 Pragmatic analysis.This lecture is about natural language content analysis. In terms of semantic analysis. So as a result, we have robust and general natural language processing techniques that can process a lot of text data. And these words can help matching documents where the original query words have not occurred. Finally we have a sentence. Finally, we're going to cover the relation between natural language processing and text retrieval. Semantic matching of terms. Another example is. So as a result, we are still not perfect, in fact, far from perfect in understanding natural language using computers. So what does that mean for text retrieval? In text retrieval, we're dealing with all kinds of text. In fact, most search engines today use something called a bag of words representation. Such relations can be extracted by using the current natural language processing techniques. They natural languages are designed for us to communicate. We can figure out whether this word in this sentence would have certain meaning in another context. Computers unfortunately, are hard to obtain such understanding. 'cause knowledge graph would contain entities and their relations, and this goes beyond the simple bag of words representation and such technique should help us improve the search engine utility significantly. And we also often dealing with a lot of text data. In our mind, we usually can map such a sentence to what we already know in our knowledge base. So in comparison, search task is all relatively easy. An in some evaluation results we have seen above 90% accuracy in terms of partial parsing of sentences. The main topic of this lecture, natural language processing can actually be interpreted in two ways in terms of the structure. This technical code would allow us to add additional words to the query and those additional words could be related to the query words. For example, we have techniques that can allow us to extract the entities and relations mentioned in text articles. First, what is natural language processing? Which is the main technique for processing natural language to obtain understanding? The second is the state of the art in NLP, which stands for natural language processing. Of course there are many queries that are not answered well by the current search engines and they do require a representation that would go beyond the bag of words representation that would require more natural language processing to be done. There are other languages designed for computers. The same word can have different syntactic categories. That's because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence. In terms of inference, we are not there yet, partly because of the general difficulty of inference and uncertainties. So that means the NLP techniques must be general, robust, and efficient, and that just implies today we can only use fairly shallow and NLP techniques for text retrieval. Some entities are harder than others. This is a general challenging in artificial intelligence. If you see matching of some of the query words in a text document, chances are that that document is about the topic, although there are exceptions. Consider the sentence. So this is called a bag of words representation. So these are examples of semantic analysis and they help us to obtain partial understanding of the sentences. Ambiguity, or PP attachment ambiguity. For example, design can be a noun or a verb. We usually think of this as processing of natural language. Some words would go together 1st and then they will go together with other words. In summary in this lecture we talked about what is an NLP and We've talked about the state of the art techniques, what we can do, what we cannot do, and finally, we also explain the why bag of words representation remains the dominant representation used in modern search engines, even though deeper NLP would be needed for future search engines. First we have to know dogs are a noun chasing is a verb etc. So here we show we have noun phrases as intermediate components and then verbal phrases. And we'll keep duplicated occurrences of words. Consider the word level ambiguity. There's no need to invent the different words for different meanings. In a shallow way, meaning we only do superficial analysis. But the fundamental reason why a natural language processing is difficult for computers is simply because natural language has not been designed for computers. Because we have a lot of background and knowledge to help us disambiguate the ambiguity. As a result, we omit a lot of common sense knowledge because we assume everyone knows about that. However, in the long run we still need deeper natural language processing techniques in order to improve the accuracy of the current search engines, and it's particularly needed for complex search tasks. And common sense reasoning is often required. They're not perfect, but they can do well for some entities. In terms of parsing, we can do partial parsing pretty well. And that context can help us naturally prefer documents where Java is referring to program language 'cause those documents would probably match applet as well if Java occurs in the document in a way that it means coffee. In order to understand the speech actor of a sentence. To some extent, but in general we cannot really do that. Like we say something to basically achieve some goal. And to get this structure we need to do something called a syntactic analysis or parsing, and we may have a parser. There was another reason why we have not used the sophisticated NLP techniques in modern search engines, and that's because some retrieval techniques actually naturally solve the problem of NLP. So that makes natural language processing difficult for computers. It also would have to maintain a large knowledge knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world. And so we can't even do one hundred percent part of speech tagging. That is to turn text data into simply a bag of words, meaning we will keep individual words, but will ignore all the orders of words. And we need to figure out the syntactic categories of those words. As you see from this picture, this is really the first step to process any text data, text data in natural languages. We are far from being able to do a complete understanding of a sentence. In case you haven't been exposed to that. We can also do word sense disambiguation to some extent. For example, recognizing the mentions of people, locations, organisations, etc in text. The data that are very different from the training data, so this pretty much summarizes the state of the art of natural language processing. Alright, so this is an example of syntactic ambiguity where we have different structures that can be applied to the same sequence of words. Finally, presupposition is another problem. So although the sentence looks very simple, it actually is pretty hard, and in cases when the sentence is very long. So those techniques also helped us bypass some of the difficulties in natural language processing. We don't have any problem with understanding this sentence. They don't have such a knowledge base, they are still incapable of doing reasoning under uncertainties. And then we also talk a little bit about what we can't do. reliably. Now from this representation we could also further infer some other things, and we might indeed naturally think of something else when we read the text, and this is called inference. So computers have to understand natural language to some extent in order to make use of the data. And, if you don't restrict the text domain or the use of words, then these techniques tend not to work well. So this is called lexical analysis or part of speech tagging. Although this is still an open topic for research and exploration. This ambiguity can be very hard to disambiguate, and you can imagine example where you have to use a lot of knowledge in the context of the sentence or from the background in order to figure out who actually had the telescope. So this is how computer would obtain some understanding of this sentence. Another common example of an ambiguous sentence is the following. For example, parts of speech tagging or partial parsing or recognizing sentiment, and those are not deep understanding 'cause we're not really understanding the exact meaning of a sentence. Then you would never match applet or with very small probability, right? So this is the case when some retrieval techniques naturally achieve the goal of word sense disambiguation. We can do part of speech tagging pretty well, so I showed 97% accuracy here. Speech Act analysis is also far from being done, and we can only do that analysis for various special cases. Because of these reasons, this makes every step in natural language processing difficult. On the other hand, the deeper understanding techniques tend not to scale up well, meaning that they would fail on some unrestricted text. They may work well based on machine learning techniques on the data that are similar to the training data that the program has been trained on, but generally wouldn't work well. So this is some extra knowledge that you would infer based on understanding of the text
