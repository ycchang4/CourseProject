This lecture is about natural language content analysis. And these words can help matching documents where the original query words have not occurred. So what does that mean for text retrieval? In text retrieval, we're dealing with all kinds of text. Semantic matching of terms. So this achieves to some extent. So this is called a bag of words representation. This technical code would allow us to add additional words to the query and those additional words could be related to the query words. So that's the topic of this lecture. Some entities are harder than others. Finally we have a sentence. For example, design can be a noun or a verb. We don't have any problem with understanding this sentence. That is to turn text data into simply a bag of words, meaning we will keep individual words, but will ignore all the orders of words. Another example is. If you see matching of some of the query words in a text document, chances are that that document is about the topic, although there are exceptions. So this is called lexical analysis or part of speech tagging. They natural languages are designed for us to communicate. So this is how computer would obtain some understanding of this sentence. Computers unfortunately, are hard to obtain such understanding. We are far from being able to do a complete understanding of a sentence. Such relations can be extracted by using the current natural language processing techniques. So as a result, we have robust and general natural language processing techniques that can process a lot of text data. As a result, we omit a lot of common sense knowledge because we assume everyone knows about that. We can figure out whether this word in this sentence would have certain meaning in another context. Finally, we're going to cover the relation between natural language processing and text retrieval. In terms of semantic analysis. So this is some extra knowledge that you would infer based on understanding of the text. The numbers might be lower. So that means the NLP techniques must be general, robust, and efficient, and that just implies today we can only use fairly shallow and NLP techniques for text retrieval. Although this is still an open topic for research and exploration. This is especially useful for review analysis, for example. Alright, so this is an example of syntactic ambiguity where we have different structures that can be applied to the same sequence of words. topic itself, but because of its relevance to the topic we talk about, it's useful for you to know the background. They're not perfect, but they can do well for some entities. That could be one possible intent to reach this level of understanding would require all these steps. So these are examples of semantic analysis and they help us to obtain partial understanding of the sentences. This is called. For example, we have techniques that can allow us to extract the entities and relations mentioned in text articles. We can also do word sense disambiguation to some extent. So for example, if you believe that if someone is being chased and this person might be scared with this rule, you can see computers could also infer that this boy may be scared. We can also do sentiment analysis, meaning to figure out the weather sentence is positive or negative. Those are harder for us, so natural languages is designed to make our communication efficient. To some extent, but in general we cannot really do that. For example, parts of speech tagging or partial parsing or recognizing sentiment, and those are not deep understanding 'cause we're not really understanding the exact meaning of a sentence. Consider the sentence. That's because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence. 'cause knowledge graph would contain entities and their relations, and this goes beyond the simple bag of words representation and such technique should help us improve the search engine utility significantly. Another common example of an ambiguous sentence is the following. We may be able to recognize the relations, for example this person visited that place or this person met that person, or this company acquired another company. It also would have to maintain a large knowledge knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world. You might be able to think of other meanings. There are other languages designed for computers. Of course there are many queries that are not answered well by the current search engines and they do require a representation that would go beyond the bag of words representation that would require more natural language processing to be done. It's not giving us a complete understanding as I showed it before for this sentence, but it would still help us gain understanding of the content, and these can be useful. In summary in this lecture we talked about what is an NLP and We've talked about the state of the art techniques, what we can do, what we cannot do, and finally, we also explain the why bag of words representation remains the dominant representation used in modern search engines, even though deeper NLP would be needed for future search engines. And then we also talk a little bit about what we can't do. Now we generally don't have a problem with these ambiguities. So here we show we have noun phrases as intermediate components and then verbal phrases. And there is a reason for that. But we have some techniques that would allow us to do partial understanding of the sentence. We could overload the same word with different meanings without the problem. The main topic of this lecture, natural language processing can actually be interpreted in two ways in terms of the structure. Or for question answering. An in some evaluation results we have seen above 90% accuracy in terms of partial parsing of sentences. And to get this structure we need to do something called a syntactic analysis or parsing, and we may have a parser. How do we define owns exactly the word own is something that we understand, but it's very hard to precisely describe the meaning of own for computers. First, what is natural language processing? Which is the main technique for processing natural language to obtain understanding? The second is the state of the art in NLP, which stands for natural language processing. The two users of off may have different syntactic categories. Yet we humans have no trouble with understanding that, we instantly will get everything. Then you would never match applet or with very small probability, right? So this is the case when some retrieval techniques naturally achieve the goal of word sense disambiguation. And we also often dealing with a lot of text data. Now from this representation we could also further infer some other things, and we might indeed naturally think of something else when we read the text, and this is called inference. For computers, ambiguity is the main difficulty. The same word can have different syntactic categories. We usually think of this as processing of natural language. But this structure shows what we might get if we look at the sentence and try to interpret the sentence. And so we can't even do one hundred percent part of speech tagging. Some words would go together 1st and then they will go together with other words. In our mind, we usually can map such a sentence to what we already know in our knowledge base. Such a representation is often sufficient, and that's also the representation that the major search engines today, like a Google or Bing or using. Now, this is probably the simplest representation you can possibly think of. But imagine what the computer would have to do in order to understand it, or in general it would have to do the following. We're going to cover three things. We also keep a lot of ambiguities because we assume the receiver or the hearer could know how to disambiguate ambiguous word based on the knowledge or the context. In case you haven't been exposed to that. So this roughly gives you some idea about the state of the art. Some technical code feedback which we will talk about later in some of the lectures. And we need to figure out the syntactic categories of those words. But yet this representation tends to actually work pretty well for most search tasks, and this is partly because the search task is not all that difficult. Because we have a lot of background and knowledge to help us disambiguate the ambiguity. When you represent the text in this way, you ignore a lot of other information and that just makes it harder to understand the exact meaning of a sentence, because we've lost the order. So that makes natural language processing difficult for computers. Speech Act analysis is also far from being done, and we can only do that analysis for various special cases. As you see from this picture, this is really the first step to process any text data, text data in natural languages. This ambiguity can be very hard to disambiguate, and you can imagine example where you have to use a lot of knowledge in the context of the sentence or from the background in order to figure out who actually had the telescope. If you look at the world alone, it would be ambiguous, but when the user uses the word in the query, usually there are other words
 Push tends to be supported by recommender systems and pull tends to be supported by a search engine.In this lecture we're going to talk about text access. Typical keyword query and the search engine system would return relevant documents to users. And this is because. In the previous lecture we talk about natural language content analysis. So for example, when you search for information on the web, a search engine might infer you might be also interested in some related information. By the structures documents. In this lecture we're going to talk about some high level strategies. So in this case this is usually supported by a recommender system. And this works when the user knows what exactly are the keywords to be used. So this is about the two high level strategies or two modes of text access. Or the user doesn't know what are the key words to use in the query. So to summarize this lecture we've talked about the two high level strategies for text access, push and pull. This gives excellent discussion of the relationship between information filtering and information retrieval. In this case browsing would be more useful. So in this case. So the main question we will address here is how can a text information system help users get access to the relevant text data we're going to cover two complementary strategies, push versus pull. Again, browsing tends to be more convenient. In querying the user will just enter a query. To help users get access to the text data. You can read this article. In the pull mode we can further distinguish querying and browsing again, we generally want to combine the two ways to help users so that you can support both querying and browsing. For example, you may have a research interest in some topic and that interest tends to stay for awhile, so it's relatively stable. You can then finally get into the relevant page. You want to buy a product so you suddenly have a need to read reviews about related products. So as we're speaking Google is probably processing many queries from us and those are all or mostly all ad hoc information needs. If you want to learn about the topic again you will likely do a lot of browsing. For example, the user may type in the query and then browse results to find the relevant information. This is also important step to convert raw big text data into small relevant data that are actually needed in a specific application. In the pull mode, we can further distinguish in two ways to help users querying versus browsing. To search for information there, it's still hard to enter the query in such a case. Or simply because the user finds it inconvenient to type in a query. So for example, a news filter or news recommender system could monitor the news stream and identify interesting news to you and simply push the news articles to you. And then we're going to talk about the two ways to implement the pull mode: querying versus browsing. As a result, bag of words representation remains very popular in applications like search engines. Of course in a sophisticated intelligent information system we should combine the two. In order to browse effectively, we need a map to guide us. So this is a pull mode in contrast, in the push mode, the system will take the initiative to push the information to the user, or to recommend that information to the user. If it is, the system has seen any relevant items to your interest the system could then take the initiative to recommend information to you
 Query. So text queries tend to be ambiguous. These documents for a user to examine. You might. This is. They are generally not equally relevant. It could could be very large. Topics you by using specific vocabulary and as a result. Is defined as a sequence of words. So text retrieval is basically a task where the system would respond to a user's query with relevant documents. Is to compute an approximation of this relevant document. So this is. The expected relevant documents may be different. And then give a zero or one as output to indicate whether this document is relevant to the query or not. To determine what documents should be in this approximation, set. To our user. Relevant documents refer to those documents that are useful to the user who is in typing the query.This lecture is about the text retrieval problem. And then you are to compute the answers to this query. We might have identical documents that have similar content or exactly the same content. Now then we have a collection of documents. And this is when you thought these words might be sufficient to help you find the relevant documents. The differences between text retrieval and database retrieval. And we can assume that all the documents that are ranked above this threshold are in this set. The SQL query can be regarded as a computer specification for what should be returned. Finally, we're going to talk about the document selecting versus document ranking as two strategies for responding to a users query. So we must prioritize. Functioning we are going to do basically classify them into two groups, relevant documents and non relevant ones. In the case of texy retrieval, we're looking for relevant documents. Be cause in effect, these are the documents that we deliver to the user. Which is a set of documents that would be useful to the user. For example, you might expect the relevant documents to talk about all these. This set of two random documents consist of these pluses these documents. Where as in database search. But in general, documents are longer than the queries. That would simply give us a value that would indicate which document is more likely relevant. Second, we're going to make a comparison between text retrieval and the related task, database retrieval. Unfortunately, this set of relevant documents is generally unknown. And that means which algorithm is better must be judged by the users. So we have a threshold. All the documents that. Next we have the query, which is a sequence of words. So you can think about what might be an example of that case. To indicate the relevant documents so we can see the true relevant documents here. There are two strategies that we can use. First, we'll define text retrieval. We assume it's generally not very useful for the user to see another similar or duplicated one. the query the documents. Where collection is so large the user doesn't have complete knowledge about the whole collection. So this function then can be used to rank the documents. And similarly there is a relevant document that's miss classified as non relevant. So what is text retrieval? It should be a task that's familiar to most of us because we're using web search engines all the time. Keyword queries or natural language queries tend to be incomplete also. And this would allow us to compute the score for each document independently, and then we're going to rank these documents based on those scores. As opposed to absolute relevance. The relevant document in the set is defined as follows, it basically. So it's not going to make a call whether this document is relevant or not, but rather it would say which document is more likely relevant. Or data. Vocabularies will be used in those random documents. The query given to us by the user is only a hint on which document should be in this set. Basically, to support the query. So formally, we can see the task is to compute this r prime of Q, an approximation of the relevant documents. First text retrieval is an empirical defined problem. The documents may be very short. Because of this difference, we can also see that text information tends to be more ambiguous, and we talked about that in the natural language processing lecture. In that it doesn't really fully specify what document should be retrieved. So for example. Now this task is often called information retrieval. Because this is a very important topic for search engines. And so in this case you can see the system must decide if a document is relevant or not. A user will typically give a query to the system to express information need and then the system would return relevant documents to users. And this collection can be very large, so think about the web. Where as in the database search. So in this case the system only needs to decide if one document is more likely relevant than another, and that is it only needs to determine relative relevance. And that's what ranking is doing. That's a function that would take a document and query as input. Think about the difference between the queries that you typically specify for a database system. Relevant relative relevance would be easier to determine their absolute relevance because in the first case, we have to say exactly whether a document is relevant or not. Think about the data and information managed by search engine versus those that are managed by a database system. That also means we must rely on empirical evaluation involving users. The query and document pair. So first it's about the text retrieval problem. set so we denote by r prime of Q. So how can we do that? Imagine if you are now asked to write a program to do this. Basically it needs to know exactly whether it's going to be useful to the user. These documents could be all the web pages on the web. Are these two tasks are similar in many ways? But there are some important differences. because if the documents are independent then we can evaluate the utility of each document separately. So to summarize this lecture, the main points. Now typically the documents are much longer than queries. Match no random documents because in the collection no others have discussed the topic using these vocabularies. What's the difference between the two? OK, so if we think about the information or data managed by the two systems, we will see that. 2nd, a user would be assumed to browse the results sequentially. So For these reasons, ranking is generally preferred now. Each is relevant. Now, in the case of tax retrieval, what should be the right answers to a query is not very well specified as we just discussed. In some other cases you might see a scenario where one document that may not be useful to the user, but when three particular documents are put together, they provide answer to the users question. So it's unclear what should be the right answers to a query, and this has very important consequences and that is text retrieval is an empirically defined problem. And then the goal of text retrieval is to find the set of relevant documents which we denoted by R of Q because it depends on the query and this is in general a subset of all the documents in the collection. Versus the queries that are typed in by users on the search engine. So in this case you can see the document. The first strategies will do document selection and that is we're going to have a binary classification function or binary classifier. So this picture shows how it works. And therefore it would make sense to feed the users with the most relevant documents. So in this case you will see there is this problem of. We are going to talk about three things in this lecture. In the database search we are retrieving records or match records with. So here we can see in the approximation of the relevant documents we have got some non relevant documents. Do these two assumptions hold? Now I suggest you to pause the lecture for a moment to think about this. Basically it has to say whether it's one or zero. First, the utility of the document to a user is independent of the utility of any other document. And user dependent in the sense that for the same query typed in by different users. And this will help users prioritize examination of search results. But it turns out that they are not sufficient, and there are many distraction documents using similar words. So this is a collective relevance and that also suggests that the value of the document might depend on other documents. It will be useful to pause the lecture at this point. And then we're going to let the user decide where to stop when the user looks at the documents. And this is called absolute relevance. Of course the classifier will not be perfect so it will make mistakes. So as I said, ranking is generally preferred. So clearly the utility of document is dependent on other documents that user has seen. We also still want to rank these red documents because. Have a value of 1 by this function. Second, document ranking is generally preferred. But there are also cases where. No relevant documents to return in the case of overly constraint query. We have text data in all kinds of languages. What would you do now? Think for a moment
 To indicate whether a document is relevant to a query. To characterize this information. Tends to be very similar. We then define the score of document with respect to a query as the probability that this random variable R is equal to 1 given a particular document and query. We might also denote as c of presidential and d. And we achieve this goal by designing a retrieval model which gives us a formalization of relevance. When optimized, these models tend to perform similarly. Any term is expected to occur more frequently. BM25, query likelihood, PL2. Now clearly this means our function must be able to measure the likelihood that a document d, is relevant to a query Q. Such as presidential campaign and news. It's not as significant as. One is classic probabilistic model. We have a query that has a sequence of words and a document that's also a sequence of words and we hope to define a function F. Basically, we assume that if a document is more similar to the query, then another document is then we will say the first document is more relevant than the second one. Bag of words representation remains the main representation. So with this assumption, the score of a query, presidential campaign news. So to summarize. And This is to use the document length for scoring. We explained that the main problem is to design a ranking function to rank documents for a query in this lecture we will give a overview of different ways of designing this ranking function. That can compute a score based on the query and document. That also means we have to have some way to define relevance. So in this case, the ranking function is defined as the similarity between the query and the document.This lecture is the overview of text retrieval methods. This is called a term frequency or TF. Another factor is how long is the document. These are often combined in interesting ways and we'll discuss how exactly they are combined to rank documents in the lectures later. Pre requires a computational definition of relevance and we achieve this goal by designing appropriate retrieval model. In this family of models we follow a very different strategy, where we assume that. There are different cases of such a general idea. And we call this document frequency or DF of presidential. Queries and documents are all observations from random variables. Use a probability. First, these models are all based on the assumption of using bag of words to represent text, and we explained this in the natural language processing lecture. And in some other models we might also. In the later lecture we will talk more about one case which is language model. And they fall into different categories. First bag of words representation. Is contributing more to the overall score than matching a common term. So all these are trying to characterize the popularity of the term in the collection in general, matching a rare term in the collection. The third kind of models are based on probabilistic influence, so here the idea is to associate uncertainity to inference rules, and we can then quantify the probability that we can show that the query follows from the document. With respect to a document d here would be based on scores computed based on each individual word. In the previous lecture we introduced the problem of text retrieval. In general. The word occurs more frequently in the document then the value of this function would be larger. TF and document frequency of words, such information is used in. And document lengths. One well known example in this case is vectors space model, which we will cover more in detail later in the lecture. The second kind of models are called probabilistic models. Among all these BM25 is probably the most popular
 So for example, one document might be represented by this vector, D1. Now this is different from another document which might be represented as a different vector D2 here. And then we're going to measure the similarity between the query vector and every document vector. Vector space model is a special case of similarity based models, as we discussed before, which means we assume relevance is roughly similarity between the document and the query. As we will see later, there are other ways to model relevance. Basically, we assume that if a document is more similar to a query than another document then the first document will be assumed to be more relevant than the second one, and this is the basis for ranking documents in this approach. So the relevance in this case would be assumed to be the similarity between the two vectors. So in this case, for example, we can easily see D2 seems to be the closest to this query vector, and therefore D2 will be ranked above others. And we're going to assume that all our documents and the query will be placed in this vector space. So this shows that by using this vector space representation we can actually capture the differences between topics of documents.This lecture is about the vector space retrieval model we're going to give a introduction to its basic idea. Therefore, our ranking function is also defined as the similarity between the query vector and document vector. Basically I showed you some examples of query and document vectors, but where exactly should the vector for a particular document point to? So this is equivalent to how to define the term weights. First, we represent a document and query via term vector. So here are term can be any basic concept, for example a word or a phrase. And they will be pointing to all kinds of directions. And similarly, we're going to place our query also in this space as another vector. So depending on how you assign the weights, you might prefer some terms to be matched over others. Vector space model is a framework. So to be more precise. To be more precise. For example, the orders of words are simply ignored, and that's because we assume that the bag of words with representation. We clearly assume the concepts are orthogonal, otherwise there will be redundancy. In this lecture we're going to talk about this specific way of designing a ranking function called vector space retrieval model. So with this representation you can already see D1 seems to suggest a topic about presidential library. That might be about present in your program. Because it would be as if you match the two dimensions when you actually match one semantic concept. Now you can also imagine there are other vectors, for example D3 is pointing to that direction. Each document vector is also similar. So this is basically the main idea of the vector space model. Finally, how to define the similarity measure is also not given. Here you can see we assume there are N dimensions. Basically, what we see here is only the vector representation of the document. And in fact that we can place all the documents in this vector space. And we're going to give a brief introduction to the basic idea. Now we can consider vectors in this 3 dimensional space. How do you compute those element values in those vectors? Now this is a very important question because term weight in the query vector indicates the importance of term. So how do we solve these problems? Is the main topic of the next lecture. Each term is assumed to define one dimension
 When user 0/1 bit vector to represent a document or a query. D4 and D3 are relevant documents and D1, D2, and D5 are non relevant. This is how we interpret this score. So you can imagine in order to implement this model we have to say, specifically, how we compute these vectors? What is exactly Xi and what is exactly Yi? This will determine where we place a document vector, where we place a query vector. As we discussed in the previous lecture. Now that we have the two vectors. To summarize, in this lecture we talked about how to instantiate a vector space model. In this case, we basically only care about word presence or absence. So think about how we can interpret this score. As long as you have 1 zero then the product will be 0. Using a vector space model and then we make assumptions about how we place vectors in the vector space and how we define the similarity. And 3rd is to define the similarity between two vectors, particularly the query vector and the document vector. forming, forming product and these two will generate another product and these two will generate yet another product and so on so forth. So as a result this scoring function basically matches how many unique query terms are matched in the document. They match news, presidential and campaign. So. A lot of words will be absent in a particular document.In this lecture we're going to talk about how to instantiate vector space model so that we can get a very In this lecture, we are going to talk about how to instantiate vector space model so that we can get a very specific ranking function. Now, what about the documents? It's the same, so T1 has two words news and about. So what do you think is the vector representation for the query? Note that we are assuming that we only use zero and one to indicate whether the term is absent or present in the query or in the document, so these are 0/1 bit vectors. In the vectors space model of course we want to 1st compute the vectors for these documents and the query. Because if a document, if a term is matched in the document, then there will be 2 ones. We have to solve these problems. But this model doesn't do that. They cover different terms in the query. So what do you think is the query vector? The query has four words here, so for these four words there will be one and for the rest will be 0. So intuitively, we would like D3 we ranked above D2. The second is to decide how to place documents as vectors in the vector space. So this is to continue the discussion of the vector space model, which is one particular approach to design ranking function. Can we expect this function to actually perform well when we used to rank the documents for users' queries? So it's worth thinking about. When it's one, it means the corresponding word is present in the document or in query. Exactly how to define the similarity function? What would you do? So think for. And to also place a query in the vector space as a vector. And finally, it did not say how we should measure the similarity between the query vector and the document vector. So let's first think about how we actually use this model to score documents. The query is news about the presidential campaign and we have 5 documents here. So as we discussed in the previous lecture, the Vector Space model is really a framework. The document vector in general will have more ones of course. Now, if I ask you to rank these documents how would you rank them? This is basically our ideal ranking: when humans can examine the documents and then try to rank them. So intuitively D3 is better because matching presidential is more important than matching about even though about and presidential above in the query. So if we can provide a definition of the concepts that would define the dimensions and these Xi's or Yi's, namely weights of terms for query and document, then we will be able to place document vectors and query vector in this well defined space and then if we also specify similarity function then we'll have a well defined the ranking function. And this is a topic that we're going to cover in the next lecture. Which is based on the bit vector represntation dot product similarity and bag of words representation. It did not say how we place a query vector in this vector space. And if you look at the these documents for a moment, you may realize that some documents are probably relevant and some others are probably non relevant. Indeed, that's probably the simplest vector space model that we can derive. So now we see that we have defined the the dimensions we have defined the vectors and we have also defined the similarity function, so now we finally have the simplest of vector space model. In the case of D3 presidential, could be an extended matching. But you still have to figure out how to compute these vectors. Now note that this matching actually that makes sense, right? It basically means if a document matches more unique query terms then the document will be assumed to be more relevant, and that seems to make sense. And we're going to talk about how we use the general framework of the vector space model as a guidance to instantiate the framework to derive a specific ranking function. So it looks like these documents are probably better than the others, so they should be ranked on top. Suppose you are asked to implement this idea. And we're going to use dot product so you can see when we use dot product we just multiply the corresponding elements, right? So these two will be. These zeros. And with such a instantiation, and we showed that the scoring function is basically to score a document based on the number of distinct query words matched in the document. Now you can easy to see if we do that. So let's see how we can do that and think about the simplest instantiation. Because if whenever we have zero the product will be 0. Similarly, if the document has a term, but the term is not in the query, there will be a zero in the query vector, so those don't count. We actually don't have to care about. However, if we examine this model in detail, we likely will find some problems. Whereas D1 only match two. When it's zero it's going to mean that it's absent. So when we take a sum over all these pairs then the zero entries will be gone. In this case, we use each word to define a dimension. Each word defines one dimension and this is basically the bag of words representation. And you can easily verify their correct because we're basically counting the number of unique query terms matched in each document
 They are as we expected. So now as a result of IDF weighting, we will have D3 to be ranked above D2 becauses it matched rare word whereas D2 matched common word. If we can do that, then we can penalize common words which generally have a low IDF and reward rare words which will have high IDF. So suppose we change the vector representation into term frequency vectors. Will be interesting for you to think about what's minimum value for this function? This could be interesting exercise. So this can be seen as a more accurate representation of our documents. So this means, by using term frequency we can now rank D4 above D2 and D3 as we hope to. We're going to focus on how to improve the instantiation of this model. So that is to say, now the elements of both queries vector and document vector will not be 0 or 1s, but instead they will be the counts of word in the query or the document.In this lecture we are going to talk about how to improve the instantiation of the vector space model. So to summarize this lecture we've talked about how to improve the vector space model, and we've got to improve the instantiation of the vector space model based on TF IDF weighting. As a result, now the score for D4 is high. The query vector is the same because all these words occur exactly once in the query, so the vector is 001 vector. Now a specific function may not be as important as the heuristic to simply penalize popular terms. Document frequency is the count of documents that contain a particular term. So let's add this back so we can do then represent a document by a vector with term frequency as elements. So in this natural, let's see how we can improve the model to solve these two problems. So let's see how can improve this. Now, without the IDF weighting before, we just have term frequency vectors, but with IDF weighting, we now can adjust the TF weight by multiplying with the IDF value. Can you come up with any statistical approaches to somehow distinguish presidential from about? If you think about it for a moment, you will realize that what differences that words like about occurs everywhere. Perhaps we will have to use different ways to instantiate the vector space model. So how can we fix this problem? Intuitively, we would like to give more credit for matching presidential than matching about, but how can we solve the problem in a general way? Is there any way to determine which word should be treated more importantly, and which word can be basically ignored about is such a word at which does not really care that much content? We can essentially ignore that we sometimes call such a word stop word. We discussed this in previous lecture, so here's a specific way of using it. So this would bring in additional information about the document. But intuitively we would like D4 to be ranked above D3 and D2 is really non relevant. So if we score with these new vectors, then what would happen is that of course they share the same weights for news and campaign. By multiplying it by the idea of the corresponding word as shown here. So this shows that the IDF weighting can solve problem 2. It's not very important to match this word, so with the standard idea you can see it's basically assuming that they all have lower weights. In particular, it has to do with how we place the vectors in the vector space. For example, here we can see is adjustment, and in particular for about there is adjustment by using the IDF value of about which is smaller than the IDF value of presidential. As shown on this slide, in particular, if you look at these three documents, they will all get the same score because they matched 3 unique query words. So how effective is this model in general? When we use TF IDF weighting, let's look at the obvious documents that we have seen before nicely. So if you look at these, the IDF will distinguishing these two words as a result of adjustment here would be larger, would make this weight larger. If we can do that, then we can expect the D2 will get the overall score to be less than 3. In order to consider the difference between a document where aquarium occurred multiple times and one where the query term occurs just once, we have to consider the term frequency: The count of a term in the document. It's 4 now. In general, you can see it would give a higher value for a low DF word, a rare word. And we have seen that this improvement model indeed works better than the simplest vector space model. So intuitively we want to focus more on the discrimination of Low DF words rather than these common words. So this idea suggest that we could somehow use global statistics of terms or some other information to try to down weight the element that for about in the vector representation of D2. At the following heuristics: First, we would like to give more credit to D4 because it matched the presidential more times than these three; Second, Intuitively, matching presidential should be more important than matching about because about this very common word that occurs everywhere. Of course, which one works better still has to be validated by using the empirical created dataset and we have to use users to judge which results are better. This is the continued discussion of the vector space model. But D4 would be different. So more specifically, the IDF can be defined as a logarithm of (M + 1) / K, where M is the total number of documents in the collection, K is the DF or document frequency, the total number of documents containing the word W. So if you count the occurrence of the word in the whole collection, then we would see that about has much higher frequency than presidential which tends to occur only in some documents. They can be essentially ignored and this makes sense when the term occurs so frequently, and let's say a term occurs in more than 50% of the documents, then the term is unlikely very important, and it's basically a common term. We can come up with a simple scoring function that would give us basically a count of how many unique query terms of matching the document. So then, naturally, in order to fix these problems, we have to revisit those assumptions. And so the way to incorporate this into our vector representation is then to modify the frequency count. These are the new scores of the new documents, but how effective is this new weighting method and new scoring function? So now let's see overall how effective is this new ranking function with TF IDF weighting? Here we show all the five documents that we have seen before, and these are their scores. So how can we do this systematically? Again, we can rely on some statistical counts, and in this case the particular idea is called the inverse document frequency. But the matching of about and presidential with discriminate them
 BM stands for best matching. Now we are more sure that this document is talking about this word. So to summarize this lecture. This BM25 transformation that we talked about is very interesting. This is the number of documents that contain this word W. Once we see a word in the document, it's very likely that the document is talking about this word. In this lecture we're gonna talk about how we can use TF transformation to solve this problem. We have just seen that this is not desirable. And will discuss how we can further improve this formula in the next lecture. In particular, we're going to talk about the TF transformation. If you plug this function into our TF IDF weighting vector space model. And that is M. Indeed, it has received the highest score among all these documents, but this document is intuitively non relevant, so this is not desirable.In this lecture we continue the discussion of vector space model. In the previous lecture, we have derived a TF IDF weighting formula using the vector space model. In other words, this upper bound might also ensure that all terms would be counted when we aggregate the weights to computer score. So it has an idea of component where we see two variables. So we precisely recover the 01 bit transformation. And so that we can prevent a spammer from just increasing the count of one term to spam all queries that might match this time. So intuitively, in order to lower the score for this document, we need to somehow restrict the contribution of the matching of this term in the document. So what we want is something like this. So this transformation function is going to turn the raw count of word into a term frequency, wait for the word in the document. So in this sense, this transformation is very flexible. This is also difference between this transformation function and the logarithm transformation which doesn't have upper bound. We probably shouldn't reward multiple occurrences. And inside the sum, each matching query term has a particular weight and this way it is TF IDF weighting. Then adding one extra occurrence is not good to tell us more about the evidence 'cause we already sure that this document is about this word. And we have shown that this model actually works pretty well for these examples, as shown on this slide except for D5, which has received very high score. That is a 01 bit transformation and the linear transformation. So the count of campaign in this document is a four which is much higher than the other documents and has contributed to the high score of this document. So for example, if we set K to 0 now you can see, the function value would be 1. So this is the formula and if you look at the formula carefully, then you will see it involves a sum over all the matched query terms. And that is the idea of TF transformation. For example, in the 01 bit vector representation, we actually use researcher transformation function as shown here. Or we might want to even bend the curve more by applying logarithm twice. But so far what works the best seems to be this special transformation called  BM25 transformation. The main point is that we need to do some linear TV TF transformation, and this is needed to capture the intuition of diminishing return from higher term counts. Now in this transformation you can see there's a parameter K here
 When we use BM25 for such a document. As we discussed before, and this is to achieve a sub linear transformation. As we explained there is this. Such as IDF inverse document frequency.This lecture is about document Length normalization. We can use phrases to define dimensions. If you generate a long document randomly by simply sampling words. And these are all possible. So for example, cosine measure can be regarded as the dot product of two normalized vectors. On the web pages, those are the text fields that describe links to other pages, and these can all be combined with appropriate weights of different fields to help improve scoring for a document. But when we combine these word frequencies together, we just do the transformation. And we have. Basically proportional to the similarity between the query and document, so naturally that implies that the query an document must be represented in the same way, and in this case we represent them as vectors in high dimensional vector space where the dimensions are defined by words or concepts or terms in general. Is empirically an analytically shown to be better than BM25. So to summarize, all what we have said about the vector space model. Is. Therefore we should penalize them. So. We use. So that's why we need to be careful about. So this is illustrated using this slide. So in this sense we should penalize long documents because they just naturally have better chances for matching any query. Those are sequences of N characters for dimensions. First, in such a model we use the similarity notion relevance, assuming that the relevance of a document with respect to a query is. However, there is another case when the document is long and that is when the document simply has more content. So this is the case where we probably should penalize the matching of. So for example. And there is a reward for short documents. These major heuristics are the most important heuristics to ensure such a general ranking function to work well for all kinds of text. Length, we can measure the length by words or by characters. But one might reason that D6 May have matched these query words In a scattered manner. So in this sense we see there is a panelization for long documents. Here F stands for Field and this is to use BM25 for documents with the structures. So that the computation and computing will all become the same and they can be matched. In this lecture, we're going to continue the discussion of the vector space model. OK, so this is one of the two most effective vector space model formulas. Then long papers generally have higher chance of matching query words. When we compare the matching of words in such a long document with matching of the words in a short abstract. If you look at the matching of these query words, we see that in D6 there are more matchings of the query words. Those are the words that are have been transformed into the same root form. So far we have talked about the mainly how to place the document vector in the vector space. Long documents such as full paper. is seem to be Interpolation of 1 and the normalized document length controlled by a parameter b here. Again, if a document is longer than the term weight of this model So you can see after we have gone through all the analysis that we talked about. There is also a query term frequency component. That means we'll assume that for the average length documents, the score is about right, so the normalizer would be 1. From a distribution of words, Then eventually you probably will match any query. and IDF weighting and document length normalization. In one case, the document may be long because it uses more words. So far in the lectures about the vector space model, we have used various signals from the document to assess the matching of the document, with the query. In general, if you think about long documents, they would have a higher chance to match any query in fact. For example, we can measure the cosine of the angle between two vectors, or we can use Euclidean distance measure. In particular, we're going to discuss the issue of document length normalization. For example, can we further improve the instantiation of the dimension of the vector space model? Now We've just assumed that the bag of words representation, so each dimension is the word. And a reference in the end has details about derivation of this model and here we see that it's basically the TF IDF weighting model that we have discussed. I should also mention that sometimes we need to do language-specific and domain-specific tokenization, and this is actually very important as we might have variations of terms. Whereas if we set b to a nonzero value then the normalizer would look like this so the value would be higher for documents that are longer than the average document length. In such a case, obviously we don't want to over penalize such a long document. We can do stop word removal. Now I have to say that I've put BM 25 in the category of vector space model. For example, stemmed words. We can simply add a small constant to the TF normalization formula, But what's interesting is that we can analytically prove that by doing such a small modification. And these are in fact the state of art vector space model formulas. a word might correspond to 1 character or two characters or even 3 characters. In the vector space model. And this is of course controlled by the parameter b here. So you can see here, when we first divide the length of the document by the average document length, This not only gives us some sense about how this document is compared with the average document length, but also gives us a. And then apply BM 25. Now consider another case of a long document where we simply concatenated a lot of abstracts of different papers. This is to remove Some very common words that don't carry any content like "the", "a" or "of". So it's Easier in English when we have a space to separate the words, but in some other languages we may need to do some natural language processing to figure out where all the boundaries for words. The idea of component should be very familiar now to you. So if we plug in this length normalization factor into the vector space model ranking functions that we have already examined. There are some additional readings. But if a document is longer than the average document length, then there will be some penalization, whereas if it's a shorter, then there's even some reward. I just mentioned that the BM 25 seems to be one of the most effective formulas. That means we first normalize each vector and then we take the dot product that would be equivalent to the cosine measure. We can even use latent semantic analysis to find some clusters of words that represent a latent concept as one dimension. By adjusting b which varies from zero to one, we can control the degree of Length normalization. And a query TF component here. That you have seen here, so as effective retrieval function BM25 should probably use a heuristic modification of the IDF to make it even more look like a vector space model. And we generally need to use a lot of heuristics to design the ranking function. And in this case we see we used a double logarithm
 For postings, they're huge. And they would contain information such as document IDs, term frequencies or term positions etc. Those words are very, very frequent. This is a typical text retrieval system architecture. In this lecture we will discuss how we can implement text retrieval method to build a search engine. So I want the relevant document to match both term A and the term B right? So that's one conjunctive query? Or I want the relevant documents to match term a or term b. Just look up a sequence of document IDs and frequencies for all the documents that match a query term, so we would read those entries sequentially. What about the multi term keyword query? We talked about vector space model for example and we would match such query with document generated score and the score is based on aggregated term weights. So they tend to be often used in queries, and they also tend to have high TF IDF weights, these intermediate frequency words.This lecture is about the implementation of text retrieval systems. Those words are actually very useful for search. They are in fact too frequent to be discriminated and they are generally not very useful for retrieval. It's actually pretty simple, right? With this structure we can easily fetch all the documents that match a term and this will be the basis for scoring documents for a query. These are the highest frequency words they occur very frequently. Also, if a user happens to be interested in such a topic, but because they're rare. These words tend to occur in quite a few documents, but they are not like those most frequent words and they're also not very rare. Those words don't occur very frequently and there are many such words. And in general we don't have to have direct access to a specific entry we generate with. Now, because they are very large, compression is often desirable. Tokenization is to normalize lexical units into the same form so that semantically similar words can be matched with each other. The basic idea is do pre-compute as much as we can basically. There are some language independent patterns that seem to be stable. That's a disjunctive query. So formally, if we use F(w) to denote the frequency, r(w) to denote the rank of a word, then this is the formula. So a search engine system then can be divided into 3 parts. This way all these different forms of computing can be matched with each other. We have three documents here and these are the documents that you have seen in some previous lectures. Then we would maintain a dictionary in the dictionary we will have one entry for each term, and we're going to store some basic statistics about the term. So these interaction signals can be used by the system to improve the ranking accuracy by assuming the view of the documents are better than the skiped ones. Basically, it's like a or b. That means, although the general phenomenon is applicable or is observed in many cases, the exact words that are common may vary from context to context. It's often true that the users are unnecessary interested in those words, but retain them would allow us to match such a document accurately, and they generally have very high IDFs. This term ocurred in all the three documents. So for example, computer computation and computing can all be matched to the root form compute. Once we do tokenization then we would index the text documents and that is to convert the documents into some data structure that can enable fast search. So think about how you can preprocess the text data so that you can quickly respond to a query with just one word? If you have thought about that question, you might realize that what the best is to simply create a list of documents that match every term in the vocabulary. And you might also realize we need this count of documents or document frequency for computing some statistics to be used in the vector space model. Basically, it's similar to disjunctive Boolean query. Suppose we want to create inverted index for these documents. For example in this case. And the queries representation would then be given to the scorer which would use the index to quickly answer a users query by scoring the documents and then ranking them. It's also true that most frequently words in one corpus may have to be rare in another. This can all be checked quickly by using the position information. And so, for example news. So when you see a term, you can simply just fetch the ranked list of documents for that term and return the list to the user. This is the document ID, document one, and the frequency is 1, the TF is 1 for news. This word occured in only one document, document three, so the document frequency is one. So in this case it matched three documents and store information about these three documents here. You can just compute the score for each document and then you can then score them. So they account for a large percent of occurrences of words. And the query will be going through a similar process in step, so that tokenizer would be applied to the query as well so that the text can be processed in the same way the same units will be matched with each other. We can see the documents are first processor via tokenizer to get that tokenized units, for example words, and then these words or tokens will be processed by a indexer that would create the index which is a data structure for the search engine to use to quickly answer query. Now how can we answer such a query by using inverted index? If you think a bit about it would be obvious cause we had simply fetch all the documents that match term a, and also fetch all the documents that match term B and then just take the intersection to answer a query A&amp;B or to take the Union to answer the query A or B. Now if you look at these words, we can see they can be separated into three groups. The results will be given to the user and then the user can look at the results and provide some feedback that can be expressed judgments about which documents are good, which documents are bad or implicit feedback such as click slows so the user doesn't have to do any, anything extra the user would just look at the results and skip some and click on some results to view. So in this case it's not a Boolean query. We take the union of all the documents that match at least one query term, and then we would aggregate the term weights. The X axis is basically the world rank and this is r(w), Y axis is a word frequency or F(w). And this data structure can then be used by the online module, which is a scorer to process users query dynamically and quickly generate search results. And these patterns are basically characterized by the following pattern of few words, like the common words the, a, or we occur very frequently in text. So with the document account here, it's easy to compute the IDF, either at this time or when we build index or running time when we see a query. To understand why we want to use inverted index, it would be useful for you to think about how you would respond to a single term query quickly. Now, in addition to these basic statistics, we also store all the documents that match the news, and these entries are stored in a file called postings. And therefore, because it's large, we generally have store postings on disk, so they have to stay on disk. So the count of documents is 3. So this is all very easy to answer. The main challenge is to manage a lot of text data and to enable a query to be answered very quickly and to respond to many queries. Now the position information is very useful for checking whether the matching of query terms is actually within a small window of let's say 5 words or 10 words or whether the matching of the two query terms is in fact a phrase of two words. So if the query has just one word news and we can easily look up this table to find the entry and go quickly to the postings and fetch all the documents that match news. It basically says the same thing, just mathematical term we will see is basically a constant, right? So as so there is also parameter Alpha that might be adjusted to better fit any empirical observations. So they, are often removed and this is called stop words removal, so you can use pretty much just the count of words in the collection to kind of infer what words might be stop words. So that's the fastest way to respond to a single term query
 And this will be followed by document IDs 2s. For example, 3 is encoded as 101. And what about the document IDs that might be compressed using d-gap. And we're going to do that for all the documents. Those are the frequencies of terms in all those documents. Once we look up a term we fetch all the document IDs that match the term. Therefore, the document IDs are all 1s in this case. So for example, three would be encoded as two ones, followed by zero, whereas five will be encoded as 4 ones, followed by zero etc. These lexicons are to map stream-based representations of document IDs or terms into integer representations or to map back from integers to the stream representation. In this case, we can see IDs of documents that match the term one will be grouped together. So that's why we have 101 for 3. The large numbers will not be frequent. Notice that here we're using the term IDs as a key to sort. So this is one reason why we tend to map these strings into integers. We're going to do sequential decoding. So that's why we can afford using unary code for that. They are also easy to compress. In this stage, we generally sort the frequencies by document IDs because we process each document sequentially. So this is for the value 2. And we can imagine if a term has matched many documents, then there will be long list of document IDs. So this is an illustration of this method. Similarly, 5 is encoded as 110 followed by 01 and in this case, the unary code encodes 3. So we will first have the Unary code for this log of X. But because we tend to see many small values, they are very frequent. We have to sequential process the data. And this is also possible because in order to uncover or uncompress these document IDs. We can sort them and then this time we're going to sort based on term IDs. Now this was possible because we only need to have sequential access to those documents IDs. So this is how you can decode Gamma code. It would be a disaster. Unfortunately, in most retrieval applications, that data set would be large and they generally cannot be loaded into memory at once. Now we mentioned earlier that because of postings are very large, it's desirable to compress them. Since this is 2, then we know that the floor of log of X is actually one. They would be appropriate for a certain distribution, but none of them is perfect for all distributions. In this case, an integer that's at least one would be encoded as x-1, one bits followed by zero. Then we sequentially process them. In this lecture we will continue the discussion of system implementation. Now, if you think about what kind of values are most frequent there, you probably will be able to guess that small numbers tend to occur far more frequently than large numbers. So now you can imagine how many bits do we have to use for a large number like 100. We're going to scan these documents sequentially, and then parse the document, and count the frequencies of terms. And it's easy to show that for this value this difference, we only need to use up to this many bits and floor of log of X bits. In particular, we're going to discuss how to construct the inverted index. And on the top we can see these are the old entries for the documents that match term ID1. And this is basically precise X - 1 to the floor of the log of X. What about the document IDs that we also saw in postings? They are not distributed in the skewed way right. So how do I uncompressed invert index when we just talked about this first, you decode those encode integers and we just I think discussed how we decode unary coding and gamma coding. The reason why we are interested in using integers to represent these IDs is because integers are often easier to handle. So it's very natural. And so this is the unary code 110. This is easy to understand, if the difference is too large then we would have a higher floor of log of X. But this time we're going to use 2 bits, cause with this level floor of log of X we could have more numbers 5, 6, 7 they would all share the same prefix here,110. Because we store the difference and in order to recover the exact the document ID, we have to 1st recover the previous document IDs and then we can add the difference to the previous document ID to restore the current document ID. So all the entries that share the same term would be grouped together. And so the floor of log of X is 2. And that means we are going to compute the difference between 5 and 2 to the two, and that's one, and so we now have again one at the end. You will start another number. And then once you collect those counts, you can sort those counts based on terms so that you build a local partial inverted index, and these are called rounds. In this case, for unary you can see it's very easy to see the boundary. And there are many different methods for encoding. Basically you will not count the terms in a small set of documents. But before we do that, we can sort them just use whatever memory we have. Eventually we will get a single inverted index with their entries are sorted based on term IDs. The problem is that when our data is not able to fit to the memory then we have to use some special methods to deal with it. Those gaps will be small, so we will again see a lot of small numbers. So this creates some skewed distribution that would allow us to compress these values. And we're going to write this into the disk as a temporary file, and that would allow us to use the memory to process the next batch of documents. So this is where you can see the advantage of converting document IDs into integers, and that allows us to do this kind of compression and we just repeat until we decode all the documents every time we use the document ID in the previous position to help you recover the document ID in the next position. So how can we deal with that? Well, it turns out that you can use a trick called d-gap and that is to restore the difference of this term IDs. We can save on average, even though sometimes when we see a large number we have to use a lot of bits. And in this method we are going to use unary coding for a transformed form of the value, so it's one plus the floor of the log of X. You can say 8 bits or 32 bits, then you will start another code. How do you decode this code? Since these are variable length encoding methods and you can't just count how many bits, and then they just stop
 when we consider query terms. So this is g. This would give us all the documents that match this query term. And then we're going to update the score accumulator for this document. That's the key to enable faster response to a user's query. We can imagine there will be allocated but then they will only be allocated as needed. Imagine we have all these score accumulators to store the scores for these documents. So we'll compute the weighted contribution of matching this query term in this document. So all these can be done at this time, so that would mean when we process all the entries for information, these weights will be adjusted by the same IDF, which is IDF for information. So to summarize this you can see we first processed the query term information. You can also see how we can incorporate the IDF weighting so they can easily be incorporated when we process each query term. For example, documents normalization. And so topic of all those models are implemented there. Note that we don't have to touch any document that didn't match any query term. These are adjustment factors of document and query so they are at the level of a document and query. We're going to compute the function g. So the form of this function is as follows. And this h function would then aggregate all these weights, so it will for example, take a sum of all the matched query terms. Basically to exploit the inverted index to accumulate the scores for documents matching or query term. We see this scoring function of document d and query q is defined as first a function of f(a). We allocate another score cumulative for d3 and add 1 to it. An we fetch the first query them what is that? That's information. And what we do is to add 3 to the existing value which is 4.This lecture is about how to do fast search by using inverted index. And this aggregate functioning would then combine all these. That's d4 and 1 so we would update the score for d4 and again we add 1 to d4. We processed all the entries in the inverted index for this term. But it can also be a product or could be another way of aggregating them. Of course, for the query we have the computed at query time, but for document, for example, document lengths can be precomputed. Then we processed the security. We only need to process the document that matched at least one query term. For example document lengths. So these score accumulators obviously will be initialized at 0. Right, so those are all heuristics for further improving the accuracy here. When we fetch the inverted index, we can fetch the document frequency and then we can compute the IDF. And then we'll maintain a score accumulator for each document d to compute h. And what do we do? Well, we do exactly the same as what we did for information, so this time we're going to change the score accumulator d2, since it's already allocated. This general form covers actually most state of the art retrieval functions. So this is the main part of the scoring function. So this is basically a general way to allow us to do computer all functions of this form by using inverted index. So we now get the 7 for d2. 3 is the occurrences of information in this document. A rare term would match fewer documents and then the score contribution would be higher because the idea of value will be higher. In this lecture, we're going to continue the discussion of system implementation. So there are some tricks to further improve the efficiency. Since our scoring function assumes that the score is just a sum of these raw counts, we just need to add 3 to the score accumulator to account for the increase of score due to matching this term information in document d1. It processed all the contributions of matching information in these four documents. And we explore the Zipf's law to avoid attaching many documents that don't match any query term. So keeping them in the memory would help and these are general techniques for improving efficiency. In the end, then, we're going to adjust the score to compute this function Fa, and then we can sort. For that purpose, we can then prune the accumulators. In particular, we're going to talk about how to support fast search by using inverted index. The function g gives us the weight of a match query term t(i) in document d. Finally, we process d5 and 3. At this point, we're going to allocate 1 for d5, and we're going to add 3 to it, So those scores on the last row are the final scores for these documents if our scoring function is just a simple sum of TF values. Here are two additional readings that you can take a look if you have time and you're interested in learning more about this. When we index the documents at that time, we already computed the IDF value that we can just fetch it. Now inside this edge function there are functions that would compute the weights of the contribution of a matched query term t(i). So we're going to fetch all the inverted index entries for security. So let's think about what a general scoring function might look like. And then finally, this adjustment function would then consider the document level or query level factors to further adjust the score. d2 score is increased because it matched both the information and security. Then for each entry dj and fj are particular match of the term in this particular document dj. And the basic idea is to preprocess the data as much as we can, and we want to do compression when appropriate so that we can save disk space and can speed up IO and processing of inverted index. And h is the aggregation function of all the matched query terms. So this general form would cover many state of the art retrieval functions. So to summarize, all the discussion about the system implementation, here are the major takeaway points. Of course, at this point that we will allocated the score accumulator as needed. We only need to return high quality subset of documents that likely are ranked on the top. Since we have not yet allocated a score accumulator for d5
 These are basically. These are judgments of which documents should be returned for which queries. So, for example d1 is judged as being relevant to Q1, D2 is judged as being relevant as well. And d3 is judged as non relevant. We compared it with database retrieval. This is to simulate users queries.This lecture is about the evaluation of text retrieval systems. You might be able to imagine for some users may be system A is better. So evaluation must rely on users. So this is the main topic of this lecture. These would be created by users. And we might need to define multiple measures. And there are some differences, and there are some documents that are returned by both systems. But how to evaluate a search engines quality or accuracy is something unique to text retrieval, and we're going to talk a lot about this. The binary judgments of documents with respect to a query. So this is remember we talked about. So in that case we will have to also define measures to quantify them. So next we talk about what to measure right? There are many aspects of a search engine that we can measure we can evaluate. Then we'll have to have relevance judgments. And R sub B is system B's approximation of relevant documents. So this is an illustration of how this works. There we mentioned that text retrieval is empirically defined problem. Basically, the question is how useful is a system for real user tasks. So as I said, we need the queries that are shown here. Ideally they have to be made by users who formulated the queries, 'cause those are the people that know exactly what documents would be useful, and then finally we have to have measures to quantify how well systems result matches the ideal ranked list that would be constructed based on users relevance judgments. Thus they don't have to accurately reflect the exact utility to users. So which one is better and how do we quantify this? Obviously this question highly depends on the users task and it depends on users as well. One is effectiveness or accuracy. We can also have a sample set of queries or topics. Which system works better would have to be judged by our users So this becomes a very challenging problem. For example, if you are doing a literature survey, you might be in the segment category and you might find that system B is better. The measures actually only to be correlated with the utility to actual users. We have Q1Q2, etc. So how exactly would this work? We're going to have a sample collection of documents and this is just to simulate the real document collection in search application. But how do we know which one works the best? In order to answer this question, we have to compare them and that means we have to evaluate these retrieval methods. One such a test collection is build. On the one hand,  one can also imagine the user might need to have as many relevant documents as possible. But once we have these and then we basically have a text collection and then if you have two systems you want to compare them then you can just run each system on these queries and documents and each system would then return results. If the user is not interested in getting all the relevant document. And typically this has to be done by using user studies and using the real search engine. In the previous lectures we have talked about a number of text retrieval methods, different kinds of ranking functions. This has been very important for comparing different algorithms and for improving search engine system in general. And this is usually done through a test collection, and this is the main idea that we'll be talking about in this course. In this case, we're measuring systems capability of ranking relevant documents on top of non random ones
 Are the retrieval results all relevant. These two are the two basic measures in text retrieval evaluation. To summarize, we talked about precision.This lecture is about the basic measures for evaluation of text retrieval systems. In this lecture we're going to discuss how we design basic measures. B for documents that are not retrieved but relevant, etc. To quantitatively compared to retrieval systems. Now with this table, then we could define precision as the. Suppose we have a total of 10 relevant documents in the collection for this query. We talked about F measure as a way to combine precision and recall. Result, so we just assume that there are 10 relevant documents in the collection. And this shows that system A is better by precision. A document can be also relevant or non relevant depending on whether the user thinks this is useful document. We can have A to represent the number of documents that are retrieved and relevant. But we also talked about System B might be preferred by some other users who like to retrieve as many relevant documents as possible. Which addresses the question have all the relevant documents being retrieved. So intuitively it looks like system A is more accurate and this intuition can be captured by a measure called precision where we simply compute: to what extent, all the retrieval results are relevant if you have 100% precision, that would mean all the retrieval documents are relevant. We can have perfect recall easily. They are useful for many other tasks as well. That's the number of retrieval relevant documents. We just said that there tends to be a tradeoff between precision and recall, so naturally it would be interesting to combine them. A document that can be retrieved or not retrieved, right? Because we're talking about the set of results. So precision and recall are the basic measures and we need to use them to further evaluate search engine. Now it's easy to see that if you have a larger precision or larger recall than F measure would be high. OK, so now let's define these two measures more precisely, and these measures are to evaluate a set of retrieval documents. Beta is a parameter that's often set to one. That's why although these two measures are defined for a set of retrieval documents, they are actually very useful for evaluating a ranked list. Then we have a perfect recall. The relevant retrieved documents A to the total number of retrieval documents, so this is just A divided by the sum of A&amp;C. We can have a test collection that consists of queries, documents and relevance judgments. We often are interested in the precision at 10 documents for web search. We also talked about the tradeoff between precision and recall and. In reality, however, high record tends to be associated with low precision. So why is this not as good as F1? Or what's the problem with this? Now. OK, So what would be an ideal result? You can easily see in the ideal case we have precision and recall, or to be 1. But such results are clearly not very useful for users, even though the average using this formula would be relatively high. We also talked about the recall. And after some transformation you can easily see it would be of this form. But what's interesting is that. In this case is a sum of precision and recall. So that means we are considering that approximation of the set of relevant documents. Now this is not desirable because one can easily have a perfect recall.0 that means we have got 1% of all the relevant documents in our results and all the results that we return are relevant. And in any case, this is just a combination of precision and recall. So in that case will have to compare the number of relevant documents, then retrieve and there is another measure called recall. Depending on a parameter beta. Can you imagine how? It's probably very easy to imagine that we simply retrieve all the document in the collection. There are fundamental measures in text retrieval and many other tasks. And once you think about multiple variants, it's important to analyze their difference. We can then run two systems on these datasets to quantitatively evaluate their performance. This measures the completeness of coverage of relevant documents in your retrieval. This is a slide that you have seen earlier in the lecture where we talked about. Is system a better or system B better? So let's now talk about how to actually quantify their performance. So this means F1 encodes different. Did not include all the ten, obviously and we have only seen three relevant documents there, but we can imagine there are other relevant documents in judge before this query. Now we can see by recall system B is better and these two measures turn out to be the very basic measures for evaluating search engines, and they are very important because they also widely used in
 Precision and Recall. So we this is one precision and this is another with different recall.This lecture is about how we can evaluate a ranked list. What about the recall ? Note that we assume that there are 10 relevant documents for this query in the collection, so it's one out of 10. But as a convenience, we often assume that the precision is 0. So note that this dinominator is 10. Therefore, you might favor system A. Note that here we are not dividing this sum by 4, which is a number of retrieved relevant documents. But if we use this measure to compare systems, it wouldn't be good because it wouldn't be sensitive to where these four relevant documents are ranked. We have got one document and that's relevant. Some users might like system A, some users might like system B. In this lecture we will continue the discussion of evaluation. In the previous lecture we talked about. So we have all these, add it up. But as we talked about Ranking before, we framed the tax retrieval problem as a ranking problem. So we divided by 10 and which is a total number of relevant documents in the collection. In that case, you emphasize high recall, so you want to see as many relevant documents as possible. So in all those cases we just assumed they have zero precisions. The actual precision would be higher, but we may make this assumption in order to have an easy way to compute another measure called average precision that we will discuss later. So the precision levels are marked as 0. So how do we do that? And that needs a number to summarize a range. OK, so now that we have this precision recall curve, how can we compare 2 ranked lists right? So that means we have to compare two PR curves. So for all these reasons it's desirable to have one single number to measure that. Now we can further link these points to form a curve as you see, we assumed that all the other precision that start high level records to be 0 and that's why they are down here, right, So they are zero in this. Now, so that also means it depends on whether the user cares about the high recall or low recall, but high Precision. The user doesn't care about high recall. In particular, we're going to look at how we can evaluate the ranked list of results. So we plotted these precision recall numbers that we have got as points on this picture. We don't know where it is. These are the two basic measures for quantitatively measuring the performance of search result. So of course this is a pessimistic assumption. So this is very good because it's a very sensitive to the ranking of every relevant document. 'cause this would be an underestimate for all the methods. And this is basically computed in this way, and it's called Average Precision. Or when we compare many different methods in research, ideally we have one number to compare them with, so that we can easily make a lot of comparisons. In this case D5. So it combines precision and recall, and furthermore you can see this measure is sensitive to a small change of a position of a relevant document. So that means which one is better actually depends on users, and more precisely user's task. Now imagine if I divide by 4, what would happen? Now think about this for a moment. How can we use precision and recall to evaluate a ranked list? Naturally, we have to look at the precision and recall at different cut offs because in the end the approximation of relevant documents set given by a ranked list is determined by where the user stops browsing, right? If we assume the user sequentially browses the list of results, the user would stop at some point and that point will determine the set, and then that's the most important cut off that will have to consider when we compute the precision recall without knowing where exactly the user would stop, then we have to consider all the positions where the user could stop. In general. Now this we don't count this one because the recall level is the same. What's the precision? What do you think? Well, it's easy to see. It can tell small differences between 2 ranked lists and that's what we want. That would be the ideal system. So this is the different levels of recall. It has to have perfect precision at all the recall points, so it has to be this line. Now in this case, which one is better? What do you think? Now this is a real problem that you actually might face. So, we also need to evaluate the quality of a ranked list. In fact, you are favoring a system that would retrieve very few rather than documents, as in that case the denominator would be very small, so this would be not a good measure. As I said, it's a real decision that you have to make if you are building your own search engine, or if you're working for a company that cares about search. So if we you divide this by 4, it's actually not very good. What if the user stops at third position? Well, this is interesting because in this case we have not got any additional relevant document. Which one is better? What do you think? In this case, clearly system B is better because the user is unlikely examining a lot of results. Now we missed the mini random documents. So when would we see another point where the recall would be different? Now if you look down the list it won't happen until we have seen another relevant document. But the precision is lower because we've now got a random number. An we can do then look at this number and that's the precision at a different recall level, etc. Basically, we're going to take a look at every different recall point. The actual curve probably will be something like this, but as we just discussed, it doesn't matter that much for comparing two methods
 So what are those values? Those are basically large values that indicate the lowly ranked results.So average precision is computed for just one query. The average precision is low. This is another way. You can have a high average precision, where as gMAP tends to be affected more by lower values and those are the queries that don't have good performance. It combines precision and recall and it's sensitive to the rank of every relevant the document. Now you've got the average precisions for all these topics. Also the average would be then dominated by where those relevant documents are ranked in the lower portion of the ranked list, but from a user's perspective we care more about the highly ranked documents. You might have thought about using r directly as the measure. So if you think about improving the search engine for those difficult queries than gMAP would be preferred. Some users will examine more than others and average precision is the standard measure for comparing two ranking methods. So this would give us what is called Mean Average Precision or MAP. As a special case of the mean average precision, we can also think about the case where there is precisely one relevant document. But as I just mentioned in another lecture, is this good? Recall that we talked about the different ways of combining precision and recall. And we called this kind of average gMAP map. In that case, there is precisely one relevant document, or in another application like a question answering. So to summarize, we show the Precision recall curve, can characterize the overall accuracy of a ranked list. But we generally experiment with many different queries and this is to avoid the variance across queries. But if you use this, there will be a big difference. If you use more queries then you would also have to take average of the average precision over all these queries. Depending on the queries you use, you might make different conclusions, so it's better to use more queries. So one natural question here is, why not simply using R? Now imagine if you are to design a measure to measure performance of the ranking system when there is only one relevant item. So if that document is ranked on the very top, R is 1 and then it's one for reciprocal rank. So how can we do that? You can naturally think of just doing arithmetic mean as we know. It is a very popular value for known item search or any ranking problem where you have just one relevant item. Right, so this is not the desirable. Now again, here you can see this R actually is meaningful here, and this R is basically indicating how much effort an user would have to make in order to find that relevant document. So do you use MAP or gMAP? Again, that's important question. It will make a big difference. So by taking this transformation by using reciprocal rank, here we emphasize more on the difference on the top and think about the difference between one and two. If you think about different ways, naturally you would probably be able to think about another way, which is geometric mean. In this case we take arithmetic mean of all the average precisions over set of queries or topics. But which which strategy would you use? Now first you should also think about the question, would it make a difference? Can you think of scenarios where using one of them would make a difference? That is, they would give different the rankings of those methods. Now you test it on multiple topics. Basically the difference is if you take some of R directly, then again will be dominated by large values of R. On the other hand, that if you just want to have improvement over all the kinds of queries or particular popular queries, that might be easy and you want to make the perfect and maybe MAP would be them preferred
 That means a document is judged as being relevant or non relevant. For a topic like this one we have 9 highly relevant documents. So imagine you can have ratings for these pages. So when we take a such a sum than a lower rank document will not contribute contribute that much as a highly ranked document. They are ok, they are useful perhaps. Overall these 10 documents. And we only know there is one highly relevant document one marginally relevant document, two non relevant documents.This lecture is about the how to evaluate the text retrieval system when we have multiple levels of judgments. And further from non relevant documents, those are not useful. Two is the rank position of this document. We don't really care where they are ranked. So are we happy with this? Well, We can use this rank systems. Then the highest DCG that any system could achieve for such a topic will not be very high. We're going to look at the how to evaluate the text retrieval system when we have multiple level of judgments. In this lecture we will continue the discussion of evaluation. Those are very useful documents from your moderately relevant documents. The difference however is when we have multiple topics. Right? So this will be an ideal ranked list. And finally it will do normalization to ensure comparability across queries. So all these would have to be 3 and then this will be followed by a two here because that's the best we could do after we have run out of threes. Now how do we evaluate search engine system using these judgments? Obviously the map doesn't work. We don't want the average to be dominated by those high values. But otherwise, in general you will be lower than one. And then we can compute the DCG for this ideal ranked list. Can be more than binary, not only more than binary. So more in the more general way, this is basically a measure that can be applied to any rank the task with multiple level of judgments. So this would be given by this formula that you see here, and so this ideal DCG would then be used as the normalizer DCG. And this ideal DCG will be used as a normalizer. So imagine if we have 9 documents in the whole collection. So this gain intuitively matches the utility of a document from a user's perspective. Ideally we want these two to be ranked on the top and which is the case here. So, so far we have talked the about binary judgments. For example, very relevant document here, as opposed to here. So this gain basically can measure how much gain of relevant information the user can obtain by looking at each document. So we divide this gain by the weight based on the position, so log of 2. But how can we capture that intuition? Well, we have to say this is 3 here. Right? And we marked the reading levels or relevance levels for these documents as shown here, 32113, etc. And we call these "Gain". For example here I show example of three levels, 3 for relevant sorry 3 for very relevant, two for marginally relevant and one for non relevant. Now, why do we want to do this? Well, by doing this will map the DCG values into a range of zero through one, so the best value or the highest value for every query would be one. But all these positions would be threes. It's useful for measuring ranked list based on multiple level relevance judgments. But earlier we also talk about the relevance as a matter of degree, so we often can distinguishing very high relative documents. And what's that? Well, that's simply the sum of these and we call it a cumulative gain. And the main idea of this measure I just to summarize is to measure the total utility of the top K documents. Precision and recall doesn't work because they rely on binary judgments
 These are text retrieval systems.4, so they are identical. This is 0. In this case, a new system might be penalized because it might have nominated some relevant documents that have not been judged, so those documents might be assumed to be non relevant. Another way to evaluate IR or Text retrieval is user studies, and we haven't covered that. In this case probability is 1. And the other unjudged documents are usually just assumed to be non relevant. And then design measures to measure that.This lecture is about some practical issues that you would have to address in evaluation of text retrieval systems. So you can see this is 0. We have to consider carefully what the users care about. MAP and nDCG are the two main measures that should definitely know about and they are appropriate for comparing ranking algorithms. For the matching of relevant documents, with the queries we also need to ensure that there exists a lot of relevant documents for each query. And then we simply combine all these top K sets to form a pool of documents for human assessors, to judge.4 and then this is twice as much as 0. You intuitively. And we hope these methods can help us nominate likely relevant documents. First, the documents and queries must be representative. So the goal is to figure out the relevant documents we want to make judgments on relevant documents, because those are the most useful documents from users perspective. Actually System A is better and this is another case. In this lecture we will continue the discussion of evaluation we'll cover some practical issues that you have to solve in actual evaluation of text retrieval systems. Now, of course there are many documents that are duplicated bcause many systems might have retrieved the same relevamnt documents. So then we're going to have each to return top K documents. In this case, because these numbers seem to be consistently better for system B . Of course the users don't see and which result is from which method the users would judge those results or click on those documents in search engine application. It means it surely is random fluctuation. But how can we quantitatively answer this question? And, This is why we need to do statistical significance test. Now, so if we look at this picture, then we see that. Then we should believe that unless you have used a lot of queries, the results might change if we use another set of queries. So there will be some duplicate documents and there are also unique documents that are only returned by one system and so the idea of having diverse set of ranking methods is to ensure the pool is broad and can include as many possible relevant documents as possible. So this is actually a major challenge. But we assume that because of random fluctuations depending on the queries. So in that case we can then conclude the difference must be real. So to illustrate this, let's think about the such a distribution and this is called the null distribution. System A is better. through the clicked documents, if the user tends to click on one, the results from one method, then it's just that the method may may be better, so this is the leverage the real users of a search engine to do evaluation. This say we start with the assumption that there's no difference between the two systems. We actually have 4 cases where system B is better, but 3 cases System A is better. And the question here is how sure can you be that observed difference that doesn't simply result from the particular queries you choose, so here are some sample results of average position for system A and system B in two different experiments. And then the users would. If we have inappropriate experiment design, we might misguide our research or applications and we might just draw wrong conclusions. In terms of relevance judgments, the challenge is to ensure complete judgments of all the documents for all the queries, yet minimizing human effort because we have to use human labor to label these documents, it's very labor intensive and as a result it's impossible to actually label all the documents for all the queries, especially considering a giant dataset like the web. If system B is better than system A, we have a plus sign when System  A is better, we have a minus sign, etc. But if you observe a value in this region or a difference on this side, then the difference is unlikely from random fluctuation, right? So there's a very small probability that you will observe such a difference just because of random fluctuation. So the fact that the average is larger doesn't tell us anything and we can reliably conclude that, and this can be quantitatively measured by a P value and that basically, means the probability that this result is infected from random fluctuation. If a query has only one, let's say rather than the document in the collection, then you know it's not very informative to compare different methods using such a query, because there's not that much room for us to see difference, so ideally there should be more relevant documents in the collection, but yet the queries also should represent the real queries that we care about. That means if you participated in contributing to the pool, then it's unlikely that it will penalize your system because the top ranked documents have all been judged. You will see them often in research papers. These are three mini books about evaluation and they all excellent in covering a broad review of information retrieval, evaluation, and discovered some of the things that we discussed. Yet if you look at the these exact average precisions for different queries. And we have seen this in some of our discussion, so make sure to get it right for your research or application. But the if the pool is not very large, this actually has to be reconsidered and we might use other strategies to deal with them, and there are indeed other methods to handle such cases, and such a strategy is generally ok for comparing systems that contributed to the pool. They must represent the real queries, and real documents that the users handle, and we also have to use many queries and many documents in order to avoid biased conclusions. So to summarize, the whole part of text retrieval evaluation, it's extremely important because the problem is empirically defined problem. Right, so this is not. What's not covered is Some other evaluation strategy like A-B Test where the system would mix 2, the results of two methods randomly and then will show the mixed results to users. The takeaway message here is that you have to use many queries to avoid jumping into a conclusion, as in this case to say System B is better
 Should.This lecture is about a probabilistic retrieval model. Suppose we're trying to compute this probability for D1D2 and D3 for Q1. So intuitively this probability. This document how likely the user would oppose this query. probability of relevance. Being probabilistic models, we define the ranking function based on the probability that this document is relevant to this query. These probabilities would give us some sense about which document might be more relevant or more useful to a user who typing this query. In other words, D2 is assumed to be relevant to Q1. Often queries and documents. And this part. In particular, we're going to discuss the query likelihood retrieval model. And that is a user formula to query based on an imaginary relevant document. And all this could be the same query typing by. Note that in the vector space model we assume they are vectors, but here we are assumed. There are even more unseen queries because you cannot predict what queries would be typing by users. And this part shows that we're interested in how likely the user would actually enter this query. Here you can see we compute all these query likelihood probabilities. In this lecture, we will discuss another subclass in this. Inquiry likelihood Our assumption is that this probability of relevance can be approximated by the probability of query given a document and relevance. What about the D1 and D2? They are here. Once we have these values, we can then rank these documents based on these values. So we ask this question and we quantify the probability and this probability is conditional probability of. There will be a lot of unseen documents. Which is one of the most effective models in probabilistic models. And in this case we have a ranking function that's basically based on the probability of a query given the document, and this probability should be interpreted as the probability that a user who likes document D would pose queria Q. We assume they are the data observed from random variables. Then what about other queries? Now we can imagine we have a lot of such data. It's not obvious we're making this assumption. So you can see with this approach, we can actually score these documents for the query, right? We now have a score for D1D2 and D3. We assume that the user likes the document because we have seen that the user clicked on this document. So obviously this approach won't work if we apply it to unseen queries or unseen documents. And this has to do with. To use this new conditional probability to help us score, then this knew conditional probability will have to somehow be able to estimate this conditional probability without relying on this big table. For this query we can simply rank them based on these probabilities, and so that's the basic idea of probabilistic retrieval model, and you can see it makes a lot of sense. Because we have a relevance value here and then we asked the question about the how likely will see this particular query from this user. So and then we just. Now the question of course, is how do we compute this conditional probability? At this, in general has to do with how to compute the probability of text, because Q is attached. Basically, we can do assume that whether the user types in this query has something to do with whether user likes the document. We can ask the question, how can we then estimate the probability of relevance? Right, so how can we compute this probability of relevance? Or intuitively that just means? If we look at the all the entries where we see this particular D and this particular Q, how likely will see a one on the third column? So basically that just means we can just collect those accounts. Called a language modeling approaches to retrieval. In general we only collected data from the documents that we have shown to the users. So what do we do in such a case when we have a lot of unseen documents and then some queries where the solutions that we have to approximate in somewhere, right? So in this particular case code query like whole retrieval model, we just approximate this by another conditional probability. P of Q given D an R is equal to 1. Model called the Language model and this kind of models are proposed to model text. The likelihood of queries given each document. So this is the basic idea. In this category of models there are different variants. The classical problem is model has led to the BM 25 retrieval function which we discussed in the vector space model. So to summarize, the general idea of modern relevance in the probabilistic model is to assume that we introduce a binary random variable R here, and then let's a scoring function be defined based on this conditional probability. In this lecture, we're going to continue the discussion of tax retrieval methods. We also talked about the approximate in this by using the query likelihood. For example, in this line it shows that query one. What is the estimated probability? Now think about that. So this also. So let's look at how this model work for our example, and basically what we are going to do in this case is to ask the following question which of these documents is most likely the imaginary relevant document in the user's mind when the user formulates this query. We have made an interesting assumption here. Observing this query if a particular document is infected, imaginary relevant document in the user's mind. Have you seen that if we are interested in Q1 and D1 will be looking at these two pairs? And in both cases. So more specifically, we would be very interested in the following conditional probability as issuing this here if the user
 This model gives. These common words. All these words are related to computer. These words have. This has to do with how do you model text data with probabilistic models so it's related to how we model query based on a document. Some text will have higher probabilities than others. Distribution. We have one probability for each word, and all these probabilities must sum to one. We assume that we generate the text by generating each word independently. Whereas these common words will not have a high probability. So in this lecture we talked about. Or consequences of this is, of course we're going to assign zero probabilities to unseen words. Language model, which is basically probability distribution over text. The first one clearly suggests a topic of text mining, because the high probability words are all related to this topic. Information tells us what words are common in general. We can ask the question which model which word distribution has been used to generate this text. The Mortal Council be regarded as a probabilistic mechanism for generating text. So we call this document language model and we call this collection language model. In this lecture we're going to give an introduction to statistical language model. We can use this text to estimate the language model and the model might look like this. As I said, text can be assumed to be assembled drawn from this world distribution. Another example, given that we observe baseball 3 times and game once in a news article, how likely is it about sports? This obviously is related to text categorization, an information retrieval. This is the background language model. But it could have generated any other sequences.This lecture is about the statistical language model. So what would be your guess? Have to decide what probability is. if we have not oveserve a word there will be no incentive to assign a non zero probability using this approach. We can see what would we see there. We're going to talk about what is the language model and then we're going to talk about the simplest language model called a unigram language model, which also happens to be the most useful model for text retrieval. Also, given that a user is interested in Sports News, how likely would the user used baseball in a query? Now this is clearly related to the query likelihood that we discussed in the previous matching. But this model now only needs N parameters to characterize. So then we can use these two models to somehow figure out the words that are related to the computer. Suppose we now have available of particular document. So this means the probability of a sequence of words. How can we do that? It turns out that it's possible to use, langage model to do that. So we can imagine what gender the text that looks like a text mining. You can see the probability is given to these sentences or sequences of words can vary a lot depending on the model. In the next lecture, we're going to talk about how, then which model can be used to design retrieval function. That means if we specify all the probabilities for all the words, then the models behavior is completely specified, whereas if we don't make this assumption we would have to specify probabilities for all kinds of combinations of words. Therefore it's clearly context dependent. Text mining etc would have. So if we use this background model, we would know that these words are common words in general, so it's not surprising to observe them in the context of computer. What is the language model? It's just a probability distribution over word sequences. But if we just use this model, we cannot just say all these words are semantically related to computer. One use is simply to use it to represent the topics. So this assumption is not necessarily true, but we make this assumption to simplify the model. text has a probability of 10 out of 100 because I've seen text 10 times an there are in total 100 words, so we simply not simply normalize these counts. So they should not have zero probabilities even though they are not observed in abstract. Right, this is the background model. So what does that mean? We can imagine this is a mechanism. We talked about the simplest language model called unigram them model which is also just a word distribution. Here I show two unigram language models with some probabilities and these are high probability words that are shown on top. What words do you think it would be generated? Well, maybe text or maybe mining. They are very common, but as we go down we will see words that are more related to computer science, computer software, text, etc. Relatively higher probabilities, in contrast in this distribution, will text has relatively small probability. We can then ask the question, how likely will observe a particular text from each of these three models? I suppose we sample words. That doesn't mean we cannot generate this paper from text mining. This means it can be used to represent the topic of the text. There can be expected to occur on the top, but soon we will see text mining Association clustering. So we're going to cover this a little more later in discussing the query likelihood retrieval model model. And normally they're not independent. But it's a non zero probability if we assume none of the words have non zero probability. So the background language model precisely tells us this. In the extreme case, you might imagine we might be able to generate the attacks paper text mining paper that would be accepted by a major conference. As you can see, even though we have not asked the model to generate the sequence, it actually allows us to compute the probability for all the sequences. Why 'cause that would take away probability mass for these ovbserved words? And that obviously wouldn't maximize the probability of this particular observer text data. It looks similar because these words occur everywhere. For example, we can simply take the ratio of these two probabilities or normalize the topic language model by the probability of the world in the background language model. As we always do so in this case, this language model gives us the conditional probability of seeing the world in the context of computer and these common words will naturally have high probabilities. And later you will see how they are used in retrieval function. In this estimator we assume that the parameter settings are those that would give our observed data the maximum probability. Given that we see John and Fields. But in general, high probability words with likely show up more often. We lack all the knowledge to understand language. And so, although here we might also see these words, for example computer. Another source is because we don't have complete understanding. Now if we do the same, we have another distribution again. So this shows that the even with these simple language models we can do some limited analysis of semantics. So how can we know what words are very common so that we want to kind of get rid of them? What model would tell us that? Maybe you can think about that. We talked about the two uses of a language model one is represented topic in a document in the collection or in general the other is rediscovered water associations. So by making this assumption, it makes it much easier to estimate these parameters, so let's see a specific example here. In such a case. Now, in this case we will get the distribution that looks like this. So for example, now we can ask the device or the model to stochastic in general words for us instead of sequences
This lecture is about the query likelihood probabilistic retrieval model. We assume that this document is generated using this unigram language model. Now this model. As we derive this ranking function. And then use this as a query word. Now we do this. So that we can compute the probability of each query word given by this document. Probability that we observe this query given that the user is thinking of this document. But we do this for both documents and then we're going to score these two documents and then rank them. In the query likelihood retrieval model. Then we can assume the user would use this document as a basis to post a query to try to retrieve this document. And this is assumed to be product of probabilities of all individual words. Whether user actually followed this process. That works as follows, where we assume that the query is generated by sampling words from the document. Now this of course is assumption that we have made about how a user would pose a query. Our idea is to model how likely a user who likes a document would pose a particular query. Well, since we are computing the query likelihood. For example, presidential. So with this these assumptions, we now have actually simple formula for retrieval, right? We can use this to rank our documents. In this lecture we continue the discussion of probabilistic retrieval model. In this sum, We have it all over the query words N query words. So for example, a user might pick a word like presidential from this document. And another word, campaign. In particular, we're going to talk about the query likelihood retrieval function. So that's the basic idea of this query, likelihood retrieval function. Instead of this particular document. Here different ways to estimate these document language model would lead to a different ranking function for query likelihood. Then we might see a problem. Would be what? Would be 0, right? So that caused a problem because we cause all these documents to have zero probability of generating this query. So we can imagine the user could use a process. So we're still considering only these N words. Now if you look at this, these numbers or these formulas for scoring all these documents. Then the probability here is just the probability of this particular query, which is a sequence of words. And then this part is log of the probability of the word given by the document language model. And similarly we can get probabilities for the other two documents. The value is log of the probability of this word given by the document. So we have to examine what assumptions have been made. So for example, the probability of presidential given the document. Doesn't necessarily assign zero probability for update. So how do we compute this query likelihood? If we make this assumption? Well, it involves 2 steps, right? The first is to compute this model. So according to our assumption that a user would pick a word from a document to generate a query, then the probability of obtaining a word like update. We should think about what has caused this problem. In fact that we consume this model does not assign zero probability for any word. So in this case you can imagine if a user likes this particular document about the presidential campaign news. And how do we fix it? Right, so if you think about this for a moment, you realize that that's because we have made assumption that every query word must be drawn from the document in the user's mind. That is unigram language model instead of a fixed document. Now we actually often score the document for this query by using log of the query likelihood as shown on the second line. For example, I've shown two possible language models here is made based on two documents. The second step would just compute the likelihood of this query and by making independent assumptions we could then have this probability as a product of the probability of each query word. And we make the assumption that each word is generated independently, so as a result, the probability of the query is just a product of the probability of each query word. And this is why we can use this idea to them. But we are using a different form, as if we're going to take sum over all the words in the vocabulary. Now the difference is that this time we can also pick a word like update even though update does not occur in the document to potentially generate the query word like update so that a query with update want to have zero probabilities. If we try a different query like this one presidential campaign update
 These are the query words that are matching the document. So more formally, we will be estimating the probability of a word given a document as follows. So all these probabilities must sum to one. It will tell us which unseen words will have likely higher probability. All query words that are not matched in the document. To formulate a query. So the main task now is to estimate this document language model. There is a log of probability of word given by the document or document language model. Obviously this sum has extra terms that are. In this lecture, we're going to continue talking about probabilistic retrieval model. They all occurred in the document. So they occur in the query. These seen words have a different probability. So what if we plug in this smoothing formula into our query likelihood running function? This is what we will get. In this case, these words have this probability because of our assumption about the smoothing. And the key question here is what probability should be assigned to those unseen words.This lecture is about the smoothing of language models. Is to control the amount of probability mass that we assign to unseen words. So this is as I said, this is some of all the query words. One sum is over all the query words that are matching the document. Due to this this term, but they don't occur in the document. We're going to assume that the probability of this word would be proportional to the probability of the word in the whole collection. So we're going to then decompose this sum into two parts. So to make this transformation and to improve the maximum likelihood estimate by assigning non zero probabilities to words that are not observed in the data. In our smoothing method, we assume that the words that are not observed in the document we have somewhat different form of probability namely it's for this form. This is over all the query words that are not matched in the document. And this makes sense, because here we are considering all query words and then we subtract the query words that are matched in the document. And this is the sum over all the query words that are matching the document. All the words match the query words matched in the document and with this kind of terms. And they also have to of course have a non 0 count in the query, so these are the words that are matched. But imagine a user who is interested in the topic of this subject. Then the probability would be a discounted. We have all probability of zero. So in this lecture we're going to look into this in more detail. So how do we estimate this language model? The obvious choice would be the maximum likelihood estimate that we have seen before, and that is we're going to normalize the word frequencies in the document. So how do we improve this, well? In order to assign a non zero probability to words that have not been observed in the document. A common way that we will use. This sum has extra terms that are not in this sum. So here we pretend that they are actually. As we said before, different methods for estimating this model would lead to different retrieval functions. Obviously, all these probabilities must sum to one, so Alpha sub D is constrained in some way. Note that for words that have not occurred in the document here they all have zero probability, so we this know this is just like the model that we assumed earlier in the lecture, where we assume that the user would sample word from the document. Right, this is another frequent account that has a different probability. unseen words in this abstract. If we do this, then we have different forms of terms inside these sums. We would have to take away some probability mass from the words that are observed in the document. For the document, the author might have written other words. In particular, we're going to talk about the smoothing of language model in the query likelihood retrieval method. And note that this is a very interesting formula because here we combined these two. Here we have another sum. So now you can see in this sum we have. That is to say if we don't observe a word in the document. That means if we don't observe the word in the data set, we're going to assume that it's probability is kind of governed by another reference language model that we will construct. Which means all the words that have the same frequency count will have identical probability. It's not matched in the document. The user might actually choose a word that's not in the abstract to use as query. So for example here we have to take away some probability mass because we need some extra probability mass for the unseen words. One idea here that's very useful for retrieval is let the probability of unseen word be proportional to its probability given by reference language model
 This. Then alpha sub d could be expected to be to be large. Normalization in particular alpha sub d might be related to document. for a long document. With this assumption, we've shown that we can derive a general ranking formula for query likelihood that has the effect of TF IDF weighting and document length normalization. I just say that this term is related to IDF weighting. We need to do more smoothing. We just need to assume that if we smooth with this collection language model, then we would have a formula. That is, the probability of an unseen word is assumed to be proportional to its probability in the collection. And that's not good for scoring a query with such an unseen world. used logarithm of query likelihood for scoring. After we smooth the document language model, we send you to have non zero probabilities for all the words. The general idea of smoothing in retrieval is to use the collection language Model to Give us some clue about the which unseen word should have a higher probability. How much smoothing do we want to do ? Intuitively, if a document is long then we need to do less smoothing because we can assume that data is large enough. length, so it encodes how much probability mass we want to give to unseen words. So to summarize, we talked about the need for smoothing document language model, otherwise would give zero probability for unseen words in the document. So this term appears to penalize long documenting in that the alpha sub d would tend to be longer than larger than. And so this may not actually be necessary. It's also interesting to note that the last term here is actually independent of the document, since our goal is to rank the documents for the same query, we can ignore this term for ranking. But what's nice with probabilistic modeling is that we are automatically given a logarithm function here. Because it's going to be the same for all the documents. And we turned the product into a sum of logarithm of probability and that's why we have this logarithm. This collection probability, but it turns out that this term here is actually related to the document length. So this new form of the formula is much easier to score or to compute. We actually have a smaller weight and this is precisely what IDF weighting is doing. In particular, we see that the main part of the formula is a sum over the matched query terms. Only that we now have a different form of TF and IDF. Again, can you see which factor is related to the document length? In this formula. After we make the assumption about the smoothing the language model. And if we heuristically design the formula, we may not necessary end up having such a specific form. And it's also necessary in general to improve the accuracy of estimating the model represents the topic of this document.So I showed you how we rewrite the query likelihood retrieval function into a form that looks like the formula of this slide. But as we will see later when we consider some specific smoothing methods, it turns out that they do penalize long documents just like in TF IDF weighting and document length normalization formulas in the vector space model. If you look at the assumptions that we have made, it will be clear it's because we have. We probably have observed all the words that the author could have written, but the document is short. In particular, we're going to show that from this formula we can see smoothing with the collection language model will give us something like a TF IDF weighting and length normalization. Imagine if we drop this logarithm, we would still have TF and IDF weighting. What's also interesting is that we have very fixed form of the ranking function. So, have you noticed that this p of seen is related to the term frequency? In the sense that if a word occurs very frequently in the document, then the estimated probability here would tend to be larger
 Which is. This picture shows how we estimate. To the observed text. As a result, we have more counts. So this is a smoothing parameter. The larger lambda is, the more smoothing we will have. For example, if we compute the smooth probability for text.This lecture is about the specific smoothing methods for language models used in probabilistic retrieval model. So the idea of smoothing then is to rely on collection language model where this word is not going to have a zero probability to help us decide what non zero probability should be assigned to such a word. Of course this part. Have you notice that this part is basically alpha sub D? So we can see this case. Now in this form we can easily to see what changes we have made to the maximum likelihood estimate which would be this part right? So normalized count by the document length So in this form we can see what we did is we add this to the count of every word. Would be of this form so that the two coefficients would sum to one. We pretend. Is to maximize the probability of the observed text as a result, if a word like network. So in this approach in what we do is we do a linear interpolation between the maximum likelihood estimate here and the collection language model and this is controlled by the smoothing parameter Lambda. Document the language model by using maximum likelihood estimate that gives us word counts normalized by the total number of words in the text. This is a constant. But the collection probability is this, so we just combine them together with this simple formula. How do we set alpha? So in order to answer these questions, we have to think about this very specifically, smoothing methods and that is the main topic of this lecture. In particular, we're going to need to know how to estimate the probability of a word Exactly and. So what does this mean? This is basically. The probability of network would be just this part. And so the probability of the text would be of this form naturally. In this lecture we will continue the discussion of language models for information retrieval, particularly the query likelihood retrieval method, and we're going to talk about the specifics smoothing methods used for such a retrieval function. For the term in the document. We are gonna talk about the two smoothing methods. And when we combine this with the count here, essentially we are adding pseudo counts. Now this is one way to understand that this smoothing Basically it means it's a dynamic coefficient interpolation There is another way to understand this formula. If you think about this and you can easily see now the  Alpha sub D in this smoothing method is basically Lambda. So this is how this method works. Right, so for text again, we will have 10 as an original count that we actually observe, but we also add some pseudo count. So this is the retrieval function based on these assumptions that we have discussed, you can see it's a sum over all the matched The query terms here. It is often called the Dirichlet Prior or Bayesian smoothing. We can rewrite the smoothing method in this form. Is not observed in the text, it's going to get zero probability as shown here. So this seems to make more sense than fixed coefficient smoothing. So this is a slide from a previous lecture where we show that with the query likelihood ranking and smoothing with the collection language model we end up having a retrieval function that looks like the following. As pseudo count to this data pretend we actually augment the data by including some pseudo data defined by the collection language model. Something related to the probability of the word in the collection, and we multiply that by the parameter mu. The word network which used to have zero probability now is getting a non zero probability. So it's easy to see
 So, this is a general smoothing. We also see IDF weighting which is given by this. So this of course should be plugged into this part. So to summarize this part we've talked about the two smoothing methods. In alpha sub d so this would be plugged into this part. So it's easy to see that this can be rewritten as this. That is tight to users. And then the form dictated by probabilistic model. So we can plug this into here. So this actually can be interpreted as expected count of the word. If you do that, the expected count of a word W would be precisely given by this denominator. vector element. And when we compute this ratio, while we defined that is that the ratio is equal to this. sorry, general ranking function for smoothing with collection language model. So all these heuristics are captured in this formula. In both cases we can see by using these smoothing methods we would be able to reach a retrieval function, whether assumptions are clearly articulated, so they're less heuristic. And you can interpret this as the element of a document vector. So overall this shows by using probabilistic model we follow very different strategy than the vector space model. And in this case, you can easily see this is precisely a vector space model, because this part is the sum over all the matched query terms. So this is a major advantage of probabilistic model where we don't have to do a lot of heuristic design. And this is the probability of unseen word, or in other words, lambda is basically the alpha here. So one plus this, so it's going to be a non-negative log of this. Into a product of probabilities of all the words in the query. This is the element of the query vector what do you think is the element of the document vector? It's this, so that's our document. It is a query words are generated independently that allows us to decompose the probability of the whole query. And if this counter is larger than the expected count, this part, this ratio would be larger than one. And then the third assumption that we have made is if a word is not seen in the document that we're going to let its probability with proportional to its probability in the collection of the smoothing with the collection language model, and finally we've made one of these two assumptions about the smoothing. The first assumption is that the relevance can be modeled by the query likelihood and the second assumption we've made. So what do you think Is the denominator here? This is the length of document, total number of words multiplied by the probability of the word given by the collection. You have seen this before.So let's plug in these smoothing methods into the ranking function to see what we will get. Suppose we don't have logarithm, then there's no sub linear transformation. In this case, of course, we still need to set the smoothing parameter, but there are also methods that can be used to estimate these parameters. In particular, how do we use query likelihood to derive a model that would work consistently better than BM25? Currently we still cannot do that. Dirichlet Prior, this is to add pseudocounts to every word and is doing adaptive interpolation in that the coefficient would be larger for shorter documents. And then here, what's the value for alpha? What do you think? It will be just Lambda, right? And, what would happen if we plug in this value here? If this is lambda, what can we say about this? Does it depend on the document? No, so it can be ignored. And in this case note that there is a specific form and we can see whether this form actually makes sense. Experiment results also show that these retrieval functions also are very effective, and they are comparable to BM 25 or pivoted length normalization. But what's interesting here is that we are doing another comparison here now. So we either use JM smoothing or the Dirichlet smoothing. And let's recall what assumptions we have made in order to derive the functions that we have seen in this lecture. If we're going to draw a word from the collection language model and we want to draw as many as the number of words in the document. The actual count of the word in the document with the expected count given by this product. So that's example of the gap between formal model like this and the relevance that we have to model, which is really a subjective machine. So Lambda is parameter and let's look and this is a TF. So that just means if we score this function we have to take a sum over all the query words and then do some adjustment of the score based on the document. So that's also why, for example, BM 25 remains very competitive and still open challenge how to use probabilistic model to derive a better model than BM25. But this is no longer a simple dot product, right? Because we have this part. As a result, what we get is the following function here, and this is again a sum over all the matched query words. And this is one advantage of using this kind of probabilistic reasoning. Where we have made explicit assumptions, and we know precisely why we have a logarithm here and why we have these probabilities here. The larger the count is, the higher the weight will be. Although it's TF IDF weighting and stopping the length normalization , for example, it's unclear whether we have sub-linear transformation. So now let's see what what's the value for alpha sub D here
 These results will be shown to the user. We talk about pseudo feedback where we simply assume top-ranked documents to be relevant. So this is all relevance feedback. And we simply assume that the top ranked documents to be relevant.This lecture is about the feedback in text retrieval. We can assume these display. Assumed documents to learn and to improve the query. You can imagine these top ranked documents are actually similar to relevant documents, even if they are not relevant. Now you might wonder how could this help if we simply assume the top ranked documents to be relavant well. This can be very useful to the system we learn. And then the query would be sent to a retrieval engine or search engine. They look like relevant documents, so it's possible to learn some related terms to the query from this set. The text is probably relevant, is interesting to user. Now this also is a clue about whether a document is useful to the user. What exactly is interesting to the user. We can see the user with typing the query. So in this lecture we're going to continue the discussion of text retrieval methods. In particular, we're going to talk about the feedback impacts retrieval. The feedback is based on relevance judgments made by the users. Relevance feedback, where the user makes explicit judgments. We don't have to involve the user, therefore we could do that actually, before we return the results to the user. And then we would then use these. So this is the effective for improving the search result. In fact, there you may recall that we talked about using language model to analyze word association to learn related words to the word "computer" right, and then what we did is we first use computer to retrieve all the documents that contain computer. So if we make a contrast between these two, what we can find these that would learn some related terms to the word computer? As we have seen before and these related words can then be added to the original query to expand the query and this would help us bring documents that don't necessarily match computer but match other words like a program and software. So we can learn from such information and this is called implicit feedback. And also use the document collection to try to improve ranking. And then we're going to then use the background language model to choose the terms that are frequent in this set, but not frequent In the whole collection. So the feedback module would then take this as input. This is a diagram that shows the retrieval process
 And these pluses are relevant documents. And these are relevant documents. Those are the documents that are assumed to be relevant or judgement be relevant. That's basically this. Or the document that are viewed by users. They are not as reliable as the relevance feedback. We might adjust weights of all terms or assign weights to new terms. Now this method can be used for both relevance feedback and pseudo relevance feedback. Those are documents known in non-relevant, and they can also be the document that escaped by users. And these are all the documents. This is the average vector of these two. When we take the average of these vectors, then we are computing the centroid of these vectors and similarly this is the average non relevant document vectors. And, this average basically is the centroid vector of relevant documents. We just add this with this one. And we're going to use this new query vector. So this is precisely what we want from feedback. We will have positive examples. And then these minuses are negative documents like this. We have zero weights of course and these are negative documents. Where is positive documents tend to be clustered together and they will point you to a consistent direction. So it's a centroid of this two. In this lecture we continue talking about feedback in text retrieval.This lecture is about the feedback in the vector space model. And we have. The task of a Text Retrieval system is to learn from examples to improve retrieval accuracy. Now in the Rocchio Feedback Method, we're going to combine all these with the original query vector, which is this. When we add these two vectors together, we are moving the query closer to the centroid. Those terms of typing by the user and the user has decided that those terms are most important. And this is especially true for pseudo relevance feedback. In the case of relevance feedback, we obviously could use a larger value, so those parameters they have to be set in imparallelly. And as a result, in general the query will have more terms, so we often call this query expansion. This newer query vector will then reflect the move of this original query vector toward this relevant centroid vector and away from the non relevant centroid vector. We have 5 documents here. What we simply just, so it's very easy to see. Here you can see this is original query vector. This one to rent the documents. Two relevant the documents here. These documents would be basically the top ranked of the documents. Each is for one term. So this is the main idea of Rocchio feedback and after we have done this we will get a new query vector which can be used to store documents. As we have discussed before, in the case of feedback. Particularly, we're going to talk about feedback in the vector space model. For example is relevant, etc. So our goal here is trying to move this query vector to some position to improve the retrieval accuracy. So when we use the query vector and use a similarity function to find the most similar documents, we are basically drawing a circle here and then. In the case of pseudo feedback up, the parameter beta should be set to a smaller value because the relevant examples are assumed to be relevant. This is for efficiency concern. In the end what we have is this one. Right? They are displayed in red and these are the term vectors and I have just assumed some TF IDF weights. Algebraically, it just means we have this formula. We will see one potential problem. Now, if you think about this picture, you can realize that in order to work well in this case you want to query about that to be as close to the positive vectors as possible. And we do exactly the same for other terms. The general method in the vector space model for feedback. An the original query terms are still very important. Is to modify our query vector and we want to place the query vector in a better position to make the accurate. I only showed the vector representation of these documents. This is basically the same. We illustrate this idea by using a 2 dimensional display of all the documents in the collection and also the query vector. There are two here
 These are the click the documents by users or relevant documents judged by users or simply top ranked blocking that we assume to be relevant. We consider two distributions. All this. This is the user generated mixable. One is the query model denoted by this distribution. So these are relevant to this topic and they if combined with the original query. Approach is simply to assume these documents are generated from this language model as we did before. The feedback model that we want to estimate. So this picture shows that we have this model. These documents that should not belong to this topic model, right? So now what we can do is to assume that, well, those words are generated from background language model, so they were generated those words like the. In this model. Now imagine how we can compute the centroid for these documents by using language Model 1. So those will be common here. And then this model can be combined with the original query model. I and then we're going to feed that mixture model to this 10. We are going to use maximum likelihood estimator to adjust this model to estimate the parameters. Into a language model. So this can be added to the original query to achieve feedback. As a result, this topic model must assign high probabilities, so the high probability words according to the topic model would be those that are common here, but rare in the background. So we can then choose this probability distribution to maximize this locali code. Right, so all these are parameters. As a result, it would then assign higher probabilities to other words that are common here. In this lecture we will continue the discussion of feedback in text retrieval. We have to have another model which is this one to help explain the world. And this is of course a pseudo feedback. We could do that and there will be another way to do this, but here we are going to talk about another approach which is more principled approach. Document set. So we are going to, let's say set this to a parameter that say 50% of the words are noise or 9% are noise and this can be assumed to be fixed if we assume this is fixed. And these are the words learned using this approach. Once we have done that, will obtain the serial F that can be there, interpreted with the original query model to do feedback. These words would work as follows. And will the basis is the feedback documents. Basically we're going to adjust well this parameter. We talked about how to use Rock You to do feedback in vector space model and how to use query model is missing for feedback in language model and we briefly talked about the mixture model and the basic idea. So it's not very good for feedback because we would be adding a lot of such words to our query when we interpret this with the original query model. But this basically means this is exactly like a vector space model 'cause we computer vector for the document the computer, another vector for the query and then we compute the distance only that these vectors are of special forms their probability distributions. And if they are common, they would have to have high probabilities according to maximum likelihood estimator. Given by the probabilistic model here to characterize what the user is looking for versus the count of query words there. Of course this approach is based on generated model. So when it do something in particular will are trying to get rid of those common words and we are we have seen actually one way to do that by using background language model. Topic model here that we would like to estimate and we're going to then generate award here. There are many other methods, for example, the relevance model is a very effective model for estimating query model. We just we just have to solve this optimization problem. This is very easy to see once you plug this into. This is the probability of award given by the feedback model in both cases. Let's assume they are most inactive. If we don't rely much on background model, we still have to use this topic model to account for the common words, whereas if we set Lambda to a very high value, we will use the background model very often to explain these words. This is very similar to Rock Hill which updates the query vector. Sorry, mostly positive documents, although we could also consider both kinds of documents. We talked about the three major feedback scenarios, relevance feedback, sooner feedback, and in principle feedback. The here and in this case it's not appropriate to use the background language model to achieve this goal, because this model would assign high probabilities to these common words. So in this approach, then we assume this machine that was generated. One is important research paper that's about relevance based language models, and it's a very effective way of computing query model. So to summarize, in this lecture we have talked about the feedback in language model approach. Imagine we flip a coin here to decide what distribution to use with probability of Lambda. But we assume that the query is generated by assembling words from a language model in the query likelihood method. The difference now is that we are not asking this model alone to explain this. Whichever model is basically to generalize. So we derive the query likelihood ranking function by making various assumptions. This can be interpreted as measuring the care divergent of two distributions. And then we got the results and we can find some feedback documents. Now the question is whether this distribution is good for feedback. And in this case, then, feedback can be achieved through simple query model estimation or updating. An if they are rare here. When we don't use the background more often, remember Lambda can use the probability of using the background model to generate the text. In particular, we're going to talk about the feedback in language modeling approaches. And this model is actually going to make the query likelihood retrieval function much closer to vector space model.This lecture is about the feedback in the language modeling approach. What we could do is do just normalize the word frequency here and then we got this world distribution. So here are some examples of the feedback model learned from a Web document collection, and we do sudo feedback. So this is how single feedback works. This model is forced to assign high probabilities to award like that because it occurs so frequently here. Then We only have these probabilities as parameters, just like in the simplest unigram language model. And which one is used would depend on Lambda and that's why we have this form. So this is how we can use the care divergent model to them the feedback the picture shows that we first estimate the document language model. And if we use maximum likelihood estimator, note that if all the words here must be generated from this model, then. And note that we also have another parameter Lambda here, but we assume that the Lambda denotes the noise in the feedback document. Are we just use the top ten documents and we use this mixture model so the query is airport security? What we do is we first retrieve 10 documents from the web database. Then there's no burden on explaining those common words in the feedback documents by the topic model. Assume that the top ten documents that are assumed to be random in there could be based on using fractions like a feedback based on pixels or implicit feedback. If we ignore the original query and this is generally not desirable, right? So this unless you are absolutely sure you have seen a lot of relevant documents and the query terms are not important. So in both cases you can see the highest probability words include very relevant words to the query, so airport security, for example
 For example, using links that we can leverage to improve scoring. In this case, we're going to crawl just some pages about a particular topic. Now the algorithm that we talked about, such as a vector space model are general algorithms. And these are techniques that can allow us to improve search results by leveraging extra information. That would crawl pages. So spam detection we have to prevent those spam pages from being ranked high. And in general we can classify two scenarios. If it is, then it means it's a high utility page and then that's it's more important to ensure such a page to be fresh. In this lecture we're going to talk about one of the most important applications of text retrieval: web search engines.This lecture is about the web search. Because it's unlikely that it will be changed frequently. And there can be applied to any search applications, so that's the advantage. For example, all pages about the automobiles. So these are also some interesting challenges that have to be solved. And there are also techniques to achieve robust ranking, and we're going to use a lot of signals to rank pages so that it's not easy to spam the search engine with a particular trick. Those are URLs that may not be linked to any page, but if you truncate the URL to a shorter path that you might be able to get some additional pages. So here are some general challenges. The second problem is that this low quality information and there are often spams. Web pages are linked with each other, so obviously the link information is something that we can also leverage. And this is typically going to start with a query and then you can use the query to get some results from a major search engine and then you can start with those results and gradually crawl more. The third component that is a retriever that would use inverted index to answer users query by talking to the users browser and then the search results will be given to the user and then the browser would show those results and to allow the user to interact with the web so we are gonna talk about each of these components. One is parallel indexing and searching and this is to address the issue of scalability. There are many additional heuristics. On the other hand, there are also some interesting opportunities that we can leverage to improve search results. So the two major factors that you have to consider are first will this page be updated frequently and do I have to crawl this page again if the page is a static page that hasn't been changed for months, you probably don't have to re-crawl it everyday. If they are, then you can probably find them by re-crawling the old page. So to summarize, web search is one of the most important applications of text retrieval, and there are some new challenges, particularly scalability, efficiency, quality information. And finally, you may be interested in the discover hidden urls. And the third line of techniques is link analysis. First we're going to talk about the crawler. Compared with another page that has never been fetched by any users for a year, then even though that page has been changed a lot, then it's probably not necessary to crawl that page, or at least it's not as not as urgent as to maintain the freshness of frequently accessed page by users. In general you don't have to re-crawl everything right? Or it's not necessary. And in general, in web search we're going to use multiple features for ranking, not just a link analysis, but also exploit in all kinds of clues, like the layout of web pages or anchor text that describes a link to another page. Second, there are techniques that are developed for addressing the problem of spams. In this case you need to optimize the resource. So because of these challenges and opportunities, there are new techniques that have been developed for web search or due to the need for web search. So in this case you your goal is to minimize the resource overhead by using minimum resources to just still crawl the updates to pages. The new pages are constantly created and some pages may be updated very quickly, so it makes it harder to keep the index fresh so these are some of the challenges that we have to solve in order to build a high quality web search engine. Also, parallel crawling is very natural because this task is very easy to parallelize
This lecture is about the web indexing. So this will then be sent to a Reduce function. Or search. So these output values will be then collected together to form the final output. And this will be collected together and this will be also fed into the reduce function. So how can we solve this problem? One natural thought is that. So all it needs to do is simply to concatenate them into a continuous chunk of data, and this can be then written into a file system. Each key is a word. So this is how we can do parallel index construction for web search. So once we see all these pairs then we can sort them based on the key which is the word. But there are new challenges that we have to solve for web scale indexing and the two main challenges. So it's all the words in that document, and so the map function will do something very similar to what we have seen in the word count example. In general, we can use the standard information retrieval techniques for creating the index, and that is what we talked about in the previous lecture. Of course, the second line will be handled by a different map function, which will produce a similar output. It simply groups all the counts of this word in this document together and it would then generate the set of key value pairs. So if we can do it in parallel it will be much faster and this is done by using the MapReduce framework. In this lecture we will continue talking about the web search and we're going to talk about how to create a web scale index. From this map function. We know this kind of counting would be useful to, for example, assess the popularity of a word in a large collection, and this is useful for achieving effect of IDF weiging. This is a general software framework for supporting parallel computation. An similarly we do that for other words like "Hadoop", "Hello" etc. So in this case we can assume the input to map function is a pair of a key, which denotes the document ID and the value denoting the stream for that document. And the counts. Which is a key and a set of values to produce another set of key values as the output. And the output that we want to generate is the number of occurrences of each word, so it's the word account. So the counts of Java in those documents. Basically we have got the counts, but what's missing is the document IDs and the specific frequency counts of words in those documents, so we can modify this slightly to actually build inverted index in parallel. And these counts represent the occurrences of this word in different lines. So once we crawled the web, we've got a lot of web pages. So now we've got a pair of a key and a set of values that are attached to this key. On that line so this key value pair will be sent to a map function. And then there will be for the sorted based on the key, and the result is that all the values that are associated with the same key would be then grouped together. And then finally output the key and the total count, and that's precisely what we want as the output of this whole program. Similarly, another document D2 can be processed in the same way, so in the end that again there was a sorting mechanism that would group them together and then we will have just a key like "Java" associated with all the documents that match this key or the documents where "Java" occurred. So we will collect all the counts of a word like a "Bye" here together. This task can be done in parallel by simply counting different parts of the file in parallel, and then in the end we just combine all the counts, and that's precisely the idea of what we can do with MapReduce. These chunks are replicated to ensure reliability, so this is something that the programmer doesn't have to worry about. Now of course, each Reduce function will handle a different key, so we will send these output values to multiple, reduce functions, each handling unique key. The application client would then talk to this GFS master and then obtain specific locations of the files that they want to process. So here's one way to do that. And a program with the specify these two functions to program on top of map reduce and you can see basically they are doing what I just described. Now, what exactly is in the value will depend on the data, and it's actually a fairly general framework to allow you to just partition the data into different parts, and each part can be then processed in parallel. All these details are hidden from the programmer. So this is the reduce function On the other hand, simply concatenates all the input that it has been given and then put them together as one single entry for this key. It's all taken care of by this fire system, so from the application perspective, the programmer would see this as if it's a normal file. Right, so this is the general framework of MapReduce. And these key value pairs are the output of the map function and all the outputs of all the map functions would be then collected. We can parallelize on lines in this input file. And the value is the count of this orld in this document, plus the document ID. (And) Another feature is that the data transfers directly between application and chunk servers, so it's efficient in this sense. Each word gets a count of one, and these are the output that you see here on this slide. Now you can easily see why we need to add document ID here. So now we have got a new pair of a key and a set of values and this pair will then be feeding to reduce function. As a result, the programmer can make a minimum effort to create a application that can be run on large cluster in parallel. So to summarize, web scale indexing requires some new techniques that go beyond the standard traditional indexing techniques, mainly will have to store the index on multiple machines, and this is usually done by using file system like a Google File System, a distributed file system. Here we see two functions
 So this is. So for such queries we might benefit from using link information.This lecture is about link analysis for web search. We want to see how we can improve ranking of pages. One is to provide extra text for matching and the other is to provide some additional scores for the web pages to characterize how likely a pages have, how likely a pages authority. Think about one page pointing to another page. The query would match this anchor text. And that's the main topic of this lecture. So. You also look at the what are those pages that are pointing to you. That means many other pages are pointing to this page and this shows that this page is quite useful. So of course, then if a page is cited often, then we can assume this page to be more useful, in general. In this lecture we're going to talk about web search. Or query likelihood to score different parts of documents or to provide additional features based on content matching, but link information is also very useful so they provide additional scoring. Many of them are based on the standard visual models such as BM25 that we talked about. One line is to exploit links to improve scoring. This means the link information can help in two ways. But, if those pages that are pointing to you are not being pointed to by other pages, they themselves don't have many in links then, well, you don't get that much credit. Right, so the description here actually is very similar to what the user will type in the query box when they are looking for such a page, and that's why it's very useful for ranking pages. So people then of course proposed ideas to leverage these in this link information. Now this description text is called anchor text. The main topic of this lecture is to look at the ranking algorithms for web search. So this kind of query is often called navigational queries. Of the document to improve scoring and finally information quality varies a lot, so that means we have to consider many factors to improve the ranking algorithm. Signals. Here and then, this actually provides evidence for matching the page that's being pointed to. This is very similar to one paper citing another paper. This would give us a more robust way to rank the pages, making it harder for any spammer to just manipulate the one signal to improve the ranking of a page. If you look at the bottom part of this picture you can also see there are some patterns of links, and these links might indicate the utility of a document. In the previous lecture we talked about how to create index now that we have got index. For example, people might search for a web page or entry page and this is different from the traditional library search where people are primarily interested in collecting literature information. Essentially we're trying to imagine there are many virtual links that will link all the pages together so that you actually get pseudo citations from everyone. And particularly focusing on how to do link analysis and use the results to improve search. So if you match the anchor text that describes a link to a page, actually that provides good evidence for the relevance of the page being pointed to, so anchor text is very useful. There are a lot of other clues such as the layout, title or link information. So as a result, people have made a number of major extensions to the ranking algorithms. The purpose is to navigate into a particular target page. On the left side you can see, this is another page that points to many other pages, so this is a directory page that would allow you to actually see a lot of other pages
 This one. And these are linear equations. So this is how we updated the vector. Instead it's going to jump to only those pages that are relevant to a query.5 for d3 and d4. So you can see this now if we assume alpha is . So this is assumed random surfing model. Basically, that's to say alpha would be 1. So. So. The sum of the scores to this document d1. That would be like this. For example, if the queries is about the sports, then we could assume that when it's doing random jumping it's going to randomly jump to a sports page. Let's say that's 1 /4 for each, and then we're going to use this matrix to update these scores. So there are two interpretations. And this probability is precisely what Pagerank computes. So mathematically, how can we compute this probability in order to see that we need to take a look at how this probability is computed. Randomly initialized vector p and then we repeatedly just updated this p by multiplying the matrix here by this p vector. So in this random surfing model. One extension is to do topic specific Pagerank. Can be captured by that there are virtual links between all the pages now. Basically, we're going to combine the scores of the pages that possibly would lead to reaching this page, so we'll look at all the pages that are pointing to this page and then combine their scores and propagate the score. For this and then we just revise the scores we generate a new set of scores and the updating formula is this one. What are these two possibilities? One is through random surfing and one is through following a link as we just explained. Now if you rewrite this matrix model multiplication in terms of just, individual equations, you will see this. There are N pages, so it's a sum over all the possible N pages. And this is Basically the updating formula for this particular pages Pagerank score so you can also see you if you want to compute the value of this updated score for d1 you basically multiply this rule. Right, by this column. Right? So this basically means we now have a system of N equations with N variables. And then the random surfer could decide to just randomly jump into any page. Mainly because we have lost some probability mass when we assume there's some probability that the surfer will try to follow links, but then there's no link to follow. And at that point that we will just have the Pagerank scores for all the pages. That's p sub t of d,i. One is pointing to d3, the other is pointing to d4, so the random surfer could pick any of these two to reach d3 and d4. You can imagine if you compute the Pagerank scores for social network where a link might indicate friendship relation, you'll get some meaningful scores for people. Then With some probability that random surfer will follow the links. By doing this, then we can bias and Pagerank to topic like sports and then if you know the current query is about sports and then you can use this specialized Pagerank score to rank documents that would be better than if you use a generic Pagerank score. Inside the sum is a product of two probabilities. And multiply by the vector again. Now, how can we compute the probability of a surfer visiting a page? If you look at the surf model then basically we can compute the probability of reaching a page as follows. And one possible solution is simply to use a page specific damping factor and that could easily fix this. Now, if you still remember some knowledge that you've learned from linear algebra and then you will realize this is precisely the equation for item vector, right when you multiply the matrix by this vector, you get the same value as this vector. And because they are of the same form, we can imagine there's a different matrix that's a combination of this M and that uniform matrix, where every element is 1 / N, and in this sense Pagerank uses this idea of smoothing and ensuring that there's no zero entry in such a transition matrix. So there are many extensions of page rank. And this is why. The Pagerank scores of different pages and in this iterative approach or power approach, we simply start with. Now this would give us N equations because for each page we have such equation and if you look at the what variables will have in these equations there are also precisely N variables. So we can make Pagerank query specific, however, so for example in the topic specific Pagerank we can simply assume when the surfer is bored the surfer is not going to randomly jump to any page on the web. For example, row one would indicate the probability of going to any other 4 pages from d1, and here we see there are only two non zero entries, each is 1 over 2. And this can be solved by using iterative algorithm. The random surfer will go from one page to another, so each row stands for a starting page. Can you see why? If you look at this formula and then compare that with this graph? And can you imagine how we might be able to interpret this as essentially propagating scores over the graph? I hope you will see that indeed we can imagine we have values initialized on each of these pages, so we can have values here. So if it does that, it would be able to reach any of the other pages, even though there's no link directly from d1 to that page. In general, the element in this matrix M sub i, j is the probability of going from d,i to d,j and obviously for each row the values should sum to 1 because the surfer would have to go to precisely one of these other pages, right? So this is the transition matrix. So the probability is the probability of being at d,i at time t multiplied by the probability of going from that page to the target page. So this is a nice way to capture both indirect and direct links. In that case, if the page does not have any out link then the probability of these pages would not sum to one basically the probability of reaching the next page from this page will not sum to one. Now we can imagine if we want to compute the average probabilities, the average probabilities probably would satisfy this equation without considering the time index. Right? That includes the graph, the actual links, and we have this smoothing transition matrix uniform transition matrix representing random jumping and we can combine them together with a linear interpolation to form another matrix. So we look at the scores that represent the probability that the random surfer will be visiting the other pages before it reached d1, and then just do the propagation to simulate the probability of reaching this page
So we talked about page rank as a way to. Intuitions are pages that are wider. Both can generate scores for web pages that can be used in the ranking function. And similarly, we can do a transformation to have equation for just the authority scores. So to summarize, in this lecture we have seen that link information is very useful. The text representation of a page and we also talk about page rank and hits on as two major link analysis algorithms. And if we do that and then we'll basically get hits algorithm to compute the hub scores an authority scores for all the pages. But mathematically, then we will be computing the same problem. So what we have to do, what we have to do is after each iteration, we're going to normalize and this would allow us to control the growth of value, otherwise they would grow larger and larger. And these scores can then be used in ranking just like a page rank scores. And then we're going to define the Hub score of page as the sum of the authority scores of all the pages that it points to. So whether you are good authoritie would depend on whether those pages that are pointing to you are good hubs so you can see this forms iterative reinforcement mechanism. Would be also improved because they are pointed to by a good hub and this algorithm is also general. And this is basically the first equation right? And similarly the second equation can be returned as the authoritie vector is equal to the product of A transpose multiplied by the hub vector and these are just different ways of expressing these equations. So if there's a link, there is one. So whether you are a hub really depends on whether you're pointing to a lot of good authority pages. So in hits we typically would initialize the values that said, one for all these values and then we would iteratively apply these equations, essentially and This is equivalent to multiply that by the Matrix A and A transpose. Now these two equations can be also written in the matrix format. So if you do that, you can actually then eliminate the authoritie vector completely and you get the equation of only hub scores, right? The Hub score vector is equal to A * A transpose multiplied by the hub score vector again. We first also construct the matrix, but this time we're going to construct the adjacency matrix and we're not going to normalize the values
This lecture is about learning to rank. So these can all be clues about whether this document is relevant or not. So we will assume that the given a query document appear Q and D. Those are the data that have been judged by users so that we already know the relevance judgments we already know which documents should be ranked high for which queries. And this information can be based on real judgments by users, or this can also be approximated by just using click through information where we can assume the clicked documents are better than the skiped documents or clicked documents are relevant and skiped documents are non relevant. They can generate the content based scores for matching documents with a query and we also talked about the link based approaches like page rank that can give additional scores to help us improve ranking. The question is of course, how can we combine them? In this approach we simply hypothesize that the probability that this document is relevant to this query is function of all these features. And these features can vary from content based features such as a score of the document with respect to the query according to a retrieval function such as BM25, or query likelihood or pivot length normalization PL2 etc. Naturally, the next question is how do we estimate those parameters and how do we know which features should have a higher weight than which features should have lower weight, so this is the task of training or learning, right? So in this approach, what we will do is to use some training data. We have talked about some retrieval models, like BM25 or query like code. So all these features can then be combined together to generate the ranking function. But by hypothesising that relevance is related to these features in a particular way, we can then combine these features to generate the potentially more powerful ranking function and more robust ranking function. So the question that we address in this lecture is how we can combine many features to generate a single ranking function to optimize search results. In the previous lectures we have talked about a number of ways to rank documents. It can be also application of retrieval models to the anchor text of the page, those are the text descriptions of links that point to this page. Now the question now is how can we combine all these features and potential many other features to do ranking and this will be very useful for ranking web pages
 Both this and this. So that's what we will get. So that means we're going to compute this. OK, we would want to maximize this probability, since this is a relevant document. Make it as large as possible. So to compute the non relevance from relevance we just. So this is the probability of relevance. The beta values are still unknown, but this gives us the probability that this document is relevant if we assume such a model. For that pair and then we just use this formula to generate the ranking score and this scoring function can be used to rank documents for a particular query. So in this approach, we simply assume that the relevance of document with respect to query is related to a linear combination of all the features. And this is precisely what we want. This is similar to expressing the probability of document. Why? Because It's a non relevant document. So. So. One is BM25 score of the document and query one is the page rank score of the document, which might or might not depend on the query. Since this expression. What we do for the second document? Well, we want to compute the probability that the prediction is non relevant. Would be completely specified, so for any new query and new documents, we can simply compute the features. So now we have this combination function. We might have a topic sensitive Pagerank that would depend on query. And we hypothesize that the probability of relevance is related to features in this way, so we're going to see for what values of beta. This expression. We can predict the relevance well. So what we can do is we use the maximum likelihood estimator to actually estimate the parameters. This value is large, then it would mean this value is small and therefore this probability this whole probability would be large and that's what we expect. Can we predict the relevance? Yeah, and of course the prediction will be using this function. These are then the feature values for a particular Doc document query pair. And we assume that these features can be combined in a linear manner. Also, as large as possible, which is equivalent to, say, make this the part as small as possible. Only that we're not talking about the probability of words, but talking about probability of relevance one or zero. So what's the probability of this document? The relevant if it has these feature values. And we can have as many features as we would like. So then this function would be well defined once beta values are known. Basically, we are going to predict the relevance status of the document that based on the feature values that is given that we observe these feature values here. So once we do the training now we will know the beta values. The next task is to see how we to estimate the parameters so that the function can actually be applied without knowing the beta values, it's harder to apply this function, so let's see how we can estimate beta values. And what that means is if look at the function is we are going to choose betas to make this as large as possible and make this. This would mean we have to compute 1 minus. So that's the basic idea of learning to rank. What we have seen above, only that we replaced these Xi(s) with now specific values, right? So for example, this . But in this case it's not relevant. In this example we have 3 features. This allows us then to connect the probability of relevance which is between zero and one to a linear combination of arbitrary features
 To summarize this lecture we have talked about the using machine learning to combine multiple features to improve ranking results. Now these learning to rank approaches are actually general, so they can also be applied to many other ranking problems, not just the retrieval problem. In contrast, we might have another case where we predicted values all around . Note that the optimization objective function that we have seen on the previous slide is not directly related to retrieval measure By maximizing the prediction of one or zero, we don't necessarily optimize the ranking of those documents. But the ranking can be wrong, so we might have got a larger value for D2 and than D1.There are many more advanced learning algorithms, then the regression based approaches and they generally attempt to direct the optimizer retrieval measure. Modern search engines are all used, some kind of machine learning techniques combined with many features to optimize ranking and this is a major feature of these commercial engines such as Google or Bing. like MAP or nDCG. Such data won't available before, so the data can provide a lot of useful knowledge about the relevance and machine learning methods can be applied to leverage this.5, so it's kinda of in the middle of zero and one for the two documents. Here are some additional readings that can give you more information about how learning to rank works and also some advanced methods
 So this is another feature that we would see. Users and mostly model users based on keyword queries. These vertical search engines can be expected to be more effective than the current general search engines because they could assume that the users are a special group of users that might have a common information need, and then the search engine can be customized to serve such users. So this is pushing for personalization and complete the user modeling and this is a major direction in research. We have to find the patterns or converted the text information into your knowledge that can be used in application or actionable knowledge that can be used for decision making. Whereas if it's a complaint that you might be able to. We can also see we can go beyond bag of words representation to have entity relation representation. You might be might be able to provide a support for automatically generating related work section for research paper, and this would be closer to task support, right? So then we can imagine this would be a literature assistant if we connect to online shoppers with blog articles or product reviews, then we can help these people to improve shopping experience so we can provide for example data mining capabilities to analyze the reviews to Compare products, compare sentiment of products and to provide a task support or decision support to help them choose what product to buy. Another trend is that we might see systems that try to go beyond, search it supports the user tasks. So this calls for large scale semantic analysis, and perhaps this is more feasible for vertical search engines. Because we have a better understanding of the users. For example, consumers might search for opinions about products in order to purchase a product and choose a good product to buy. So for example if you can connect everyone with web pages. All these are trying to help people to improve the productivity. Once the information is found and this step has to do with analysis of information or data mining. In general, you can think about any intelligent system, especially intended information system, as being specified by these three nodes, and so if we connect these three into a triangle, then we'll be able to specify information system, and I call this data user service triangle. Another trend that we can expect to see is The search engine will be able to learn over time. So search navigation and recommendation or filtering might be combined to form a full fledged information management system. Some of them might be already relevant to what you are currently working on. For example, particular words may not be ambiguous in such a domain, so we can bypass the problem of ambiguity. These are different modes of information access, but these modes can be combined. Now imagine we can connect all these in different ways. Look at the user search history and then further model the user completely to understand the users task, environment, task need context or other information. And this is of course very attractive, because that means the search engine will self improve itself as more people are using it. For example for researchers, you might want to find the relevant literature or site of the literature, and then there's no support for finishing tasks such as writing a paper. And that would be convenient for people. And furthermore the knowledge would be used to help user to improve productivity in finishing a task. And similarly, in the pull mode, querying and browsing could also be combined then in fact we're doing that basically today with the current search engines we are querying. Search engines will become better and better and this is already happening because search engines can learn from the implicit feedback, more users use it and the quality of the search results for the popular queries that are typed in by many users will likely become better. In this lecture we're going to talk about some possible future trends of web search and intelligent information retrieval systems in general. If you connect the scientists with literature information to provide all kinds of service, including search, browsing or alert of new relevant documents, or mining, analyzing research trends, or provide the task support or decision support, for example. And we can imagine a system that can provide analysis of these emails to find the major complaints of the customers.This lecture is about the future of web search. So this is a trend and so basically in this dimension we anticipate in the futur. In order to further improve the accuracy of search engines, it's important to consider special cases of information need, so one particular trend could be to have more and more specialized and customized search engines and they can be called vertical search engines. So in this case it would be beneficial to support the whole workflow of purchasing a product or choosing a product. In the future, we would like to have knowledge representation where we can add perhaps inference rules and then the search engine will become more intelligent. What do you get? That's web search, right? What if we connect the UIUC employees with organization documents or enterprise documents to support the search and browsing. Now I should also emphasize interactive here because it's important to optimize the combined intelligence of the users and the system so we can get some help from users in some natural way and we don't have to assume the system has to do everything when the human user and the machine can collaborate in an intelligent way in an efficient way, then the combined intelligence will be high and in general we can minimize the user's overall effort in solving problem. So basically the three questions you ask would be who are you serving and what kind of data you are managing. For example, a decision making task
 Then we are talking about the content based filtering or content based recommendation. A search engine for information filtering. or like most of those users. Here we're going to talk about how to extend retrieval system.This lecture is about the recommender systems. So how can we build such a system? And there are many different approaches. Frst it has to make a filtering decision so it has to be a binary decision maker binary classifier given a text. Right, so you can see the document vector could be fed into a scoring module which is already exists in a search engine that implements a vector space model and the profile will be treated as a query essentially and then the profile vector can be matched with the document vector to generate the score. It has to say yes or no, whether this document should be delivered or not. We can measure the similarity between profile text, description and document, and then we can use the score threshold for the filtering decision. And this will be to feed the system with the initial user profile. And another component that we have to add is for is of course to learn from the history and here we can use the traditional feedback techniques to learn to improve scoring. And then the accepted document will be those that have passed the threshold according to the classifier. So here's the basic idea for extending a retrieval system for information filtering. If it says yes and then the documents will be sent to the user and then the user could give some feedback and the feedback information would have been ,would be used to both adjust to the threshold and to adjust the vector representation so the vector learning is essentially the same as query modification or feedback. We do retrieval, and then we kind of find the scores of documents and then we apply a threshold to see whether document is passing the threshold or not, and if it's passing the threshold we are going to say it's relevant, and we're going to deliver it to the user. So in other words, we're trying to decide the absolute relevance. This is probably because of we know that web search engines are by far the most important applications of text retrieval and they are the most useful tools to help people convert big raw text data into a small set of relevant documents. An we mentioned that recommender systems are the main systems to serve users in the push mode where the systems would take initiative to recommend the information to user or push a relevant information to the user. So this utility function has to be designed based on the specific application. First we can reuse a lot of retrieval techniques to do scoring, right, so we know how to score documents against queries, etc. Now what's the criteria for evaluating such a system? How do we know this filtering system actually performs well? Now, in this case we cannot use the ranking evaluation measures like a map because we can afford waiting for a lot of documents and then rank the documents to make a decision for the user. And the system Clock collect a lot of information about these users interest and this can then be used to improve the classifier. And but we have to develop a new approaches to learn how to set the threshold and we need to set it initially and then we have to learn how to update the threshold overtime. And so in this sense they are kind of similar. Text document and profile description of the user. It has to be absolutely sure that it's not a non-relevant one. When the system has a good knowledge about what the user wants. And we know Rocchio can be used for scoring improvement. And then this score would be fed into a threshold module that would say yes or no, and then the evaluation would be based on utility for the filtering results. So in other words, we could kind of just. Inside the system there will be a binary classifier that would have some knowledge above the users‘ interest. So in this case, one commonly used strategies is user utility function to evaluate the system. So that's the decision module and there should be a initialization module. So here's what the system might look like if we just generalize the vector space model for filtering problems. One interesting question here is how should we set these coefficients? Now I just showed 3 and negative 2 as a possible coefficients. What's the difference? What do you think? How would this utility function affect the system's threshold decision? But you can think of these two extreme cases, 10&amp;nbsp; -1 versus 1 -10. There should be also an initialization module that would take a users input, maybe from a users specified keywords or chosen category etc. This is what the system would look like. Because we count in learn from the user about their preferences on the delivered documents if we don't deliver document to the user, we would never know would never be able to know whether the user likes it or not. You can imagine that the system would be very reluctant to deliver a lot of documents. So this is a slide that you have seen before when we talked about the two different modes of text access - pull and push. Will you like item X? And there are two ways to answer this question. Note that in this case typical users information need is stable, so the system would have a lot of opportunities to observe the users, if the user has taken a recommended item has viewed that, and this is the signal to indicate that the recommended item may be relevant if the user discarded it, it's not relevant, and so such feedback can be a long term feedback and can last for a long time. So far we have talked about a lot of aspects of search engines
5 and it's relevant. So to summarize, there are two strategies for recommender systems or filtering systems. So it's not as high as the optimal utility. So we would explore. So in other words, we're going to explore the space of user interests by testing whether the user might be interested in some other documents that currently are not matching the users' interests so well. So here I show a ranked list of all the training documents that we have seen so far, and they are ranked by their positions. In content-based filtering system, We generally have to solve several problems related to filtering decision and learning etc. In this lecture, we've covered the content based filtering approach in the next lecture we're going to talk about collaborative filtering. And so this can be just adjustment factor. What if I can cut at a different scoring threshold point what would happen, what's utility? Since these are training data, we can kind of compute the utility, right? We know their relevance status or we assume that we know relevant status that's based on approximation of clickthroughs. Now this means we also want to explore the document space a little bit and to see if the user might be interested in documents that we haven't delivered. So this is a dilemma. So that's the basic idea of this approach. But this of course doesn't account for exploration that we just talked about. So how do we do that? Well, we could lower the threshold a little bit and do just deliver some near misses to the user to see what the user would respond, to see how the user would respond to this extra document. Of course we have a lot of documents for which we don't know the status because we have never delivered them to the user. You might miss opportunity to learn another interest of the user. So how do we solve this problem where we generate and as I said, we can lower the threshold to explore a little bit, so here's one particular approach called better gamma threshold learning. Now the question is how should we set alpha? And when should we deviate more from the optimal utility point? Well this can depend on multiple factors and one way to solve the problem is to encourage this threshold mechanism to explore up to the zero point, and that's a safe point, but we're not going to necessarily reach all the way to the zero point, but rather we're going to use other parameters to further define alpha, and this specifically is as follows. So there will be a beta parameter to control the deviation from the optimal threshold, and this can be based on for example can be accounting for the overfitting to the training data let's say. So exploitation means you would exploit what you learned about user. Suppose I cut at this position and that would be the utility. One is content based which is looking at the item similarity. So as you can see here, we only see the judgments of documents delivered to the user, so this is not a random sample, so it's censored data. So it's possible that the discarded item might be actually interesting to the user. that depending on the cut off position, we will have a utility that means. As I just explained, it's desirable to explore the interest space, so it's desirable to lower the threshold based on your training data. The other is collaborative filtering, which is looking at the user similarity. So that means in general we want to set the threshold somewhere in this range. Now how do we solve these problems? In general, I think one can use the empirical utility optimization strategy, and this strategy is basically to optimize the threshold based on historical data, just as you have seen on the previous slide. The optimal point theta optimal is the point when we would achieve the maximum utility if we had chosen this threshold. Of course this function depends on how you specify the coefficients in the utility function, but we can then imagine. So in general we can only get upper bound for the true optimal threshold, because the threshold might be actually lower than this. And such a system can actually be built based on a search engine system by adding a threshold mechanism, and adding adaptive learning algorithm to allow the system to learn from long-term feedback from the user. So this gives us a dynamic strategy for exploration, right? The more examples we have seen, the less explosion we're going to do, so the threshold would be closer to the optimal threshold. So then we can just choose the threshold that gives the maximum utility on the training data
 That's content-based filtering. It can be applied to any items. And then we're going to predict the user preferences based on the preferences of these similar users. So this means this approach is very general. In this lecture, we will look at the user similarity.This lecture is about the collaborative filtering. So another assumption we have to make is that there are sufficiently large number of user preferences available to us. Alright, so those who are interested in information retrieval research probably all favor SIGIR papers. And that is, we say, we will infer individual's interest or preferences from that of other similar users. So for example. Users with the same interest where have similar preferences. We can also assume that if we see people favor SIGIR papers, then we can infer their interest is probably information retrieval. Given a user U, we're going to 1st find the similar users u_1 through u_m. So what you see here is that we have shown some ratings available for some combinations, so some users have watched some movies they have rated those movies. So this is in general a sparse matrix. So for example, if you see a lot of ratings of users for movies, and those indicate their preferences for movies, and if you have a lot of such data, then collaborative filtering can be very effective. So u_1 through u_m and we also considering a number of objects, let's say N objects denoted as o_1 through o_n and then we will assume that the users will be able to judge those objects and the user could for example give ratings for those items, for example, those items could be movies. And we have observed some values of this function. Now the user similarity here can be judged based on their similarity in preferences on a common set of items. In this lecture, we're going to continue the discussion of recommender systems. So first, what is collaborative filtering? It is to make filtering decisions for individual user based on the judgments of other users. You have seen this slide before when we talked about the two strategies to answer the basic question will user U like item X. If the interest of the user is in information retrieval, then we can infer the user probably favor SIGIR papers. That's the assumption that we make and if this assumption is true, Then it would help collaborative filtering to work well. And how can we figure out the function based on the observed ratings? So this is the setup
 They can be text documents. And then these ratings are weighted by their similarity. So this. So this is. Once we make this prediction. We could also combine that with content based filtering. They can be movies they can book so they can be products. This would recover a meaningful rating for this user. So the user u_a is the user that we're interested in recommending items to and We now are interested in recommending this o_j, so we're interested in knowing how likely this user will like this objec. So this basically measures whether the two users tend to all give higher ratings to similar items, or lower ratings to similar items. Because some users might be more generous and they generally given high ratings. And of course, naturally, this way that should be related to the similarity between u_a and this particular user u_i. Because it's a little similar to storing all the user information and when we are considering a particular user, we're going to try to retrieve the relevant users or the similar users to this user case and then try to use that user information about those users to predict the preference of this user. Now this is it will normalize these ratings so that the ratings from different users would be comparable. So we need to do this normalization. So. Another measure is the cosine measure, and this is to treat the rating vectors as vectors in the vector space. Basically the normalized rating that's more meaningful, but when they evaluate these collaborative filtering approaches. Using the ratings of similar users to this active user, this is called a memory based approach. So you can imagine W of a an I is just a similarity of user A and user I. So if this user is generous than the average would be somewhat high and when we add that the rating will be adjust to a relatively high rating. But not all users contribute equally to the average, and this is controlled by the weights. Can be based on the average ratings of similar users. This n_i is needed because we would like to normalize the ratings of objects by this user. And So this allows such approach to be applied to a wide range of problems. We could use more context information and those are all interesting approaches that people are still studying. Item to a user. So there are some obvious ways to also improve this approach. And n_i is the average rating of all objects by this user. And so this means basically, if you consider the weight here together with K and we have coefficients or weights that would sum to one for all the users. So indeed there are many different ways to compute this function or this weight, w and specific approaches generally differ in how this is computed. Now when you recommend. It didn't matter what these items are. But some others might be more critical, so their ratings cannot be directly compared with each other or aggregate them together. And inside the sum, We have their ratings, well, their normalized ratings as I just explained, the ratings need to be normalized in order to be comparable with each other. Clearly we know more about the user, not just these preferences on these items. Just like any other heuristics to improve these similarity functions, another idea which is actually very similar to the idea of IDF that we have seen in Text research is called inverse user frequency. And mainly we would like to improve the user similarity measure and there are some practical issues to deal with here as well.And here we are going to talk about basic strategy and that would be based on similarity of users and then predicting the rating of. Right, so this is basically the main idea of memory based approaches for collaborative filtering
 So in future we could anticipate the such a system to be more useful to user. Now the context here could be the context of the user and that it could be also context of documents or items. You can use for example, more user information to assess their similarity instead of using the preferences of these users on these items, there may be additional information available about the user. And we also talked about the two strategies for filtering task one is content based, where we look at item similarity. Obviously the tool should be combined and they can be combined to have a system that can support the user with multiple mode information access. The other is collaborative filtering where we look at the user similarity and they obviously can be combined in a practical system. You can imagine the general would have to be combined so that will give us a hybrid strategy for filtering. And then you're going to see. The thing about the news filtering as soon as you see the news and you have to decide whether the news would be interesting to a user. You have to make a decision when you see this item. Articles that can give you an overview of a number of specific approaches to recommended systems. In particular, those new algorithms tend to use a lot of context information. And we also could recall that we talked about push versus pull as two strategies for getting access to the text data and recommended system is to help users in the push mode and search engines are certain users in the pull mode. However, filtering is actually much harder task than retrieval because it has you have to make a binary decision and you can't afford waiting for a lot of items
 And we then talked about how to evaluate the text retrieval system. We then talked about learning to rank. And then we talked about how to implement a retrieval system. So the text mining course, or rather text mining analytics course will be dealing with what to do once the user has found the information. This map shows the major topics we have covered in this course.This lecture is a summary of this course. One is content based, one is collaborative filtering and they can be combined together. This is a very important evaluation methodology that can be applied to many tasks. We talked about the major evaluation measures, so the most important measures for search engine--map(mean average precision),  nDCG(normalized, discounted cumulative gain), an also precision and recall are the two basic measures. And this is a topic that you can learn more by reading this book. We only managed to cover some basic topics in text retrieval and search engines. We started with the overview of vector space model and the probabilistic model, and then we talked about the vector Space Model in depth. So this is the second step in this picture where we would convert the text data into actionable knowledge. In indexing we introduce the map reduce and then we talked about how to use linking information on the web to improve search. We then talked about Web search, and here we talked about how to use parallel indexing to solve the scalability issue. So if you have not taken the text mining course in this data mining specialization series, then naturally the next step is to take that course as this picture shows to mine big text data we generally need two kinds of techniques, one is text retrieval, which is covered in this course and these techniques would help us convert the raw big text data into small relevant text data which are actually needed in the specific application. And we talked about a number of retrieval methods. And we then talked about the feedback techniques and we talked about the Rocchio in the vector space model and the mixture model in the language modeling approach. And then we talked about a number of issues related to search engines we talked about the search problem, and we framed that as a ranking problem. And this has to do with helping users to further digest the found information or to find the patterns and to reveal knowledge buried in text and such knowledge can then be used in application system to help decision making or to help user finish a task. First we talked about the natural language content analysis. And it's often sufficient for the most of the search tasks, but obviously for more complex such tasks than we need a deeper natural language processing techniques. And in this course we have covered various strategies to help users get access to the most relevant data. And we talked about how to perform search by using the inverted index. We talked about page rank and HITS as the major algorithms to analyze links on the web. And then finally we talked about the recommender systems and these are systems to implement the push mode and we talked about the two approaches. These techniques are also essential in any text mining system to help provide provenance, stand to help users interpret in the patterns that user would find through text data mining. and we also later talked about the language modeling approaches, and that's a probabilistic model and here The main takeaway messages is that modern retrieval functions tend to look similar, and they generally use various heuristics
