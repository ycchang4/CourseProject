 Text mining is also related to text retrieval, which is an essential component in any text mining systems. So I've taught another separate MOOC on text retrieval and search engines, where we discussed various techniques for text retrieval. Both text mining and text analytics mean that we want to turn text data into high quality information or actionable knowledge. So this is a high level introduction to the concept of text mining and the relation between text mining and retrieval. Now it's interesting to view text data as data generated by humans as subjective sensors. For example, we might be able to determine which product is more appealing to us all, a better choice for a shopping decision. So this course will cover specialized algorithms that are particularly useful for mining text data.In this lecture we give an overview of text mining and analytics. Text retrieval is very useful for text mining in two ways: First, text retrieval can be a pre-processor for text mining, meaning that it can help us turn big text data into a relatively small amount of most relevant text data, which is often what's needed for solving a particular problem. Text retrieval is also needed for knowledge provenance and this roughly corresponds to the interpretation of text mining as turning text data into actionable knowledge. If you have taken that MOOC, you will find some overlap and it would be useful to know the background of text retrieval for understanding some of the topics in text mining. So in this case, text mining supplies knowledge for optimal decision making. But if you have not taken that MOOC it's also fine, because in this more context mining and analytics we're going to repeat some of the key concepts that are relevant for text mining. And here we distinguish two different results one is high quality information, the other is actionable knowledge. Here we emphasize the utility of the information or knowledge we discover from text data. In the case of high quality information we refer to more concise information about the topic, which might be much easier for humans to digest than the raw text data. But text data is also very important, mostly because they contain a lot of semantic content and they often contain knowledge about the users, especially preferences and opinions of users. So this slide shows an analogy between text data and non text data and between humans as subjective sensors and physical sensors such as network sensor or thermometer. And in general would be dealing with both non text data and text data and of course the non text data are usually produced by physical sensors. It's actionable knowledge for some decision problem, or some actions to take. So this means that data mining problem is basically taking a lot of data as input and giving actionable knowledge as output. First, let's define the term text mining and the term text analytics. Now text retrieval refers to finding relevant information from a large amount of text data. The title of this course is called Text Mining and Analytics, but the two terms text mining and text analytics are actually roughly the same. So here we are looking at the general problem of data mining, and in general we would be dealing with a lot of data about our world that are related to a problem. So in both cases we have the problem of dealing with a lot of text data and we hope to turn these text data into something more useful to us than the raw text data. Once we find the patterns in text data or actionable knowledge, we generally would have to verify the knowledge by looking at the original text data so the users would have to have some text retrieval support to go back to the original text data to interpret the pattern, or to better understand the knowledge or to verify whether the pattern is really reliable. So we are not going to really distinguish them, and we're going to use them interchangeably
 And then we're going to talk about opinion mining and sentiment analysis. So this can be regarded as one example of mining knowledge about the observer. In general, non-text data can be very important for such prediction tasks. Now non-text data can be also useful for analyzing text by supplying context. And that human. And for this reason, we might be concerned with joint mining of text and non text data and so in this course we're going to focus more on text mining. We actually hope to cover most of these general topics. As so, this has much to do with mining the content of text data. Second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge about a language. First, we are going to cover natural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining. Or extracting high quality information about a particular aspect of the world that we're interested in. And so specifically we can think about the mining, for example, knowledge about the language. In fact, when we have a non-text data, we could also use the non-text data to help prediction. So a more general picture would be to include non text data as well. We could analyze the sentiment in different context, so this is fairly general landscape of the topics in text mining and analytics. In this course we're going to selectively cover some of those topics. If you look further then you can also imagine we can mine knowledge about this observer himself or herself. Third, we're going to talk about the topic mining and analysis, and this is only one way to analyze content of text, but it's a very useful way of analyzing content. This can often be biased also. So this picture basically covered multiple types of knowledge that we can mine from text in general. So this is 1 type of mining problems where the result is some knowledge about language which may be useful in various ways. And we're going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in (the) real world. But we can also touch how to join the analysis of both text data and non-text data. And will use this as outline for the topics that will cover in the rest of this course. But text data generally have also context associated. Similarly, we can partition text data based on locations or any metadata that's associated to form interesting comparison scenarios. And we hope to be able to uncover some aspect in this process. It's also one of the most useful techniques in text mining. So this slide also serves as a road map for this course. So this has also to do with using text data to infer some properties of this person. For example, the time, the location, of that associated with the text data and these are useful context information. For example, after we mine the content of text data, we might generate some summary of content, and that summary could be then used to help us predict the variables of the real world. When we infer other real world variables, we could also use some of the results from mining text data as intermediate results to help the prediction. In that case, we might have text data of mixed languages for different languages. Now in this case, obviously the historical stock price data would be very important for this prediction, and so that's example of non-text data that would be very useful for the prediction and we can combine both kinds of data to make the prediction. And finally, we are going to cover a text based prediction problems where we try to predict some real world variable based on text data
 Sentiment analysis is another aspect of semantic analysis that we can do. And this .This lecture is about the naturalÂ language content analysis. So connecting these phrases with what we know is understanding. That means we can tag the sentences general positive when it's talking about product. When we read such a sentence, we don't have to think about it to get meaning of it. In particular tagging these words with these syntactic categories is called a part of speech tagging. So we are going to first talk about this. Because we assume all the. with a factor how we can represent text data? And this determines what algorithms can be used to analyze and mine text data. So one is. And finally, we might even further to infer. Or syntactical analysis or parsing of natural language sentence. For computer, would have to formally represent these entities by using symbols. Natural language content analysis is the foundation of text mining. As well talking about this topic so it's processing of natural language. First, the computer needs to know what are the words, how to segment the words. The ordinary meaning that we will be getting. For semantic analysis, we can also do some aspects of semantic analysis, particularly extraction of entities and relations. might further infer what This sentence is requesting or why the person who said the sentence is saying this sentence. Now this is a very simple sentence. As a result, for example, we omit a lot of common sense knowledge. So for example, natural language processing can actually be interpreted in two ways. And so this has to do with understanding the purpose of saying this sentence, and this is called SPEECH Act analysis or pragmatic analysis. Think about the word level ambiguity a word like design can be a noun or a verb, so we've got ambiguous part of speech tag. Once we reach that level of understanding, we might also make inferences. all of us have this knowledge, there's no need to encode this knowledge. That tells us a structure of the sentence so that we know how we can interpret the sentence. For example, if we assume there's a rule that says if someone is being chased than a person can get scared, then we can infer this boy might be scared. it can also do a word sense disambiguation for some extent. This is the inferred meaning based on our additional knowledge. And there are certain way for them to be connected together in order to generate the meaning. And we also represented the chasing action as a predicate. And this is called a lexical analysis. And this is again because we assume that we have the ability to disambiguate a word, so there's no problem with having the same word to mean, possibly different things in different context. Statistical analysis methods to try to get as much meaning out from the text as possible. Parsing would be more difficult, but for partial parsing, meaning to get that some phrases correct, we can probably achieve 90% or better accuracy. So for example Dog is a noun, chasing is the verb, boy is another noun, etc. Therefore, we have to use whatever we have today particular statistical machine learning methods, or. So A and the dog will form a noun phrase. Now we don't generally have this problem, but imagine for once a computer to determine the structure, the computer would actually have to make a choice between the two. It doesn't mean on any data set to the accuracy will be precisely 97%. I'm going to explain these concepts using a simple example that you are seeing here. But there is also another possible interpretation, which is to say language processing is natural. But in general we can do part of speech tagging fairly well, although not perfectly. Ambiguity is a main killer, meaning that in every step there are multiple choices and the computer would have to decide what's the right choice and that decision can be very difficult, as you will see also in a moment. This is called syntactic parsing. Or it can be the root of a plant. So in order to get the meeting would have to map these phrases and these structures into some real world entities that we have in our mind
 For this reason, in this course, the techniques that we cover are, in general, shallow techniques for analyzing text data, to mine text data. Not feasible for large scale text mining. And they are generally based on statistical analysis, so they are robust, and general, and, And they are in the category of shallow analysis, so such techniques have the advantage of being able to be applied to any text data in any natural language about any topic. For that we have to rely on deeper natural language analysis techniques. And we have humans to help as needed in various ways. And is the main topic of this course and they are generally applicable to a lot of applications. So obviously the better we can understand the text data, the better we can do text mining. So they are practically more useful than some of the deeper analysis techniques that require a lot of human effort to annotate text data. So in practical applications we generally combine the two kinds of techniques with the general statistical and methods as backbone as the basis, since it can be applied to any text data and on top of that we're going to use humans to annotate more data and to use supervised machine learning to do some tasks as well as we can, especially for those important tasks. But this course will cover the general statistical approaches that generally don't require much human effort. So to summarize this lecture, the main points to take away are, first NLP is the foundation for text mining. Robust and general NLP tends to be shallow, while deep understanding does not scale up. They are in some sense also more useful techniques. So to bring humans in, into the loop to analyze, fix, to analyze text data more precisely
This lecture is about the text representation. In this lecture we're going to discuss text representation. And these words can be used to form topics. When we combine related words together and some words are positive, some words are negative, so we can also do sentiment analysis. So in this case it might be a request. By identifying words we can, for example, easily count what are the most frequent words in this document or in the whole collection, etc. And they are less accurate. I should also say that text data are generated by humans and are meant to be consumed by humans, so as a result in a text data analysis text mining, humans play a very important role. We can perhaps count how. Meaning that we should optimize the collaboration of humans and computers. We don't necessarily replace the original word sequence representation Instead, we add this as an additional way of representing text data, so that now the data is represented as both a sequence of words, and a sequence of part of speech tags. Now, this is desirable because as we can represent text at the level of knowledge, we can easily extract the knowledge. But unfortunately, using such a representation would not help us do semantic analysis, which is often needed for many applications of text mining. That's the purpose of text mining. Yet this is the most general way of representing text, because we can use this to represent any natural language text. And unfortunately, such techniques would require more human effort. Author of this sentence, what's the intention of saying that? What scenarios, what kind of actions will be made? So this is. So that also means it's still necessary to combine such deep analysis with shallow analysis based on, for example sequence of words. So you have to rely on some special techniques to identify words. Another level of analysis that would be very interesting. So the sequence of words representation is not as robust as string of characters. Then we can obtain a representation of the same text, but in the form of a sequence of words. If we could go further for semantic analysis, then we might be able to recognize dog as animal and we also can recognize boy as a person and playground as a location. So this is very useful representation an it's also related to the Knowledge Graph that some of you may have heard of. That means there are mistakes. Now, once we do that, we can count for example, the most frequent nouns or what kind of nouns are associated with what kind of verbs, etc. So in that sense, it's OK that computers may not be able to have completely accurate representation of text data and patterns that are extracted from text data can be interpreted by humans, and humans can guide the computers to do more accurate analysis by annotating more data by providing features to guide the machine learning programs to make them work more effectively. Now with this level of representation, we certainly can do a lot of things, and this is mainly because words are the basic units of human communication in natural language, so they are very powerful. We can represent this sentence in many different ways. When we store a natural language sentence as a string of characters, we have perhaps the most general way of representing text, since we can always use this approach to represent any text data. Now if we go further to do natural language processing, we can add a part of speech tags. So representing text data as a sequence of words opens up a lot of interesting analysis possibilities. Note that I use the plus sign here, because by representing text as a sequence of part of speech tags. So this makes this level of representation less robust, yet it's very useful
So, as we explained, different textual representation tends to enable different analysis. So in general we want to do as much as we can. It can enable a lot of analysis techniques such as word relation analysis, topic analysis and sentiment analysis, and there are many applications that can be enabled by this kind of analysis. And some applications are related to this kind of representation. First, they are general and robust, so they are applicable to any natural language. Now they are very effective, partly because the words are invented by humans as basic units for communications. Text representation determines what kind of mining algorithms can be applied. And finally such a word based representation and the techniques enabled by such a representation can be combined with many other sophisticated approaches. Now this course is covering techniques mainly based on word based representation. And this can be the case. For example, Thesaurus discovery has to do with discovering related words and topic and opinion related applications are abundant, and there are for example, and people might be interested in knowing the major topics covered in the collection of text. Word based representation is very important level of representation. And these different representations should in general be combined in real applications to the extent we can. So as a string text can only be processed by using stream processing algorithms, but it's very robust, it's general. We can also generate the structure based feature features and those are features that might help us classify text objects into different categories. When we add entities and relations, then we can enable lot of techniques such as knowledge graph analysis or information network analysis in general and this analysis would enable applications about entities, for example, discovery of all the knowledge and opinions about the real world energy entity. For example, stylistic analysis generally requires syntactical representation. And when different levels are combined together, we can enable richer analysis. These techniques are general and robust and thus are more widely used in various applications. So that makes this kind of word based representation also powerful. This course, however, focuses on word based representation. And in general there are many applications that can be enabled by the representation at this level. So this table summarizes what we have just seen. Although knowing word boundaries might actually also help. But obviously all these other levels can be combined and should be combined in order to support sophisticated applications. So they're not competing with each other. So they are actually quite sufficient for representing all kinds of semantics. Third, these techniques are actually surprisingly powerful and effective for many applications. So in order to support this level of application, we need to go as far as logical representation. More powerful analysis. And business intelligence people might be interested in understanding consumers opinions about their products and competitors products to figure out the what are the winning features of their products
 That means these two words are semantically related.This lecture is about the Word Association mining and analysis. So A&amp;B don't have to be words and they can be phrases example. In general there are two word relations, and these are quite basic. In this lecture we're going to talk about how to mine associations of words from text. And this can be used to introduce additional related words to a query to make the query more effective. We are gonna assume the words that have high context similarity to have paradigmatic relation. You can see they generally occur in similar context. For example, we might be interested in understanding positive and negative opinions about iPhone 6. Another application is to use word associations to automatically construct the topic map for browsing where we can have words as nodes and associations as edge. For paradigmatically relation we represent each word by its context, and then compute the context similarity. We are gooing to first talk about what is word Association and then explain why discovering such relations is useful and finally we are going to talk about some general ideas about how to mine word associations. This is an example of knowledge about natural language that we can mine from text data. Word relations can be also very useful for many applications in text retrieval and mining. So how can we discover such associations automatically? Now, here are some intuitions about how to do that. For syntagmatic relation, we will count how many times two words occur together in a context which can be a sentence, paragraph or a document even. Or you can use related words to suggest related queries to the user to explore the information space. So this is in contrast to the question about eats and meat. But here we are interested in knowing what other words are correlated with the verb eats. In that paradigmatically related words tend to have syntagmatic relation with the same word that they tend to be associated with the same word, and that suggests that we can also do join the discovery of the two relations. For example, in search in text retrieval we can use word associations to modify a query. And that might improve the all words in the sentence or in sentences around this word. In general they will be very different from what words occur after cat. And we're going to compare their Co occurrences with their individual occurrences. So these two are complementary and basically relations of words, and we're interested in discovering them automatically from text data. So to summarize, the general ideas for discovering word associations are the following. We're going to assume words with high co-occurrences, but relatively low individual occurrences to have syntagmatic relations because they tend to occur together, and they don't usually occur alone. First, such relations can be directly useful for improving accuracy of many NLP tasks, and this is because this is part of our knowledge about the language. In this case, the two words that have this relation can be combined with each other. So this is different from paradigmatic relation and these two relations are in fact so fundamental, that they can be generalized to capture basic relations between units in arbitrary sequences. So we can call this left context. What words occur before we see cat, cat or dog. If you think about the general problem of the sequence mining, then we can think about the units in the sequence data, and then we think of paradigmatic relation as relations that are applied to units that tend to occur in similar locations in a sentence or in a sequence of data elements in general. And then imagine what words occur after computer. So this is just suggesting that we can discover paradigmatic relation by looking at the similarity of context of words. So this intuition can help us discover syntagmatic relations. So A&amp;B have syntagmatic relation If they can be combined with each other in a sentence. And grammar learning can be also done by using such techniques because If we can learn paradigmatic relations, then we form classes of words. So they occur in similar locations relative to the neighbors in the sequence. So the important question to ask for syntagmatic relation is whenever eats occurs, what other words also tend to occur? So the question here has to do with whether there are some other words that tend to co-occur together with eats, meaning that whenever you see eat, you tend to see the other words. Finally, such word associations can also be used to compare and summarize opinions. Meaning that if we do that, the sentence will become somewhat meaningless. A&amp;B have paradigmatic relation if they can be substituted for each other. For example, we can look at the what words occur in the left part of this context. Now again, consider example- How helpful is the occurrence of eats for predicting occurrence of meat? knowing whether eats occurs in a sentence would generally help us predict the Whether meat also occurs indeed as if we will see eats occur in a sentence, and that should increase the chance that meat will also occur. So if you know these two words or synonyms, for example, and then you can help a lot of tasks. And then we can ask the question what words tend to occur to the left of eat and what words tend to occur to the right of eat? Now thinking about this question would help us discover Syntagmatic relations. So this is the basic idea of discovering paradigmatic relation. The user could navigate from one word to another to find information in the information space
 So they can be regarded as a pseudo document. So this may not be desirable. It may not be robust. By definition, 2 words are paradigmatically related if they share similar contexts. So in general, we can represent a pseudo document or context of cat as one vector. So this vector can then be placed in this vector space model. Now of course these xi's will sum to 1 because they are normalized frequencies. And then we can measure the similarity of these two vectors. An imaginary document. So this is another problem of this approach. We can call this context left1 context. An another word dog might give us a different context, so d2.This lecture is about the paradigmatic relation discovery. Now those of you who have been familiar with information retrieval or text retrieval techniques would realize that vector space model has been used frequently for modeling documents and queries for search. So that's exactly what we want. So the general idea of discovering paradigmatic relations is to compute the similarity of context of two words. So the idea here is represent a context by award vector where each word has a weight that is equal to the probability that a randomly picked word from this document vector is this word. And this would give us then two probability distributions representing two contexts. In this lecture we're going to talk about how to discover a particular kind of word Association called paradigmatic relations. So this intuitively makes sense for measuring similarity of contexts. Now to answer this question it's useful to think of bag of words representation as vectors in the vector space model. So that gives us one perspective to measure the similarity. And this can be interpreted as a probability that you would actually pick this word from d1 if you randomly pick the word. And there is this dot product infact gives us the probability that two randomly picked words from the two contexts are identical that means if we try to pick a word from one context and try to pick another word from another context, we can then ask the question, are they identical? If the two contexts are very similar, then we should expect that we frequently will see the two words picked from the two contexts are identical. So the two questions that we have to address is first how to compute each vector, that is, how to compute the xi or yi? And the other question is, how do you compute the similarity? Now in general there are many approaches that can be used to solve the problem, and most of them are developed for information retrieval. xi is defined as the normalized count of word wi in the context. And of course we can also assign weights to these different similarities to allow us to focus more on particular kind of context, and this would be naturally application specific, but again here that main idea for discovering paradigmatically related words is to compute the similarity of their context. And they have been shown to work well for matching a query vector and a document vector, but we can adapt the many of the ideas to compute the similarity of context documents for our purpose here. EOWC expected overlap of words in context. We can call this context right1. So by viewing context in the vector space model, we convert the problem of paradigmatic relations discovery into the problem of computing the vectors and their similarity. So that addresses the problem how to compute the vectors? Next, let's see how we can define similarity in this approach. And this means the vector is actually probability distribution over words. In the next lecture, we're going to talk about how to address these problems. So naturally, our idea for discovering such relation is to look at the context of each word and then try to compute the similarity of those contexts. Of course, this might be desirable in some other cases, but in our case we should intuitively prefer a case where we match more different terms in the context so that we have more confidence in saying that the two words indeed occur in similar context. Namely, they occur in similar positions in text. Now you might want to also take a look at the exact formulas and see why this can be interpreted as the probability that two randomly picked words are identical. And you can see we are seeing some remaining words in the sentences that contain cat. But here we also find it convenient to model the context of a word for paradigmatically relation discovery. Well, here we simply define the similarity as a dot product of two vectors and this is defined as the sum of the products of all the corresponding elements of the two vectors. We can call. For example, we can look at the word that occurs before the word cat. But intuitively we know matching the isn't really surprising because the occurs everywhere, so matching the is not as such a strong evidence as matching a word like eats which doesn't occur frequently. Then we have N dimensions as illustrated here
 These are the words that share similar context. And they generally form a sub linear transformation. Most specifically, we're going to introduce some retrieval heuristics used in text retrieval, and these heuristics can effectively solve these problems, as these problems also occur in text retrieval when we match a query vector with document vector. That is, we can reward matching a rare word and this heuristic is called IDF term weighting in text retrieval. So how can we add these heuristics to improve our. Matching one frequent term can contribute a lot. And more specifically, we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations. So here we define in this case we define the document vector. In general, when we represent a term vector to represent the sorry to represent context with the term vector, we would likely see some terms have higher weights and other terms have lower weights depending on how we assign weights to these terms, we might be able to use these weights to discover the words that are strongly associated with the candidate word in the context. IDF stands for inverse document frequency. IDF stands for inverse document frequency. Now there is also another interesting transformation called a BM25 transformation which has been shown to be very effective for retrieval and in this transformation we have a form that. And so that will be denoted by TF of W&amp;D as shown in the Y axis. So to summarize, the main idea for discovering paradigmatic relations is to collect the context of a candidate word to form a pseudo document, and this is typically represented as a bag of words. So for this reason, the highly weighted terms in this IDF weighted vector can also be assumed to be candidate for Syntagmatic relations. So you may recall this sum indicates all the possible words that can be a overlap between the two contexts. In this case, we're going to say, any non zero counts will be mapped to one. And so this is 1 parameter. So now we are going to talk about how to solve these problems. And we have each Xi, defined as a normalized weight of BM 25. But this is a reasonable way where we can adapt the BM25 retrieval model for paradigmatic relation mining. This is to again ensure all the x(i) will sum to one in this vector. So now the highest weighted terms will not be those common terms because they have lower IDFs. And that's what we had before. Finally, Syntagmatic relations can also be discovered as a bi-product when we discover paradigmatic relations. So this would be very similar to what we had before in that this vector is actually something similar to word distribution or the exercise with sum to one. That means we'll introduce weight for matching each term. A common word will be worth less than rare word, so we emphasize more on matching rare words now. Which can be interpreted as the probability that to randomly pick the words from the two contexts are identical, we also discuss the two problems of this method. So those are clearly the words that tend to occur in the context of the candidate word, for example, cat. And then zero count will be mapped to 0. We basically take Y as the same as X. That's when the word occurs just once in the context. In this method, we represent each context by a word vector that represents the probability of word in the context and we measure the similarity by using the DOT product. In our case, what will be our collection? We can also use the context that we can collect for all the words as our collection and that is to say, a word that's popular in the collection in general would also have a low IDF. So we use the raw count as representation. And the Xi and Yi probabilities of picking the word from both contexts, therefore it indicates how likely will see a match on this word. Instead, those terms would be the terms that are frequent in the context, but not frequently in the collection. In this case, all the lengths of all the context documents that we are considering. And then compute the similarity of the corresponding context documents of two candidate words. That is, to convert the raw count of word in the document into some weight that reflects our belief about how important this word in the document. But if we apply IDF weighting as you see here, we can then we weight these terms based on IDF That means the words that are common, like 'the' will get penalized. But we can't just say any frequent term in the context that would be correlated with the candidate word. This of course measure is used in search where we naturally have a collection. Even a common word like 'the' would contribute equally as content word like 'eats'. And in the next lecture, we're going to talk more about how to discover Syntagmatic relations. As we vary K, if we can simulate the two extremes. So how can we address that problem? In this case we can use the IDF weighting that's commonly used in retrieval. And this is a parameter to controlÂ  length normalization. We roughly have the 01 vector. So here we show that the IDF measure is defined as a logarithm function of the number of documents that match the term, or document frequency. Now interestingly we can also use this approach to discover syntagmatic relations. An then we can take the highly similar word pairs and treat them as having paradigmatic relations. So this transformation function is by far the most effective transformation function for text retrieval, and it also makes sense for our problem set up. But it clearly shows the relation between discovering the two relations. So with this modification, then the new function will likely address those two problems. More specifically, we have used the BM25 and IDF weighting to discover paradigmatic relation and these approaches also represent the state of the art in text retrieval techniques. Because this is a constant. Now with this definition, then we have a new way to define our document vectors and we can compute the vector D2 in the same way. The difference is that the high frequency terms will now have a somewhat lower weights and this would help control the influence of these high frequency terms. So now we're going to talk about the two heuristics in more detail. And indeed they can be discussed, discovered in a joint manner by leveraging such associations. I saw it's (K + 1) * X /( X + K) where K is a parameter. So one case is set to zero
 An entropy. Now we can assume high entropy words are harder to predict. If they are associated with eats, they tend to occur in the context of eats. We have a probability of 1. Here we denoted by X sub w, w denotes a word. In particular, we can talk about how to discover syntagmatic relations. So now let's see how we can use entropy for word prediction. In this lecture, we're going to continue talking about word Association mining. And we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations. So this random variable is associated with precisely one word. So more specifically, our prediction problem is to take any text segment, which can be a sentence, paragraph or a document, and then I asked the question is a particular word present or absent in this segment.This lecture is about the syntagmatic relation discovery. Right, so this would force us to think about what other words are associated with eats. When it's zero, it means the word is absent, and naturally the probabilities for one and zero should sum to 1. That means when we see one word occurs in the context, we tend to see the occurrence of the other word. So we can think about the two cases. In this case, the probability that X = 1 is . Now, what's interesting is that some words are actually easier for it, in other words. And so we will now have quantitative way to tell us which word is harder to predict. So the intuition we discussed earlier can be formally stated as follows. Which one do you think it is easier to predict? Now, if you think about it for a moment, you might conclude that. And I can bet that it doesn't occur in this sentence. An we clearly would expect the meat to have a high entropy, then the OR Unicorn.0 and the entropy is 0. So this is the entropy function and this function will give a different value for different distributions of this random variable. The question that is, can you predict what other words occur? To the left or to the right. And sometimes when we have 0 log of 0, we would generally find that as zero because log of 0 is undefined. The coin shows up as head hotel equally likely, so the two probabilities would be, 1/2 right so both will have both equal to 1/2. Because a word is either present or absent in the segment. Now, entropy in general is not negative and that can be mathematically proved. So now we can compute the entropy of this random variable, and this entropy indicates how difficult it is to predict the outcome of a coin for coin tossing. We see some words that might occur together with eats like a cat, dog or fish is right. By definition, Syntagmatic relations hold between words that have correlated Co occurrences. It can be head or tail, so we can define a random variable X sub coin so that it's one when the coin shows up as head, it's zero when the coin shows up as tail. Particularly, think about their entropies. But meat is somewhere in between in terms of frequency, and it makes it hard to predict because it's possible that it occurs in the sentence or the segment more accurately. But it may also not occur in the segment. When the value of the variable is 1, it means this word is present. The is easier to predict because it tends to occur everywhere, so I can just say with the in the semtence. So an interesting question
 So this is 0. So this suggests that we can use conditional entropy for mining syntagmatic relations. They all measure how hard it is to predict W1. We're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations. Because it tells us to what extent we can predict the one word given that we know the presence or absence of another word. Would be. Earlier we talked about using entropy to capture how easy it is to predict the presence or absence of a word. Basically. So as a result, if we replace these probabilities with their corresponding conditional probabilities in the entropy function, we will get the conditional entropy.This lecture is about the syntagmatic relation discovery and conditional entropy. In this lecture, we're going to continue the discussion of word association mining an analysis. Now that would change all these probabilities to conditional probabilities where we look at the presence or absence of meat. And of course, we can also define this conditional entropy for the scenario where we don't see eats. Now we address the different scenario where we assume that we know something about the text segment. We're going to consider both scenarios of the value of eats zero or one, and this gives us the probability that eats is equal to 0 or 1. Because in order to do that, we have to ensure that these conditional entropies areÂ  comparable across different words. Basically, whether eats is present or absent, and this of course is the entropy conditional entropy of meat in that particular scenario. For each word W1, we're going to enumerate the overall other words W2, and then we can compute the conditional entropy of W1 given W2. So this is a case of. And if we frame this using entropy, that would mean we are interested in knowing whether knowing the presence of eats could reduce uncertainty about the meat or reduce the entropy of the random variable corresponding to the presence or absence of meat. Note that we need to use a threshold to find these words. So let's see how we can use conditional entropy to capture syntagmatic relations. Whereas in the case of eats, eats is related to meet, so knowing presence of eats or absence of eats would help us predict wether meat occurs so it can help us reduce entropy of meat, so we should expect the second term, namely, this one to have a smaller entropy. And that's also when this conditional entropy reaches the minimum. So how do we address this problem? Later we'll discuss we can use mutual information to solve this problem. So these questions can be addressed by using. Now this would allow us to mine the most strongly correlated words with a particular word W1 here. Where you see the involvement of those conditional probabilities. This means we know whether meat occurs in the sentence and we hope to predict whether the meat occurs in the sentence. And in this case they all comparable right? So the conditional entropy of W1 given W2 and conditional entropy of W1 givenÂ  W3 are comparable. That means by knowing more information about the segment, we won't be able to increase the uncertainty. Now in general, for any discrete random variables X&amp;Y we have. In this case of discovering Syntagmatic relations for a target word like W1, we only need to compare the conditional entropies For W1 given different words. Another concept, called the conditional entropy. So we now also know when this w is the same as this meat then the entropy conditional entropy would reach its minimum which is 0? And for what kind of words would it reach its maximum? Well, that's when this W is not really related to meat. And this then tells us the entropy of meat after we have known eats occurring in the segment
 So for W1, we have two probabilities shown here. Either they both occur. The entropy of this word. And they are shown here. It's going to be larger than or equal to the mutual information between eats and another word. There are two scenarios. They should sum to 1 because a word can either be present or absent in the segment. This is another term in information theory that measures the divergance between two distributions. So we're going to compute the mutual information between eats and other words. And if we do that, and it's basically a based on the same intuition as in conditional entropy, we will see that words that are strongly associated with each will tend to have high mutual information, whereas words that are not related. Because when two random variables are independent, they joined distribution is equal to the product of the two probabilities. And similarly for the second word, we also have two probabilities representing presence or absence of this word, and this sums to one as well. So mathematically, it can be defined as the difference between the original entropy of X and the conditional entropy of X given Y. And similarly, using this equation we can compute the probability that we observe only the second word. This probability can be calculated by using this equation, because now this is known and this is also known and this is already known right? So this can be easier to calculate. Now the question we ask for syntactic relation mining is whenever eats occurs, what other words also tend to occur? So this question can be framed as a mutual information question, that is, which was have higher mutual information with eats. Now these equations allow us to compute some probabilities based on other probabilities. If there were independent. An so we this will take care of the computation of these probabilities of presence or absence of each word. Once we know how to calculate these probabilities, we can easily calculate the mutual information. So now we're going to introduce mutual information, which is another concept in information theory that allows us to, in some sense, normalize the conditional entropy to make. And if we know the presence of the probability of presence of the second word, then we can easily compute their absence probability, right? It's very easy to use this equation to do that. The mutual information between eats and meats, which is the same as between meats and eats cause major information is symmetric is expected to be higher than The mutual information between eats and the. And so, for example, this one means if we add up the probabilities that we observe the two words occur together and the probabilities when the word the first word occurs and the second word doesn't occur, we get exactly the probability that the first word is observed.This lecture is about the syntagmatic relation discovery and mutual information. It can also be defined as a reduction of entropy of Y, because of knowing X. And then finally we. In other words, and when the word is observed when the first word is observed and there are only two scenarios depending on weather second word is also observed. More specifically the question we're interested in here, is how much reduction in the entropy of X can we obtain by knowing Y. Now, in order to do that, we often. The bottom part of the denominator can be interpreted as the expected joint distribution of the two random variables. This is easy to understand becausw the original entropy is always not going to be lower than the possibly reduced conditional entropy. Now it's easy to see that we can actually compute the all the rest of these probabilities based on these. And here I listed all the probabilities involved and it's easy for you to verify that basically we have first 2 probabilities corresponding to the presence or absence of each word. In these two cases, one of the random variables will be equal to 1 and the other would be 0. So more specifically, and if we know the probability that a word is present, and in this case right? So if we know this. So this slide shows that we only need to know how to compute these three probabilities that are shown in the boxes, namely the presence of each word and the Co occurrence of both words in a segment. In this lecture, we're going to continue discussing syntagmatic relation discovery. Specifically, for example, using this equation, we can compute the probability that the first word occurred and the second word did not, because we know these probabilities in the boxes. So in that case both variables will have a value of one or one of them occurs. And they're summing up to 1, so these are the probabilities involved in the calculation of mutual information. In other words, picking any other word, and computing mutual information between eats and that word, you won't get any mutual information larger than the mutual information between eats and itself. So ranking based on mutual information is exactly the same as ranking based on the conditional entropy of X given Y. So that means knowing why did not help at all, and that's when X&amp;Y are completely independent. Right, so now this can be calculated. So now let's think about how to compute the mutual information
 These are fairly general. Once we have these counts, we can just normalize. So if we do that, we can see we can compute these probabilities as follows for estimating the probability that we see a word occurring in segment, we simply normalize the counts of segments that contain this word. From these pseudo segment. These approaches can be applied to any text with no helmet human effort. Their indicator as once for both columns. Of a word like a cat, and if we do the same for all the words, then we can cluster these words or computer similarity between these words based on their context similarity. A relation discovery an. So to summarize, this whole part about word Association mining, we introduce the two basic associations, called Paradigmatic and Syntagmatic relations. Any other applications in both information retrieval and text data mining. They can be applied to any items in any language, so the units don't have to be worse than they can be phrases or entities. And For estimating these probabilities, we simply need to collect the three counts. The third account is when both words occurred, so this is time we're going to count the segments where both columns have ones. And similarly this . These counts by n, which is the total number of segments and this will give us the probabilities that we need to compute mutual information. Are we introduced multiple statistical approaches for discovering them? Then it showing that pure statistical approaches are visible? Available for discovering both kinds of relations, and they can be combined to perform. And in some segments you see both words occur. So this basically concludes the discussion of how to compute the mutual information, how to use this for syntagmatic relation discovery. But here, once we use mutual information to discover Syntagmatic relations, we can also represent the context with this mutual information as weights. Observed more data than we actually have.In general, we can use the empirical counts of events in the observed data to estimate probabilities. They are based on counting of words. Yet they can actually discover interesting relations of words. So in the actual segments that we observed, it's OK if we haven't observed all the combinations. We can also use different ways to define context and segment and this would lead to some interesting variations of applications. So this provides yet another way to do term waiting for paradigmatic. So this would give us another way to represent the context. We get . So, to summarize, select the cinematic relation can generally be discovered by measuring correlations between occurrences of two words. We will pretend we observe some pseudo segments that are illustrated on the top on the right side of the slide and these pseudo segments would contribute additional counts of these words so that no event will have zero probability probability. We introduce the three concepts from information theory, entropy, which meshes uncertainly over random variable X conditional entropy, which measures the entropy of X. And of course in some other cases, none of the words occur, so they are both zeros. and a commonly used technique is called a maximum likelihood estimate, where we simply normalize the observed accounts. And similarly, counting Co occurrences using, let's say mutual information to discover syntagmatic relations, we also have to define the segment and the segment can be defined as an arrow, text window or longer text article and this would give us different kinds of associations. So to address this problem we can use a technique called smoothing and that's basically to add some small constant to discounts and then so that we don't get a zero probability in any case. For example, the context can be very narrow, like a few words around a word or sentence or maybe paragraphs and using different contexts, which allows you to discover different flavors of paradigmatic relations. Now, in particular, we introduce the four pseudo segments. These discovery associations can support them
 as C.This lecture isÂ  about topic mining and analysis. Or we are interested in knowing about the research topics. And we call that topic mining and analysis. All such meta data or context variables can be associated with the topics that we discover. Now when we have some non-text data then we can have more context for analyzing the topics. First, we have as input a collection of N text documents. What are these K topics? OK, major topics in the text data. An we can assume that these probabilities sum to one, because a document won't be able to cover other topics outside the topics that we discussed we discovered. The main topics. Now, starting from this lecture, we're going to talk about mining another kind of knowledge, which is content mining and trying to discover knowledge about. In general, we can view topic as some knowledge about the world. In this case K topics. And then we also would like to know which topics are covered in which documents, to what extent. Document one we might see that topic 1 is covered a lot, topic 2 and topic k are covered with a small portion. Here we can denote that text connection. For example, we can talk about the topic of a sentence. The second task is to figure out which documents cover which topics to what extent. So more formally, we can define the problem as follows. For example, we might be interested in knowing what are Twitter users talking about today? Are they talking about NBA sports or talking about some international events, etc. And then we can use these context variables to help us analyze patterns of topics. So obviously for each document we have a set of such values indicate. Also we want to generate the coverage of topics in each document d sub i and this is denoted by Ï sub i j and Ï sub i j is the probability of document d sub i covering topic theta sub j. As you see on this roadmap, we have just covered mining knowledge we have just covered mining knowledge about the language namely discovery of word associations such as paradigmatic relations relations and syntagmatic relations. So now the question is how do we define theta sub i? How do we define the topic now? This problem has not been completely defined until we define what is exactly theta., right? So now you can see there are generally two different tasks or subtasks. Or perhaps we're interested in knowing what are the major topics debated in 2012 presidential election? And all these have to do is discovering topics in texts and analyzing them, and we're going to talk about a lot of techniques for doing this. Now the output would then be the K topics that we would like to discover denoted as theater sub one through theta sub k. To what extent did the document covers each topic. For example, one might be interested in knowing what are the current research topics in data mining and how are they different from those five years ago
 word sports. We are going to talk about using a term as topic.This lecture is about the topic mining and analysis. A term can be a word or a phrase. these counts as our estimate of the coverage probability for each topic. If we're dealing with tweets, we could also favor hashtags which are invented to denote topics. So now of course, as we discussed. Here, candidate terms can be words or phrases. Our idea here is to define a topic simply as a term. So we need to count related words. It can be applied to any language and any text. Then we're going to design a scoring function to measure how good each term is as a topic. Those terms are very frequent in English. Here, for example, we might want to discover to what extent document 1 covers sports and we found that 30% of the content of document 1 is about sports. We can't simply just count the topic. These words then become candidate topics. So a particular approach could be based on, TF-IDF weighting from retrieval. And in general, we can use these terms to describe topics, so our first thought is just to define a topic as one term. OK so after this then we will get K topical terms and those can be regarded as the topics that we discovered from the collection. For example, in news we might favor title words. First, when we count what words belong to the topic, we also need to consider related words. This forms a distribution over the topics for the document to characterize coverage of different topics in the document. In the task definition for topic mining and analysis, we have two tasks, one is to discover the topics and the 2nd is to analyze the coverage. They are semantically similar or closely related or even synonyms. So let's first think about how we can discover topics if we represent each topic by a term. But when we apply such an approach to a particular problem, we might also be able to leverage some domain specific heuristics. So that means we need to mine K topical terms from a collection. But it cannot represent the complicated topics that might require more words to describe. Now if we define a topic in this way, we can analyze the coverage of such topics in each document. So that would mean we want to favor a frequent term. It does not suggest what other terms are related to the topic, even if we're talking about the sports, there are many terms that are related, so it does not allow us to easily count related terms toward contributing to coverage of this topic. Finally, there's this problem of word sense ambiguation, a topical term or related term can be ambiguous. That means the coverage of sports will be estimated as zero. So in general the formula would be to collect the counts of all the terms that represented the topics and then simply normalize them so that. similar. So we need to deal with that as well. And TF stands for term frequency IDF stands for inverse document frequency and we talked about some of these ideas in the lectures about the discovery of word associations. 'cause we know the content is about sports. For example a very specialized topic would be hard to describe by using just a word or one phrase, we need to use more words, so this example illustrates some general problems with this approach of treating a term as topic. So this obviously is also not desirable. We're going to talk about a natural way of doing that, which is also likely effective. So this estimate has problem. For example, we can use pure statistics to design such as scoring function. Next let's think about how we can compute the topic coverage pi sub i j So looking at this picture, we have sports, travel and science and these topics and now suppose you are given a document How should we figure out the coverage of each topic in the document? One approach can be to simply count occurrences of these terms. So in terms of the content, it's about the sports. So how can we design such a function? Well, there are many things that we can consider
 We're going to introduce probabilistic topic models.This lecture is about the probabilistic topic models for topic mining and analysis. So we design a probabilistic model to model how the data are generated. So this is a general idea of using a generative model for text mining. We simply assume that they are generated this way and inside the model, we embed some parameters that were interested in denoted by Lambda. We talked about the task of topic mining and analysis when we define a topic as a distribution, so the input is a collection of text articles. First, we design a model with some parameters that we are interested in, and then we model the data. But now we're going to use a word distribution to describe the topic. So this is to say that we first pretend we already have these word distributions and coverage numbers, and then we're going to see how we can generate data by using such distributions. As a set of words that determines what units would be treated as the basic units for analysis. Now you can also see there are some words that are shared by these topics. When we have more words that we can use to describe the topic, we can describe complicated topics, to address the second problem, we need to introduce weights of words. So now each topic is word distribution. We adjust the parameters to fit the data as well as we can. These are sports-related terms and of course it would also give a non zero probability to some other words like "travel" which might be related to sports. So this is a slide that you have seen earlier where we discussed the problems with using a term as a topic. In general, we can imagine a non zero probability for all the words and some words that are not relevant would have very very small probabilities and these probabilities will sum to one. We have non zero probabilities, it's just that for a particular topic of some words we have very very small probabilities. Third, because we have probabilities for the same word in different topics. By varying the model, of course we can discover different knowledge. So what you see now is the old representation, where we represent each topic with just one word or one term or one phrase. Now obviously we can already see N * K parameters for pi's. Now, once we set up with a model, then we can fit the model to our data, meaning that we can estimate the parameters or infer the parameters based on the data. It turns out that all these can be done by using a probabilistic topic model, and that's why we're going to spend a lot of lectures to talk about this topic. And since we have N documents so we have N sets of pis. Each is word distribution. So this would be then our estimate of the parameters and these parameters note that are precisely what we hope to discover from text data, so would treat these parameters as actually the outcome or the output of the data mining algorithm. So the coverage of each of these K topics would sum to one for a document. A document is not allowed to cover a topic outside the set of topics that we are discovering. Now intuitively, this distribution represents a topic in that if we sample words from the distribution, we tend to see words that already do sports. So the basic idea here is improved representation of topic as a word distribution. So how do we model the data in this way? And we assume that data are actually samples drawn from such a model that depends on these parameters. And these parameters in general, will control the behavior of the probabilistic model, meaning that if you set these parameters for different values, it will give some data points higher probabilities than others. And also the coverage of all the topics in each document and these are formally represented by theta_i's and pi_i's and we have two constraints here for these parameters. In most cases, we use words as the basis. So that it forms a distribution of all the words. Now in this case, of course, for our tax mining problem, or more precisely topic mining problem, we have the following parameters. So this capital lambda actually consists of all the parameters that we're interested in. So to summarize, we introduced a new way of representing a topic, namely represented as word distribution, and this has advantage of using multiple words to describe a complicated topic. But as a distribution, this topic representation can in general involve many words to describe the topic and can model subtle differences in semantics of the topic. So given a set of text data, we would like to compute all these distributions and all these coverages as you have seen on this slide. Similarly, we can model travel and science with their respective distributions. In this lecture we're going to continue talking about the top mining and analysis. We also introduce the general idea of using a generative model for text mining and the idea here is to first design a model to model the generation of data. What we're interested in here is what parameter values will give our data set the highest probability. And for each word distribution, we know that all the probabilities should sum to one over all the words in the vocabulary. And then we can infer the most likely parameter values lambda star given a particular data set, and we can then take the Lambda star as knowledge discovered from the text for our problem, and we can adjust the design of the model and parameters with this discover various kinds of knowledge from text. What we're interested in here is to find the Lambda star that would maximize the probability of the observed data. And similarly you can see "star" also occurred in sports and science with reasonably high probabilities, because they might be actually related to the two topics. Now the output would consist of as first a set of topics represented by Theta i's Each theta_i is a word distribution
 Such a model can also be regarded as a probabilistic mechanism for generating text. These models are general models that cover probabilistic topic models as special cases. Now this signals a topic about text mining, because when we sample words from such a distribution we tend to see words that often occur in text mining context. Some word sequences might have higher probabilities than others. One is given a model. So when we have a model, we generally have two problems that we can think about. So here we assume we have N words, so we have N probabilities, one for each word, and they sum to one. In such a case, we simply assume that text is generated by generating each word independently. Now, in this case, we're going to assume that we have observed data. So now we can assume our text is a sample drawn according to this word distribution.00001 So as you can see, such a distribution clearly is context dependent.This lecture is about an overview of statistical language models which cover probabilistic topic models as special cases. And so this suggests that such a distribution can actually characterize topic. So in general, in order to characterize such a distribution, we must specify probability values for all these different sequences of words. For this reason, we also call such a model generative model. So that just means given a particular distribution, different text will have different probabilities. So for such a model we have as many parameters as the number of words in our vocabulary. So now Given a model, we can then sample sequences of words. And that just means we can view text data as data observed from such a model. And query as a relatively small probability, just observd once. So, for example, we might have a distribution that gives "Today is Wednesday" a probability of 0. In this case, let's assume we have a text mining paper. So as you can see, with N probabilities, one for each word, we actually can characterize the probability distribution over all kinds of sequences of words, and so this is a very simple model. We can try to sample words according to a distribution. Ignore the word order, so it may not be effective for some problems such as speech recognition, where you may care about the order of words. So in this case, if we ask the question about what is the probability of generating a particular document, then we likely will see text that looks like a text mining paper of course. That is, we're interested in the sampling process. In this lecture we're going to give an overview of statistical language models. Obviously it's impossible to specify that, because it's impossible to enumerate all the possible sequences of words. But the same sequence of words might have a different probability in a different context. And this is called a maximum likelihood estimate. But it turns out to be quite sufficient for many tasks that involve topic analysis, and that's also what we're interested in here. The first one has higher probabilities for words,Â  text, mining, association, etc. Now, in general, the words may not be generated independently, but after we make this assumption, we can significantly simplify the language model. So specifically we can compute the probability of "today is Wednesday". So first, what is the statistical language model? A statistical language model is basically the probability distribution over word sequences. And similarly another sentence, "The eigenvalue is positive", might get a probability of 0. Assuming that no word has a zero probability in the distribution and that just means we can essentially generate all kinds of text documents, including very meaningful text documents
 It's basically a word distribution. We assume that we have some prior belief about the parameters. And later you will see this can be useful for topic mining where we want to inject some prior knowledge about the topics. And this estimate is a more general estimate than the maximum likelihood estimate. So this is our prior. As a special case, we can assume F of Theta is just equal to Theta. That includes our prior knowledge about the parameters. And in such a case, we can maximize this posterior probability to find a theta that would maximize this posterior probability. Given that we have observed Y, now what do we believe about X now, do we believe some values have high probabilities than others? Now, the two probabilities are related through this can be regarded as the probability of the observed evidence Y here given a particular X. Where we define the estimate as arg max of the probability of X given Theta. So we're interested in which value of data is optimal. And this probability of X given Y is a conditional probability, and this is our posterior belief about X, because this is our belief about X values after we have observed Y. In this case we must define a prior on the parameters P of Theta, and then we're interested in computing the posterior distribution of the parameters which is proportional to the prior and the likelihood. So to summarize, we introduced the language model which is basically probability distribution over text. The prior tells us some theta values are more likely than others. So in our case, what we're interested in is inferring the theta values so we have a prior here. That means before we observe any other data, that's our belief about X, what we believe some X values have higher probability than others. It's also called a generative model for text data. But this zero probability for all the unseen words may not be reasonable sometimes, especially if we want the distribution to characterize the topic of text mining. That means the most likely parameter value according to our prior before we observe any data. We also talked about the Bayesian estimation or influence. And then we have our data likelihood. So in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. Otherwise your estimate may not necessarily be more accurate than maximum likelihood estimate. The simplest language model is unigram language model. There's no free lunch, and if you want to solve the problem with more knowledge, we have to have that knowledge and that knowledge ideally should be reliable. So one way to address this problem is actually to use Bayesian estimation, where we actually would look at both the data and all our prior knowledge about the parameters. In general, in Bayesian inference we are interested in the distribution of all these parameter values. So we think about all the possible values of Theta. So you can think about X as our hypothesis. So I showed F of Theta here as an interesting variable that we want to compute. And by using Bayes rule that I have shown here, we can then combine the likelihood function with the prior to give us this posterior probability of the parameter. And we have some prior belief about which hypothesis to choose and after we have observed Y, we will update our belief and this updating formula is based on the combination of our prior here and the likelihood of observing this Y if X is indeed true. So this is a general illustration of Bayesian estimation and Bayesian inference. We introduced the concept of likelihood function which is the probability of data given some model. So the problem of Bayesian inference is to infer this posterior distribution and also to infer other interesting quantities that might depend on Theta. So in this case the value of argmax is Theta. Given a data point, sorry, given a data sample X, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum likelihood estimate. Because once if we define our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically would go back to the maximum likelihood estimator because in such a case it's mainly going to be determined by this likelihood value here. And then when we combine the two, we get the posterior distribution and that's just a compromise of the two. And allows us to write down one conditional probability of X given Y in terms of the conditional probability of Y given X. This point is the maximum likelihood estimate that represents the theta that gives the data the maximum probability
This lecture is a continued discussion of probabilistic topic models. Since we know all the theta I's must sum to one, we can plug this into this constraint here, and this will allow us to solve for Lambda. And when we set it to zero, we can easily see theta sub i is related to Lambda in this way. We're going to talk about how to use probabilistic models to somehow get rid of these common words. And for convenience we're going to use theta sub I to denote the probability of word W sub I. In this lecture, we're going to continue discussing probabilistic models, we are going to talk about a very simple case where we are interested in just mining one topic from one document. Our model is a unigram language model, a word distribution that we hope to denote a topic and that's our goal. So the main goal is just to discover the word probabilities for this single topic, as shown here. So we will have as many parameters as many words in our vocabulary, in this case M. So this is the simplest case of topic modeling. And this is multiplied by the logarithm of the probability. So it's additional parameter. As always, when we think about using a generative model to solve such a problem, we'll start with thinking about what kind of data we're going to model or from what perspective we're going to model the data or data representation. In the output we also no longer have coverage because we assumed that the document covers this topic 100%. So in this simple setup we are interested in analyzing one document and trying to discover just one topic. Now when we do this transformation, we then would need to introduce a count function here. Which means we'll take the estimated parameters as a knowledge that we discover from the text. Next, we're going to find the theta values, or probabilities of these words that would maximize this likelihood function. So if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document D here, let's imagine this document is a text mining paper. So our goal is to maximize this likelihood function. So our data in this case is just the document which is a sequence of words. Because these words might have repeated occurrences. As a topic representation, you will see this is not ideal, right? The because of the high probability words are functional words they are not really characterizing the topic. And since some word might have repeated occurrences, so we can also rewrite this product in a different form. So now we're just interested in optimizing this Lagrange function. So after all this math, after all, we have just obtained something that's very intuitive, and this will be just our intuition where we want to maximize the theta by assigning as much probability mass as possible to all the observed words here. This denotes the count of word one in document. So one question is how can we get rid of such common words? Now this is a topic of the next lecture. On the top you will see the high probability words tend to be those very common words, often functional words in English, and this will be followed by some content words that really characterized the topic well like text, mining etc and then in the end you also see various more probabilities of words that are not really related to the topic, but they might be externally mentioned in the document. And this Lambda is simply taken from here. If we do that, you will see the partial derivative with respect to theta i here is equal to this. The objective function is the likelihood function, and the constraint is that all these probabilities must sum to one. Each word here is denoted by X sub I. Now, what does the likelihood function look like? This is just the probability of generating this whole document given such a model. Now this content is beyond the scope of this course. And obviously these thetas of i's would sum to one. And the likelihood function will have some parameters in the function and then we are usually interested in estimating those parameters, for example by maximizing the likelihood which would lead to maximum likelihood estimator and these estimated parameters would then become the output of the mining algorithm. It's not sufficient though, so. But since Lagrange multiplier is very useful approach, I also would like to just give a brief introduction to this for those of you who are interested. So let's see how we can solve this problem. As you may recall from calculus, an optimal point would be achieved when the derivative is set to 0
 Then we're going to use this word distribution to generate a word. So mathematically, this model. And we're going to use the background word distribution to generate the word. Specifically, Theta sub D which is intended to denote the topic of document D and Theta sub B which is representing a background topic that we can set to attract the common words. In this lecture we will continue discussing probabilistic topic models.This lecture is about a mixture of unigram language models. Note that they sum to one. This is the probability of selecting the background word distribution denoted by Theta sub B. When we generate the word, however, we're going to 1st decide which of the two distributions to use, and this is controlled by another probability: probability of theta sub D and probability of theta sub B here. One is the two word distributions. Because common words would be assigned high probabilities in this model. and probability of Theta sub B. This way the two distributions can be mixed together to generate the text data and will let the other model which we called background topic model to generate the common words. So the basic idea of a mixture model is just to treated these two distributions together as one model. But we can still think of this as a model for generating text data and such a model is called a mixture model. And the model is a mixture model with two components: two unigram language models. And this is basically a more complicated mixture model. So this is the probability of selecting the topic word distribution. In particular, we're going to introduce a mixture of unigram language models. In particular, we have to say that this distribution doesn't have to explain all the words in the text data, or we're going to say these common words should not be explained by this distribution. The form you're seeing now is more general form than. The coverage of each topic and this is determined by probability of Theta sub D. So in this sense, the mixture model is more general than the previous model where we have just one distribution and it can cover that as a special case. So one natural way to solve the problem is to think about using another distribution to account for just these common words. So to summarize, and we talked about the mixture of two unigram language models. Well, this is obviously because these words are very frequent in our data and we are using a maximum likelihood estimate and then the estimator obviously would have to assign high probabilities for these words in order to maximize the likelihood. So now once we set up the model and we can write down the likelihood function as we see here, the next question is how can we estimate the parameter or what to do with the parameters given the data? Well, in general we can use some observed text data to estimate the model parameters and this mission would allow us to discover the interesting knowledge about the text, so in this case, what do we discover? Well, these are represented by our parameters, and we have two kinds of parameters. Now in this case I just give example where we can set both to . An eventually we generated a lot of words. The first case is to use the topic word distribution to generate the word, and in such a case, then the probability would be the probability of Theta sub D, which is the probability of choosing the model multiplied by the probability of actually observing the word from that model. It will be useful to think about why we end up having this problem. Since we now have two distributions, we have to decide which distribution to use when we generate the word, but each word will still be sampled from one of the two distributions, right? So text data is still generating the same way. This is nothing but to just define the following generative model where the probability of word is assumed to be a sum over 2 cases of generating the word. One is the word probabilities in each topic must sum to one, the other is the choice of each topic must sum to one. So as I just said, we can treat this as just a generative model and it's often useful to think of just the likelihood function. This is a slide that you have seen earlier where we talked about how to get rid of the background words that we have on top of estimated language model for one document. So in such a case we have a model that has some uncertainty associated with the use of a word distribution. The basic question is, so what's the probability of observing a specific word here? Now we know that the word can be observed from each of the two distributions, so we have to consider 2 cases. Two ways in this case. So to summarize, what we observe here, the probability of a word from a mixture model is in general a sum over all different ways of generating the word. This way our target is the topic theta here would be only generating the content words that characterize the content of the document. But in general these probabilities don't have to be equal, so you might bias towards using one topic more than the other. Now obviously the probability of text the same is all similar, right? So we also consider two ways of generating text, and each case is a product of the probability of choosing a particular word distribution multiplied by the probability of observing the word from that distribution. In this case, what's the probability of observing the word w? Now here I showed some words like "the" and "text", so as in all cases, once we set up the model, we're interested in computing the likelihood function
 We are going to assume we have knowledge about others.This lecture is about mixture model estimation. They are precisely the two probabilities of the two words text and the given by theta sub D. We can't give both a probability of 1. In particular, we're going to talk about how to estimate the parameters of a mixture model. So we assume the background model is already fixed. Furthermore, we are going to assume there are precisely two words: the and text. In this lecture, we're going to continue discussing probabilistic topic models.5 multiplied by the probability of observing text from that model. If there were no constraint, of course we would set both probabilities to their maximum value, which would be one to maximize this. And this is because we have assumed all the other parameters are known. But we can't do that because text and the must sum to one. And this also means that by using a background model that is fixed to assign high probabilities to background words, we can indeed encourage the unknown topic world distribution to assign smaller probabilities for such common words, instead put more probability mass on the content words that cannot be explained well by the background model. So in order to compensate for that we must make the probability of text given by theta sub D somewhat larger so that the two sides can be balanced. This is not the case when we have just one distribution and this is clearly because of the use of the background model which assigns a very high probability to the and low probability to text. So this is in fact a very general behavior of this mixture model, and that is if one distribution assigns a high probability to one word than another, then the other distribution. In particular, you will see in order to make them equal and then the probability assigned by theta sub D must be higher for a word that has a smaller probability given by the background. So specifically in this case. So now the question is how should we allocate the probability mass between the two words? What do you think? Now it would be useful to look at this formula for moment and to see what intuitively what we do in order to set these probabilities to maximize the value of this function. And they will tend to bet high probabilities on different words to avoid this competition in some sense. So the idea is to assume that the text data actually contain two kinds of words. So this is the case of customizing a probalistic model so that we embed the unknown variables that we are interested in. Now, although we designed the model heuristically to try to factor out this background words. Now you can imagine we could have assumed that we also don't know the background word distribution, but in this case our goal is factor out precisely those high probability background words. We're going to assume that we already know the parameters of all the values for all the parameters in the mixture model except for the word distribution of theta sub D, which is our target. So there's some constraint. So we further assume that the background model gives probability of point nine to the word the and text point one. And the problem here is how can we adjust theta sub D in order to maximize the probability of the observed document here and we assume all the other parameters are known. Now if we plug that in, it will mean that we have to make the two probabilities equal. OK, if we look into this further then we'll see some interesting behavior of the two component models in that they will be collaborating to maximize the probability of the observed data which is dictated by the maximum likelihood estimator. So to understand why this is so, it's useful to examine the behavior of a mixture model. Then the product of them, which is maximum when they are equal and this is a fact that we know from algebra. So in order to solve this problem of factoring out background words, we can set up our mixture model as follows. Or to gain advantage in this competition. And, this is obvious from examining this equation because the background part is weak for text it's small. Once you understand what's the probability of each word, which is also why it's so important to understand what exactly the probability of observing each word from such a mixture model
 Should it be increased to be more than 0.9 which is now 0.1 or should we decrease it to less thanÂ  0. It also encourages the unknown distribution theta sub d to assign somewhat higher probability to this word. Since in this case all the additional terms are the, we're going to just multiply by this term for the probability of the. So. The more likely a component that is being chosen, it's more important than to have higher values for these frequent words. And this is to collaboratively maximize likelihood. We also talk about the special case of fixing one component to a background word distribution, and this distribution can be estimated by using a collection of documents. As we add more words, we know that, we have to just multiply the likelihood function by additional terms to account for the additional occurrences of the. A large collection of English documents, by using just one distribution and then we'll just have normalized frequencies of terms to give us the probabilities of all these words. Right, so this means there is another behavior that we observe here that is high frequency words generally will have high probabilities from all the distributions. So now, as you can imagine, it would make sense to actually assign a smaller probability for text and to make room for a larger probability for the. 1st Every component component model attempts to assign high probabilities to high frequency words in the data. OK, So what you're seeing now is basically the likelihood function for the two word document, and we know in this case the solution is to give text a probability of 0. Now, we've being so far, assuming that each model is equally likely and that gives us 0. The question is which word to have a reduced the probability and which word to have a larger probability? And in particular, let's think about the probability of the. This is also an example of imposing a prior on the model parameters and the prior here basically means one model must be exactly the same as the background language model, and if you recall what we talked about in Bayesian estimation and this prior would allow us to favor a model that's consistent with our prior. So to summarize, we have just discussed the mixture model and we discussed the estimation problem of mixture model and in particular we discussed some general behavior of the estimate an that means we can expect the our estimator to capture these intuitions. Now when we use such a specialized mixture model, we show that we can effectively get rid of background words in the other component. This means the behavior here, which is high frequency words tend to get higher probabilities are affected or regularised somewhat by the probability of choosing each component. Now it's interesting to think about a scenario where we start adding more words to the document. This is also an issue that we will talk more later. Now you will see these terms for the will have a different form where the probability of 'the' would be even larger because the background that has a high probability for the word and the coefficient in front of 0.1 will no longer be optimal for this new function, right? But the question is how should we change it? Well in general they sum to one. An added the probability mass to the other word. So all the more word occurs, then it's it makes more sense to give such a word a high probability because the impact would be more on the likelihood function
 They are equally likely. So we are interested in computing this estimate. Now that we are interested in the word text, so text can be regarded as evidence. distribution. So now all the parameters are known for this mixture model. These blue words are then assumed to be from the topic word distribution. If we had known which words are from which distribution precisely. So in other words, we're going to compare these two probabilities. Then we simply we can simply normalize them to have estimate of the probability that the word text is from theta sub d or from theta sub b. And by this we are going to say text is more likely from theta sub d. So the only thing unknown is this word probabilities are given by theta sub d update. By the theta sub d, than by the background model, which has a very small probability. But suppose we actually know which word is from which distribution, so that would mean, for example these words: the is and we are known to be from this background word distribution. We're going to choose word that has a higher likelihood. In this lecture, we're going to continue the discussion of probabilistic topic models. The probability of generating text is another product of similar form. And in this lecture were going to look into how to compute this maximum likelihood estimator. We are going do tend to guess the distribution that gives the word higher probability and this is likely to maximize the likelihood right so. And we're going to try to adjust these probability values to maximize the probability of the observed document, and know that we assume that all the other parameters are known. When Z is zero, it means it's from the topic theta sub d when it's one, it means it's from the background theta sub b. So more specifically, let's think about the probability that this word text has been generated. So now we have the probability that text is generated from each. On the other hand, the other words, text, mining, clustering, etc are known to be from the topic word distribution. Specifically, given all the parameters can we infer the distribution the word is from So let's assume that we actually know tentative probabilities for these words in theta sub D. So the question is, do you think text is more likely have been having been generated from theta sub d or from theta sub B? So in other words, we want to infer which distribution has been used to generate this text. So when we multiply the two together, we get the probability that text has in fact has been generated from theta sub d Similarly, for the background model an. Of the word given by each distributions. One group would be explained by the background model, the other group would be explained by the Unknown topic word distribution After all, this is the basic idea of mixture model. And equivalently, the probability that Z is equal to 0 given that the observed evidence is text. And this is in fact Making this model no longer mixture model because we can already observe which distribution has been used to generate which part of the data, so we actually go back to the Single word distribution problem, and in this case let's call these words that are known to be from theta d pseudo document d prime and then all we need to do is just normalize these word counts for each word w sub I. But this gives us the idea of perhaps we can guess which word is from which. Now, this inference process is a typical Bayesian inference situation where we have some prior about These two distributions so can you see what is our prior here? Well the prior here is the probability of each distribution, right? So the prior is given by these two probabilities
 This is given by this formula. re estimate our parameters. That were interested in so these will help us re estimate these parameters. For the word distribution that we're interested in. So these words are believed to be more likely from the topic. For example, is made to what extent this document has covered background words. We have high probability like this one text. To help us re estimate the parameters. For example, we have N + 1. So what are the relevant statistics? Well, these are the word counts. So assume we have just 4 words and their counts are like this and this is our background model that assigns high probabilities to common words like the. By the probability that we believe this would has been generated by using the theta sub d. So here this probability that we're interested in is normalized into an uniform distribution over all the words.0, so we're going to just get some percentage of the counts toward this topic, and then we simply normalize these counts. If this is 1. In our case the parameters are mainly the probability of a word given by status update. We can then normalize the count to estimate the probabilities or to revise our estimate of the parameters. So we're going to adjust the count. Of the distribution that has been used to generate each word, we can see we have different probabilities for different words. So in this setting we have assumed that the two models have equal probabilities and the background model is known. Why that's be cause these words have different probabilities in the background. And then once we have a new generation of parameters, we're going to repeat this. Then it's 0 for Z etc. So compare this with this one and will see at the probability is different. But note that before we just set these parameter values randomly, But with this guess we will have a somewhat improved estimate of this. And you can see this. And this is done by this multiplication. And this when we add this up or take the average will kind of know to what extent it has covered background versus content words that are not explained well by the background. In this case the topic distribution, and you might wonder whether this would be also useful because our main goal is to estimate these word distribution right? So this is our primary goal.0 Then we just get the full Council of this word for this topic. So this is what would actually happen when we compute these probabilities using the EM algorithm. And one use is to. We just imagine there are such a social values of Z attached to all the words. What's also interesting is do not last column, and these are the inferred word split, and these are the probabilities that a word is believed to have come from one distribution. Now the idea that we talked about before for predicting the word distribution that has been used with the general the world is it'll predict this. We're going to use the E-step again to improve our estimate of the hidden variables, and then that would lead to another generation of re estimate the parameters. Now, of course we don't observe those Z values. We hope to have a more discriminating world distribution. But in general, as I said, it's not going to be 1. And So. And this these new in further values of these will give us then another generation of the estimate of probabilities of the words. And note that these log likelihood is negative becausw the probability is between zero and one when you take logarithm, it becomes a negative value. So 4 must be multiplied by this point three three in order to get the allocated counts toward the topic. Not only that, we also see some words that are believed to have come from the topic. So these formulas are the EM formulas that you see before, and you can also see there are superscripts here N to indicate the generation of parameters. That means we have improved parameters from here to
 This is a general algorithm for computing. These are basically part of this lower bound. Be cause it's a lower bound, we are guaranteed to improve this gas. Require more. The general idea is that we will have two steps to improve the estimate of parameters in the E step. And this is the one that we hope to maximize an we hope to find a set of value at this point to maximize this. So the two would be equal. So the E step is basically to compute this lower bound. We find this point. We see the likelihood function. Maximum regular is made of all kinds of mixture models. And that we can then map to the original like role function. To summarize, in this lecture we introduce the EM algorithm. That's your starting point and then you try to improve this by moving this to another point where you can have a higher like recorder. Convert it to some stable value. In the end step, then would exploit such augmented data, which would make it easier to estimate the distribution to improve the estimate of parameters. We have set up value. I so we already know it's improving the lower bound, so we definitely improve this original like record function which is above this lower bound. And we don't direct it, just computed this likely or function, but we computed the latent variable values and. In our case, this is the distribution that has been used to generate the world. Note that it's not necessary that we will have a stable converged parameter values, even though the likelihood function is insured to increase. That means we're not going to just say exactly what's the value of a hidden variable, but we're going to have a probability distribution over the possible values of these hidden variables, so this causes a split of counts of events probabilistically and in our case, will split the world counts between the two distributions. So we have to resolve a numerical algorithm. So here for example, how do we start from here? Then we gradually just climb up to this top, so that's not optimal, and we'd like to climb up all the way to here. So here is just the illustration of what happened an A detailed explanation this. Knowledge about some of the inequalities that we haven't really covered yet. So this curve is reaching or like roller function, right? So this one. There are some properties that have to be satisfied in order for the parameters also too
 Specifically we have K topics. K among these K topics. Lambda sub B versus non background. These are pie some ideas and they tell us which document covers which topic to what extent. All the words must have probabilities that sum to 141 distribution. So we hope to generate these as output because there are many useful applications if we can do that. Say this is the most basic topic model.This lecture is about the probabilistic latent semantic analysis or P LSA. After we have made this decision, we have to make another decision to choose one of these K distributions. So these are the tasks of topical model. We're going to generate the world by using one of these distributions, assume here. One is we decided not to talk about background, so that's probability 1 minus Lambda sub b. Those are the most important parameters that we are asked, so one is pies and these are the coverage of topic in the document. So here I illustrate how we can generate the text that I was multiple topics. And background help. The only difference is that we're going to have more than two topics. So it's the background. In this lecture we're going to introduce probabilistic latent semantic analysis, often called the PLSA. First we can flip a coin to decide whether we will talk about the background. We can use a large collection of text or use all the tests that we have available to estimate the water distribution. So next, once we have the likelihood function, we would be interested in knowing the parameters right? So to estimate the parameters. If we can discover these topics can color this words as you see here to separate the different topics, then you can do a lot of things such as summarization or segmentation of the topics, clustering of sentences, etc. And the other is the word distributions that characterize all the topics. But now we have multiple is. How many unknown parameters are there? Now trying to figure out that question would help you understand the model in more detail, and it would also allow you to understand what would be the output that we generate when we use PLSA to analyze text data, and these are precisely the unknown parameters. Also, one of the most useful topic models. First, we have to have chosen the background model. The other is the topic coverage distribution. One is the topic category characterization Seedies HCI is a water distribution and 2nd it's the topic coverage for each document. We also see background words or mixed with them, so the goal of topic analysis here is try to decode these topics behind the text. Anna Document will have to cover precisely these K topics, so the probability of covering each topical would have to sum to one. So again, it's a constrained optimization problem like what we have seen before, only that we have a collection of text and we have more parameters to estimate and we still have two constraints, different constraint, two kinds of constraints. second, we see the background language model and typically we also assume this is known. So segment of the topics to figure out which words are from which distribution and to figure out the first one of these topics. So before we have just one topic besides the background topical, but now we have more topics. So we first for example. Now, this kind of models can in general be used to mine multiple topics from text documents, and PLSA is one of the most basic topic models for doing this, so let's first examine this problem in a little more detail. A topical theta sub k. The probability that the world is generated from the background model is Lambda multiplied by the probability of the world from the background model, right? Two things must happen. Now all these are topics that we assume that exist in the text data, so the consequences that our switch for choosing a topic now is multiway switch before it's just a two way switch. You can see in the article we use words from all these distributions. So this one minus Lambda B gives us the probability of actually choosing a topic. So the next line then is simply to plug this in to calculate the probability of document. This represents the percentage of background words. That would believe exist in the text data and this can be unknown value that we set empirically. Just accounting for more documents in the collection. This is really key to know to for understanding all the topic models and indeed a lot of mixture models, so make sure that you really understand the probability. So the idea of PLSA is actually very similar to the two component mixture model that we have already introduced
 Represented by the topic. This is different. You can see this is the same discounted count, it tells us to what extent we should allocate this word to topic theta sub-j. We can assign a document to the topic cluster that's covered most in the document. In our case we are interested in all those coverage parameters-- pis--and word distributions, thetas. Allocated counts for each topic. And the higher probability words can be regarded as in belonging to one cluster. So we simply normalize this over all the words. So to summarize, we introduced the PLSA model, which is a mixture model with K unigram language models representing K topics. And similarly, the bottom one is to re-estimate the probability of word for topic. and re-estimate with normalizing. And that might give a different guess of word for word in different documents, and that's desirable. In fact, each topic can be regarded as a cluster, so we already have term clusters. And in addition to this, we can also cluster terms and cluster documents. So we're going to predict for word whether the word has come from one of these K+1 distributions. We can also aggregate topics covered in documents associated with a particular author, and then we can characterize the topics written by this author, etc. So, for example, we can normalize among all the topics to get re estimate of Pi the coverage. This tells us how likely this word is actually from theta sub-j, and when we multiply them together we get the discounted count that's allocated for topic theta sub-j and we normalize this over all the topics we get the distribution over all the topics to indicate the coverage. So in this case we can re estimate our coverage probability and this is re estimated based on collecting all the words in the document. The pis are tied to each document. We can assign the document to the topic cluster that has the highest pi. This gives us different distributions and these tells us how to improve the parameters? And as I just explained in both E step formulas, we have a maximum likelihood estimator based on the allocated word counts to topic theta sub-j. And then we're going to normalize them in different ways to obtain the re-estimate. Or we can renormalize based on the. This equation allows us to predict the probability that the word W in Document D is generated from topic theta sub j And the bottom one is the predicted probability that this word has been generated from the background. For all the words and that would give us a word distribution. It would be useful to take a comparison between the two. But the normalization is different because in this case we are interested in the word distribution. And we split words among the topics. And then we're going to look at the to what extent this word belongs to the topic's theta sub-j, and this part is our guess from E-step. Because this background language model can help attract the common terms. And such detailed characterization of coverage of topics in documents can enable a lot of further analysis. At all, sorry, these are probabilities of observing the word from each distribution, so you can see basically the prediction of word from topic theta sub-j is based on the probability of selecting that theta sub-j as a word distribution to begin to generate the world multiplied by the probability of observing the word from that distribution. So remember pis indicate to what extent each topic is covered in the document. And we just randomly normalize them. Note that we use Document D here to index the word. Namely this one. And, We show that with maximum likelihood estimator we can discover topical knowledge from text data
 When we re estimate word distributions, we are going to add. For example, we might have seen those tags topic tags assigned to documents. For example, we might expect to see retrieval models as a topic in information retrieval. We can use this prior, which is denoted as P of Lambda to encode. Document D will set to the Pi value to 0 for that topic. The document must be generated by using the topics corresponding to the assigned tags. So we can say the third topic should not be user generated. Now, a user may also have knowledge about the topic coverage. And those tag could be treated as topics if we do that, then a document that can only be generated using topics corresponding to the tags already assigned to the document. In this lecture, we're going to continue talking about topic models. Or we can prevent a topic from being used in generated document. We can also use the prior to favor set of parameters with topics that assign high probabilities to some particular words. In practice, when we apply PLSA to analyze text data, we might have additional knowledge that we want to inject to guide the analysis. For example, we can force the document D to choose topic one with probability of 1/2. And you may recall in Bayesian inference we use prior together with data to estimate parameters, and this is precisely what will happen. This is also very useful, but sometimes a user might have some expectations about which topics to analyze. And we may know which topic is definitely not covered in which document or is covered in the document. If the document is not assigned to a tag, we're going to say there's no way for using that topic to generate the document. And this is only reasonable, of course, when we have prior knowledge that strongly suggests this. But now we force this distribution. We have 0 pseudocounts because their probability is zero in the prior and when you see this is also controlled by a parameter mu and We're going to add mu multiplied by the probability of W given our prior distribution to the connected counts. to precise this distribution, so in this case it is this distribution, and that's why we said the background language model is in fact a way to enforce a prior, because we force one distribution to be exactly the same as what we give, that's the background distribution. In fact, we are going to set this one. For example, we can set any parameters for constraints, including zero as needed. So what would happen is that we're going to have an estimate that listens to the data and also listens to our prior preferences. We are going to just fit data as much as we can and get some insight about data. For example, we may want to set one of the pis to 0. And this would mean we don't allow that topic to participate in generating that document. Now this can be encoded as a prior because we can say the prior for the parameters is only a non zero if the plan does contain one topic that's equivalent to the background language model. So this is one way to impose our prior. And this is convenient for computation. So the plan for this lecture is to cover two things. We also may be interested in certain aspects such as battery and memory when looking at the opinions about the laptop, because the user is particularly interested in these aspects. When we re estimate the when we re estimate the this world distribution right? So this is the only step that changed and the changes happened here and before we just collect the counts of words that we believe have been generated from this topic. Basically is to maximize the posterior distribution probability and this is a combination of the likelihood of data and the prior. So the prior says that the distribution should contain one distribution that would assign high probabilities to battery, and life. To give more probabilities to these words by adding them to the pseudocounts so to artificially in effect, we artificially inflated their probabilities and to make this distribution we also need to add this many pseudocounts to the denominator. So the question is, how can we incorporate such knowledge into PLSA? It turns out that there's a. As a result, we can combine the two and the consequences that you can basically convert the influence of the prior into the influence of having additional pseudo data because the two functional forms are the same and they can be combined. The second is to extend the PLSA as a generative model fully generated model
 So these word distributions. To be more likely than others. Now in this model, all these parameters are free to change and we do not impose any prior, so these word distributions are now represented as theta i vectors. And this is for convenience to introduce LDA and we have one vector for each document. It means it can compute the topic coverage and topic word distributions as in PLSA. By using the parameters of alphas and beta. And in this case in theta we have one vector for each topic. So more specifically they will be drawn from 2 Dirichlet distributions respectively. Instead, we're going to force them to be drawn from another distribution. And you can achieve the same goal as PLSA for text mining. These parameters that we are interested in, namely the topics and the coverage, are no longer parameters in LDA. So this is a likelihood function for LDA. For example, you might favor a choice of relatively uniform distribution of all the topics, or you might favor generating skewed coverage of topics, and this is controlled by Alpha. So we cannot compute the pis for future document. And then we're going to use the pi to further choose which topic to use, and this is of course very similar to the PLSA model. And PLSA is the basic topic model, and in fact the most basic topic Model. The Dirichlet distribution is a distribution over vectors, so it gives us a probability for a particular choice of a vector. Now I've shown some distributions and they intuitively suggest the topic, but what exactly is the topic? Can we use phrases to label the topic to make it more easy to understand? And this paper is about the techniques for doing that. Dirichlet is just a special distribution that we can use to specify prior. The conclusion is that they tend to perform similarly. Now, once we impose these price than the generation process will be different an we all start with drawing pi's from this Dirichlet distribution and this pi will tell us these probabilities. Each topic is characterized by a word distribution, and we're going to also output proportions of these topics covered in each document. Take for example pis, right? So this Dirichlet distribution tell us which vector of pis is more likely, and this distribution itself is controlled by another vector of parameters of alpha's. A similar here we're not going to have these distributions free. You will see there are many fewer parameters. This, however, is not necessary problem for text mining because here we are often only interested in fitting the training documents that we have. So LDA is proposed to improve that and it basically to make PLSA a generative model by imposing a Dirichlet prior on the model parameters. So we can use the maximum likelihood estimated to compute that. The 2nd paper has a discussion about how to automatically label a topic model. So to summarize, our discussion of probabilistic topic models and these models provide a general principal way of mining and analyzing topics in texts with many applications. So it's very important to understand this. So this is a very important formula as I have stressed but multiple times and this is actually the core assumption in all the topic models and you might see other topic models that are extensions of LDA or PLSA and they all rely on this. In this case we have to use Bayesian inference or posterior inference to compute them based on the parameters Alpha and beta. So here and the other set of parameters are pis and we present as a vector also. Now you might think about how many parameters are there in LDA versus PLSA. And this is also often adequate for most applications. This has led to theoretically more appealing models. Unfortunately, this computation is intractable, so we generally have to resort to approximate. You can see why, and that's because the pies are needed to generate the document, but the pis are tied to the document that we have in the training data. And this gives us the probability of getting a word from a mixture model. You will see there are fewer parameters in LDA because in this case the only parameters are alphas and betas. The best basis test setup is to take tax data as input, and we're going to output the key topics. The third one is empirical comparison of LDA and PLSA for various tasks. So you will see them when you use different toolkits for LDA, or you read the papers about that these different extensions of LDA. The topic word distributions are drawn from another Dirichlet distribution with beta parameters and note that here Alpha has K parameters corresponding to our inference on the k values of pis for a document, whereas here beta has N values corresponding to controlling the N words in our vocabulary. And Secondly, it has many parameters and I've asked you to compute how many parameters exactly there are in PLSA and you will see there are many parameters . And there are many methods are available for and then. That's why we have to take the integral to consider all the possible pi's that we could possibly draw from this Dirichlet distribution
 Might. Results returned for a query. We may be able to cluster terms in this case. They are very different. So what is text clustering ? Clustering actually is a very general technique for data mining. This is very important that technique for doing topic mining an analysis. For assessing similarity. Terms are objects. For example, we might extract all the text segments about the topic, let's say by using a topic model. So for example we might cluster websites. So in this case documents are the units to be clustered. In particular, in this lecture organ to start with some basic questions about the clustering: What is text clustering and why we are interested in text clustering? In the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results. So we can treat all the articles published by author as one unit for Clustering. Now, once we've got those text objects, then we can cluster. A sense about the major topics or what are some typical or representative document in the collection? And clustering help us achieve this goal. We can understand what other major complaints about. And so, for example, we can cluster documents in the whole text collection. For example, they can be documents, turns, passages, sentences or websites.This lecture is the first one about the text clustering. So, for example, you might be interested in getting. And when the query is ambiguous, this is particularly useful. So for example. So there are in general many applications of text clustering any. And Cluster of terms can be used to define the concept or theme or topic. For example. We may also use text clustering to induce additional features to represent text data when we cluster documents together, we can treat each cluster as a feature and then we can say when a document is in this cluster and then the feature value would be one and if a document is not in this cluster, then the future value is zero and this helps provide additional discrimination that might be used for texture classification as we will discuss later. Sometimes they are about the same topic and by linking them together we can have more complete coverage of the topic. So more generally, why is text clustering interesting? Well, it's brcause it's a very useful technique for text mining, particularly exploratory text analysis. In this way, we might group authors together based on whether they are published papers or similar. So there are many examples of text clustering. One is to cluster search results for example and You can imagine a search engine can cluster the search results so that user can see overall structure of those. And the problem lies in how to define similarity. And when you define a clustering problem, it's important to specify your perspective for similarity or for defining the similarity that would be used to group similar objects 'cause otherwise. In fact, the topic models that you have seen some previous lectures. So this is a case of combining text clustering with some other techniques, and in general you will see a lot of text mining algorithms can be actually combined in a flexible way to achieve. Another example is to just a cluster any texts segments, for example passages, sentences or any segments that you can extract the from a large text objects. So as you can see clearly here, depending on the perspective will get different clustering results, so that also clearly tells us that in order to evaluate the clustering result we must use perspective. Without perspective, it's very hard to define what is the best clustering result. So are the two words similar It depends on how you look at it. The segments that we've got to discover interesting clusters that might also represent the subtopics. What do you mean by similar objects? Now this problem. These objects might be duplicated content for example, and in that case such a technique can help us remove redundancy, removing duplicated documents. And then you hope to get the sense about what are the overall content of the collection. Similarly, we can also cluster articles written by the same author, for example. Similarity is not well defined
 For document clustering where we hope this document will be generated from precisely one topic.This lecture is about the generative probabilistic models for text clustering. So this document must share some topics. for all. So that's N documents and topics. So here we see that when we generate a document. And here we hope to estimate a topic model or word distribution based on precisely one document, and that's when we assume that this document covers precisely one topic. We can also assume that we are going to assume there are fewer topics. So to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models. If we assume one topic is a cluster. Now this is not what we want to see for text clustering. One topic that we discussed earlier. Is theta one or theta two right? And then it's this probability is multiplied by the probability of observing this document from this particular distribution. Model that will give us a probability of a document. One is a set of topics denoted by Theta i's. Basically we make the decision once for this document and stay with this to generate all the words. But we can further allow these documents share topics and then. One is generating probabilistic models, which is the topic of this lecture, and later will also discuss similarity based approaches. If we really have one topic to correspond to one cluster of documents, then we would have a document to be generated from precisely one topic. As shown here. But in general, ideas are all mixture models that we can estimate these models by using the EM algorithm as we will discuss more later. For example, we can consider there are N documents, each covers different topic. Of course, in this case these documents are independent and these topics also independent. In this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering So this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. In this lecture we're going to talk about how to do text clustering, in general, as you see on this slide, there are two kinds of approaches. Of using a particular distribution is made of just once for this document. Now a main difference between this and text clustering problem is that here a document is assumed to possibly cover multiple topics, and indeed in general document will be covering more than one topic with non zero probabilities. So. The topics and each document that covers one topic, and we hope to embed such such preferences in a generative model. In text clustering, however, we only allow a document to cover one topic. Using a particular distribution is made just once for document clustering model, whereas in the topic model it's made multiple times. But when we generate each each word, we have to make a decision regarding which distribution we use. so in this case what's the likelihood function or it's going to be very similar to what we have seen before in topic models, but it will be also different. We generated each word independently. And if we have N documents for share k topics, then will again have precisely the document clustering problem. So this is what you're seeing here and we again would have to make a decision regarding which is distributing to use to generate document, because the document that could potentially be generated from any of the K word distributions that we have. It has allowed multiple topics to contribute the words to the document. So we have sum there for each word. But if you think about the main difference between this problem and the topic model that we talked about earlier and then you will see a main requirement is how can we force every document to be generated from precisely one topic instead of K topics? As in the topic model. topic model with two components here. So this is a detailed view of two component mixture model and when we have K components it looks similar. Here we show that we have input of text collection C and number of topics K and vocabulary V, and we hope to generate as output two things. Now we first make this decision regarding which distribution should be used to generate the world, and then we're going to use this distribution to sample word. So this is a topic coverage and it's also visualized here on this slide you can see that this is what we can get by using a topic model. So because of these connections, naturally we can think about how to use a probabilistic generating model to solve the problem of text clustering. So the question now is what generating model can be used to do clustering. The second is that word distribution here is going to be used to generate all the words for a document. Now inside this model there's also this, which of choosing a different distribution and we don't observe that, so that's a mixture model. As in all cases of designing a generative model, we hope the generative model would adopt the output that we hope to generate, or the structure that we hope to model. All the words in one document. But this time, once we have made the decision to choose one of the topics, we're going to stay with this distribution to generate the all the words in the document. And after that we can do is to look at the how to estimate the parameters. So that's the connection that we discussed earlier. Multiple distribution could have been used to generate the words in the document. We just stick with one particular distribution. So if you are map, this formula is kind of formula to our problem. That means if we change the topic definition just slightly by assuming that each document can only be generated by using precisely one topic. A product of two probabilities and one is the probability of choosing a distribution. So this form should be very similar to the topic model, but it's also useful to think about the difference and for that purpose I am also copying the probability of. So if you realize this problem, then we can naturally design alternative mixture model for doing clustering. But in the case of topic modeling, one distribution doesn't have to generate with all the words in a document. And of course, the main problem in document clustering is to infer
 For this mixture model. An P of theta i can be interpreted as. With two distributions in two component mixture model for document clustering.This lecture is a continued discussion of generative probabilistic models for text clustering. So this is precisely what we're going to use. This is general presentation of the mixture model for document clustering. But of course, given a particular document that we still have to infer which topic is more likely. But we have similar parameters. Now we can talk about how to do parameter estimation. So that means we're going to choose one of those distributions that gives D highest probability. So as more cases we follow these steps using a generated model. Now in this lecture, we're going to generalize this to include the K clusters. So this means once we can estimate the parameters of the model, then we can easily solve the problem of document clustering. We first choose a theta I according to probability of theta I and then generate all the words in the document using this distribution. Now, if you think about what we can get by estimate such a model, we can actually get more information than what we need for doing clustering, right? So theta. And they will tell us what the cluster is about. And then we talk about the model. So we're going to favor a cluster that's large and also consistent with the document. To generate the document so in that sense, we can still have a document dependent probability of clusters. In this lecture, we're going to continue talking about the tax capture text clustering, particularly generative probabilistic models. So next we have to discuss how to actually compute the estimate of the model. That most likely has been used to generate D. In this case, we design a mixture of K unigram language models. As you will see later. We have a set of theta i's denote the word distributions corresponding to the K unigram language models. In this case the prior is P of Theta I. So as a result, we can also recover some other interesting. And this, as you see later, would allow us to assign a document to the. Cluster that has the highest probability of being able to generate the document. It helps summarize what the cluster is about to look at the top terms in this cluster or in this word distribution. Note that it's important that we use this distributed generator. And together, that is, we're going to use the base formula to compute the posterior probability of Theta given D. We have P of each theta I as the probability of selecting each of the K distributions to generate the document. The more likely a cluster is used to generate the document, we can assume the larger the cluster size is. Think about the model. Of this equation
6 for Theta 1 so Theta 1 is more likely than Theta 2. That's most likely to have generated the document. So basically this normalizes the probability of generating this document by using this average word distribution.This lecture is a continued discussion of generative probabilistic models for text clustering. Most specifically, we're going to for example. Basically we need to re estimate all the parameters. Now how do we normalize them? These probabilities of these words must sum to one. In this lecture we're going to finish the discussion of generative probabilistic models for text clustering. And if this constraint then you can easy to compute this distribution as long as. And then so this is for P of Theta sub I. Given the document. So equally likely. So this is the same as what happened before for topic models. So to summarize, our discussion of generating models for clustering. We can use a normalized. So in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. Before we observe anything, we don't have any knowledge about which cluster is more likely, but after we have observed these documents, then we can collect the evidence. So once we have this problem that we can easily compute the probability of Z = 2 given this document, how we're going to use the constraint? Right now it's going to be 1 - 100 /101 1,000,000 one. These counts into probabilities so that the probabilities on all the words some to one. And more specifically, basically we're going to apply Bayes rule to infer, or which distribution is more likely to have generated this document or computing the posterior probability of the distribution. we're going to infer which distribution has been used to generate each document. Because that's the output from what mixture model. Discount them by the probabilities that each document is likely be generated from Theta 1. Instead, we can do that. what it is proportional to, right? So once you compute this product that you see here, then you simply normalize this these probabilities to make them some to 1 what over all the topics. I've shown some initializer values for the two distributions. An we know it's proportional to the probability of selecting this distribution P of Theta I and the probability of generating this whole document from that distribution, which is a product of all the probabilities of words for this document, as you see here. Indeed in this case the document is more likely to be generated from Theta 1,Â  much more likely than from than Theta 2. In this lecture, we're going to talk more about how exactly we're going to compute the maximum likelihood estimator. And so this gives us all the evidence about using topic. And this does the average distribution will be comperable to each of these distributions. I have two clusters. So that's E step after E Step we would know which distribution is more likely to have generated this document D, which is unlikely. There are two words, sorry, two occurrences of text and two occurrences of mining. So we can then divide the numerator and the denominator both by this normalizer. So there are many probabilities and they have to send one. So you can see the probability of Theta 2 would be naturally . To infer which cluster is more likely, and so this is proportional to the sum of the probability of Z sub DJ is equal to I. Use all the counts of text in these documents to estimate the probability of tax given still awhile, but we're not to use their raw counts or total account. A document that is generated by first choosing a unigram language model and then generating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models. So this is basically what I just said. One is P of Theta and this is the probability of selecting a particular distribution. What about these world probabilities? What we do the same? And intuition is the same, so we're going to see in order to estimate the probabilities of words in Theta one, we're going to look at which documents have been generated from Scylla and we're going to pull together the words in those documents and normalize them. So this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function looks like and we can also compute the maximum liklihood estimate to estimate the parameters. And here we just pulled the counts of words that are in documents that are inverted to have been generated from a particular topic Theta I here and this would allow us to then estimate how many words have actually been generated from Theta I. well, whereas the probability of a word given Theta is a probability distribution over all the words. So in our case the output from a mixture model or the observation from mixture model is a document not a word. And this probabilistic assignment that sometimes is useful for some applications. Note that it's very important to understand these constraints as they are precisely the normalizers in all these formulas, and it's also important to know that distribution is over what? For example, the probability of Theta is overall the key topics and that's why these K probabilities sum to 1. So there estimation involves two kinds of parameters. So from the E step we can see our estimate of which distribution is more likely to have generated a document, and you can see D1 is more likely from the first topic. How do we estimate that? Intuitively, you can just pull together the Z probability Z probabilities from E Steps, right? So if all these documents say they're more likely from silouan, then we intuitively would give a high probability to see that one right? So in this case, so we can just take the average of these probabilities that you see here, and we obtain the 
This lecture is about the similarity based approaches to text for clustering. So this is. And that is similarity based approaches. An term clusters can be in general generated by representing each term with some text content. Of course, term clusters can be generated by using generative models as well as we have seen. So this is a general idea. And this is. Both approaches can and generate the term clusters and document clusters. In this lecture, we're going to continue the discussion of how to do a text clustering. We also talked about the similarity based approaches. So it can be also shown this process will converge to a local minimum. We did not talk about it, but we can easily design generated model to generate a hierarchical clusters. Given K clusters. These approaches are more flexible. So the general idea of similarity based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects. It's harder to. I think about this process for a moment. Now this is in contrast with a generative model where we implicitly define the clustering bias. Can be expected to generate the loose clusters. And once we have these Central Eastside, we're going to assign a vector to the cluster hosts entry that is closest to the current vector. Now the next we're going to adjust the centroid, and this is very similar to M step where we re estimate the parameters. So to summarize our discussion of clustering methods, we first discussed the model based approaches, mainly the mixture model. In particular, we're going to cover a different kind of approaches than generative models. And so we're not going to have a probability, but we're going to just put one object into precisely one cluster. Now, there are many different methods for doing similarity based clustering. That's when we'll have a better estimate of the parameter. Now, if you think about how to implement this algorithm, you will realize that we have everything specified except for how to compute the group similarity. Now this is to have tentative. Now we're going to start with some tentative clustering result by just selecting Kate randomly selected vectors as centroids of K clusters and treat them as sentence as they represent each cluster. We are only given the similarity function or two objects, but as we group groups together we also need to assess the similarity between two groups. We can also use prior to further customize clustering algorithm to for example, control the topic of 1 cluster or multiple clusters. Now, these different ways of computing group similarities will need to different clustering algorithms, and they will generally give different results. But rather we make a choice. So basically we're going to measure the distance between this vector and each of the centroids, and see which one is closest to this one, and then just put this class this object into that cluster. So it's essentially similar to eastep. That's why we can use potentially a different model to recover different instruction. And then we can certainly cluster terms based on actually their tax representations. And the clustering structure is built into a generated model. So more specifically, we also initialize these. Sometimes we want to do that, but it's very hard to inject the such a explicit definition of similarity into such a model. Complex generative models can be used to discover complex clustering structures. Which is called K means clustering will represent each text object as a term vector and then assuming similarity function defined onto objects. Or we can use the threshold to cut or we can cut at this high level to get just the two clusters. In that when we allocate vector into one of the clusters based on our tentative clustering, it's very similar to inferring the distribution that has been used with generally the document in the mixture model. So start with all the text objects and we can then measure the similarity between them. Maybe these two now have the highest similarity. Every link is in between, so it takes average of all these pairs. That is, to ensure the objects that are put in the same group to be similar, but the objects that are put into different groups to be not similar, and these are the general goals of clustering. Now in this case it's also based on individual decision, so it could be sensitive to outliers. Directly specify similarity functions. For example, take the context of each term as a representation of each term as we have done in paradigmatic relation learning. So we're going to make sure that if the two groups are having a high similarity, then every pair of the two the objects in the group will be insured to have high similarity. Average link defines the similarity as average of similarity of all the pairs of the two groups. In this case, we are giving a similarity function calls to measure similarity between two objects and then we can gradually group similar objects together in a bottom up profession to form larger and larger groups, and they also form a hierarchy and then we can stop when some stopping criterions that. So here we have a better clustering result by adjusting the centroid. Sometimes they may be very far away. And then we're going to iteratively improve it, and the process goes like this. So once we have a similarity function, we can then aim at optimally partitioning to partitioning the data into clusters or into different groups. However, one disadvantage of this approach is that there is no easy way to direct or control the similarity measure. In the E step, however, we do a probabilistic location, so we split the counts. Indeed, this algorithm is very similar to the EM algorithm for the mixture model for clustering. It might remind you the EM algorithm for mixture model. As long as they are very close orders, say the two groups are very. So in this case these two pairs objects would define the similarity of the two groups. A cluster, so this is also similar to the M step, where we do counts pull together counter and normalize them, or the difference of course is also because of the difference in the instep, and we're not going to consider probabilities when we count the points in this case, for K means we're going to only count the objects allocated to this cluster, and this is only a subset of data points. We're going to make a call if this data point is closest to cluster two that were going to say you are in class too. So one is bottom up that can be called agglomerative, where we gradually Group A similar object into larger and larger clusters until we group everything together. So here are two groups G1 and G2 with some objects in each group, and we know how to compute the similarity between two objects. But other times you might need a tight clusters, then completely completely better, but in general you have to empirically evaluate these methods for your application to know which one is better. And then we can gradually group them together in every time we're going to pick the highest similarity similarity pairs to group
This lecture is about evaluation of text cluster. There are many approaches that can be used for text clustering and we discussed them: Model based approaches and similarity based approaches. In general, we can evaluate text clusters in two ways. This is often needed to explore text data. So to summarize text clustering, it's a very useful unsupervised general text mining technique as particularly useful for obtaining an overall picture of the text content. In this case what we care about is the contribution of clustering to some application. F measure is another possible measure. The 2nd way to evaluate text clusters is to do indirect evaluation. So we often have a baseline system to compare with. And they will use their judgments based on the need of a particular application to generate what they think are the best clustering results. This could be the current system for doing something and then you hope to add clustering to improve it or the baseline system could be using a different clustering method and you then what you are trying to experiment with and you hope to have a better idea for clustering. So what counts as the best clustering result would be dependent on the application. So this perspective is also very important for evaluation. And this is sometimes desirable. So in this case the question to answer is how useful are the clustering results for the intended applications? Now this of course is application specific question, so usefulness is is going to depend on specific applications. So far we have talked about multiple ways of doing text clustering but how do we know which method works the best? So this has to do with evaluation. One is direct evaluation and the other is indirect evaluation. Because two objects can be similar depending on how you look at them, we must clearly specify the perspective of similarity. And finally, evaluation of clustering results and can be done both directly and indirectly. So here I just want to discuss some high level ideas that would allow you to think about how to do evaluation in your applications. And that's precisely why the perspective or clustering bias is crucial for evaluation. Mutual information captures the correlation between these cluster labels and normalized mutual information is often used for quantifying the similarity for this evaluation purpose. Now how do we do that exactly? The general procedure would look like this. And finally, you can see in this case we essentially inject the clustering bias by using humans. For example, if you are clustering search results, then obviously you don't want to generate 100 clusters, right? So the number can be dictated by the interface design. If you look at this slide and you can see we have two different ways to cluster these shapes. And then we're going to compare the performance of your clustering system and the baseline system in terms of the performance measure for that particular application. And ideally we want the system results to be the same as human generated results, but in general they are not going to be the same, so we would like to then qualify the similarity between the system generated clusters and the gold standard clusters, and this similarity can be also measured from multiple perspectives and this will give us various measures to quantitatively evaluate a cluster clustering result and some of the commonly used measures include purity, which measures whether a cluster has similar objects from the same cluster in the gold standard and normalized mutual information is a commonly used measure which basically measures based on the identity of or the cluster of object in the system-generated results. And this would be then used to compare with the system generated clusters from the same test set
 to text content. Distinguish it relevant documents from non relevant documents for a particular query. And then we're going to talk about how to do text categorisation followed by how to evaluate the categorisation results so. That's when you have multiple categorization tasks that are related. First is related to topic mining analysis. When we do text categorization, we have a lot of text objects to be processed by a categorisation system.This lecture is about the text categorization. For example, topic categories. Applications of text categorisation. These are categories that characterize content of text object. Often together with non text data specifically to text. Finally, it's also related to text based prediction. That they produce. Secondly, it's also related to opinion mining and sentiment analysis, which has to do with discovering knowledge about the observer that the human sensor. In this lecture we're going to talk about the text categorization. Or we can have any other meaningful categories associated with text data, as long as. There could be more than two categories, so topical categorisation is often such example where you can have multiple topics. For example, K category categorisation task can be actually performed by using binary categorization. In that case, we can treat this as a categorization problem. For example, authors or entities associated with the content that they produce. It is relevant to discovery of various different kinds of knowledge as shown here. And so this is a very important technique for text data mining. First we're going to talk about what is text categorization and why we are interested in doing that in this lecture. One is text Categorization helps us enrich text representation, and that's to achieve more understanding of text data that's always useful for text analysis. So for example, sentiment categories could be already very useful, or author Attribution might be directly useful. So now with categorisation, text can be represented in multiple levels, meaning keyword bag of words representation as often used for a lot of text processing tasks. And text categorisation allows us to infer the properties of such entities that are associated with text data. So this is a general way to allow us to use text mining tool. So without doing categorization it will be much harder to aggregate such opinions. Sorry, text categorization to discover knowledge about the world. This is a very important technique for a text, data mining and analytics. And the system will in general assign categories to these documents as shown on the right. Sentiment categorization of product reviews or tweets is yet another kind of applications where we can categorize content into positive or negative or positive and negative or neutral. Analytics, where we are often interested in using text data as extra sensor data collected from humans to infer certain desicion factors. Semantic categories assigned can also be directly or indirectly useful for application. We can further categorize into sub categories etc. In nature this all the problems are all the same and that's as we defined and it's a text categorization problem. Or sentiment categories and they generally have to do with the content of the tax objects, direct Characterization of the content. And so we can use their content, determine which author has written which part, for example, and that's called author attribution. We're given a set of predefined categories. Try to leverage the dependents of these tasks to improve accuracy for each individual task. Secondly, categories can also vary, and we can generally distinguish two kinds of categories. So first text objects can vary, so we can categorize a document. So this means we can use text categorization to discover knowledge about the world in general, as long as we can associate the entity with text data, we can always use the text data to help categorize the corresponding entities. Once we can make the connection, then we can make predictions about those values. Another example is when semantic categories can facilitate aggregation of tax content, and this is another case of. For example, we might collect a lot of reviews about a restaurant. So it's useful to think about the information network that will connect the other entities with text data. And the categorisation results. The problem of texture categorisation is defined as follows. And we often assume the availability of training examples, and these are the documents that are tagged with known categories, and these examples are very important for helping the system to learn patterns in different categories, and this would further help the system then learn how to recognize. And that's because it has to do with analyzing text data based on some predefined topics. So this is example of external category. And then these text data can help us infer properties of product or a restaurant. Or a lot of reviews about the product. So also two categories. And this is ontology of terms characterize content of literature articles in detail. Because we can often use text categorization techniques to predict some variables in the real world that are only remotely related to text data
 Conditional. The main topic could be another topic, different topic then sports. So all these. So in this lecture were going to discuss how to do text categorization.This lecture is about the methods for text categorization. And once we can estimate these models, then we can compute this conditional probability of label given data based on. To learn a classifier to map a value of X into a map of Y. But they're not as powerful as non linear combination, but nonlinear models might be more complex for training. Now we also should have sufficient knowledge. For example, if there is some special vocabulary that is known to only occur in a particular category, and that would be most effective because we can easily use such a vocabulary or pattern of such a vocabulary to recognize this category. So soft rules just means we're going to still decide which category should be assigned to the document. Obviously we can't do this for all kinds of categorization problems. Things like that and this would allow us to deterministically decide which category A document should be put into. So these discriminative classifiers attempted to model the. And then the train classifier can be applied to a new text object to predict the most likely category, and that's to simulate the prediction of what a human would assign to this text object. You can easily identify text data. So we need to provide some basic features for the computers to look into. So we first model distribution of labels and then we model how the data is generated given a particular label here. More specifically, we have to use human experts to help in two ways. Now this is the most important thing 'cause this conditional probability of the label can then be used directly to decide which label is most likely. And, this can then be factored out to a product of Y. So using each word as a feature is a very common choice to start with. So in general, all methods would rely on discriminating features of text objects to distinguish different categories, so that's why these features are very important and they have to be provided by humans. But one can also imagine some text articles that mention these keywords. It's the basis for learning. But we're not going to say exactly for sure, but instead we're going to use probabilities or weights so that we can combine multiple evidences, so the learning process basically is going to figure out which features are most useful for separating different categories. In this case. So in general, we can distinguish the two kinds of classifiers at a high level one is going to generative classifiers. First, the human experts must annotate datasets with category labels, will tell the computer which documents should not receive which categories. So that's one disadvantage of this approach, and then finally the rules may be inconsistent and this would need to concern about robustness more specifically, and sometimes the results of categorization may be different depending on which rule to be applied. There are many methods for text categorization In such a method, the idea is to determine the category based on some rules that we design carefully to reflect the domain knowledge about the categorization problem. And this is called a training data. We have to do it From scratch for a different problem, becauses different rules would be needed so it doesn't scale up as well. And in the case of text, natural choice would be the words. But that may not be exactly about the sports, or only marginally touching sports. So in such approaches, the objective function is actually likelihood, so we model how the data are generated, so only thus it only indirectly captures the training errors. But it's not going to be used using a rule that is deterministic, so we might use something similar to saying that if it matches game sports many times, it's likely to be a sports. They also tend to vary in their ways of combining the features, so linear combination for example is simple is often used. Secondly, the categories have to be easy to distinguish based on surface features in text, so that means superficial features like keywords or punctuations or whatever. But if we can model the data in each category accurately, then we can also classify accurately. So when we use machine learning for text categorization, we can also talk about the problem in the general setting of supervised learning. So once human experts can provide this, then we can use machine learning to learn soft rules for categorization from the training data
 To estimate all these parameters.This lecture is about how to use generative probabilistic models for text categorization. And so this is a slide that we've talked about before about text clustering, where we assume there are multiple topics represented by word distributions. Before we observe any document. So what are those words? Well those are the common words. So this is a score of a document for these two categories. Then what we can do is precisely like what we did for text clustering. We would then favor this category slightly. And this is what we can collect from document. And we simply just normalize this count to make this a probability. And so naturally, we can then decompose this likelihood into a product. This time we can do this for each category. So here we're going to collect evidence about which category is more likely. So which word has higher probability? Well, we simply count the word occurrences in the documents that are known to be generated from theta i. We'll say that it's more likely to be from category One, and the more we observe such a word, the more likely the document will be classified as theta one. In this lecture, we're going to talk about the generative models. Once you see some word and other words will more likely occur. There are in general are two kinds of approaches to text categorization by using machine learning. So basically with naive Bayes classifier, we're going to score each category for a document by this function. The other is a likelihood part, that is this part. Indeed, in text categorization, we typically assume that there are training data available and those are the documents that are known to have been generated from which category. So in this case it will be useful for you to think about what are the constraints on these two kinds of probabilities. The probability of theta i and this indicates how popular each category is or how likely we would have observed the document in that category. Or we can understand how we can use generative models to do text categorization from the perspective of clustering. So how do we do smoothing? Well in general we added pseudo counts to these events. These documents have been all generated from category one, namely have been all generated using this same word distribution. So more rigorously, this is what we would be doing, so we're going to choose the topic that will maximize this posterior probability of the topic given the document. Now, this addition of background model would cause nonuniform smoothing of this word distributions we are going to bring the probability of those common words, or to a higher level because of the background model. So let's say we are considering category I or Theta I. Namely, we are going to assign document D to the category that has the highest probability of generating this document. when a word doesn't occur in the document. So in general, in naiyes bayes categorization we have to do such smoothing and once we have these probabilities, then we can compute the score for each category for a document and then choose the category with the highest score as we discussed earlier. That we need to consider if a topic or cluster has a higher prior then it's more likely that the document has been from this cluster, so we should favor such a cluster. Theta sub b Now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model. An we now can see clearly how we can assign a documentary to a category based on the information about word distributions for these categories and the prior on these categories. When we add Delta because we're going to add a constant pseudo count to every word. What would happen? Or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. And we're going to score based on this probability ratio. That's just because these documents are known to be generated from a specific category, so once we know that it's in some sense irrelevant what other categories we are also dealing with. It is no longer mixture model. And then we just normalize these counts to make this distribution of all the words make all the probabilities of all these words sum to one. In other words, we're going to maximize this posterior probability as well. Essentially we are comparing the probability of the word from the two distributions and if it's higher according to theta one, then according to theta 2 then this weight would be positive and therefore it means when we observe such a word. Now, how can you compute the probability that particular topic word distributions theta i has been used to generate this document? In general, we use bayes rule to make this inference. To what extent observing this word helps contributing to our decision to put this document in Category One. One is generative probabilistic models, the other is discriminative approaches. So to estimate the probability of each category. Get posterior becausw this one P of Theta i is the prior, that's our belief about which topic is more likely. So the idea then is to just use the observed training data to estimate these two probabilities. Now if we do this generalization, what we see is that in general we can represent the document by feature vector F, FI here. And we want to pick a topic that's high by both values. will likely have higher probability, and that's just the maximum likelihood estimator indeed, and that's what we could do. Because every category has some help from their background for words, like the, a which have high probabilities. Therefore in the sum we have to also add K multiplied by Delta as a total pseudo counts that we add to the estimate. So the estimation problem of course would be simplified, but in general you can imagine what we want to do is to estimate these probabilities that I marked here and what are the probabilities that we have to estimate in order to do categorization where there are two kinds. So the problem of text categorization is actually very similar to document clustering in that we assume that each document belongs to one category or one cluster. Therefore it's no longer so important that each category has documents that contain such a lot of occurrences of such word, or the estimate is more influenced by the background model and the consequences that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities. So we have to do smoothing to make sure it's not zero probability. Right, so think for a moment that how do you use all these training data, including all these documents that are known to be in these K categories. Right, so these are the words that are observed in the document, but in general we can consider all the words in the vocabulary. We know there is one word distribution that has been used to generate documents. In other words, these are the documents with known categories assigned, and of course human experts must do that. And in general we can do this separately for different categories. We pretend that every category has actually some extra number of documents represented by Delta. That means the more we observe such a word, the more likely the document is actually from theta 2. We have observed some data from some model and we want to guess the parameters of this model. And this is generally not accurate. And what about the basis for estimating the probability of word in each category? Well, the same and you'll be just assuming that words that are observed frequently in the documents that are known to be generated from a category. And this is related to how well this word distribution explains the document here, and the two are related in this way. And this has to do with whether the topic word distribution can explain the content of this document well. So once we estimate such model, we faced the problem of deciding which cluster document d should belong to and this question boils down to decide which theta i has been used to generate D. Then the estimated probability would be 0 in this case if we have not seen a word in the training documents for, let's say, category I, then our estimate would be 0 for the probability of this word in this category. So in this case our prior knowledge says that no words should have zero probability, so smoothing allows us to inject this prior to make sure that no word has a zero probability. Now we can do the same for the word distribution, but in this case we sometimes we find it useful to use a non-uniform pseudo counts for the words. But this conditional probability here Is the posterior probability of the topic after we have observed the document d. And in the denominator we also add K multiplied by Delta because we want the probability to sum to one. Now the question is what will be your guess or estimate of the probability of each word in this distribution and what will be your guess of the prior probability of this category? Of course, this second probability depends on how likely that you will see documents in other categories. The other kind is word distributions and we want to know what words have high probabilities for each category. T sub I and that's denoted by C of w and T sub I. All evidence that we can collect from all these features. Obviously we don't want to do that. When the data set is small, we tend to rely on some prior knowledge to to solve the problem. So one is the prior. Therefore this is the negative evidence for supporting category one. then based on these categories and training data, we would like to allocate a document to one of these categories, or sometimes multiple categories. But because of the similarity of the two problems, we can actually adapt document clustering models for text categorization. So this is just something that you have seen in document clustering. So in nature they will be all using Bayes rule to do classification, but the actual generative model for documents in each category. So what are the parameters? Well These betas are the weights, and with appropriate settings of weights then we can expect the such a scoring function to work well to classify documents. Actually, this general form is very close to a classifier called logistical regression, and this is actually one of those conditional approaches or discriminative approaches to classification. And this is also called IDF weighting inverse document frequency weighting that you have seen in mining word relations. Right, so it's suppose we have just two categories and we're going to score based on their Ratio of probability, so this is Ann let's say this is our scoring function for two categories. So if the ratio is larger then it means it's more likely to be in category one, so the larger the score is, the more likely the document is in category One. This is often the function that we actually use to score each category, and then we're going to choose the category that has the highest score by this function. And here you see that T1 represents the set of documents that are known to have been generated from category one, and T2 represents the documents that are known to have been generated from category two, etc. Main difference is that in clustering we don't really know what are the predefined categories or what are the clusters. So in this case you can see this is a proportional to the count of the word in the collection of training documents. Each topic is 1 cluster. We could, for example, assume the words may be dependent on each other, so that would make it a bigram language model or trigram language model. Obvious Delta is a smoothing parameter here, meaning that the larger delta is and the more we will do smoothing and that means we'll more rely on pseudo counts and we might indeed ignore the actual counts if delta is set to Infinity. For example, if you have seen a word like a text, and then it makes categorization or clustering more likely to appear And if you have not seen text. And to answer the question which category is most popular, then we can simply normalize the count of documents in each category. And this is related to the prior and the likelihood an as you have seen on the previous slide. So smoothing is the important technique to address data sparseness. But you should know that this kind of model doesn't have to make this assumption. And this count of the word serves as a feature and to represent the document. First, what's the basis for estimating the prior or the probability of each category? Well, this has to do with whether you have observed a lot of documents from that category. So now the question is, how can we make sure each theta i actually represents category i accurate? Now, in clustering we learned this category i or the word distributions for category i from the data. Suppose D has L words represent represent as Xi here. So effectively we're just have the product over all the words in the document
 That are denoted as X. Since these are training documents, we know they're categories. When I maximize this one.This lecture is about the discriminative classifiers for text categorization. Note that this is a conditional probability of Y given X. And if you think about when we want to maximize this probability we will basically going to want this probability to be as high as possible when the label is one. Because they sum to one. Now this similarity function. If it's one will be taking this form. So in our case we are interested in maximizing this conditional likelihood. So in this lecture we're going to introduce some discriminative classifiers. As you see. It's not like a modeling X, but rather we're going to model this. And then we're going to assign the category that's most common in these neighbors. So the key assumption that we made in this approach is that the distribution of the label given the document or probability of a category given document. In this lecture, we're going to continue talking about how to do text categorization and cover discriminative approaches. Now this kind of scoring function can be actually a general scoring function where we can in general represent text data as a feature vector. Then what we are saying is that in order to estimate the probability of a category given a document, we can try to estimate the probability of the category given that entire region. Basically now we can use the known categories of all the documents in this region to estimate this probability. So as I mentioned that KNN can be actually regarded as estimate of conditional probability of Y given X, and that's why we put this in the category of discriminative approaches. Since we're not modeling the generation of this vector and we can plug in any signals that we want, so this is potentially advantages for doing text categorization. As directly as a function of these features. If we have all the betavalues already known, all we need is to compute The Xi's for that document. For example, probability of theta I given document D is locally smoothed and that just means we're going to assume that this probability is the same for all the documents in this region. Since we have a probability here. The probability that the document is in category one. In this approach, we're going to also estimate the conditional probability of label. And we mentioned that this is precisely similar to logistic regression. But mathematically, this can also be regarded as a way to directly estimate the conditional probability of label given data that is P of Y given X. But what about this if it's 0? Well, if it's zero then we have to use a different form and that's this one. Y I if it's one, it has a different form than when it's 0. Basically, we're going to allow these neighbors to vote for the category of the object that we're interested in classifying. It's equivalent to minimize this one. So according to this approach we're going to find the neighbors. That means the document is in topic one. If D is not different, very different than we're going to assume that the probability of theta given D would be also similar, and so that's a very key assumption, and that that's. You can see, and that's precisely what we want. And this is also precisely what we want for classification. An here we use the notation of X vector, which is more common when we Introduce such machine learning algorithms, so X is our input, it's a vector. So most specifically, we assume that log of the ratio of probability of y = 1 and the probability of y = 0. And then denominator is just a total number of documents training documents in this region, so this gives us a rough estimate of which category is most popular in this neighborhood, and we're going to assign the popular category to our data objective since it falls into this region. Of course the features don't have to be all the words and their features can be other signals that we want to use. So given a particular XI, how likely we are going to observe the corresponding Y i of course, Y I could be one or zero and in fact the function form here would vary depending on whether Y sub I is one or zero. But if the document is not we are going to maximize this value, and what's going to happen is actually to make this value as small as possible. So you can see basically the if we maximize the conditional likelihood we're going to basically try to make the prediction on the training data as accurate as possible. They try to model the conditional distribution of labels given the data directly rather than using Bayes rule to compute that indirectly. And another advantage of this kind of approach is that it would allow many other features than words to be used in this vector. And so in logistic regression, we basically assume that the probability of y = 1Â  given X is dependent on this linear combination of all these features. But here we actually would assume explicitly that we would model our Probability of Y given X. Basically the results might depend on the K and indeed K is an important parameter to optimize. Now in general, I should say there are many such approaches. And then plugging those values that will give us a estimate. The condition likelihood here is basically to model y given the observed X. Basically, you're going to separate your training data into two parts and then you're going to use one part to actually help you choose. So the idea is to keep all the training examples and then once we see a text object that we want to classify, we're going to find the K examples in the training set and that are most similar to this text object. And with M features. And so it's a function of X, and it's a linear combination of these feature values, controlled by beta values. So this is a log odds ratio. Now how do we get this one? That's just one minus the probability of y = 1, right? And you can easily see this now. So, as in other cases, when compute the maximum likelihood estimator Basically lets go find a beta value, a set of beta values that will maximize this conditional likelihood. So the parameter as there has to be set empirically and typically you can optimize such a parameter by using cross validation. So in our categorization problem we have two categories, lets say theta 1 and theta 2, and we can use the Y value to denote the two categories. And suppose we draw a neighborhood and we're going to assume in this neighborhood, since the data instances are very similar, we're going to assume that the conditional distribution of the label, given the data, would be roughly the same. Obviously the assumption would be a problem, and then the classifier would not be accurate. Now that means if most of them have a particular category, lets say category 1 then we're going to say this current object will have category one. So this is the basic idea of logistic regression, and it's a very useful classifier that can be used to do a lot of classification tasks, including text categorization. And since we know that probability of y = 0 is 1 minus probability of y = 1, and this can be also written in this way. So that the Xs will be mapped into a range of values from zero to 1. In general, we're going to adjust these parameter values, optimize the performance of classifier on the training data. Let's proceed with this assumption. So the numerator that you see here c of Theta and R is a count of the documents in region R with category Theta I. So if we rewrite this question to actually express the probability of Y given X in terms of X by taking by getting rid of the logarithm and we get this functional form and this is called a logistical function, it's a transformation of X into Y. There are other methods as well, but in the end will we're going to get the set of beta values once we have the beta values, then we have a well defined scoring function to help us classify a document right? So what's the function? Well, it's this one. And when Y is 1 it means the category of the documents first class theta 1 Now the goal here is to model the conditional probability of Y given X directly as opposed to model the generation of X&amp;Y as in the case of Naive Bayes
 An another common scene is that they might be. So this is. As you will see. As features. Discriminative classifiers for text categorization. An we want to classify documents into these two categories and we're going to represent again a document by a feature vector X here. And this is. But We can. This can allow you to obtain. So we see transpose of W vector multiplied by the feature vector. Functions about it would be positive. So this is our assumption or setup. So. Humans invention for communication and they are generous sufficient for representing content for many tasks. So this to have a small value for this expression. Imagine the words are original feature representation, but the representation can be mapped to the topic space representation. And then try to somehow align these categories with the categories defined by the training data where we already know which documents are in which category. So that means these methods can be actually applied to any text categorization problem as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all these classifiers can be easily applied to those. For example word count. Why I multiplied by the classifier value must be larger than or equal to 1? An obviously when? Why is just one you see. I so let's just assume that beta one is negative and beta two is positive. So again we let's recall that our classifier is such a linear separator where we have weights for all the features and the main goal is to learn these weights W&amp;B. So feature design tends to be more important than the choice of specific classifier. A line defined by just three parameters here beta0 beta 1 beta 2. And there are instances do represented as X. So to train a sentiment categorizer meaning we want to distinguish positive from negative opinions and categorize reviews into these two categories then. And we want this value to be less than or equal to negative one. This is where it's convenient when we have chosen why I as negative one for the other category cause it turns out that we can easily combine the two into one constraint. And you can imagine once we know which are support vectors, then this center separate line will be determined by them so. But then they can still be useful. Now the margin can be shown to be related to the magnitude of the weights. We could assume five star reviews are all positive training examples. Although in Texas domain cause words are excellent representation of text content because these are. Do that effectively. Meaning to recognize the weights. Let's say we have K topics, so a document cannot be represented as a vector of justice K values corresponding to the topics. But there are also some ways to help with this, so one is to assume some low quality training examples can also be used so those can be called a pseudo training examples. The words. And as we said, if this value is positive we're gonna say this is in category one, and if it's negative it's going to be in category Two. We want every instance we classified accurately, but if we allow this to be. So now you can see this is basically optimization problem, right? We have some variables to optimize and these are the weights and B and we have some constraints. So as a general rule, and if you can improve the feature representation an and then provide more training data, then you can generate do better. So that just means if Yi, the known label, for instance XI is one we would like this classify value to be large. This way you can. Now this is equivalent to finding values for W&amp;B because they would determine where exactly the separator is. Now we also have the objective that's Tide to maximization of margin and this is simply to maximize sorry to minimize W transpose multiplied by W and we often denote this by file W.This lecture is a continued discussion of. So P is a biased constant and W is a set of weights and with one wait for each feature we have M features and so have aim weights and are represented as a vector. Some error to the constraint so that now we allow. Is the heading in this direction, so it shows that as we increase X1, X2 will also increase. The cause from the unlabeled data we some are labeled as category ones and more labeled as category two. So you can see the margin of this side is as I've shown here. So this is the basic idea of ecfmg. So know that beta1 and beta2 have different signs or one is negative and there is positive. An so the original word features can be also combined with such such latent dimension features or low dimensional space features to provide a multiresolution representation, which is often very useful. So it turns out that we can easily modify it as VM to accommodate this. So what do you think? Basically working to evaluate its value by using this function. It's another kind of classifier where you can have intermediate features embedded in the model so that it's highly non linear classifier. So in our case actually it's easy to see the mixture model can be used for both text clustering and categorisation, so even imagine if you have a lot of unable text data for categorization then you can actually do clustering on these text data to learn categories. So in this lecture will introduce yet another discriminative classifier called a support vector machine or VM, which is a very popular classification method, and there has been also shown to be effective for text categorization. We're going to choose a linear separator to maximize the margin. Typical biclustering of features in various ways, so metrics factorization has been used to do such a job, and this and some of the techniques are after very similar to the topic models that we discussed, so topic models. Here that you see and it's very similar to what you have seen or just for logistic regression. So we want to then also minimize this CI. That means this will be positive, right? So we know in general for all the points on this side, the. And symbol approaches that would combine different methods and tend to be more robust and can be useful in practice. So we could imagine there are other lines that can do the same job. The training laid out would be basically like a in other classifiers we have a set of training points where we know the X vector and then we also the corresponding label, why I? An here we define why I as two values, but these two values are not 01 as you have seen before, but rather negative one and positive one and their corresponding to these two categories as I've shown here. Now the idea of this classifier is do design. And the classifier will say X is in category one if it's positive. So this is a quadratic program with linear constraints and there are standard algorithms that are available for solving this problem. And once we solve, the problem, will obtain the weights W&amp;B and then this would give us a well defined the classifier, so we can then use this classifier to classify any new texture objects. And then we're going to assume the high confidence classification results, or actually reliable. It means all the eyes must be small. So apparently this parameter C has to be actually set. To allow us to characterize content of text concisely with categories or the predictor, some properties of real world variables that are associated with text data. The decision boundary between two categories. These are the two different instances, different kinds of cases and how can we combine them together now. Most techniques that we introduce the use supervised machine learning and which is a very general method. To improve categorization method and so this idea is very powerful and when the enable data and training data are very different and we might need to use other advanced machine learning techniques called domain adaptation or transfer learning, this is when we can borrow some training examples from a related problem that may be different or from a categorisation task that. So what that means is that the F value for this point should be higher than the F value for this point on the line. So we've just assume that we have a constraint for the getting the data on the training set to be classified correctly. I can you can verify when we multiply these two vectors together, take the dot product that we get the same form of the NIA separate as you have seen before. We also mention negative opinions so that rain example is not all of that high quality, but they can still be useful. So in the linear is UVM, we're going to then seek these parameter values to optimize the margins and then the training error. So what do you think is the sign of this expression? To examine the sign, we can simply look at this expression. Now I use this way so that it's more consistent with what notations people usually use when they talk about SVM. To maximize the margin. So specifically, you will see we have added something to the optimization problem. Otherwise, we're going to say it's in Category 2, so that makes 0 value. And what would happen if we. But the optimization problem will be very similar. Basically you look at the empirical data to see what values should be set to in order to optimize the performance. And this line, of course, is determined by the vector beta, the coefficients, different coefficient will give us a different line. So here are illustrated to support vectors for one class and two for the other class. The feature representation is very critical an so that these methods all require effective feature representation and to design effective feature set that we need domain knowledge and humans definitely play important role here. It has shown some promise and one important advantage of this approach in relationship with the feature design is that they can learn intermediate representations or compound features automatically, and this is very valuable for learning effective representation for text localization. In this case this corresponds to a line that you can see here. Basically, we can use, let's say a naive Bayes classifier to classify all the unlabeled text documents. So what you see here is very similar to what you have seen before, but we have introduced the extra variables. So because of this reason, the value of deep learning for text processing tends to be lower than for computer vision and speech recognition, where there aren't corresponding wedding design. So let's assume they are actually training label examples and then we combine them with the true training examples. And then finally we can leverage some machine learning techniques. But deep learning is still very promising for learning effective features, especially for complicated tasks like a sentiment analysis, and has been shown to be effective because it can provide replenishing that goes beyond bag of words. An another way to figure out effective features is to do error analysis on the categorisation results. Be performing similarly on the data set but with different mistakes and so their performance might be similar, but then the mistakes that make might be different, so that means it's useful to compare different methods for particular problem and then maybe combine multiple methods 'cause this can improve the robustness and they want to make the same mistakes so. But basically when the two domains are very different than we need to be careful not to overfit the training domain, but yet we can still want to use some signals from the related training data. So for example, feature selection is a technique that we haven't really talked about, but it's very important and it has to do with trying to select the most useful features before you actually trainer for classifier, and sometimes training a classifier would also help you identify which features have high values. Now one question is to take a point like this one and to ask the question what's the value of this expression or this classifier for this data point. The computers of course here are trying to optimize the combinations of the features provided by human an. So we can let each topic define one dimension. That means they may not look as nice as you have seen on the previous slide where align can separate all of them. And there are also other ways to ensure the sparsity of the model. And we're going to also say that if the sign of this function value is positive, then we're going to say the object is in Category 1. So for example, training categorisation on news might not give you an immediately effective classifier for classifying topics in tweets, but you can still learn something from news to help categorizing tweets, so there are machine learning techniques that can help you. That involves data that follow very different distributions from what we are working on. Now you might wonder why we don't define them as zero and one, but instead of having negative 11 and this is purely for mathematical convenience, as you will see in a moment. The error can be very, very large, so naturally we don't want this to happen. So the logistical regression classifier that you have seen earlier actually uses some criteria to determine where this line should be, and it's a linear separate as well and uses a conditional likelihood on the training data to determine which line is the best
 This. Basically has assigned this category to this document or no, so this is denoted by Y or N. So we will see. To understand these will have to. In multiplied by K. For short documents. So these are all the cases where this human says the document should have this category. Categories have been assigned to those documents by humans and we want to quantify the similarity of these decisions. And we're going to compare our systems decisions on which documents should get which category with what. An we when we are interested in knowing the relative difference of these methods. So ideally we would like to model such differences. For example, it might be also interested in knowing which category performs better, which category is easy to categorize, etc. That's the system to decision. An recall would tell us has the category being actually assigned to all the documents that should have this category. Where we already every document is tagged with the desired categories, or in the case of search for which query, which documents should have been retrieved and this is called ground truth. How to we have to know how to evaluate categorisation results? So first some general thoughts about the evaluation in general for evaluation of this kind of empirical tasks such as categorisation, we use methodology that was developed in 1960s by information retrieval researchers called Cranfield Evaluation Methodology.This lecture is about the evaluation of taxable categorization. Ann It's also controlled by a parameter beta two to indicate the weather precision is more important, or recall is more important when beta is set to one, we have a measure called F1, and in this case we just take a equal weight on both precision and recall. Or equivalently, to measure the difference between the system output and desired ideal output generated by the humans? So obviously the higher similarity is, the better the results are. And if you're interested in some documents and this would tell us how well we did that those documents a subset of them might be more interesting than others. And so, in general, when we use classification accuracy as a measure, we want to ensure that the classes are balanced. In APA category or per document basis? One example that shows clearly the desicion errors are having different causes, spam filtering that could be retrieved as a two category categorization problem. So recall tells us whether the system has actually indeed assigned all the categories that it should have to this document. I basically this kind of measure will not the arithmetic mean is not going to be as reasonable FF1, which tends to prefer a tradeoff between precision and recall. Is sometimes also useful to combine precision and recall as one measure, and this is often done by using if mesh. I saw when the system says yes, how many are corrected that means looking at this category to see if all the documents that are assigned with this category are indeed in this category. The basic idea is to help humans to create test collection. If one is very often used as a measure for categorisation. And 98% of instances are in category one only 2% are in category Two. So in our case, then we're going to compare our systems categorization results with the categorisation ground truth created by humans. You might see a pattern here for this kind of documents along documents it doesn't do as well as. And this gives you some insight for improving the better. And as in the previous case, we can define precision and recall and it will just basically answer the questions from a different perspective. Whether the document that has called all the categories it should have. So that the two values are about equal, so we if there's an extreme case where you have 041 value and one for the other, than F1 will be low, but the arithmetic mean would still be reasonably high. In this case, it's going to be appearing to be very effective, but in reality this is obviously not a good result. Now with this ground truth test collection, we can then reduce the collection to test many different systems and compare different systems. It's often OK not to consider such a cost variation when we compare different methods. So the measure of classification accuracy is similar to count how many of these decisions are correct and normalize that by the total number of decisions we have made. So the first measure that we will introduce is called classification accuracy, and this is basically to measure the percentage of corrective decisions. In this case we're going to look at the how good are the decision on a particular category. Alright, so then we can have some meshes to just better characterize the performance by using these phone numbers and so 2 popular measures of precision and recall. The similarity can be measured in different ways. And if you think about that, you will see that there is indeed some difference and sum. In general, different categorization mistakes, however, have different costs for a specific application, so some errors might be more serious than others. So we've talked about many different methods for taxi categorisation, but how do you which method works better? And for a particular application, how do you this is the best way of solving your problem. So this represents the old categories that it should have got an. The other recall the other meshes called Recall an this measures. Basically, it would be obvious to you if you think about a case when the system says yes for all the category and nothing appears. The test set where most instances are in category one. But if you read many papers in texture catalyzation, you will see that they don't generally do that, and instead they will use a simplified measure. So it's OK to introduce some bias as long as the bias is not correlated with a particular method. If one is more popular and it's actually useful to think about difference. For example, it may be more important to get the decisions right on some documents than others, and or maybe more important to get the divisions right on some categories than others, and this would call for some detailed evaluation of this results to understand. Also, if the human has assigned a category to the document, there will be a plus sign here. So the question here is, how could other divisions on this document? Now, as in the general cases of all decisions, we can think about four combinations of possibilities. And that would lead to different measures, and sometimes it's desirable also to measure the similarity from different perspectives just to have a better understanding of the results in detail. How to address these problems? We of course would like to also evaluate the results in other ways and in different ways
 And, this may not be desirable. Once we compute that, then we can aggregate them. And similar we can do that for recall and F score, so that's how we can then generate the overall precision, recall and F score. An then compute the precision and recall. Recall and F. We generally need to consider how will the results be further processed by a user and then think from a user's perspective what quality is important. Again, for each category, we can compute the precision recall and F1 so for example, for category C one. For example, it might be passed to humans for further editing. So. We have precision P1 recall R1 and F value F1 and similarly we can do that for Category 2 and all the other categories. So for example, ranking documents in the search engine can also be framed as a binary categorization problem, distinguishing relevant documents that are useful to users from those that are not useful. So in this case, the results have to be prioritized. So to reflect the utility for humans in such a task, it's better to evaluate the ranking accuracy, and this is basically similar to search again. Now this is because. Or we do that for each document and then aggregate over all the documents. And if the system can give a score to the categorisation decision or confidence, then we can use the scores to rank these decisions and then evaluate the results as a ranked list, just as in search engine evaluation, where you rank the documents in response to the query. Now in this lecture we're going to do furtherÂ  examine how to combine the performance on these different categories or different documents. Sometimes there are tradeoffs between multiple aspects, like precision and recall, and then, so we need to know for this application is high recall more important or high precision is more important. Earlier we have introduced measures that can be used to compute the precision and recall for each category and each document. So for example, we can aggregate all the precision values for all the categories to compute the overall precision and this is often very useful. In this case, what we do is to pull together all the decisions. What aspect of quality is important. And then after we have completed the computations for all these documents we were going to aggregate them to generate the overall precision, overall recall and overall F score. Categorisation results are sometimes or often indeed passed to human for various purposes. But typically we frame this as a ranking problem and we evaluated as a ranked list. Finally, sometimes ranking may be more appropriate, so be careful. So here are two suggested readings are one is some chapters of this book where you can find more discussion about evaluation measures. That's be cause people tend to examine the results sequentially, so ranking evaluation more reflects the utility from users perspective. Basically computing the values to fill in this contingency table and then we can compute precision recall just once.This lecture is continued discussion of evaluation of textual categorisation. The second is a paper about the comparison of different approaches to text categorization and it also has excellent discussion of how to evaluate the text categorisation. Then we can actually compute, for example, weighted classification accuracy where you associate the different cost or utility for each specific decision. But it may be appropriate for some applications, especially if we associate, for example, the cost for each combination. In general, you want to look at the results from multiple perspectives and for particular application in some perspectives would be more important than others, but for diagnosis, analysis of categorization methods and it's generally useful to look at as many perspectives as possible to see subtle differences between methods or to see where a method might be weak, from which you can obtain insights for improving a method. So, to summarize, categorization evaluation, first evaluation is always very important for all these tasks, so get it right
 In particular, we're going to talk about the opinion mining and sentiment analysis.This lecture is about opinion mining and sentiment analysis covering its motivation. So the task of opinion mining can be defined as taking text data as input to generate a set of opinion representations. As we discussed earlier, text data can be regarded as the data generated from humans as subjective sensors. Now it's interesting to think about other tasks that might be also simple, because those are the cases where you can easily build applications by using opinion mining techniques. We also Would be interested in others opinions. So this analysis shows that there are multiple elements that we need to include in order to characterize an opinion. We can have simple context, like different time or different locations, but there could be also complex text such as some background topic being discussed. But it could be about a group of products. So these are interesting variations that we need to pay attention to when we extract opinions. What's this opinion about? And 3rd, of course we want opinion content and So what exactly is the opinion? If you can identify this, we get a basic understanding of an opinion and can already be useful. In this lecture we are going to start talking about mining a different kind of knowledge, namely knowledge about the observer or humans that have generated text data. So in this case we have the author's opinion. Could be about the product from a company in general. When we decide whom to vote, for example. For example, we can optimize the product search engine, optimize recommender system if we know what people are interested in, what people think about products. Ideally we can also infer opinion sentiment from the content and context to better understand the opinion. And in general, we can gain a lot of advantage in any prediction task because we can leverage the text data as extra data about any problem and so we can use text based prediction techniques to help you make prediction or improve the accuracy of prediction. So this lecture and the following lectures will be mainly about how we can mine and analyze opinions buried in a lot of text data. In each representation we should identify opinion Holder, target content and context. So you can see product reviews are fairly easy to analyze in terms of obtaining a basic opinion representation. Instead, would you just get all the sentences that are about opinions that are useful for understanding the person or understanding the product that we are commenting on. Now this is directly related to humans as sensors, and we can usually aggregate opinions from a lot of humans to kind of assess the general opinion. Or we want to know that the sentiment of this review is positive, and so this additional understanding of course adds value to mining the opinions. So when opinion expressed in the particular discourse context, it has to be interpreted in different ways than when it's expressed in another context, so the context can be very rich to improve the entire discourse context of opinion. What are the winning features of their product or winning features of competitive products? Market research has to do with understanding consumers opinions and this is clearly very useful, directed for that. Also, for this reason about the indirect opinions. It can be about 1 entity, a particular person, a particular product, a particular policy, etc. So that's one general kind of applications. And we can also then identify the context. You can identify one sentence opinion or one phrase opinion, but you can also have longer text to express the opinion like a whole article. From computational perspective, we're most interested in what opinions can be extracted from text data, so it turns out that we can also differentiate distinguish different kinds of opinions in text data from computation perspective. In contrast, the text might also report opinions about others so the person could also make observation about another person's opinion and report this opinion. Now unlike in the product review, all these elements must be extracted by using natural language processing techniques. Finally, the opinion context can also vary. For example, it can help understand peoples preferences. So and this could help us better serve people. Data Driven social science research can benefit from this because they can do text mining to understand the people's opinions. It could be about someone else opinion and one person might comment on another persons opinion etc. Now the main difference between text data and other data like video data is that it has rich and rich opinions and the content tends to be subjective because it's generated from humans. Sometimes if you want to understand further, we want to enrich the opinion representation. I can help us optimize our decisions. So now that we have talked about what is opinion mining and we have defined the task, let's also just talk a little bit about the why opinion mining is very important and why it's very useful. For example, in what time was it expressed? We also would like to deeply understand opinion sentiment and this is to understand, what the opinion tells us about the opinion holder's feeling, for example, is this opinion positive or negative? Or perhaps the opinion holder was happy or sad. We can mine the text data to understand the opinions, understand the people's preferences, how people think about something. Now this is would be very useful for business intelligence, where product manufacturers want to know, where their products have advantages over others. So opinion is subjective statement. And so such understanding obviously goes beyond just extracting the opinion content and needs some analysis. It's more difficult than the analysis of opinions in product reviews. So we can distinguish positive versus negative or neutral or happy versus sad, etc. So let's start with the concept of opinion that it's not that easy to formally define opinion, but mostly we would define opinion as a subjective statement describing what a person believes or thinks about something. In this case, this actually explicit opinion Holder and explicit target, so it's It's obviously what's opinion Holder, and that's just a reviewer, and it's also often very clear what's the opinion target, and that's the product being reviewed. And this is a key differentiating factor from opinion, which tends to be not easy to prove wrong or right because it reflects what a person thinks about something. So whose opinion is this, second must also specify the target. But of course, if you want to get more information, we might want to know the context. Now the content of course is the review text that's in general also easy to obtain. Those statements can be proved right or wrong. So that's the target of the opinion. So clearly the two kinds of opinions need to be analyzed in different ways and sometimes in product reviews you can see, although mostly the opinions are from this reviewer. Now you can see in this case the task is relatively easy, and that's because. Sometimes a reviewer might mention opinions of his friend or her friend, right? And another complication is that there may be indirect opinions or inferred opinions that can be obtained by making inferences on what's expressed in the text that might not necessarily look like opinion. For example, I don't like this phone at all, and that's opinion of this author. I just gave a good example, in the case of product reviews where the opinion Holder and opinion target are often explicitly identified, and that's not why this turns out to be one of the simplest opinion mining tasks. But there is also another target, which is a hurricane of 1938
 That could be useful for sentiment analysis. They can enrich text representation. But of course such a representation would not be as discriminative as words. We can also learn word clusters empirically, for example we talked about mining associations of words and so we can have cluster of paradigmatically related words or sementically related words.This lecture is about the sentiment classification. And these clusters can be features to supplement the word based representation. One is polarity analysis where we have categories such as positive, negative or neutral. One is to use more sophisticated features that may be more appropriate for sentiment tagging, as I will discuss more in a moment. So this is a case of just using sentiment classification for understanding opinion. For example, the word great might be followed by a noun and this could become a feature, a hybrid feature. So NLP enriches text representation. As we've seen before, so it's a special case of text categorization. Furthermore, we can also have frequent pattern syntax and these could be frequent word set. In our case, for text categorization, or more specifically, sentiment classification. That's not desirable. But we also have locations where the words might occur more closely together. The output is typically, a sentiment label or sentiment tag, and that can be designed in two ways. Sentiment classification can be defined more specifically as follows: The input is opinionated text object. For example, we could use ordinal regression to do, and that's something that will talk more about later. So as you can see, the task is essentially a classification task or categorisation task. We can also consider part of speech tag n-grams, if we can do part of speech tagging and for example, adjective, noun could form a pair. Or could be semantic and they might represent concepts in theÂ thesaurus or ontology like word net. This also means any text categorization method can be used to do sentiment classification. And these features can then be further analyzed by humans through error analysis. Or they can be recognized the named entities like people or place and these categories can be used to enrich the representation as additional features. So now let's talk about some features that often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis. Uni Grams are actually often very effective for a lot of text processing tasks and that's mostly because words are well designed features by humans for communication, and so they often good enough for many tasks, but it's not good or not sufficient for sentiment analysis clearly. But with NLP we can enrich the representation with a lot of other information such as part of speech tags, parse trees or entities, or even speech act. In general you have just discrete categories to characterize the sentiment. And so in that sense, you wanted features to be frequent. Specificity requires the feature to be discriminative, so naturally infrequent features tend to be more discriminating, so this really caused tradeoff between frequent versus infrequent features, and that's why feature design is generally an art. And this is also robust to spelling errors or recognition errors, right? So if you misspelled the word by 1 character and this representation actually would allow you to match this word when it occurs in the text correctly. If we assume that most of the elements in the opinion representation are already known, then our only task maybe just the sentiment classification as shown in this case. So suppose we know who is the opinion holder and what's the opinion target and also know the content and context of the opinion. This is the problem in general, and the same is true for parse tree based features where you can use a parse tree to derive features such as frequent subtrees or paths, and those are even more discriminating, but they also are more likely to cause overfitting. You can just have a sequence of characters as a unit, and they can be mixed with different n(s),Â  different lengths. So misspelled word and the correct form can be matched because they contain some common n-grams of characters. For example, we might see a sentence like it's not good or it's not as good as something else. And in General, Patton discovery algorithms are very useful for feature construction, because they allow us to search in a larger space of possible features that are more complex than words that are sometimes useful. So from these words we can only derive simple world n-grams representations or character n-grams. Then we mainly need to decide the opinion sentiment of the review. So next we can also have word classes, so these classes can be syntactic like a part of speech tags
 So now with this approach we can now do rating prediction as follows. Now, what if it's not as large as . That's alot alot of parameters.This lecture is about the ordinal logistic regression for sentiment analysis. Intuitively, the features that can distinguish Category 2 from 1, or rather rating 2 from 1, may be similar to those that can distinguish K from K - 1. So these training data for different classifiers. In general, words that are positive would make the rating higher and for any of these classifiers, for all these classifiers. Now when we train categorisation program by treating these categories as independent, we would not capture this. So the predictors are represented as X and these are the features and there are M features altogether, which feature value is a real number, and this can be representation of a text document. You may recall that in logistic regression we assume the log of probability that Y is equal to 1 is assumed to be a linear function of these features as shown here. So this is intuitively appealing assumption. So these equations are the same as you have seen before. For example, positive words generally suggest a higher rating. So when Y_j is equal to 1, it means rating is J or above. Larger than or equal to negative of alpha_j as shown here. There are many parameters. And the other is to allow us to share the training data, because all these parameters are assumed to be equal. And that's our classifier one, and then we're going to have another classifier to distinguish K - 1 from the rest. So we should be able to take advantage of this factor. When it's zero, that means the rating is lower than J. We have an opinionated text document D as input an we want to generate as output already in the range of one through K, so it's discrete rating and thus this is a categorization problem. So this is just a direct application of logistical regression for binary categorization. So this is our problem set up for a typical sentiment classification problem, or more specifically, rating prediction. So this classifier will tell us whether this object should have a rating of K or above. So suppose we just want to distinguish it positive from negative and then it's just a two category categorization problem. Now the idea of ordinal logistic regression is precisely that A key idea is just the improvement over the K -1 independent logistical regression classifiers, and that idea is to tie these beta parameters and that means we are going to assume the Beta parameters these are the parameters that indicate the influence of those weights. And the idea is we can introduce multiple binary classifiers and each case we ask the classifier to predict whether the rating is J or above all the ratings lower than J. And we're going to assume these better values are the same for all the K - 1 premise, and this just encodes our intuition that positive words in general would make a higher rating more likely. It's reasonable for our problem set up when we have this order in these categories. So now let's see how we can use such a method to actually assign ratings. Now can you count how many parameters we have exactly here? Now this may be interesting exercise. That's classifier two, and in the end we need a classifier to distinguish two and one So altogether we'll have K - 1 classifiers. Can then be shared to help us set the optimal value for beta. And then of course, this is a standard two category categorization problem. So basically, if we want to predict rating in the range of one through K, we first have one classifier to distinguish K versus others. And if its probability according to this logistical regression classifier is larger than . Specifically, we have N + K - 1 because we have M beta values and plus K minus one alpha values. So if you think about it for a moment and you will see now the plan that we have far fewer parameters. So apha subject is different. When we need to decide whether it's two or one, so this will help us solve the problem, right? So we can have a classifier that would actually give us a prediction of rating in the range of one through K, unfortunately, such a strategy is not the optimal way of solving this problem, and specifically there are two problems with this approach. These problems are actually dependent. It turns out that with this. What if we have multiple categories, multiple levels? We actually use such a binary logistic regression program to solve this multi level rating prediction. So in sum, in this approach we're going to score the object
 Infer this aspect ratings. And As for aspect. They thought we are modeling.This lecture is about the latent aspect rating analysis or opinion mining and sentiment analysis. Now of course these sentiment weights might be different for different aspects. We can generate a aspect level opinion summary. So we are interested in the conditional probability of R sub T given D. So with this we would be able to obtain aspect segments. And this model is set up as follows. As shown here, and that's denoted by beta sub I and W. In particular, we're going to introduce. With these seed words and that would allow us to segment the text into segments. Note that here the sentiment weights are specifically to aspects, so beta is indexed by I. When they are combined together, we can have a more detailed understanding of the opinion. What we want to do is first we're going to segment the aspects. In this lecture, we're going to continue discussing opinion mining and sentiment analysis. As a bi product that will also get the beta vector and these are the aspects of specifica sentiment, weights of words, so more formally. Or price to retrieve the relevant the segments and then from those segments we can further mine correlated words. So. So we're going to assume we have aspect weights in order by of R sub of D. So what we want to do is to decompose this overall rating. So next question is, how can we estimate these parameters and so we collectively denote all the parameters by Lambda here. And for example, we can do opinion based and the ranking. And we can assume the overall rating is simply a weighted average of this aspect ratings. So the aspect rating is assumed to be a weighted combination of these word frequencies where the weights are the sentiment weights on the words. So we hope to discover them. So we have for each aspect a set of sentiment weights. So obviously that's one of the important parameters, but in general we can see we have these parameters. And then we're going to set up a generation probability for the overall rating given the observed words. So if we can decompose overrating two ratings on these different aspects. And so we have seen such cases before in, for example, PLSA, where we predict the text data. So for more details about this model, you can read this paper cited here. This is neither because of the same word might have positive sentiment for once back, but negative sentiment for another aspect. Now, so this means when we generate our overall rating, we're going to first draw. So the task here is to get these reviews and their overall ratings as input and then generate the both the aspect ratings, decomposed aspect ratings and the aspect of weights as output. So now more specifically, we can now once we estimate the parameters, we can easily compute the abstract rating for aspect I or sub I of D and that's simply to take all the words that occurred in the segment I and then take their accounts and then multiply that by the sentiment weight of each word and take a sum. We can also analyze reviewers preferences, compare them or compare their preferences on different hotels. Now, what about the aspect weights? Alpha sub I of D? It's not part of our parameter, right? So we have to use Bayesian inference to compute it. In this case, the aspect ratings and aspect of weights. As I said, the overall rating is assumed to be a weighted average of aspect ratings. Into ratings on different aspects such as value, rooms, location and service. 2 computer this alpha value. So we're going to figure out what words are talking about location in what words are talking about, the room conditioning, etc. In particular, we're going to obtain the counts of all the words in each segment, and this is denoted by C supply of WND. Basically we're going to maximize the product of the prior of our according to our assumed market valued Gaussian distribution and the likelihood in this case likely is the probability of generating this observed overall rating given this particular Alpha value and some other parameters. And of course there are also reviews that are in text. Later, we're going to also mention that we can do this in the unified model. So the task in general is given a set of review articles about the topic with overall ratings. And to generate the overall rating. This can be done by using seed words like location and room. Then it would increase the aspect rating for location. But as always when we make this assumption, we have a formal way to model the problem, and that allows us to compute interesting quantities. And each review documents denoted by AT and overall rating is denoted by R sub D and these pre segmented into K as their segments and we're going to use C sub W and D and to denote the count of world W in aspect segment I. But if you can see if we can uncover these parameters, that would be nice because also R of D is precisely the aspect ratings that we want to get, and these are decomposer ratings on different aspects of our sub ID is precisely the aspect weights that we hope to get. And this would be used to take a weighted average of the aspect ratings, which are denoted by our supply of the. I will show you some results. Now we can, as usual, use The maximum likelihood is made and this will give us the settings of this premise that with the maximizer observed
 So given an entity, we can assume there are aspects that are described by word distributions. For solving this problem. And once we can end users better, we can serve these users better. And this is what we call a personalized or rather query specific recommendation. Now what we can do is such a query is that we can use reviewers that we believe have a similar preference to recommend the hotels for you. As a result, we can learn those useful information when fitting the model to the data. So this is yet another application of this technique. And shows that by doing text mining we can understand the users better. Topics and then we can use a topic model to model the generation of the review text. It can be applied to any reviews with overall ratings, so this is very useful technique that can support a lot of text mining applications. You can't really tell much difference between these hotels, but by decomposing these ratings into aspect ratings we can see some hotels have higher ratings for some. And such knowledge can be useful for manufacturers to design their next generation of products. And as a task sentiment analysis can be usually done by using just text categorization, but standard techniques tend not to be enough and so we need to have enriched feature representation. We have also shown that generative models are powerful for mining latent user preferences, in particular in the generating model for letting the rating regression, we embed some interesting preference information and sentiment weights of words in the model. So to summarize our discussion of opinion mining in general, this is a very important topic and with a lot of applications. The positive sentence is negative sentences about each aspect.This lecture is a continued discussion of latent aspect rating analysis. The next two papers are about the generative models for letting the aspect rating analysis. So this shows that by using this model we can infer some information that's very hard to obtain, even if you read all the reviews. So with these results we can build a lot of interesting applications. So this is a case where text mining algorithms can go beyond what humans can do to review interesting patterns in the data, and this of course can be very useful. Now, it's also possible to develop a unified generative model for solving this problem, and that is we not only modeling, we not only model the generation of overrating based on text, we also model the generation of text and so a natural solution would be to use topic model. We did not really have to do this, but the design of the generative model has this component and these are sentiment waits for words in different aspects. So because we can infer the reviewers weights on different dimensions, we can allow a user to actually say what do you care about. Our assumed the words in the review text are drawn from these distributions. And then we can then plug in the latent regression model to use the text to further predict the Overall rating and that means we first predict the aspect rating and then combine them with aspect weights to predict the overall rating. So this shows that with this apology, we can also learn sentiment information directly from the data. How can we know that we can infer the weights of those reviewers on different aspects? We can find the reviewers whose weights or more precise whose inferred weights or similar to yours and then use those reviews to recommend the hotels for you. An you can see the top results generally have much higher price than the low Group, and that's because when reviewers cared more about the value as dictated by this query and they tend to really have favor low price hotels. So this would give us a unified generative model where we model both the generation of text and the overall rating condition on text. These are MP3 three reviews an these results show that the model can discover some interesting aspects commented on low overall ratings versus those high overall ratings, and they care more about the different aspects. So we don't have time to discuss this model in detail, as in many other cases in this part of the course where we discuss the cutting edge topics. So now I'm going to show you some simple results that you can get by using this kind of generative models. The first one is about solving the problem using two stages and the second one is about the unified model where topic model is integrated with the regression model. Here are also mother results about the aspects discovered from reviews with low ratings. So what you see is average weights on different dimensions by different groups of reviewers. Or they comment more on different aspects. So this shows that the model can reveal differences in. Even if you read all the reviews, it's very hard to infer such preferences or such emphasis. In the same way as we assumed for a generative model like PSA. So now you can see the average prices of hotels are favored by toptenreviews are indeed and much cheaper than those that are favored by the bottom 10. Here are some interesting results on analyzing users rating behavior. And we also need to consider the order of those categories and we talk about the ordinal regression. Earlier we talked about how to solve the problem of Lara in two stages when we first do segmentation of different aspects and then we use a little regression model to learn the aspect ratings and letting the weights. To solve the problem using a unified model. are small books that are excellent reviews of this topic where you can find a lot of discussion about the other variations of the problem and techniques proposal for solving the problem. Remember, the model can infer whether a reviewer cares more about service or the price. So here what you see are the decomposed ratings for three hotels that have the same overall rating. Opinions of different reviewers and such a detailed understanding can help us understand better about reviews and also better about their feedback on the hotel. what you here are the prices of hotels in different cities, and these are the prices of hotels that are favored by different groups of reviews
This lecture is about text based prediction. And this is most useful for prediction about human behavior or human preferences or opinions. And the other could be to mine knowledge about the observer so sentiment analysis or opinion analysis. So this is very different from content analysis or topic mining where we directly characterize the content of text. And that's to mine text in the context defined by non text data. So those are the data mining or text mining algorithms can be used to generate the predictors. So one sub task could be mine, mine the content of text data like topic mining. So for example, topic mining can be very useful to generate the patterns or topic based indicators or predictors that can be further fed into a predictive model. Basically you can see text based prediction character serve as a unified framework to combine many text mining and analysis techniques, including topic mining and content, any content mining techniques or sentiment analysis. So topics can be intermediate representation of text. Because non text data can provide a context for mining text data. And similarly, sentiment analysis can lead to such predictors as well. In one perspective, we can see non text data can help text mining. That may not be directly related to the text, or only remotely related to text data. In general, we want to collect data that are most useful for learning. We should mine all the data together. These patterns that are generated from text and non text data themselves can sometimes already be useful for prediction, but when they are put together with many other predictors they can really help improving the accuracy of prediction. This helps discover some frequent patterns from non text data. Because we in general should treat all the data that we collected. And, this is so that we can adjust the sensors to collect the most useful data for prediction. So from text mining perspective and we're interested in text based prediction, of course sometimes text alone can make predictions. In this lecture and the following lectures, we're going to talk more about how we can predict more information about the world. It will control the generation of these features. But in general text data will be put together with non text data. So the interesting questions here would be first how can we design effective predictors? And how do we generate such effective predictors from text? This question has been addressed to some extent in some previous lectures where we talked about what kind of features we can design for text data. This data generally will help us improve the prediction accuracy and in this loop are humans will recognize what additional data needs to be collected and machines would of course help humans identify what data should be collected next. in such a prediction problem set up, we are very much interested in joint mining of non text and text data. We might want to make decisions based on that. And then you can then decide what additional data. But when there are, of course a lot of text data than machines have to help, and that's why we need to do text data mining. And this gives us two sets of text data and then we can see what's the difference and this difference in text data is interpretable because text content is easy to digest and that difference might suggest some meaning for this pattern that we've found from non text data, so that helps interpret such patterns. Now we can use the text data that are associated with instances where the pattern occurs as well as text data that are associated with instances where the pattern doesn't occur. It's also different from opinion mining or sentiment analysis, which still have to do with characterizing mostly the content only that we focus more on the subjectiveÂ content which reflects what we know about the opinion holder. It has also been addressed to some extent by talking about the other knowledge that we can mine from text. And of course we can also add non text data directly to the predictive model, but then non text data also helps provide context for text analysis that further improves the topic mining and the opinion analysis. Now the other perspective is text data can help non text data mining as well. The other question is how can we join mine text and non text data together? Now this is a question that we have not addressed yet. That would allow us to design high level features or predictors that are useful for prediction of some other variable. And such improvement often leads to more effective predictors for our problems it would enlarge the space of patterns of opinions or topics that we can mine from text. And this is because text data can help interpret patterns discovered from non text data. So in this lecture and the following lectures we're going to address this problem because this is where we can generate the much more enriched features for prediction and allows us to review a lot of interesting knowledge about the world. As we'll discuss more later, so the join analysis of text and non text data can be actually understood from 2 perspectives
 topic. context. So in general, any related data can be regarded as context, so there could be remotely related to context. Or we can obtain a set of papers about the text mining, and this can be compared with papers about another topic.This lecture is about the contextual text mining. Contextual text mining is related to multiple kinds of knowledge that we mine from text data. It's related to text based prediction because it allows us to combine non text data with text data to derive sophisticated predictors for the prediction problem. And so in general, this enables discovery of knowledge associated with different context as needed. What are the common research interests of two researchers? In this case, authors can be the context. As I'm showing here, is related to topic mining because can make topics associated with context, like a time or location, and similarly it can make opinion mining more contextualized, making opinions connected to context. So more specifically, why are we interested in contextual text mining? Well that's, first, because text often has rich context information and this can include direct context such as meta data. There could be also other text data from the same source as this one, so the other context data can be connected with this text, as well. Are there topics in news data that are correlated with sudden changes in stock prices? In this case, we can use a time series such as stock prices as context. So here's illustration of how context can be regarded as interesting ways of partitioning of text data. Indirect text context refers to additional data related to the meta data. So time is a context in this case. And in particular, we can compare different contexts, and this often gives us a lot of useful knowledge. So, for example, from authors, we can further obtain additional context, such as social network of the author or the author's age. Now, such text data can be partitioning in many interesting ways because we have context. We can get all the SIGIR papers and compare those papers with the rest or compare SIGIR papers with KDD papers with ACL papers. For example, comparing topics overtime, we can see trends of topics and comparing topics in different context can also reveal differences about the two contexts. It also in general provides meaning to the discovery topics if we gonna associate the text with context. And this is very important because this allows us to do interesting comparative analysis. And such information is not, in general, directly related to the text yet through the authors we can connect them. This would allow us to compare topics, for example in different years. For example, what topics have been gaining increasing attention recently in data mining research? Now to answer this question, obviously we need to analyze text in the context of time
 Now we have about 30 articles. Now such context information is what we hope to model as well. Recall that before when we generate the text, we generally assume we will start with some topics and then sample words from some topics. So this allows us to plot this conditional probability. Recall that in contextual text mining we hope to analyze topics in text. So we're going to choose particular coverage and that coverage. The consequences that this would enable us to discover contextualized topics make the topics more interesting, more meaningful, because we can then have topics that can be interpreted as specific to a particular context that we're interested in. This topic is indeed very relevant, to both wars. So we're not going to just model the text. And naturally, the model will have more parameters to estimate, but once we can estimate those parameters that involve the context, then we will be able to understand the context of specific views of topics or context of specific coverages of topics. So to generate such a document with context, we first also choose a view. In consideration of context so that we can associate the topics with appropriate context that we're interested in. The other is that we assume.This lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. And this is precisely what we want in contextual text mining. So what it means we're going to use the coverage to choose a topic to choose one of these three topics. Firstly it would model the conditional likelihood of text given context. So as you see here, we can assume there are still multiple topics. Or in other words, we can do let the context influence both coverage and content of a topic. And we're going to introduce contextual probabilistic latent semantic analysis As an extension of PLSA for doing contextual text mining. Before we have those classical probabilistic model logic model, Boolean model etc. And people talked about these topics. So now we have a specific version of word distributions. So such a technique would allow us to use location as context to examine variations of topics. So this is the basic idea. And this would allow us to do contextual text mining. And this view of course now could be from any of these contexts. Now you can see some probabilities of words for each topic. We pick We've picked the document specifically coverage now with the coverage and these word distributions, we can generate the document in exactly the same way as in PLSA. But the visualization shows that with this technique that we can have conditional distribution of time given a topic. So this shows that by bringing the context, in this case, different wars are different collections of text. In this lecture, we're going to continue discussing contextual text mining. One is to assume that depending on the context depending on different time periods or different locations, we assume that there are different views of the topic or different versions of word distributions that characterize a topic, and this assumption allows us to discover different variations of the same topic in different context. So in this case, the context that is explicitly specified by the topical collection. Before is fixed in PLSA and it's hard to a particular document. Now, as you can see, we assume there are different views associated with the each of the topics. And the topic we focus on is about the retrieval models and you can see the top word top words with high probability is about this model on the left. So in this approach contextual probabilistic latent semantic analysis or CPLSA The main idea is to explicitly add interesting context variables into a generated model. And these views are tide to some context variables. Now, so the main difference is when we obtain the coverage and the word distributions, we let the context influence our choice. That clearly suggests that the generation of text would then depend on context, and that allows us to bring context into the generative model. Now on the bottom you will see the theme coverage or topic coverage might also vary according to these context. So in other words, we have extra switches that are tied to this context that would control the choices of different views of topics and choices of coverage. It's to perform comparative text mining to allow us to extract the common topics shared by multiple collections and their variations in each collection. So in this case we can see the time can be used as context to reveal trends of topics. So here are some sample results from using such a model. But here we are going to add context variables so that the coverage of topics and also the content of topics will be tight little context. Now here, because we consider context so the distribution of topics or the coverage of topics can vary depending on the context that has influenced the coverage. Now if you the background, of course this is not surprising and this is. This is some additional result on special patterns and this. This these topics are obtained from block articles about the Hurricane Katrina. And these are shown as view one, view two and view three Each view is a different version of word distributions. So this technique here can use event as context. In this case it's about the topic of government response. And in this case, the estimate premise would naturally contain context variables, and in particular a lot of conditional probabilities of topics given certain context. But after 1998 that we see clear dominance of language model as probabilistic models and we see words like a language model, estimation of parameters etc. Now, in this case, the goal is to. And of course, the model is completely general, so you can apply this to any other collections of text to reveal spatial temporal patterns. Each document has just one coverage distribution. And that means depending on the time or location, we might cover topics differently. One is the start of TREC for text retrieval conference. So, for example, we might pick a particular coverage, let's say in this case. We can have topic variations, tied to these contexts to review the differences of coverage of United Nations in the two wars. Is yet another application of this kind of model where we look at the use of the model for event impact analysis. We assume we have got a word distribution associated with each topic, right? And then next to the view we choose a coverage from the bottom. And so one idea here is to model the variations of topic content in different context and this gives us different views of the world distributions. And then next time we might choose a different topic, an will get donate, etc right until we generate all the words and this is basically the same process as in PLSA
This lecture is about how to mine text data with social network as context. We're going to optimize another function F. For example topics. So this makes it possible to find the parameters that are that are both to maximize the PLSA log likelihood. This regularizer can also be any regularizer. So can you recognize which part? Is the likelihood for the text data given by a topic model? If you look at it that you will see this part is precisely the PLSA log likelihood that we want to maximize when we estimate the parameters for PLSA alone. To read more about the topic. So we want to maximize the probability of text data given the parameters generated denoted by Lambda here. They will share common distribution of the topics or have just slight variations of the topic distributions or topic coverage. We want their distributions to be similar, so they here we are computing the square of their differences and we want to minimize this difference. So first some motivation for using network context for analysis of text. strongly connected to be similar and we ensure their coverages are more similar. There are four communities of articles and we will. So the advantage of this idea is that it's quite general here the topic model can be any generative model for text. So such context connects the content. a model called Network supervised topic model. So here we are going to briefly introduce. So such heuristic can be used to guide us in analyzing topics. should be able to pick up a Co occurring words, so in general the topics that they generate represent words that Co occur with each other. If we use more topics, perhaps will have more coherent topics. And so, in general analysis of text there should be using the entire network of information that's related to the text data. The context of a text article can form a network. In this lecture, we're going to continue discussing contextual text mining. But it should still be useful to know the general ideas and to know what they can do to know when you might be able to use them. They must cover similar topics in similar ways, and that's basically what it says in English. Connections between two people in a social network and then they will have a high weight then that means it will be more important to make sure that their topic coverages are similar and that's basically what it says here. IR information retrieval. So technically what we can do is simply to add a network induced regularizers to the likelihood objective function as shown here. So for example, it's reasonable to assume that authors connected in collaboration network tend to write about the similar topics. Right, it doesn't have to be PLSA or LDA or the current topic models. So here's one suggested reading and this is the paper about the NetPLSA where you can find more details about the model and how to estimate such a model. So we often use maximum likelihood estimator to obtain the parameters and these parameters would give us useful information that we want to obtained from text data. Indeed, in many cases they tend to cover similar topics. If we have a weight that says these two nodes are strong collaborators of researchers, or these two are strong. And if you look at this formula and you can actually recognize some part fairly familiar, because are they should be fairly familiar to you by now. So a more general view of text mining in the context of network is to treat text as living in the rich information network environment. In particular, we're going to look at the social network of authors of text as context. So instead of just optimizing the probability of text data given parameters, Lambda. And that's because we can use network to impose some constraints on topics text. Hoping to see that the topic mining can help us uncover these four communities, but from these sample topics that you are seeing here that are generated by PLSA and PLSA is unable to generate the four communities that correspond to our intuition and the reason was because they are all mixed together and there are many words that are shared by these communities, so it's not that easy to use the four topics. Text also can help characterize the content associated with each subnetwork and this is to say that both kinds of data, the network and text can help each other. We can be flexible in capturing different heuristics that we want to capture. For example, the authors of research articles might form a collaboration network. Similarly, locations associated with text can also be connected to form geographical network, but in general you can imagine the meta data of the text data can form some kind of network if they have some relations. This whole Sum. We so we may be able to smooth the topic distributions on the graph on the network so that adjacent nodes will have very similar topic distributions, so you. And this is an active sign that I just mentioned, because there's a negative sign when we maximize this objective function we will actually minimize this second term here. So the general idea of network supervised topic model modeling is the following. But when the lambda is set to a larger value, then we'll let the network influence the estimate models more so as you can see, the effect here is that we can do basically PLSA but we're going to also try to make the topic coverages on the two nodes that are. But still they cannot generate such coherent results as NetPLSA is showing that the network context is very useful here. And, this in this slide we're going to give some general ideas, and then in the next slide will give some more details. A similar model could have been also used to characterize the content associated with each subnetwork of collaborations. Essentially the people that form a collaboration network would then be kind of assumed to write about similar topics and that's why we can have more coherent topics. So here we see that these topics correspond well to the four communities
 So we're going to be able to use such a model.This lecture is about using a time series as context to potentially discover causal topics in text. Now we call these topics "causal topics". So non text data provide context for mining text data and we discussed a number of techniques for contextual text mining. And this approach can be regarded as an alternate way to maximize both dimensions. If we can. So when we apply the topic models we are maximizing the coherence. And then we're going to add some history information of X into such a model to see if we can improve the prediction of Y. But we hope these topics are not just regular topics. In particular, we're going to look at the time series as a context for analyzing text to potentially discover causal topics. The topicals will be all meaningful. For example, PLSA and we can apply this to text streams. And we might figure out that topic one and topic four are more correlated and topic two and topic three are not. But as I also explained that these topics are likely very good because they are general topics that explained the whole text collection, they're not necessarily the best topics that are correlated with our time series. Text data is often combined with non text data for prediction, because for this purpose, for the prediction purpose, we generally would like to combine non text data and text data together as much clues as possible for prediction. However, they may not be so coherent semantically. Of course, then we can apply topic models to get another generation of topics, and that can be further ranked based on the time series to select the highly correlated topics. How this works. And so as a result, joined analysis of text and non text is very necessary. One is the paper about this iterative topic modeling with time series feedback, where you can find more details about how this approach works. In this lecture we're going to continue discussing contextual text mining. So what we can do in this approach is to further zoom in the word level. And this is the best we could do without any other information. So the idea here is to go back to topic model by using these, each as a prior, to further guide the topic modeling, and that's to say we ask our topic models to now discover topics that are very similar to each of these two subtopics, and this will cause a bias toward more correlated topics with the time series. And it's also very useful now when we analyze text data together with non text data we can see they can help each other. Of course, they're not, strictly speaking, causal topics or we are never going to be able to verify whether they are causal or there's a true causal relationship here. Or this is the best that we could get from this set of topics so far. Now, Text based prediction is generally very useful for big data applications that involve text, because, you can help us infer new knowledge about the word and the knowledge can go beyond what's discussed in the text. So as a topic and it's not good to mix these words with different correlations, so we can then further separate these words, we're going to get all the red words that indicate positive correlation W1and W3, and we're going to also get another subtopic, if youÂ  want, that represents a negatively correlated words W2 and W4. And we're going to look into each word in the top ranked word list for each topic. Why, because we are restricted to the topics that were discovered by PSA or LDA? And that means the choice of topics will be very limited and we know these models try to maximize light role of the text data, so those topics tend to be the major topics that explain the text data well, and they're not necessarily correlated with time series. And if the topic is correlated with the time series, there must be some words that are highly correlated with the time series. And then when we apply the selected words, as a prior to guide the topic modeling, we again go back to optimize the coherence because topic models will ensure the next generation of topics to be coherent and we can iterate, iterate, and optimize in this way as shown on this picture. So here, for example, we might discover W1 and W3 are positively correlated with time series. So each of these two subtopics can be expected to be better correlated with time series. Then we can further analyze the component words in the topic and then try to analyze word level correlation. And the output so would contain topics just like in topic modeling. In this case, we hope to use text mining, to understand the time series. Take the text stream as input and apply regular topic modeling to generate a number of topics. And we're going to look at the words in this topic - the Top words. And this is to see this was different from the standard talking models where we have just the text collection. The idea is to do a iterative adjustment of topics discovered by topic models using time series to induce a prior. Basically you're going to have auto regressive model to use the history information of Y to predict itself. But at least they are correlated topics that might potentially explain the cause, and humans can certainly analyze such topics to understand the issue better.Â  With these topics, we certainly don't have to explain the data the best in text, but rather they have to explain the data in the text, meaning that they have to represent a meaningful topics in texts semantically coherent topics, but also more important they should be correlated with the external time series that is given as a context. That's why we set time series here to serve as context. And, with some extension like a CPLSA or contextual PLSA, then we can discover these topics in the collection and also discover their coverage overtime. From another perspective these result help us what people have talked about in each case, so in the not just the people what people have talked about, but what are some topics that might be correlated with their stock prices? And so these topics can serve as a starting point for people to further look into the issues and to find the true causal relations. So all these cases are special cases of a general problem of joint analysis of text and the time series data to discover causal topics. And then get the even more correlated subtopics that can be further fed into the process as prior to drive the topic model discovery. Are there any clues in the news stream that might provide insight about this. But when we decompose the topic model words into sets of words that are strongly very strongly correlated with time series, we select the most strongly correlated words with the time series we are pushing the model back to the causal dimension to make it better in causal scoring. And then we're going to use the external time series to assess which topic is more causally related or correlated with the external time series, so we can certainly rank them. So the only component that you haven't seen in such a framework is how to measure the causality because the rest is just topic model. Those major topics in the news event. Let's say we have a topic about government response here and then with topic model, we can get the coverage of the topic overtime. But W2 and W4 are negatively correlated. So it clearly has topics that are more correlated with the external time series. We know topic one is correlated with the time series. And if you look at these topics and they are indeed quite related to the campaign. Here's another scenario where we want to analyze the presidential election. So to understand how we solve this problem, let's first just to solve the problem with the regular topic model. So they in some sense, well, we should expect so, they are in some sense more correlated with time series than the original topic one because the original topic one has mixed words here, we separate them. Now these subtopics, all these variations of topics based on the correlation analysis, are topics that are still quite related to the original topic topic one, but they already deviating because of the use of time series information, to bias selection of words. Right, so one would be interested in knowing what might have caused the stock market crash. Now we could have stopped here and that would be just like the simple approaches that I talked about earlier, right, then we can get these topics and call them causal topics. Or maybe just measure the correlation of the two? There are many measures that we can use in this framework. But this approach is not going to be very good. So here's an illustration of how this works. If we only use causality test or correlation measure then we might get a set of words that are strongly correlated with time series, but they may not necessarily mean anything
 We also talked a lot about the text similarity when we discuss how to discover paradigmatically relations, we compare their context of words, discover words that share similar contexts. And in particular, we talked about how to mine word associations. We then talked about how to analyze topic syntax, how to discover topics, and analyze them. Then we talked about the text clustering and text categorization. And in particular, we talked about how to use context to analyze topics. And then we talked about the topic mining and analysis and that's where we introduced the probabilistic topic model. This can be regarded as knowledge about the oberved of the world, and then we talked about how to mine knowledge about the observer and particularly talk about how to mine opinions and do sentiment analysis. In text clustering we talked about the, how we can solve the problem by using a slightly different than mixture model than the probabilistic topic model. And this part is well connected to text retrieval. At that point that we talked about, representing text data with a vector space model, and we talked about some retrieval techniques such as BM25 for measuring similarity of text and for assigning weights to terms, TF-IDF weighting, etc. Then we talked about the sentiment analysis and opinion mining, and that's where we introduced sentiment classification problem. So these are all general techniques that tends to be very useful in other scenarios as well. And we then also briefly reviewed some similarity based approaches to text clustering. Finally, in the discussion of text based prediction, we mainly talked about the joint mining of text and non text data as they are both very important for prediction. So in general, to build a big text data application system, we need two kinds of techniques, text retrieval and text mining. This is a practical, useful technique for a lot of text categorization tasks. We also suggested you to learn more about data mining, and that's simply because general data mining algorithms can always be applied to text data which can be regarded as a special case of general data. So this course covered text mining, and there's a companion course called text retrieval and search engines that covers text retrieval. Text mining has to do with further analyzing the relevant data to discover the actionable knowledge that can be directly useful for decision making or many other tasks. And finally, we also recommend you to learn more about the text retrieval information retrieval or search engines. Another technique that's useful is indexing technique that enables quick response of search engine to users query and such techniques can be very useful for building efficient text mining systems as well. Those are two important building blocks in any text mining application systems. In this part we also introduce some general concepts that would be useful to know. And finally we talked about how time series data can be used as context to mine, potentially causal topics in text data. And although it's a special case of text categorization, but we talked about how to extend or improve the text categorisation method by using more sophisticated features that would be needed for sentiment analysis. One is generative classifiers, they rely on base rule to infer the conditional probability of a category given text data. So it can in particular help text mining systems in two ways. In this case we use social network or network in general of text data to help analyzing topics. These are actually very general relations between elements in any sequences. There are other techniques that can be relevant here also. So they are very important because they are key to also understanding some advanced NLP techniques and naturally they would provide more tools for doing text analysis in general. Then we also talk about latent aspect rating analysis. So there are many applications of data mining techniques in particular, for example, a pattern discovery would be very useful to generate a interesting features for text analysis. And a search engine would be essential system component in any text based applications, and that's because text data are created for humans, to us to consume. Recently, information network mining techniques can also be used to analyze text information network. So to learn more about this subject, you should definitely and learn more about the natural language processing, because this is the foundation for all text based applications. So these are all good to know in order to develop effective text analysis techniques. In the case of using non text data to help the text data analysis, we talked about the contextual text mining. Moreover, some techniques and information retrieval, for example BM 25, vector space and language models, also very useful for text data mining. We then talked about how to mine knowledge about the language, natural language used to express what's observed in the world of text data. There also very important that they are very popular and they're very useful for text categorization as well.This lecture is a summary of this whole course. And in discussing this, we also discussed the rule of non-text data which can contribute to additional predictors for the prediction problem and also it can provide a context for analyzing text data. So this is another important concept to know. One is generating model and this is a general method for modeling text data and modeling other kinds of data as well. First in NLP and text representation, you should realize that NLP is always very important for any text applications because it enriches text representation the more NLP, better text representation we can have and this further enables more accurate knowledge discovery, to discover deeper knowledge buried in text. If you take it as meaning elements that occur in similar context in the sequence and elements that tend to co-occur with each other and these relations might also be meaningful for other sequences of data. And the other area that has emerged in statistical learning is the word embedding technique where they can learn vector representation of words and then these vector representations would allow you to compute the similarity of words. So it's very important to know more about these. We did not talk about that predictive modeling component, but this is mostly about the regression or categorization techniques, and this is another reason why statistical learning is important. One is to effectively reduce the data size from a large collection to a small collection with the most relevant text data that only matter for the particular application. So this is very important. In Word Association Mining and analysis, the important points are first are we introduced the two concepts for two basic plan complementary relations of words, paradigmatic and syntagmatic relations. The next point that is about the co- occurrence analysis of text and we introduced some information theory concepts such as entropy, conditional entropy and mutual information. And this is also the basis for understanding LDA, which is a theoretically more appealing model. However, the current state of the art of natural language processing is still not robust enough, and so as a result, the robust text mining technologies today tend to be based on word representation and tend to rely a lot on statistical analysis, as we have discussed in this course
