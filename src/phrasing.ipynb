{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "def tokenize(sentence):\n",
    "    return [token if token not in STOP_WORDS else \"@\" for token in sentence.split() ]\n",
    "import re\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "    return re.sub(r'\\s{2,}', ' ', sentence)\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "def build_phrases(sentences):\n",
    "    bi = Phrases(sentences,\n",
    "                      min_count=3,\n",
    "                      progress_per=1000)\n",
    "    tri = Phrases(bi[sentences], min_count=3)\n",
    "    return bi, tri\n",
    "def sentence_to_bi_grams(big,trig, sentence):\n",
    "    return ' '.join(trig[big[sentence]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<gensim.models.phrases.Phrases object at 0x000001CCC479E160>, <gensim.models.phrases.Phrases object at 0x000001CCC3EAE0A0>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(\"textanalytics.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "with open(\"textretrieval.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "sentences = [clean_sentence(s) for s in sentences]\n",
    "sentences = [tokenize(s) for s in sentences]\n",
    "bi, tri = build_phrases(sentences)\n",
    "phrased = [sentence_to_bi_grams(bi, tri, s) for s in sentences]\n",
    "ph = [w for s in phrased for w in s.split() if \"_\" in w]\n",
    "phc = [(ph.count(x) / back_phc.get(x,1), x) for x in set(ph)]\n",
    "phc = [(a / aphc.get(b,1), b) for a,b in phc]\n",
    "phc = [(a / bphc.get(b,1), b) for a,b in phc]\n",
    "phc = [(a / cphc.get(b,1), b) for a,b in phc]\n",
    "phc.sort()\n",
    "selected = [b for a,b in phc if a > 2.99 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases= [x.replace(\"_\", \" \") for x in selected]   \n",
    "with open(\"phrases.txt\", \"w\") as out:\n",
    "    for x in phrases:\n",
    "        out.write(x + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, '1_minus'), (1, 'extreme_case'), (1, 'high_quality'), (1, 'hurricane_katrina'), (1, 'important_role'), (1, 'intuitively_makes'), (1, 'ive_shown'), (1, 'machine_learning'), (1, 'main_idea'), (1, 'method_works'), (1, 'multiple_levels'), (1, 'previous_slide'), (1, 'relatively_easy'), (1, 'second_line'), (1, 'time_period'), (1, 'x_sub'), (1, 'youre_interested'), (2, 'additional_information'), (2, 'common_sense'), (2, 'continue_talking'), (2, 'decision_making'), (2, 'previous_lecture'), (2, 'social_media'), (2, 'united_nations'), (3, 'multiple_times'), (3, 'new_orleans'), (3, 'new_york_times'), (3, 'parameter_values'), (3, 'total_number'), (3, 'youre_seeing'), (4, 'deep_learning'), (4, 'high_level'), (4, 'social_network'), (4, 'special_cases'), (4, 'web_pages'), (6, 'dont_necessarily'), (7, 'whats_interesting'), (9, 'doesnt_mean'), (9, 'pay_attention'), (12, 'new_york'), (13, 'weighted_average'), (14, 'doesnt_work'), (14, 'special_case'), (14, 'starting_point'), (15, 'weve_got'), (15, 'weve_talked'), (17, 'makes_sense'), (20, 'real_world'), (23, 'x_axis'), (26, 'time_series'), (31, 'y_axis'), (32, 'different_ways'), (32, 'long_time'), (38, 'gonna_talk'), (47, 'whats_happening'), (66, 'lets_look'), (80, 'looks_like'), (286, 'im_going'), (329, 'little_bit')]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "with open(\"cs125.dat\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "with open(\"noncs.dat\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "with open(\"bkgd.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "sentences = [clean_sentence(s) for s in sentences]\n",
    "sentences = [tokenize(s) for s in sentences]\n",
    "phrase_model = build_phrases(sentences)\n",
    "phrased = [sentence_to_bi_grams(bi,tri, s) for s in sentences]\n",
    "back_ph = [w for s in phrased for w in s.split() if \"_\" in w]\n",
    "back_phc = [(back_ph.count(x), x) for x in set(back_ph)]\n",
    "back_phc.sort()\n",
    "print(back_phc)\n",
    "back_phc = dict([(b,a) for a,b in back_phc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(\"noncs.dat\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "sentences = [clean_sentence(s) for s in sentences]\n",
    "sentences = [tokenize(s) for s in sentences]\n",
    "phrase_model = build_phrases(sentences)\n",
    "phrased = [sentence_to_bi_grams(bi,tri, s) for s in sentences]\n",
    "ph = [w for s in phrased for w in s.split() if \"_\" in w]\n",
    "phc = [(ph.count(x), x) for x in set(ph)]\n",
    "phc.sort()\n",
    "aphc = dict([(b,a) for a,b in phc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(\"bkgd.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "sentences = [clean_sentence(s) for s in sentences]\n",
    "sentences = [tokenize(s) for s in sentences]\n",
    "phrase_model = build_phrases(sentences)\n",
    "phrased = [sentence_to_bi_grams(bi,tri, s) for s in sentences]\n",
    "ph = [w for s in phrased for w in s.split() if \"_\" in w]\n",
    "phc = [(ph.count(x), x) for x in set(ph)]\n",
    "phc.sort()\n",
    "bphc = dict([(b,a) for a,b in phc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(\"cs125.dat\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "sentences = [clean_sentence(s) for s in sentences]\n",
    "sentences = [tokenize(s) for s in sentences]\n",
    "phrase_model = build_phrases(sentences)\n",
    "phrased = [sentence_to_bi_grams(bi,tri, s) for s in sentences]\n",
    "ph = [w for s in phrased for w in s.split() if \"_\" in w]\n",
    "phc = [(ph.count(x), x) for x in set(ph)]\n",
    "phc.sort()\n",
    "cphc = dict([(b,a) for a,b in phc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
