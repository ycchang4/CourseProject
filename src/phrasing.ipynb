{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa0fba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "def tokenize(sentence):\n",
    "    return [token if token not in STOP_WORDS else \"@\" for token in sentence.split() ]\n",
    "import re\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "    return re.sub(r'\\s{2,}', ' ', sentence)\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "def build_phrases(sentences):\n",
    "    bi = Phrases(sentences,\n",
    "                      min_count=3,\n",
    "                      progress_per=1000)\n",
    "    tri = Phrases(bi[sentences], min_count=3)\n",
    "    return bi, tri\n",
    "def sentence_to_bi_grams(big,trig, sentence):\n",
    "    return ' '.join(trig[big[sentence]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "380a1cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<gensim.models.phrases.Phrases object at 0x000001CCC479E160>, <gensim.models.phrases.Phrases object at 0x000001CCC3EAE0A0>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1da30cd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['effective_predictors',\n",
       " 'high_quality',\n",
       " 'like_hold',\n",
       " 'maximum_likelihood',\n",
       " 'space_model',\n",
       " 'sub_d',\n",
       " 'tf_idf',\n",
       " 'united_nations',\n",
       " 'word_association',\n",
       " 'youre_seeing',\n",
       " 'high_level',\n",
       " 'multiple_times',\n",
       " 'decision_making',\n",
       " 'social_media',\n",
       " 'active_research_area',\n",
       " 'ad_hoc_information',\n",
       " 'beta_sub',\n",
       " 'big_picture',\n",
       " 'categorisation_task',\n",
       " 'causal_relation',\n",
       " 'cause_overfitting',\n",
       " 'cheaper_hotels',\n",
       " 'clicked_documents',\n",
       " 'complete_understanding',\n",
       " 'conditional_entropies',\n",
       " 'cranfield_evaluation_methodology',\n",
       " 'deeper_analysis',\n",
       " 'deeper_natural',\n",
       " 'detailed_understanding',\n",
       " 'discovering_paradigmatic',\n",
       " 'discovering_syntagmatic',\n",
       " 'eats_occurs',\n",
       " 'expected_overlap',\n",
       " 'expensive_hotels',\n",
       " 'fewer_parameters',\n",
       " 'generally_preferred',\n",
       " 'generation_process',\n",
       " 'geometric_mean',\n",
       " 'gold_standard',\n",
       " 'good_authorities',\n",
       " 'google_file_system',\n",
       " 'gradually_group',\n",
       " 'ground_truth',\n",
       " 'high_dimensional_space',\n",
       " 'high_quality_information',\n",
       " 'hub_scores',\n",
       " 'independence_assumption',\n",
       " 'inference_rules',\n",
       " 'inferred_weights',\n",
       " 'iphone_6',\n",
       " 'k_topical',\n",
       " 'key_value_pair',\n",
       " 'keyword_queries',\n",
       " 'knowledge_base',\n",
       " 'lambda_sub_b',\n",
       " 'language_processing',\n",
       " 'large_numbers',\n",
       " 'latent_aspect_rating',\n",
       " 'linear_separator',\n",
       " 'major_complaints',\n",
       " 'meat_occurs',\n",
       " 'multiple_perspectives',\n",
       " 'n_dimensions',\n",
       " 'natural_languages',\n",
       " 'news_stream',\n",
       " 'noun_phrase',\n",
       " 'observed_evidence',\n",
       " 'optimal_utility',\n",
       " 'paradigmatically_related',\n",
       " 'parallel_processing',\n",
       " 'partial_parsing',\n",
       " 'pearson_correlation',\n",
       " 'penalize_long',\n",
       " 'prepositional_phrase',\n",
       " 'probability_ranking_principle',\n",
       " 'random_surfing',\n",
       " 'recommender_system',\n",
       " 'reference_language_model',\n",
       " 'relation_discovery',\n",
       " 'research_articles',\n",
       " 'sense_disambiguation',\n",
       " 'social_networks',\n",
       " 'speech_act',\n",
       " 'speech_recognition',\n",
       " 'statistical_approaches',\n",
       " 'statistical_learning',\n",
       " 'strongly_correlated',\n",
       " 'subjective_sensors',\n",
       " 'sublinear_transformation',\n",
       " 'tentative_clustering',\n",
       " 'term_ids',\n",
       " 'tf_weighting',\n",
       " 'tfidf_weighting',\n",
       " 'training_errors',\n",
       " 'uniform_code',\n",
       " 'updating_formula',\n",
       " 'user_wants',\n",
       " 'w1_given',\n",
       " 'w_sub',\n",
       " 'web_scale',\n",
       " 'works_better',\n",
       " 'additional_information',\n",
       " 'continue_talking',\n",
       " '01_bit',\n",
       " 'absolute_relevance',\n",
       " 'actual_ratings',\n",
       " 'advanced_algorithms',\n",
       " 'aspect_weights',\n",
       " 'average_precisions',\n",
       " 'based_approaches',\n",
       " 'baseline_system',\n",
       " 'binary_random_variable',\n",
       " 'care_divergent',\n",
       " 'co_occurrences',\n",
       " 'coin_shows',\n",
       " 'collaboration_network',\n",
       " 'completely_biased_coin',\n",
       " 'conditional_likelihood',\n",
       " 'conditional_probabilities',\n",
       " 'corresponding_elements',\n",
       " 'current_search_engines',\n",
       " 'design_effective',\n",
       " 'dirichlet_prior',\n",
       " 'discriminative_approaches',\n",
       " 'email_messages',\n",
       " 'external_time_series',\n",
       " 'extreme_case',\n",
       " 'fast_search',\n",
       " 'following_lectures',\n",
       " 'good_hubs',\n",
       " 'high_level_strategies',\n",
       " 'highest_score',\n",
       " 'hub_score',\n",
       " 'human_experts',\n",
       " 'ideal_dcg',\n",
       " 'k_nearest_neighbors',\n",
       " 'key_value_pairs',\n",
       " 'knowledge_graph',\n",
       " 'label_given',\n",
       " 'linear_transformation',\n",
       " 'machine_learning_methods',\n",
       " 'map_reduce',\n",
       " 'mean_average_precision',\n",
       " 'method_works',\n",
       " 'natural_question',\n",
       " 'new_generation',\n",
       " 'nlp_techniques',\n",
       " 'non_zero_probabilities',\n",
       " 'ordinal_logistic_regression',\n",
       " 'original_entropy',\n",
       " 'pagerank_score',\n",
       " 'parse_tree',\n",
       " 'peoples_opinions',\n",
       " 'pivoted_length_normalization',\n",
       " 'predefined_categories',\n",
       " 'predicted_values',\n",
       " 'previous_lectures',\n",
       " 'probabilistic_latent_semantic_analysis',\n",
       " 'probabilistic_modeling',\n",
       " 'pseudo_count',\n",
       " 'pseudo_segments',\n",
       " 'push_mode',\n",
       " 'push_versus_pull',\n",
       " 'query_like_hold',\n",
       " 'randomly_jump',\n",
       " 'relatively_easy',\n",
       " 'relatively_high',\n",
       " 'score_accumulators',\n",
       " 'search_tasks',\n",
       " 'semantically_related',\n",
       " 'small_numbers',\n",
       " 'smoothing_method',\n",
       " 'specific_examples',\n",
       " 'statistical_significance_test',\n",
       " 'subtle_differences',\n",
       " 'suggested_readings',\n",
       " 'supervised_machine_learning',\n",
       " 'syntactic_categories',\n",
       " 'syntactic_structures',\n",
       " 'syntagmatic_relation_discovery',\n",
       " 'takeaway_messages',\n",
       " 'tool_kit',\n",
       " 'user_clicked',\n",
       " 'word_association_mining',\n",
       " 'world_w',\n",
       " 'youre_interested',\n",
       " 'average_document_length',\n",
       " 'battery_life',\n",
       " 'bayesian_estimation',\n",
       " 'best_guess',\n",
       " 'binary_categorization',\n",
       " 'categorisation_results',\n",
       " 'common_sense_knowledge',\n",
       " 'discussed_earlier',\n",
       " 'domain_knowledge',\n",
       " 'dont_observe',\n",
       " 'empirically_defined',\n",
       " 'expected_count',\n",
       " 'f_measure',\n",
       " 'fair_coin',\n",
       " 'file_system',\n",
       " 'filtering_system',\n",
       " 'function_f',\n",
       " 'general_ideas',\n",
       " 'generated_independently',\n",
       " 'given_y',\n",
       " 'government_response',\n",
       " 'hidden_variables',\n",
       " 'hurricane_katrina',\n",
       " 'important_role',\n",
       " 'improve_scoring',\n",
       " 'intelligent_information',\n",
       " 'intuitively_makes',\n",
       " 'language_modeling',\n",
       " 'large_scale',\n",
       " 'length_encoding',\n",
       " 'local_maximum',\n",
       " 'machine_learning_techniques',\n",
       " 'mixture_models',\n",
       " 'modern_search_engines',\n",
       " 'multiple_levels',\n",
       " 'parameter_b',\n",
       " 'posterior_distribution',\n",
       " 'program_language',\n",
       " 'random_fluctuation',\n",
       " 'ranking_functions',\n",
       " 'rare_term',\n",
       " 'raw_count',\n",
       " 'relatively_small',\n",
       " 'second_line',\n",
       " 'sigir_papers',\n",
       " 'similarity_based_approaches',\n",
       " 'simplest_vector_space',\n",
       " 'sub_linear_transformation',\n",
       " 'task_support',\n",
       " 'test_collection',\n",
       " 'unary_coding',\n",
       " 'user_likes',\n",
       " 'y_1',\n",
       " '1_minus',\n",
       " 'aspect_rating',\n",
       " 'basic_measures',\n",
       " 'clustering_result',\n",
       " 'continued_discussion',\n",
       " 'different_locations',\n",
       " 'doesnt_occur',\n",
       " 'equally_likely',\n",
       " 'frequent_term',\n",
       " 'gamma_code',\n",
       " 'high_frequency',\n",
       " 'implicit_feedback',\n",
       " 'information_theory',\n",
       " 'jm_smoothing',\n",
       " 'method_works_better',\n",
       " 'opinion_target',\n",
       " 'parameter_values',\n",
       " 'presidential_election',\n",
       " 'previous_lecture',\n",
       " 'r_sub',\n",
       " 'real_world_variables',\n",
       " 'reciprocal_rank',\n",
       " 'slide_shows',\n",
       " 'smoothing_parameter',\n",
       " 'speech_tags',\n",
       " 'stock_prices',\n",
       " 'tf_transformation',\n",
       " 'theta_subj',\n",
       " 'user_stops',\n",
       " 'users_perspective',\n",
       " 'x_sub',\n",
       " 'y_given_x',\n",
       " 'z_values',\n",
       " 'causal_topics',\n",
       " 'classification_accuracy',\n",
       " 'cumulative_gain',\n",
       " 'dimensional_space',\n",
       " 'dirichlet_distribution',\n",
       " 'discover_syntagmatic_relations',\n",
       " 'discriminative_classifiers',\n",
       " 'generative_probabilistic_models',\n",
       " 'human_effort',\n",
       " 'inverse_document_frequency',\n",
       " 'linear_interpolation',\n",
       " 'mining_paper',\n",
       " 'news_articles',\n",
       " 'probabilistic_models',\n",
       " 'probabilistic_retrieval',\n",
       " 'product_reviews',\n",
       " 'pseudo_counts',\n",
       " 'score_accumulator',\n",
       " 'sentiment_weights',\n",
       " 'smoothing_methods',\n",
       " 'system_says_yes',\n",
       " 'theta_2',\n",
       " 'time_t',\n",
       " 'transition_matrix',\n",
       " 'total_number',\n",
       " 'anchor_text',\n",
       " 'assign_high_probabilities',\n",
       " 'ive_shown',\n",
       " 'naive_bayes',\n",
       " 'naive_bayes_classifier',\n",
       " 'natural_language_processing_techniques',\n",
       " 'non_zero',\n",
       " 'nontext_data',\n",
       " 'picture_shows',\n",
       " 'prior_knowledge',\n",
       " 'research_papers',\n",
       " 'speech_tagging',\n",
       " 'theta_sub_b',\n",
       " 'time_period',\n",
       " 'utility_function',\n",
       " 'bayesian_inference',\n",
       " 'continue_discussing',\n",
       " 'information_access',\n",
       " 'mining_algorithms',\n",
       " 'paradigmatic_relations',\n",
       " 'presidential_campaign',\n",
       " 'pull_mode',\n",
       " 'retrieval_functions',\n",
       " 'retrieval_systems',\n",
       " 'seen_earlier',\n",
       " 'sentiment_classification',\n",
       " 'simply_normalize',\n",
       " 'unigram_language_models',\n",
       " 'word_associations',\n",
       " 'additional_readings',\n",
       " 'alpha_sub_d',\n",
       " 'based_prediction',\n",
       " 'feature_values',\n",
       " 'generative_models',\n",
       " 'linear_combination',\n",
       " 'non_zero_probability',\n",
       " 'precision_recall',\n",
       " 'random_variables',\n",
       " 'recommender_systems',\n",
       " 'relevance_feedback',\n",
       " 'semantic_analysis',\n",
       " 'syntagmatic_relation',\n",
       " 'unary_code',\n",
       " 'unigram_language_model',\n",
       " 'clustering_bias',\n",
       " 'main_idea',\n",
       " 'posterior_probability',\n",
       " 'reduce_function',\n",
       " 'theta_sub',\n",
       " 'training_examples',\n",
       " 'upper_bound',\n",
       " 'actionable_knowledge',\n",
       " 'aspect_ratings',\n",
       " 'document_length_normalization',\n",
       " 'high_probabilities',\n",
       " 'language_models',\n",
       " 'logistic_regression',\n",
       " 'overall_ratings',\n",
       " 'previous_slide',\n",
       " 'probabilistic_topic',\n",
       " 'probability_mass',\n",
       " 'relevance_judgments',\n",
       " 'average_precision',\n",
       " 'document_id',\n",
       " 'logistical_regression',\n",
       " 'map_function',\n",
       " 'optimization_problem',\n",
       " 'original_query',\n",
       " 'page_rank',\n",
       " 'random_surfer',\n",
       " 'search_results',\n",
       " 'bm_25',\n",
       " 'document_ids',\n",
       " 'document_length',\n",
       " 'hidden_variable',\n",
       " 'probabilistic_model',\n",
       " 'different_aspects',\n",
       " 'length_normalization',\n",
       " 'syntagmatic_relations',\n",
       " 'term_frequency',\n",
       " 'arithmetic_mean',\n",
       " 'beta_values',\n",
       " 'collection_language_model',\n",
       " 'k_1',\n",
       " 'lower_bound',\n",
       " 'm_step',\n",
       " 'opinion_holder',\n",
       " 'opinion_mining',\n",
       " 'overall_rating',\n",
       " 'paradigmatic_relation',\n",
       " 'vector_space',\n",
       " 'contextual_text_mining',\n",
       " 'idf_weighting',\n",
       " 'tf_idf_weighting',\n",
       " 'bayes_rule',\n",
       " 'collaborative_filtering',\n",
       " 'dot_product',\n",
       " 'random_variable',\n",
       " 'e_step',\n",
       " 'maximum_likelihood_estimate',\n",
       " 'objective_function',\n",
       " 'system_b',\n",
       " 'background_language_model',\n",
       " 'sentiment_analysis',\n",
       " 'topic_models',\n",
       " 'generative_model',\n",
       " 'conditional_probability',\n",
       " 'ranked_list',\n",
       " 'theta_sub_d',\n",
       " 'basic_idea',\n",
       " 'word_distributions',\n",
       " 'background_model',\n",
       " 'information_retrieval',\n",
       " 'natural_language',\n",
       " 'natural_language_processing',\n",
       " 'web_search',\n",
       " 'non_relevant',\n",
       " 'similarity_function',\n",
       " 'maximum_likelihood_estimator',\n",
       " 'query_likelihood',\n",
       " 'em_algorithm',\n",
       " 'non_text',\n",
       " 'common_words',\n",
       " 'scoring_function',\n",
       " 'conditional_entropy',\n",
       " 'machine_learning',\n",
       " 'search_engines',\n",
       " 'mutual_information',\n",
       " 'text_categorization',\n",
       " 'likelihood_function',\n",
       " 'ranking_function',\n",
       " 'training_data',\n",
       " 'inverted_index',\n",
       " 'mixture_model',\n",
       " 'search_engine',\n",
       " 'language_model',\n",
       " 'relevant_documents',\n",
       " 'vector_space_model']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "with open(\"textanalytics.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "with open(\"textretrieval.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "sentences = [clean_sentence(s) for s in sentences]\n",
    "sentences = [tokenize(s) for s in sentences]\n",
    "bi, tri = build_phrases(sentences)\n",
    "phrased = [sentence_to_bi_grams(bi, tri, s) for s in sentences]\n",
    "ph = [w for s in phrased for w in s.split() if \"_\" in w]\n",
    "phc = [(ph.count(x) / back_phc.get(x,1), x) for x in set(ph)]\n",
    "# phc = [(a / aphc.get(b,1), b) for a,b in phc]\n",
    "# phc = [(a / bphc.get(b,1), b) for a,b in phc]\n",
    "# phc = [(a / cphc.get(b,1), b) for a,b in phc]\n",
    "phc.sort()\n",
    "selected = [b for a,b in phc if a > 2.99 ]\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f39ffb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bec5dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases= [x.replace(\"_\", \" \") for x in selected]   \n",
    "with open(\"phrases.txt\", \"w\") as out:\n",
    "    for x in phrases:\n",
    "        out.write(x + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "127852e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, '1_minus'), (1, 'extreme_case'), (1, 'high_quality'), (1, 'hurricane_katrina'), (1, 'important_role'), (1, 'intuitively_makes'), (1, 'ive_shown'), (1, 'machine_learning'), (1, 'main_idea'), (1, 'method_works'), (1, 'multiple_levels'), (1, 'previous_slide'), (1, 'relatively_easy'), (1, 'second_line'), (1, 'time_period'), (1, 'x_sub'), (1, 'youre_interested'), (2, 'additional_information'), (2, 'common_sense'), (2, 'continue_talking'), (2, 'decision_making'), (2, 'previous_lecture'), (2, 'social_media'), (2, 'united_nations'), (3, 'multiple_times'), (3, 'new_orleans'), (3, 'new_york_times'), (3, 'parameter_values'), (3, 'total_number'), (3, 'youre_seeing'), (4, 'deep_learning'), (4, 'high_level'), (4, 'social_network'), (4, 'special_cases'), (4, 'web_pages'), (6, 'dont_necessarily'), (7, 'whats_interesting'), (9, 'doesnt_mean'), (9, 'pay_attention'), (12, 'new_york'), (13, 'weighted_average'), (14, 'doesnt_work'), (14, 'special_case'), (14, 'starting_point'), (15, 'weve_got'), (15, 'weve_talked'), (17, 'makes_sense'), (20, 'real_world'), (23, 'x_axis'), (26, 'time_series'), (31, 'y_axis'), (32, 'different_ways'), (32, 'long_time'), (38, 'gonna_talk'), (47, 'whats_happening'), (66, 'lets_look'), (80, 'looks_like'), (286, 'im_going'), (329, 'little_bit')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1_minus': 1,\n",
       " 'extreme_case': 1,\n",
       " 'high_quality': 1,\n",
       " 'hurricane_katrina': 1,\n",
       " 'important_role': 1,\n",
       " 'intuitively_makes': 1,\n",
       " 'ive_shown': 1,\n",
       " 'machine_learning': 1,\n",
       " 'main_idea': 1,\n",
       " 'method_works': 1,\n",
       " 'multiple_levels': 1,\n",
       " 'previous_slide': 1,\n",
       " 'relatively_easy': 1,\n",
       " 'second_line': 1,\n",
       " 'time_period': 1,\n",
       " 'x_sub': 1,\n",
       " 'youre_interested': 1,\n",
       " 'additional_information': 2,\n",
       " 'common_sense': 2,\n",
       " 'continue_talking': 2,\n",
       " 'decision_making': 2,\n",
       " 'previous_lecture': 2,\n",
       " 'social_media': 2,\n",
       " 'united_nations': 2,\n",
       " 'multiple_times': 3,\n",
       " 'new_orleans': 3,\n",
       " 'new_york_times': 3,\n",
       " 'parameter_values': 3,\n",
       " 'total_number': 3,\n",
       " 'youre_seeing': 3,\n",
       " 'deep_learning': 4,\n",
       " 'high_level': 4,\n",
       " 'social_network': 4,\n",
       " 'special_cases': 4,\n",
       " 'web_pages': 4,\n",
       " 'dont_necessarily': 6,\n",
       " 'whats_interesting': 7,\n",
       " 'doesnt_mean': 9,\n",
       " 'pay_attention': 9,\n",
       " 'new_york': 12,\n",
       " 'weighted_average': 13,\n",
       " 'doesnt_work': 14,\n",
       " 'special_case': 14,\n",
       " 'starting_point': 14,\n",
       " 'weve_got': 15,\n",
       " 'weve_talked': 15,\n",
       " 'makes_sense': 17,\n",
       " 'real_world': 20,\n",
       " 'x_axis': 23,\n",
       " 'time_series': 26,\n",
       " 'y_axis': 31,\n",
       " 'different_ways': 32,\n",
       " 'long_time': 32,\n",
       " 'gonna_talk': 38,\n",
       " 'whats_happening': 47,\n",
       " 'lets_look': 66,\n",
       " 'looks_like': 80,\n",
       " 'im_going': 286,\n",
       " 'little_bit': 329}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "with open(\"cs125.dat\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "with open(\"noncs.dat\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "with open(\"bkgd.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "sentences = [clean_sentence(s) for s in sentences]\n",
    "sentences = [tokenize(s) for s in sentences]\n",
    "phrase_model = build_phrases(sentences)\n",
    "phrased = [sentence_to_bi_grams(bi,tri, s) for s in sentences]\n",
    "back_ph = [w for s in phrased for w in s.split() if \"_\" in w]\n",
    "back_phc = [(back_ph.count(x), x) for x in set(back_ph)]\n",
    "back_phc.sort()\n",
    "print(back_phc)\n",
    "back_phc = dict([(b,a) for a,b in back_phc])\n",
    "back_phc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dac2072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(\"noncs.dat\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "sentences = [clean_sentence(s) for s in sentences]\n",
    "sentences = [tokenize(s) for s in sentences]\n",
    "phrase_model = build_phrases(sentences)\n",
    "phrased = [sentence_to_bi_grams(bi,tri, s) for s in sentences]\n",
    "ph = [w for s in phrased for w in s.split() if \"_\" in w]\n",
    "phc = [(ph.count(x), x) for x in set(ph)]\n",
    "phc.sort()\n",
    "aphc = dict([(b,a) for a,b in phc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "254dd643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_minus': 1,\n",
       " 'decision_making': 1,\n",
       " 'hurricane_katrina': 1,\n",
       " 'intuitively_makes': 1,\n",
       " 'ive_shown': 1,\n",
       " 'real_world': 1,\n",
       " 'relatively_easy': 1,\n",
       " 'starting_point': 1,\n",
       " 'total_number': 1,\n",
       " 'united_nations': 1,\n",
       " 'weve_talked': 1,\n",
       " 'whats_happening': 1,\n",
       " 'x_sub': 1,\n",
       " 'youre_seeing': 1,\n",
       " 'gonna_talk': 2,\n",
       " 'new_orleans': 2,\n",
       " 'previous_lecture': 2,\n",
       " 'doesnt_mean': 3,\n",
       " 'doesnt_work': 3,\n",
       " 'special_case': 3,\n",
       " 'pay_attention': 5,\n",
       " 'weve_got': 5,\n",
       " 'different_ways': 6,\n",
       " 'long_time': 6,\n",
       " 'makes_sense': 6,\n",
       " 'lets_look': 13,\n",
       " 'weighted_average': 13,\n",
       " 'little_bit': 17,\n",
       " 'looks_like': 22,\n",
       " 'time_series': 23,\n",
       " 'x_axis': 23,\n",
       " 'y_axis': 31,\n",
       " 'im_going': 123}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "with open(\"bkgd.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "sentences = [clean_sentence(s) for s in sentences]\n",
    "sentences = [tokenize(s) for s in sentences]\n",
    "phrase_model = build_phrases(sentences)\n",
    "phrased = [sentence_to_bi_grams(bi,tri, s) for s in sentences]\n",
    "ph = [w for s in phrased for w in s.split() if \"_\" in w]\n",
    "phc = [(ph.count(x), x) for x in set(ph)]\n",
    "phc.sort()\n",
    "bphc = dict([(b,a) for a,b in phc])\n",
    "bphc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b96880ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(\"cs125.dat\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')\n",
    "sentences = [clean_sentence(s) for s in sentences]\n",
    "sentences = [tokenize(s) for s in sentences]\n",
    "phrase_model = build_phrases(sentences)\n",
    "phrased = [sentence_to_bi_grams(bi,tri, s) for s in sentences]\n",
    "ph = [w for s in phrased for w in s.split() if \"_\" in w]\n",
    "phc = [(ph.count(x), x) for x in set(ph)]\n",
    "phc.sort()\n",
    "cphc = dict([(b,a) for a,b in phc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
