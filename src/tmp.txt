lecture 1
['video data might require computer vision', 'text retrieval also helps minimize human effort', 'network sensor would monitor network traffic', 'help us turn big text data', 'help people digest text data', 'distinguish two different results one', 'taught another separate mooc', 'generally need different algorithms', 'also develop special algorithms', 'geo sensor would sense', 'text mining supplies knowledge', 'two terms text mining', 'help us solve', 'two phrases literally', 'two different angles', 'cover specialized algorithms', 'relatively small amount', 'discussed various techniques', 'multimedia data like', 'understand video content', 'high level introduction', 'high quality information', 'high quality information', 'high quality information', 'text retrieval support', 'text retrieval refers', 'text mining systems', 'term text mining', 'called text mining', 'turning text data', 'treating text data', 'raw text data', 'raw text data', 'original text data', 'original text data', 'non text data', 'non text data', 'non text data', 'non text data', 'non text data', 'consuming text data', 'optimal decision making', 'data mining module', 'finding relevant information', 'two terms roughly', 'human effort', 'view text data', 'thermometer would watch', 'turn text data', 'mining text data', 'text analytics mean', 'term text analytics', 'relevant text data', 'giving actionable knowledge', 'called actionable knowledge', 'would also sense', 'data mining problems', 'network sensor', 'case text data', 'data mining problem', 'data mining problem', 'humans would express', 'often contain knowledge', 'high level', 'sense human', 'human sensors', 'generally would', 'text retrieval', 'text retrieval', 'text retrieval', 'text retrieval', 'text retrieval', 'text retrieval', 'text retrieval', 'text retrieval', 'mining algorithms', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data']
lecture 2
['picture basically covered multiple types', 'help us make context sensitive analysis', 'historical stock price data would', 'different time might also pay attention', 'extracting high quality information', 'certainly also say something', 'partition text data based', 'human sensor would perceive', 'real world variable based', 'example entity relation graphs', 'actually provide interesting angles', 'many real world problems', 'form interesting comparison scenarios', 'text data would contain', 'might partition text data', 'often called predictive analytics', 'using knowledge representation language', 'text based prediction problems', 'include non text data', 'provide interesting angles', 'stock prices based', 'cover natural language processing', 'certain interesting variables', 'human observer would look', 'generating test data', 'different people would', 'useful context information', 'completing whatever tasks', 'different time periods', 'slide also serves', 'properties could include', 'pay attention', 'human would express', 'using text data', 'using text data', 'understanding text data', 'turn text data', 'text data generally', 'text data alone', 'require text data', 'original text data', 'generating text data', 'help us predict', 'text mining algorithms', 'real world variables', 'real world variables', 'real world variables', 'analyzing text data', 'analyze text data', 'text mining analytics', 'mining text data', 'also context associated', 'useful lexical knowledge', 'general picture would', 'could also use', 'could also use', 'mine word associations', 'fairly general landscape', 'observed world ".', 'non text data', 'general data mining', 'text mining problem', 'help us', 'stock prices', 'sensor would', 'human sensor', 'different angles', 'mining problems', 'different context', 'natural language', 'word associations', 'particular entity', 'human ...', 'discover something', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'supplying context', 'could analyze', 'real world', 'real world']
lecture 3
['today particular statistical machine learning methods', 'art natural language processing techniques cannot', 'prepositional phrase attachment ambiguity meaning ...', 'boy b1 means b1 refers', 'natural language sentences precisely', 'another problem anaphora resolution', 'natural language content analysis', 'natural language content analysis', 'complete parse tree correctly', 'make human communications efficient', 'need common sense reasoning', 'understand natural language precisely', 'also another possible interpretation', 'speech tagging fairly well', 'precisely understanding natural language', 'cannot really fully understand', 'might also make inferences', 'called speech act analysis', 'statistical analysis methods', 'dog d1 means d1', 'natural language processing', 'natural language processing', 'natural language processing', 'natural language processing', 'say language processing', 'speech act analysis', 'quit smoking implies', 'probably achieve 90', 'annotate enough data', 'infer ... might', 'natural language first', 'syntactic ambiguity refers', 'still cannot solve', 'precisely 97 %.', 'using particular datasets', 'slides also shows', 'makes communication efficient', 'inferred meaning based', 'slide clearly shows', 'sentences general positive', 'also multiple meanings', 'word sense disambiguation', 'common sense knowledge', 'common sense knowledge', 'mine text data', 'got ambiguous part', 'natural language sentence', 'real world entities', 'reminding another person', 'another classic example', 'possibly different things', 'represent text data', 'word like design', 'word level ambiguity', 'text mining problem', 'john persuaded bill', 'person actually takes', 'numbers may need', 'mathematical sense etc', 'called syntactic parsing', 'language precisely', 'natural language', 'natural language', 'computer also needs', 'prepositional phrase', 'prepositional phrase', 'parse tree', 'computer would need', 'computer would actually', 'computational methods', 'generally difficult problem', 'particular tagging', 'generally cannot', 'cannot rely', 'speech tagging', 'speech tagging', 'computers today', 'fully understand', 'mathematical sense', 'make sense', 'ambiguity lies', 'another difficulty', 'another aspect', 'really understand', 'text mining', 'syntactical analysis', 'sentiment analysis', 'semantic analysis', 'semantic analysis', 'semantic analysis', 'really meaningful', 'pragmatic analysis', 'lexical analysis', 'data set', 'data set', 'noun phrase', 'multiple choices', 'formally represent', 'everything correctly', 'combinations may']
lecture 4
['deeper nlp requires common sense knowledge', 'typically would require human effort', 'somewhat different syntactic categories', 'one hundred percent correctly', 'get complete parsing correct', 'use supervised machine learning', 'deeper natural language analysis techniques', 'require much human effort', 'precise deep semantic analysis', 'use machine learning techniques', 'large scale text mining', 'deeper analysis techniques', 'use statistical nlp', 'general nlp tends', 'shallow nlp based', 'mine text data', 'analyzing text data', 'general statistical approaches', 'human effort', 'analyze text data', 'annotate text data', 'natural language', 'natural language', 'deeper understanding', 'first nlp', 'large scale', 'sense also', 'statistical analysis', 'text mining', 'text mining', 'deep understanding', 'use humans', 'shallow analysis', 'text data', 'text data', 'text data', 'general statistical', 'shallow techniques', 'generally based', 'various ways', 'take away', 'speech tagging', 'restaurant ".', 'parse depending', 'man saw', 'main points', 'limited domains', 'john owns', 'give us', 'statistical methods', 'useful techniques', 'generally combine', 'generally applicable', 'training examples', 'specific examples', 'bring humans', 'two kinds', 'sentence like', 'practical applications', 'main topic', 'important tasks', 'require', 'computers today', 'analysis', 'text', 'techniques', 'techniques', 'techniques', 'data', 'scale', 'shallow', 'general', 'general', 'analyze', 'generally', 'humans', 'examples', 'annotate', 'annotate', 'useful', 'two', 'topic', 'today', 'tasks', 'methods', 'like', 'computers', 'applications', 'also', 'also', 'working', 'well', 'versus', 'understand', 'understand', 'turned', 'turned', 'top', 'thus', 'telescope']
lecture 5
['sophisticated natural language processing techniques', 'would give us direct knowledge', 'original word sequence representation instead', 'text data analysis text mining', 'combine related words together', 'added yet another level', 'natural language processing', 'natural language processing', 'natural language processing', 'techniques would require', 'machine learning programs', 'actually give us', 'many different ways', 'many different ways', 'correcting grammar mistakes', 'would allow us', 'slightly less general', 'really analyze semantics', 'natural language text', 'shallow analysis based', 'even less robust', 'even inference rules', 'completely accurate representation', 'interesting analysis possibilities', 'representing text data', 'representing text data', 'representing text data', 'natural language sentence', 'even recognizing words', 'even syntactic analysis', 'representation less robust', 'might make mistakes', 'might make mistakes', 'might make mistakes', 'necessary deeper representation', 'also less robust', 'analyze text data', '... another level', 'identify words like', 'discuss text representation', 'natural language', 'allow us', 'special techniques', 'text mining', 'text mining', 'less accurate', 'word segmentation', 'word boundaries', 'inference rules', 'help us', 'make mistakes', 'representing text', 'representing text', 'representing text', 'shallow analysis', 'also related', 'representation would', 'text data', 'text data', 'text data', 'text data', 'text data', 'many applications', 'deeper analysis', 'deeper analysis', 'accurate analysis', 'syntactic structure', 'still necessary', 'another person', 'general way', 'general way', 'even harder', 'sentiment analysis', 'semantic analysis', 'semantic analysis', 'interesting analysis', 'interesting analysis', 'deep analysis', 'text representation', 'writing styles', 'whole collection', 'whole collection', 'speech tags', 'speech tags', 'speech tags', 'speech acts', 'semantic way', 'right types', 'right side', 'relation recognition', 'providing features', 'plus sign', 'picture shows', 'often needed', 'observer order', 'news articles', 'necessarily replace', 'little bit', 'little bit', 'knowledge graph']
lecture 6
['although knowing word boundaries might actually also help', 'might help us classify text objects', 'stylistic analysis generally requires syntactical representation', 'customer service people might want', 'would enable logic inference ofcourse', 'fragile natural language processing techniques', 'word based representation also powerful', 'business intelligence people might', 'using stream processing algorithms', 'covering techniques mainly based', 'real world energy entity', 'structure based feature features', 'different textual representation tends', 'important research topics today', 'gradually add additional representations', 'use graph mining algorithms', 'require much manual effort', 'analysis would enable applications', 'help biologists manage', 'syntactical structure representation', 'actually surprisingly powerful', 'also several advantages', 'word based representation', 'word based representation', 'word based representation', 'word based representation', 'also add ontology', 'also analysis capacities', 'add logic predicates', 'major topics covered', 'actually quite sufficient', 'word relation analysis', 'final column shows', 'deeper analysis results', 'word boundaries', 'major takeaway points', 'syntactic graph analysis', 'enable richer analysis', 'knowledge graph analysis', 'different categories corresponding', 'enable different analysis', 'people might', 'information network analysis', 'third column shows', 'first column shows', 'analyze syntactic graphs', 'text representation determines', 'intelligent knowledge assistant', 'partial structures extracted', 'adding syntactic structures', 'different authors want', 'understanding consumers opinions', 'interesting representation opportunities', 'represent text data', 'opinion related applications', 'text mining applications', 'natural language', 'support sophisticated applications', 'also use', 'discovering related words', 'biologists might', 'enabled analysis techniques', 'powerful analysis', 'manual effort', 'classify articles', 'mining algorithms', 'gradually add', 'different representations', 'also generate', 'actually written', 'actually track', 'generally need', 'real applications', 'different categories', 'would open', 'relation graphs', 'analysis techniques', 'text data', 'represent text', 'extracted information', 'support analysis', 'sentiment analysis', 'integrative analysis', 'syntactic structures', 'syntactic structures', 'syntactic structures', 'research problem', 'major complaints', 'important benefit', 'logical predicates', 'knowing', 'k authors', 'intelligent program', 'although', 'scientist want', 'logical representation', 'various applications', 'text recognition', 'string text', 'interesting applications']
lecture 7
['really help us predict whether text also occurs', 'sentence would generally help us predict', 'question would help us discover syntagmatic relations', 'syntagmatic relations would help us show', 'help us discover syntagmatic relations', 'whether meat also occurs indeed', 'many times two words occur together', 'knowing whether eats occurs', 'positive versus negative reviews', 'knowing whether eats occurred', 'introduce additional related words', 'often called query expansion', 'syntagmatic relation essentially captures', 'relatively low individual occurrences', 'two words around eats', 'whenever eats occurs', 'paradigmatically related words tend', 'become somewhat meaningless', 'suggest related queries', 'actually closely related', 'discover paradigmatic relation', 'words often either', 'similarity value would', 'feature word like', 'usually occur alone', 'user could navigate', 'many nlp tasks', 'larger expression based', 'learn syntagmatic relations', 'paradigmatic relation would', 'called syntagmatic relation', 'many different ways', 'use word associations', 'use word associations', 'really related', 'also helps explain', 'use related words', 'sentence would still', 'learn paradigmatic relations', 'two word relations', 'similar locations relative', 'discovering syntagmatic relation', 'cannot replace cat', 'capture basic relations', 'word association mining', 'mine word associations', 'discovering word associations', 'discovering paradigmatic relation', 'general replace one', 'words also tend', 'case clearly dog', 'taken away cat', 'would still', 'see eats occur', 'syntagmatic relations', 'syntagmatic relations', 'also even look', 'help', 'generally say', 'generally occur', 'high context similarity', 'call right context', 'syntagmatic relation', 'syntagmatic relation', 'syntagmatic relation', 'syntagmatic relation', 'syntagmatic relation', 'syntagmatic relation', 'syntagmatic relation', 'syntagmatic relation', 'paradigmatically relation', 'two relations', 'two relations', 'similar left context', 'sentences around', 'putting together', 'occur together', 'occur together', 'negative opinions', 'many applications', 'many applications', 'individual occurrences', 'word relations', 'computer would', 'dog would', 'taken away', 'taken away', 'describe relations', 'basically relations', 'word association', 'word associations', 'understanding positive', 'semantically related', 'see many', 'say also', 'related semantically', 'also occur', 'also done', 'paradigmatic relation', 'paradigmatic relation']
lecture 8
['gives us really semantically related words really give us', 'word based representation would actually give us interesting way', 'would give us loosely related paradigmatic relations', 'context may contain adjacent words like eats', 'might favor matching one frequent term', 'another word dog might give us', 'two probability distributions representing two contexts', 'dot product infact gives us', 'word association called paradigmatic relations', 'ate occurred three times etc', 'text retrieval techniques would realize', 'eats occured five times', 'ofcourse would allow us', 'flexibility also allows us', 'adjacent words like saturday', 'gives us one perspective', 'would give us', 'treats every word equally', 'discovering paradigmatically related words', 'see frequency vector representing', 'two randomly picked words', 'two randomly picked words', 'yi gives us', 'two words indeed occur', 'paradigmatic relations discovery', 'xi gives us', 'discovering paradigmatic relations', 'word like eats', 'word like eats', 'relatively low frequency', 'might indeed make', 'would actually pick', 'gives us', 'paradigmatic relation discovery', 'little bit questionable', 'defining one dimension', 'two contexts would', 'actually probability distribution', 'really surprising', 'two vectors actually', 'randomly picked word', 'context actually contains', 'word like dog', 'high dimensional space', 'another word like', 'two words picked', 'paradigmatically relation discovery', 'also assign weights', 'see words eats', 'naturally application specific', 'intuitively makes sense', 'allow us', 'words representation', 'eight words around', 'would work well', 'paradigmatic relations', 'vector space model', 'vector space model', 'vector space model', 'vector space model', 'paradigmatically related', 'similarity function actually', 'make sense right', 'one possible approach', 'one possible approach', 'formula little bit', 'two potential problems', 'see words like', 'pick another word', 'capture similarity based', 'dog based', 'much related', 'also different ways', 'would like', 'would like', 'contain cat', 'eowc expected overlap', 'share similar contexts', 'one term', 'dot product', 'dot product', 'seeing identical words', 'give', 'cat big cat', 'context based', 'word like', 'two words', 'one vector', 'two contexts', 'two contexts', 'two contexts', 'two contexts', 'two contexts', 'another perspective', 'text around', 'randomly pick', 'way', 'interesting', 'information retrieval', 'information retrieval']
lecture 9
['containing elements representing normalized bm 25 values', 'help us design effective similarity function', 'continue discussing paradigmatic relation discovery', 'weight would never exceed k', 'also another interesting transformation called', 'favors matching one frequent term', 'treats every word equally', 'method called expected overlap', 'generally non negetive number', 'highly similar word pairs', 'idf would give us', 'called idf term weighting', 'expected overlap account approach', 'using text retrieval models', 'bm 25 transformation', 'would help control', 'matching one frequent term', 'matching one frequent term', 'would give us', 'give us curve', 'paradigmatic relation mining', 'documents containing word', 'length formalization together', 'many common words like', 'discover paradigmatic relation', 'highly weighted terms', 'control length normalization', 'also makes sense', 'sub linear normalization', 'effective transformation function', 'high frequency counts', 'discover paradigmatic relations', 'non zero counts', 'highest weighted terms', 'apply idf weighting', 'introduced something else', 'bm 25', 'sub linear transformation', 'sub linear transformation', 'sub linear curve', 'would contribute equally', 'discovering paradigmatic relations', 'discovering paradigmatic relations', 'low value close', 'text retrieval techniques', 'inverse document frequency', 'inverse document frequency', 'discover syntagmatic relations', 'discover syntagmatic relations', 'really high counts', 'somewhat lower weights', 'little bit right', 'actually something similar', 'high frequency terms', 'high frequency terms', 'high frequency terms', 'original data set', 'share similar context', 'idf weighted vector', 'content word like', 'penalize popular terms', 'upper bound k', 'many different ways', 'bm25 retrieval model', 'average document length', 'frequently frequent term', 'matching one term', 'x /( x', 'two values', 'common word like', 'raw frequency count', 'lower weights depending', 'general would also', 'corresponding context documents', 'collected context documents', 'document frequency means', 'problems also occur', 'matching rare words', 'would likely see', 'approaches also represent', 'retrieval heuristics used', 'high counts', 'two candidate words', 'paradigmatic relations', 'paradigmatic relations', 'normalized count', 'frequent term', 'normalized weight', 'another parameter', 'normalization function', 'idf weighting', 'idf weighting', 'upper bound', 'syntagmatic relations', 'syntagmatic relations', 'linear transformation', 'linear transformation', 'linear transformation', 'similarity function', 'term frequency']
lecture 10
['random variable like x sub w', 'still predicted whether w', 'random variable x sub coin', 'function form looks like', 'head hotel equally likely', 'completely biased coin corresponds', 'function would show exactly', 'assume high entropy words', 'x sub w', 'question whenever eats occurs', 'equation looks like', 'equally likely taking', 'syntagmatic relation discovery', 'correlated co occurrences', 'random experiment like', 'completed biased coin', 'would generally find', 'function looks like', 'completely biased coin', 'completely biased coin', 'completely biased coin', 'completely biased coin', 'syntagmatic relations hold', 'discover syntagmatic relations', 'fair coin corresponds', 'clearly would expect', 'random variable inside', 'random variable equals', 'binary random variable', 'binary random variable', 'would force us', 'word association mining', 'another extreme case', 'see one word occurs', 'using coin tossing', 'random variable taking', 'coin always shows', 'measure called entropy', 'two probabilities would', 'also relatively easy', 'one quantitatively measure', 'might occur together', 'precisely one word', 'entropy reached maximum', 'three words shown', 'two values zero', 'particular word present', 'words also tend', 'world w', 'w denotes', 'random variable', 'random variable', 'random variable', 'random variable', 'random variable', 'random variable', 'random variable', 'random variable', 'random variable', 'random variable', 'completely fair', 'eats like', 'show eats', 'high entropy', 'coin shows', 'coin shows', 'coin shows', 'fair coin', 'fair coin', 'fair coin', 'occurs everywhere', 'prediction would', 'coin tossing', 'clearly depends', 'completely symmetric', 'two probabilities', 'two ends', 'two cases', 'two cases', 'tell us', 'gives us', 'always takes', 'measure introduced', 'function defined', 'three words', 'three words', 'entropy function', 'entropy function', 'entropy function', 'special cases', 'quantitative way', 'multiple values', 'might conclude', 'may also', 'mathematically proved', 'explicitly plugged', 'end point', 'easily generalized', 'discussed earlier', 'different distributions']
lecture 11
['eats would help us predict wether meat occurs', 'conditional entropy gives us directly one way', 'really tell us much', 'strongest k syntagmatic relations', 'really help us reduce', 'putting different scenarios together', 'discrete random variables x', 'help us reduce entropy', 'also help us predict', 'entropy conditional entropy would reach', 'comparable across different words', 'target word like w1', 'word like meat occurs', 'eats could reduce uncertainty', 'w1 given different words', 'always help us', 'minimum possible value would', 'conditional entropy would capture', 'help us predict', 'know whether meat occurs', 'smaller entropy means easier', 'help us mine', 'would allow us', 'potential syntagmatic relations', 'mining syntagmatic relations', 'discovering syntagmatic relations', 'discovering syntagmatic relations', 'capturing syntagmatic relations', 'helps us predict', 'helped us predict', 'syntagmatic relation discovery', 'random variable corresponding', 'intuitively makes sense', 'capture syntagmatic relations', 'target word w1', 'different upper bounds', 'different upper bounds', 'another random variable', 'strongly correlated words', 'gives us', 'known eats occurring', 'conditional entropy reaches', 'conditional entropy conditioned', 'cannot really compare', 'w1 given w2', 'w1 given w2', 'word association mining', 'minimum possible value', 'probabilities indicating whether', 'one word given', 'corresponding conditional probabilities', 'use conditional entropy', 'use conditional entropy', 'use mutual information', 'entropy conditional entropy', 'know eats occured', 'know eats occured', 'word like meat', 'particular word w1', 'w1 given w3', 'word occurs', 'special case listed', 'algorithm would look', 'knowing whether knowing', 'meat occurs', 'tells us', 'tells us', 'word given', 'upper bounds', 'words would', 'predict whether', 'word w1', 'variable x', 'predict w1', 'another word', 'would mean', 'would change', 'different scenario', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'words w2', 'reduce uncertainty', 'looks like', 'really related', 'whether eats']
lecture 12
['rank different ys using conditional entropy would give', 'continue discussing syntagmatic relation discovery', 'discover strong syntagmatic relations globally', 'comparison would tell us whether', 'conditional entropy would never exceed', 'knowing one would allow us', 'really help us predict eats', 'possibly reduced conditional entropy', 'always help us potentially', 'comparable across different pairs', 'eats cause major information', 'first 2 probabilities corresponding', 'two conditional entropies h', 'mutual information allows us', 'syntagmatic relation discovery', 'discover syntagmatic relations', 'tell us anything', 'equations allow us', 'called mutual information denoted', 'syntagmatic relation mining', 'two probabilities representing presence', 'syntactic relation mining', 'help us predicting', 'two words occur together', 'compare different pairs', 'mutual information would', 'already saw two', '2 joint distributions', 'conditional entropy computed', 'two variables taking', 'four different scenarios', 'two scenarios depending', 'four possible scenarios', 'callback labeler divergance', 'two random variables', 'two random variables', 'two random variables', 'two random variables', 'two random variables', 'two random variables', 'introduce mutual information', 'high mutual information', 'computing mutual information', 'called mutual information', 'actual joint distribution', 'equations also follow', 'also non negative', 'using mutual information', 'additional constraints listed', 'joint actual observed', 'first word occurred', 'signal word actually', 'whenever eats occurs', 'mutual information reaches', 'mutual information measures', 'lower mutual information', 'expected joint distribution', 'weather second word', 'allows us', 'mutual information larger', 'higher mutual information', 'really comparable', 'already known right', 'mutual information question', 'two values 0', 'means knowing one', 'two probabilities shown', 'different pairs', 'first word occurs', 'would mean', 'would expect', 'similarly knowing eats', 'hurt us', 'words also tend', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'random variables', 'two variables', 'two variables', 'two distributions', 'different values', 'two scenarios', 'called kl', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information']
lecture 13
['select medical relations using random walks', 'pseudo segments would contribute additional counts', 'random variable x conditional entropy', 'would give us another way', 'would give us different kinds', 'introduced multiple statistical approaches', 'using various statistical measures', 'provides yet another way', 'syntactic medical relation discovery', 'least non zero counter', 'counting co occurrences using', 'seen like medical relations', 'also use different ways', 'one single pseudo segment', 'two words occur together', 'would give us', 'would give us', 'actually discover interesting relations', 'using different contexts', 'using bm 25', 'contain world w one', 'non composition compositional', 'pure statistical approaches', 'strongest cinematical relationship', 'helmet human effort', 'commonly used technique', 'discover different flavors', 'discover lexical atoms', 'maximum likelihood estimate', 'discover syntagmatic relations', 'discover syntagmatic relations', 'four different combinations', 'word association mining', 'four pseudo segments', 'paradigmatically relation discovery', 'two basic associations', 'say mutual information', 'zero counts sometimes', 'two pseudo segments', 'syntagmatic relation discovery', 'potentially also suggest', 'one word occurs', 'least one count', 'discovering syntagmatic relations', 'longer text article', 'technique called smoothing', 'give us', 'text data mining', 'zero probability probability', 'use mutual information', 'compute mutual information', 'example hot dog', 'syntagmatic relations', 'would lead', 'would believe', 'world graphs', 'unified way', 'principled way', 'interesting variations', 'best way', 'pseudo segments', 'pseudo segments', 'relation discovery', 'different pairs', 'allows us', 'pseudo segment', 'discovery associations', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'zero probability', 'zero probability', 'second counter', 'w one', 'third one', 'paradigmatic relations', 'potentially possible', 'actually one', 'two words', 'word like', 'information theory', 'information retrieval', 'three counts', 'three counts', 'text window', 'empirical counts', 'entropy reduction', 'entropy reduction', 'two ones', 'cinematic relation', 'word occurring', 'word 2', 'whole part', 'values computer', 'three concepts', 'three concepts', 'third account', 'term waiting', 'small sample']
lecture 14
['covering topic theta sub j', 'help us analyze patterns', 'generally two different tasks', 'product like iphone 6', 'would involve first discovering', 'also covers topic k', 'five years ago', '2012 presidential election', 'tells us something', 'topic generally provide', 'language namely discovery', 'paradigmatic relations relations', 'mining another kind', 'theta sub k', 'also negative reviews', 'generally also need', 'etc ., right', 'twitter users talking', 'theater sub one', 'main idea discussed', 'case k topics', 'also would like', 'major topics debated', 'involves discovering topics', 'define theta sub', 'current research topics', 'n text documents', 'cover topic 1', 'covered topic 2', 'discover k topics', 'covered mining knowledge', 'covered mining knowledge', 'iphone 6', 'would like', 'exactly theta', 'involves discovery', 'discovering topics', 'topic k', 'define theta', 'syntagmatic relations', 'topic 1', 'π sub', 'π sub', 'often need', 'document covers', 'output would', 'topic 2', 'research articles', 'require discovery', 'topics k', 'k topics', 'k topics', 'topic mining', 'topic mining', 'topic mining', 'mining task', 'data mining', 'data mining', 'content mining', 'people like', 'major topics', 'main topics', 'different ways', 'different type', 'different granularities', 'different granularities', 'different applications', 'view topic', 'trending topic', 'article topic', 'research topics', 'word associations', 'values indicate', 'task definition', 'task definition', 'small portion', 'second task', 'roughly speaking', 'probabilities sum', 'nba sports', 'meta data', 'many applications', 'international events', 'fading away', 'documents cover', 'digital library', 'completely defined', 'automatically suggest', 'mining topics', 'discover whether', 'discover knowledge', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text connection', 'text article', 'different locations']
lecture 15
['sometimes called maximal marginal relevance ranking', 'easily count related terms toward contributing', 'extent document 1 covers sports', 'star might actually suggest perhaps', 'cannot really describe complicated topics', 'many related words like basketball', 'might also discover document 2', 'basketball star versus star', 'mine k topical terms', 'get k topical terms', 'favor title words becauses', 'could also favor hashtags', 'course many different ways', 'highest scored terms would', 'simply picking k terms', 'topic coverage pi sub', 'might favor title words', 'term frequency idf stands', 'term travel actually occurred', 'occurred four times', 'k topical terms', 'k topical terms', 'lacks expressive power', 'domain specific heuristics', 'highest scored terms', 'also likely effective', 'also high score', 'travel occurred twice', 'nba basketball game', 'favor representative terms', 'word sense ambiguation', 'obtain candidate terms', 'word like star', 'might also mean', 'document would add', 'using actual datasets', 'inverse document frequency', 'simply count occurrences', 'particular approach could', 'count related words', 'become candidate topics', 'specialized topic would', 'gradually take terms', 'general would like', 'consider related words', 'defined mostly based', 'terms like sports', 'word sports actually', 'offer one way', 'scoring function would', 'basketball star', 'estimated coverage would', 'use pure statistics', 'symbol general topics', 'estimated also zero', 'simple example illustrates', 'term science also', 'many terms', 'document 1', 'favor terms', 'would like', 'would like', 'complicated topics', 'might also', 'many things', 'would mean', 'naturally hashtags', 'idf weighting', 'highest scores', 'get balance', 'topical term', 'candidate terms', 'different topics', 'closely related', 'might require', 'might encounter', 'formula would', 'cannot represent', 'simply count', 'related term', 'also raised', 'also consider', 'sports might', 'tf stands', 'avoid picking', 'actually ambiguous', 'functional terms', 'might want', 'might want', 'candidate term', 'also want', 'also want', 'words belong', 'general terms', 'zero count', 'scoring function', 'scoring function', 'scoring function', 'scoring function', 'scoring function']
lecture 16
['likely parameter values lambda star given', 'capital lambda actually consists', 'introduce semantically related words', 'describe fairly complicated topics', 'introduce probabilistic topic models', 'general involve many words', 'problem called generative model', 'data points higher probabilities', 'see top words like', 'k topics would sum', 'distinguish subtle differences', 'also see k theta_i', 'precisely topic mining problem', 'solve text mining problems', 'specific parameter values', 'discover various kinds', 'still see one word', 'many different ways', 'reasonably high probabilities', 'hotel etc ."', 'probabilistic topic models', 'mine k topics', 'non zero probability', 'non zero probability', 'one interesting question', 'one dimensional variable', 'actually samples drawn', 'non zero probabilities', 'model subtle variations', 'model subtle differences', 'model several differences', 'much smaller probabilities', 'using statistical modeling', 'also allows us', 'uses multiple words', 'tax mining problem', 'also generally assume', 'related words together', 'high probability words', 'related terms necessary', 'text data c', 'discover different knowledge', 'describe complicated topics', 'would give us', 'output would consist', 'travel topic 0', 'data mining algorithm', 'would also give', 'words must sum', 'probabilistic topic model', 'probability obviously depends', 'using multiple words', 'already see n', 'top mining', 'particular data set', 'parameter values', 'parameter values', 'parameter values', 'parameter values', 'allows us', 'higher probabilities', 'also introduce', 'different values', 'topics k', 'much related', 'zero probabilities', 'data points', 'would like', 'would like', 'probability values', 'complicated topic', 'text mining', 'text mining', 'multiple topics', 'introduce weights', 'probabilistic model', 'probabilistic model', 'lambda star', 'lambda star', 'must sum', 'different probabilities', 'generative model', 'generative model', 'related terms', 'related terms', 'also want', 'also occurred', 'different topics', 'topic mining', 'topic mining', 'n sets', 'n documents', 'words like', 'also see', 'also see', 'k parameters', 'slide given', 'actually related', 'would maximize', 'would allow']
lecture 17
['estimate somehow even slightly', 'cover probabilistic topic models', 'cover probabilistic topic models', 'must specify probability values', 'positive ", might get', 'text mining paper publishing', 'called maximum likelihood estimate', 'food nutrition paper would', 'text mining paper would', 'text data looks like', 'positive ", sometimes', 'might get wednesday often', 'observed text 10 times', 'maximum likelihood estimate', 'words like eigenvalue might', 'observe certain kind', 'involve topic analysis', 'unigram language models', 'statistical language models', 'statistical language models', 'show two examples', 'non grammatical sentence', 'sample drawn according', 'similarly another sentence', 'text mining paper', 'text mining paper', 'might often get', 'meaningful text documents', 'unigram language model', 'statistical language model', 'statistical language model', 'simplest language model', 'view text data', 'second distribution show', 'relatively large probability', 'like many others', 'model generative model', 'might also get', 'sample words according', 'text mining context', 'observed text data', 'relatively high probability', 'likely language model', 'relatively small probability', 'actually characterize topic', 'word sequences might', 'looks like', 'looks like', 'probabilistic mechanism', 'many times', 'might get', 'even though', 'different topic', 'text mining', 'relatively higher', 'often occur', 'best estimate', 'text documents', 'language model', 'language model', 'language model', 'maximum probability', 'text data', 'text data', 'observed data', 'observed data', 'observed data', 'data observed', 'non zero', 'data points', 'many tasks', 'sequence like', 'words might', 'general models', 'different text', 'might give', 'sample sequences', 'w_n would', 'today often', 'guess text', 'wednesday ".', 'wednesday ".', 'unlikely coherent', 'two problems', 'top conference', 'speech recognition', 'special cases', 'special cases', 'somewhat smaller', 'quite sufficient', 'possible sequences', 'particular document', 'numbers together', 'know exactly', 'gonna draw', 'fake numbers', 'estimation process', 'different sequences', 'different context', 'clearly indicates']
lecture 18
['non zero probability would take away probability mass', 'estimate intuitively also makes sense', 'maximum likelihood estimator would give', 'gives us also one point', 'basically would go back', 'contain another word related', 'bias towards certain values', 'gives us another way', 'distribution would allow us', 'data also tells us', 'parameter values would maximize', 'called maximum likelihood estimate', 'word zero probability', 'would tell us', 'likely parameter value according', 'bayesian reasoning would', 'x given theta reach', 'maximum likelihood estimator', 'unseen words may', 'maximum likehood estimate', 'prior tells us', 'function reach maximum', 'unigram language model', 'simplest language model', 'trust data entirely', 'one conditional probability', 'maximum likelihood estimate', 'maximum likelihood estimate', 'maximum likelihood estimate', 'maximum likelihood estimate', 'observed 100 words', 'posterior probability combines', 'one dimension value', 'maximum likelihood estimation', 'interesting point estimates', 'using bayes rule', 'two different ways', 'zero probability', 'actually would look', 'two conditional probabilities', 'basically probability distribution', 'use bayesian estimation', 'noninformative prior meaning', 'observed evidence x', 'function maximum value', 'allow us', 'would maximize', 'one way', 'give us', 'data sample x', 'would return', 'tell us', 'also called', 'theta given x', 'x given theta', 'maximum probability', 'would say', 'allows us', 'word distribution', 'f according', 'certain thetas', 'also talked', 'conditional probability', 'conditional probability', 'estimate may', 'language model', 'parameter values', 'parameter values', 'observed words', 'posterior probability', 'posterior probability', 'posterior probability', 'bayesian estimation', 'bayesian estimation', 'bayesian estimation', 'bayesian estimation', 'take', 'also look', 'higher probability', 'higher probability', 'higher likelihood', 'map estimate', 'map estimate', 'observed evidence', 'things related', 'two probabilities', 'two probabilities', 'parameter value', 'high probabilities', 'bayes rule', 'bayes rule', 'bayes rule', 'data likelihood', 'data likelihood', 'data likelihood', 'generative model', 'likelihood value', 'general estimate', 'likelihood function', 'likelihood function']
lecture 19
['continue discussing probabilistic models', 'topic well like text', 'high probability words tend', 'easily see theta sub', 'maximum likelihood estimate problem', 'use lagrange multiplier approach', 'often see later also', 'likelihood function look like', 'use probabilistic models', 'probabilistic topic models', 'maximum likelihood estimator', 'maximum likelihood estimator', 'maximum likelihood estimator', 'topic 100 %.', 'often use later', 'text mining paper', 'introduce lagrange multiplier', 'much probability mass', 'also would like', 'since lagrange multiplier', 'cases later also', 'whole document given', 'constrained maximization problem', 'w sub one', 'unigram language model', 'also see various', 'somehow get rid', 'use theta sub', 'use theta sub', 'might also notice', 'log likelihood instead', 'estimate one topic', 'also see later', 'general topic models', 'function would combine', 'high probability words', 'would look like', 'unconstrained optimizing problem', 'often functional words', 'word w sub', 'mining one topic', 'analyzing one document', 'closed form formula', 'estimated parameters would', 'optimal point would', 'one document also', 'probabilities must sum', 'solve optimization problem', 'w sub', 'estimate would', 'looks like', 'theta sub', 'often needed', 'often easy', 'lagrange function', 'lagrange function', 'x sub', 'constrained optimization', 'also see', 'mining etc', 'mining algorithm', 'get rid', 'also rewrite', 'original likelihood', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'one topic', 'one topic', 'optimization problem', 'optimization problem', 'estimated parameters', 'functional words', 'optimal solution', 'optimal solution', 'optimal setting', 'would need', 'would lead', 'topic representation', 'topic modeling', 'single topic', 'must sum', 'one question', 'theta values', 'might see', 'mathematical problem', 'useful form', 'useful approach', 'simply taken', 'simple setup', 'repeated occurrences', 'repeated occurrences', 'really characterizing', 'really characterized']
lecture 20
['might bias towards using one topic', 'continue discussing probabilistic topic models', 'mission would allow us', 'two unigram language models', 'two unigram language models', 'maximum likelihood estimator later', 'background word distribution denoted', 'unigram language models', 'unigram language models', 'component like theta sub', 'also consider two ways', 'maximum likelihood estimator', 'called background topic model', 'usually good exercise', 'likelihood function look like', 'particular word distribution multiplied', 'theta sub b', 'theta sub b', 'theta sub b', 'theta sub b', 'events must happen', 'estimated language model', 'occurring often later', 'second part accounts', 'assigned high probabilities', 'assign high probabilities', 'maximum likelihood estimate', 'generating multiple ways', 'using another distribution', 'consider 2 cases', 'particular component distribution', 'following generative model', 'estimator obviously would', 'topic must sum', 'topic must sum', 'two distributions together', 'something quite general', 'topic word distribution', 'topic word distribution', 'topic word distribution', 'background word distribution', 'different unique words', 'one natural way', 'two word distributions', 'complicated mixture model', 'common words would', 'topic models', 'observed text data', 'one distribution right', 'still 1 document', 'generating text data', 'actually general form', 'background topic', 'topic theta', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'mixture models', 'two ways', 'give us', 'different ways', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'might recall', 'something different', 'quite different', 'two distributions', 'two distributions', 'two distributions', 'two distributions', 'two distributions', 'would happen', '2 cases', 'background words', 'component model', 'component model', 'component model', 'two topics', 'two terms', 'two terms', 'two components', 'two cases', 'mixed together', 'components together', 'complicated model', 'unique words', 'first must', 'model multiplied', 'generative model', 'generative model', 'one distribution', 'one distribution', 'one distribution', 'one distribution', 'would mean', 'might want']
lecture 21
['continue discussing probabilistic topic models', 'text data actually contain two kinds', 'unknown topic world distribution', 'use maximum likelihood estimator', 'use maximum likelihood estimator', 'background model gives probability', 'two probabilities must sum', 'maximum likelihood estimator', 'flip fair coin', 'two component models', 'high probability background words', 'bet high probabilities', 'assign high probabilities', 'assign smaller probabilities', 'topic word distribution', 'topic word distribution', 'different exact probabilities', 'simple algebra problems', 'adjust theta sub', 'mixture model except', 'mixture model estimation', 'common words like', 'two words text', 'two words text', 'simple algebra question', 'background word distribution', 'two probabilities equal', 'precisely two words', 'one distribution assigns', 'simple case like', 'smaller probability given', 'two models', 'text point one', 'smaller probabilities', 'unknown variables', 'observed data', 'two probabilities', 'two probabilities', 'two words', 'different words', 'background distribution', 'two ways', 'two variables', 'two variables', 'two variables', 'two variables', 'two sides', 'common words', 'common words', 'simple expression', 'extremely simple', 'actually end', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'background words', 'background words', 'background words', 'background words', 'high probability', 'high probability', 'word distribution', 'word distribution', 'word distribution', 'one distribution', 'maximum value', 'likelihood function', 'likelihood function', 'must sum', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'background part', 'background model', 'background model', 'background model', 'background model', 'background model', 'content words', 'two distributions', 'unclear whether', 'somewhat larger', 'somewhat larger', 'probalistic model', 'probalistic model', 'point nine', 'point nine', 'observed patterns', 'naive oversimplification', 'much larger', 'much easier', 'model heuristically', 'might feel', 'interesting behaviors']
lecture 22
['1st every component component model attempts', 'basically means one model must', 'unknown distribution theta sub', 'different component models tend', 'high frequency words tend', 'high frequency words generally', 'assign somewhat higher probability', 'must take away', 'prior would allow us', 'theta sub b', 'contributed one turn', 'effectively get rid', 'likelihood function look like', 'get higher probabilities', 'fixing one component', 'repeated many times', 'bet high probabilities', 'assign high probabilities', 'two component models', 'high frequency words', 'overall result would', 'specialized mixture model', 'maximum likelihood estimator', 'maximum likelihood estimator', 'background language model', 'relatively small impact', 'gives us 0', 'collaboratively maximize likelihood', 'becomes less important', 'zero prior probability', 'would make sense', 'background word distribution', 'theta sub', 'two word document', 'component models', 'component models', 'high probabilities', 'one distribution', 'higher values', 'actually assign', 'two words', 'different words', 'component regulates', 'would allow', 'would allow', 'give us', 'somewhat regulated', 'regularised somewhat', 'different form', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'effectively excludes', 'model parameters', 'background model', 'high probability', 'high probability', 'would make', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'make room', 'frequent words', 'background words', 'one word', 'objective function', 'new function', 'would happen', 'would happen', 'small probability', 'start adding', 'obviously changes', 'normalized frequencies', 'might want', 'little bit', 'estimation problem', 'discovered topic', 'bayesian estimation', 'another occurrence', 'many terms', 'impact would', 'choosing one', 'positive impact', 'important behavior', 'another behavior', 'another behavior', 'less important', 'general phenomenon', 'also encourages', 'add many', 'additional terms', 'additional terms', 'even larger', 'even larger', 'smaller probability', 'probability mess', 'probability mass']
lecture 23
['initialized parameter values would allow us', 'bayes formula provides provides us', 'two things must happen first', 'parameter values somewhat randomly', 'prediction essentially helped us', 'typical bayesian inference situation', 'consider word like text', 'two component mixture model', 'actually know tentative probabilities', 'background theta sub b', 'unknown topic word distribution', 'single word distribution problem', 'one topic word distribution', 'like many others', 'almost 100 %.', 'use bayes rule', 'apply bayes rule', 'word distribution theta sub', 'probabilistic topic models', 'actually go back', 'maximum likelihood estimator', 'expectation maximization algorithm', 'theta sub b', 'theta sub b', 'maybe text could', 'topic theta sub', 'latent variable z', 'one group would', 'word w sub', 'longer mixture model', 'text would depend', 'maximum likelihood estimate', 'maximum likelihood estimate', 'much higher probability', 'topic word distribution', 'topic word distribution', 'world distribution would', 'guess must also', 'might think well', 'different prior possible', 'word higher probability', 'background word distribution', 'bayes rule', 'allows us', 'bayesian inference', 'gives us', 'z values', 'thing unknown', 'mixture models', 'group would', 'higher likelihood', 'probability values', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'inference process', 'two probabilities', 'two probabilities', 'mixture model', 'mixture model', 'mixture model', 'would mean', 'would affect', 'actually know', 'two together', 'two priors', 'two groups', 'word probabilities', 'em algorithm', 'em algorithm', 'em algorithm', 'word counts', 'choose word', 'background model', 'background model', 'background model', 'background model', 'word distribution', 'two distributions', 'two distributions', 'would update', 'useful algorithms', 'still possible', 'similar form', 'really know', 'pseudo document', 'principled way', 'familiar scenario', 'fairly straightforward', 'extremely simple', 'denote whether']
lecture 24
['invoke another spec step called', 'initialized values would allow us', 'binary variable would indicate whether', 'covered background versus content words', 'e step would give us', 'really variable z hidden variable', 'invoke e step followed', 'e step 2 gas', 'topic word distributions theta sub', 'parameters would allow us', 'initialization says uniform distribution', 'additional information like z', 'sure whether taxes', 'might wonder whether', 'theater super b', 'point three three', 'help us solve', 'know exactly whether', 'log like code', 'hill climbing algorithm', 'parameter values randomly', 'often randomly set', 'e step based', 'assigns high probabilities', 'covered background words', 'less likely probably', 'hidden variable values', 'would gradually improve', 'common words like', 'allocated counts toward', 'use bayes rule', 'would actually happen', 'discriminating world distribution', 'text mining algorithm', 'high probability like', 'different guest probabilities', 'word distribution right', 'give us', 'simply take advantage', 'binary variable', 'somewhat improved estimate', 'inferred word split', 'called e', 'background world distribution', 'additional information', 'right data counts', 'hidden variable', 'hidden variable', 'hidden variable', 'theta sub', 'help us', 'help us', 'uniform distribution', 'would happen', 'two distributions', 'latent variable', 'distribution like', 'initialization stage', 'cedar sub', 'e step', 'e step', 'e step', 'another generation', 'another generation', 'algorithm would', 'would work', 'would lead', 'z values', 'z values', 'z values', 'z values', 'probabilities would', 'inferred values', 'z etc', 'z attached', 'last rule', 'word distribution', 'word distribution', 'using e', 'counts toward', 'hidden variables', 'hidden variables', 'z value', 'guess says', 'take advantage', 'log likelihood', 'em algorithm', 'em algorithm', 'em algorithm', 'em algorithm', 'em algorithm', 'social values', 'random values', 'possible values', 'initial values', 'status update', 'simply normalize', 'relevant statistics', 'primary goal', 'practice mate']
lecture 25
['original lighter holder curve', 'original like role function', 'original like record function', 'stable converged parameter values', 'good initial starting point', 'really covered yet', 'higher like recorder', 'estimated parameter values', 'like roller function', 'latent variable values', 'many local maxima', 'thought data augmentation', 'actual global maximum', 'useful hidden variables', 'simple mixture model', 'cannot easily find', 'parameter value given', 'current gas unless', 'hill climb algorithm', 'initial points', 'starting point', 'mixture model', 'hidden variables', 'hidden variable', 'predicting values', 'possible values', 'stable value', 'hill climbing', 'mixture models', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'local maximum', 'local maximum', 'local maximum', 'current generation', 'current gas', 'current gas', 'x dimension', 'two things', 'two steps', 'two distributions', 'random guess', 'numerical optimization', 'numerical algorithm', 'next guest', 'next gas', 'multiple times', 'maximum regular', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'events probabilistically', 'even though', 'em algorithm', 'em algorithm', 'em algorithm', 'em algorithm', 'done probabilistically', 'different points', 'detailed explanation', 'climbing algorithm', 'augmented data', 'analytical solution', 'always better', 'allows us', 'would use', 'would mean', 'would make', 'would exploit', 'would depend', 'two would', 'generally would', 'new point', 'another point', 'end step', 'e step', 'e step', 'general algorithm', 'top two', 'say exactly', 'much easier', 'helps determine', 'difficult problem', 'basically part', 'already know', 'probability distribution']
lecture 26
['lambda sub b versus non background', 'probability 1 minus lambda sub b', 'one minus lambda b gives us', 'topic category characterization seedies hci', 'introduce probabilistic latent semantic analysis', 'probabilistic latent semantic analysis', 'see lambda sub b', 'find applied math problem', 'lambda sub b', 'topical theta sub k', 'topic theta sub k', 'constrained optimization problem like', 'see two interesting kinds', 'first topic popular like', 'two things must happen', 'two component mixture model', 'two things happen', 'also see background words', 'many useful applications', 'problem many times', 'w given sit', 'basic topic model', 'maximum likelihood estimator', 'complete likelihood function', 'basic topic models', 'background language model', 'would also allow', 'useful topic models', 'analyze text data', 'one topic besides', 'would believe exist', 'two way switch', 'first line shows', 'mining multiple topics', 'mine multiple topics', 'example government response', 'many unknown parameters', 'likelihood function shown', 'make another decision', 'lambda multiplied', 'question would help', 'text data right', 'topic analysis', 'topic coverage distribution', 'two component', 'probabilistic modeling', 'optimization problem', 'tell us', 'set us', 'two kinds', 'two kinds', 'two kinds', 'like holder', 'like hold', 'mixture model', 'another topic', 'mixture models', 'topical model', 'government response', 'government response', 'useful exercise', 'topical would', 'background topical', 'background help', 'many premise', 'likelihood function', 'likelihood function', 'topic models', 'k among', 'key way', 'background model', 'background model', 'background model', 'background model', 'two topics', 'words must', 'unknown value', 'unknown primers', 'k distributions', 'two constraints', 'k topics', 'k topics', 'k topics', 'text data', 'text data', 'also ask', 'would want', 'topic coverage', 'background words', 'many variables', 'vocabulary set', 'usual thing', 'unknown parameters', 'set empirically', 'really key', 'parameter estimation', 'p lsa', 'often called', 'new orlean', 'make sure']
lecture 27
['hidden variable z sub dw indicates whether word', 'k unigram language models representing k topics', 'actually helps us better understand', 'coverage parameters -- pis --', 'keep counting various events', 'predetermined background language model', 'know whether likelihood converges', 'accumulated counts various counts', 'help discover discriminating topics', 'hidden variable z', 'generate computed expected count', 'gives us different distributions', 'maximum likelihood estimator based', 'k plus one values', 'case plsa allows us', 'inferred z values', 'hidden variable values', 'equation allows us', 'background language model', 'maximum likelihood estimator', 'discover topical knowledge', 'topic theta sub j', 'e step formulas essentially', 'also aggregate topics covered', 'maximum regular estimated', 'would allow us', 'discover two things', 'unknown parameters randomly', 'slightly different form', 'keep count counter', 'say stop right', 'would give us', 'topic theta sub', 'topic theta sub', 'topic theta sub', 'topic theta sub', 'many useful applications', 'hidden variable', 'potentially different pis', 'remember pis indicate', 'particular time period', 'actually general phenomenon', 'e step result', 'e step formulas', 'also cluster terms', 'higher probability words', 'allocated word counts', 'theta sub', 'theta sub', 'theta sub', 'split words among', 'plsa model', 'word whether', 'likelihood converges', 'k topics', 'tells us', 'tells us', 'tells us', 'time period', 'keep track', 'help attract', 'hidden variables', 'mixture model', 'estimated based', 'two values', 'previous likelihood', 'might give', 'current likelihood', 'also added', '1 distributions', 'right counts', 'relevant counts', 'allocated counts', 'word distributions', 'word distributions', 'different ways', 'renormalize based', 'particular parameter', 'event based', 'discounted count', 'discounted count', 'world multiplied', 'topics written', 'term clusters', 'temporal chains', 'representing', 'intentionally leave', 'first initialize', 'em algorithms', 'detailed characterization', 'compute likelihood', 'common terms', 'change much', 'bayes rule', 'basically assessing', 'b denoting', 'particular topic', 'particular topic', 'e step', 'e step']
lecture 28
['generative model fully generated model', 'dirichlet prior dirichlet distribution based', 'high pseudocounts similar life would', 'two specific extreme cases', 'would assign high probabilities', 'strongly favor certain kind', 'using maximum likelihood estimator', 'prior called conjugate prior', 'tags topic tags assigned', 'also called map estimate', 'make one distribution fixed', 'latent dirichlet allocation', 'latent dirichlet allocation', 'latent dirichlet allocation', 'generated using topics corresponding', 'background language model', 'background language model', 'model setting would', 'assign high probabilities', 'tags already assigned', 'cover two things', 'maximum likelihood estimator', 'two functional forms', 'also high pseudocounts', 'mean conjugate prior', 'would allow us', 'world distribution right', 'use map estimator', 'contain one distribution', 'similar em algorithm', 'force particular choice', 'choose topic one', 'contain one topic', 'say supplier says', 'additional pseudo data', 'precisely 1 background', 'also would listen', 'use prior together', 'smallest parameters reflect', 'estimate word distributions', 'user may also', 'analyze text data', 'add mu multiplied', 'posterior distribution probability', 'force one distribution', 'see retrieval models', 'see example later', 'user controlled plsa', 'assigned tags', 'map estimator', 'would mean', 'would make', 'strongly suggests', 'use maximum', 'background distribution', 'also controlled', 'would affect', 'topics corresponding', 'em algorithm', 'em algorithm', 'certain number', 'favor set', 'user generated', 'information retrieval', 'functional form', 'would happen', 'would happen', 'would happen', 'also may', 'specific example', 'computed using', 'prior says', 'prior right', 'may recall', 'may know', 'one dominate', 'certain aspects', 'many pseudocounts', 'also use', 'battery life', 'battery life', 'battery life', 'battery life', 'total sum', 'theoretically speaking', 'special form', 'pi value', 'even force', 'entirely focused', 'entirely concentrated', 'deal works', 'continue talking', 'changes happened', 'cannot attract', 'bayesian inference', 'analyzing reviews', 'additional counts', 'additional knowledge', 'posteriori estimate', 'one way']
lecture 29
['use algorithm called collapsed gibbs sampling', 'might favor generating skewed coverage', 'best basis test setup', 'really represents global maximum', 'would work equally well', 'approach maximum likelihood estimator', '2 dirichlet distributions respectively', 'training data looks like', 'lda formula would add', 'dirichlet distribution tell us', 'work around though', 'good local maximum', 'many local maximum', 'maximum likelihood estimated', 'general principal way', 'could possibly draw', 'local maxima problem', 'modeling future data', 'explaining future data', 'relatively uniform distribution', 'n values corresponding', 'force certain choices', 'really generating model', 'probabilistic topic models', 'probabilistic topic models', 'also output proportions', 'also often adequate', 'use different toolkits', 'take tax data', 'topic modeling problem', 'k parameters corresponding', 'estimated using exactly', 'multiple word distributions', 'basic topic model', 'basic topic model', 'another dirichlet distribution', 'use bayesian inference', 'many parameters exactly', 'topic word distributions', 'topic word distributions', 'many fewer parameters', 'might favor', 'give similar performance', 'algorithm looks', 'lda versus plsa', 'tell us', 'training data', 'training data', 'would worry', 'would overfit', 'k values', 'dirichlet distributions', 'em algorithm', 'might think', 'might find', 'training documents', 'n words', 'multiple times', 'elegant way', 'draw one', 'topic coverage', 'topic coverage', 'dirichlet distribution', 'dirichlet distribution', 'dirichlet distribution', 'dirichlet distribution', 'dirichlet distribution', 'word distributions', 'word distributions', 'word distributions', 'gives us', 'gives us', 'many methods', 'models provide', 'bayesian version', 'appealing models', 'another distribution', 'topic models', 'topic models', 'also find', 'use phrases', 'necessary problem', 'likelihood functions', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'bayesian inference', 'word distribution', 'special distribution', 'dirichlet prior', 'many parameters', 'many parameters', 'many parameters', 'word inside', 'whole collection', 'third one', 'suggested readings', 'previous slide', 'nice review']
lecture 30
['also cluster fairly large text law gets', 'becausw clusters likely represent different senses', 'link similar text objects together', 'might group authors together based', 'mean text objects may contain', 'help us remove redundancy', 'helps provide additional discrimination', 'may also use text clustering', 'group similar texture objects together', 'also clearly tells us', 'clustering help us achieve', 'may also use text', 'also cluster articles written', 'particularly exploratory text analysis', 'group similar objects together', 'would give us cluster', 'might get different clusters', 'get different clustering results', 'large text objects', 'two words like car', 'really see text objects', 'clustering problem well defined', 'well defined clustering problem', 'might also represent', 'induce additional features', 'would give us', 'discover interesting clusters', '2 dimensional space', 'represent text data', 'course texture objects', 'text clustering interesting', 'combining text clustering', 'text mining algorithms', 'natural groups well', 'best clustering result', 'group similar objects', 'might also feel', 'two words similar', 'cluster documents together', 'discover natural structures', 'must use perspective', 'whole text collection', 'removing duplicated documents', 'sometimes also want', 'feature value would', 'cluster email messages', 'general many applications', 'cluster search results', 'see overall structure', 'user must define', 'might cluster websites', 'text clusters', 'see another example', 'grouped together', 'text clustering', 'text clustering', 'text clustering', 'text clustering', 'text clustering', 'text clustering', 'text clustering', 'text objects', 'texture classification', 'group objects', 'well defined', 'well defined', 'objects based', 'text mining', 'clustering results', 'clearly defined', 'similar objects', 'three clusters', 'three clusters', 'major clusters', 'looks like', 'email messages', 'clustering result', 'search results', 'also seen', 'text object', 'two objects', 'still group', 'different ways', 'different levels', 'text data', 'text data', 'text data', 'grouped based', 'particularly useful', 'natural structures', 'different way', 'different way', 'objects might', 'clustering problem', 'results returned', 'overall content', 'many examples', 'future value', 'duplicated content', 'clustering bias']
lecture 31
['also discuss similarity based approaches', 'basically tells us document di', 'detailed coverage distributions pi ij', 'estimating one word distribution based', 'naturally design alternative mixture model', 'topic mining problem using topic models', 'document covers precisely one topic', 'also mentioned multiple times', 'possibly cover multiple topics', 'introduce generative probabilistic models', 'continue discussing text clustering', 'likelihood function looks like', 'two component mixture model', 'generative model would adopt', 'precisely one topic instead', 'using precisely one topic', 'generative probabilistic models', 'covers different topic', 'cover one topic', 'multiple topics covered', 'allowed multiple topics', 'word distribution based', 'covers one topic', 'precisely one topic', 'precisely one topic', 'precisely one topic', 'non zero probabilities', 'formula looks like', 'made multiple times', 'text collection c', 'generating probabilistic models', 'word x sub', 'force every document', 'two different distributions', 'precisely one distribution', 'share k topics', 'potential different decision', 'precisely one document', 'cluster assignment decisions', 'document must share', 'probabilistic generating model', 'theta sub two', 'output two things', 'multiple distribution could', 'pi ij', 'would allow us', 'documents share topics', 'four different words', 'use precise one', 'k word distributions', 'particular data point', 'assume one topic', 'longer mixture model', 'covering text clustering', 'theta two right', 'one particular distribution', 'text clustering problem', 'topics also independent', 'topic coverage', 'document clustering model', 'document clustering problem', 'word first make', 'multiple distributions', 'mixture models', 'choosing one distribution', 'detailed view', 'generative models', 'give us', 'likelihood function', 'likelihood function', 'topic models', 'two models', 'c sub', 'document covers', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'also different', 'one topic', 'one topic', 'one topic', 'design model', 'generative model', 'generative model', 'generative model', 'generative model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model']
lecture 32
['particularly generative probabilistic models', 'k unigram language models', 'k unigram language models', 'tax capture text clustering', 'documents n documents denoted', 'model basically would make', 'two component mixture model', 'generative probabilistic models', 'maximum likelihood estimator', 'x sub j', 'would allow us', 'little bit different', 'likelihood function would', 'potentially different choice', 'generic choice probability', 'use unique word', 'change allows us', 'word distributions corresponding', 'topic model presentation', 'intuitively makes sense', 'choose theta based', 'document actually depends', 'likely cluster would', 'document dependent probability', 'text clustering', 'two distributions', 'likelihood function', 'topic choice', 'k distributions', 'makes sense', 'general presentation', 'documents lets', 'basically still', 'mixture model', 'mixture model', 'tells us', 'tell us', 'k clusters', 'word distribution', 'topic model', 'topic model', 'top terms', 'steps using', 'second line', 'particular position', 'parameter estimation', 'may recall', 'helps summarize', 'generated model', 'generally higher', 'general notion', 'following assumption', 'estimation formulas', 'distributed generator', 'different model', 'continued discussion', 'continue talking', 'c sub', 'use likelihood', 'cluster corresponding', 'likelihood together', 'posterior probability', 'posterior probability', 'highest probability', 'highest probability', 'high probability', 'change also', 'also recover', 'also consistent', 'also available', 'standard way', 'first choose', 'choose one', 'better way', 'document clustering', 'document clustering', 'document clustering', 'simply use', 'terms like', 'product instead', 'key problem', 'following formula', 'easily solve', 'base formula', 'particular document', 'document using', 'assigning document', 'small cluster', 'assigning clusters', 'allocate clusters', 'see later', 'see later', 'might think', 'seen earlier', 'topic theta', 'actually get', 'similar parameters', 'large cluster', 'large cluster', 'lets look']
lecture 33
['compute another average district called theater bar', 'k representing k different distributions', 'hidden variable must tell us', 'using multiple unigram language models', 'first theater subway must', 'tax given still awhile', 'two things must happen', '1 hidden variable attached', 'estimation involves two kinds', 'likelihood function looks like', 'value variable could take', 'used 1 hidden variable', 'word distribution actually generates', 'sometimes also use logarithm', 'hidden variable zd', 'unigram language model', 'unigram language model', 'help preserve precision', 'capture different patterns', 'apply bayes rule', 'single language model', 'maximum likelihood estimator', 'would allow us', 'estimated model pamateter', 'harder clusters mainly', 'z sub dj', 'z probability z probabilities', 'generative probabilistic models', 'generative probabilistic models', 'achieve different goals', 'use bayes rule', 'cannot use logarithms', 'namely two occurrences', 'new mixture model', 'em algorithm starts', 'special normalization technique', 'hidden variable', 'maximum liklihood estimate', 'average word distribution', 'k probabilities sum', 'theta 2 would', 'particular topic theta', 'intuitively would give', 'take two values', 'word given theta', 'words must sum', 'four words together', 'many small probabilities', 'consider one document', 'avoid underflow problem', 'k topics', 'hidden variables', 'looks like', 'selecting theta one', 'two distributions', 'take average', 'likelihood converges', 'gives us', 'gives us', 'z values', 'mixture models', 'must use', 'topic models', 'topic models', 'z equals', 'maximum lag', 'council would', 'two occurrences', 'two occurrences', 'two occurrences', 'two clusters', 'two clusters', 'average distribution', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'theta sub', 'probabilistic assignment', 'probabilistic assignment', 'must sum', 'em algorithm', 'em algorithm', 'em algorithm', 'topic model', 'topic model', 'first topic', 'first topic', '2 given', 'top model', 'model slightly', 'using topic', 'two solutions', 'two possibilities', 'first thing', 'first choosing', 'also compute', 'numerical values', 'initializer values', 'disjoint clusters']
lecture 34
['selecting kate randomly selected vectors', 'assuming similarity function defined onto objects', 'many different clustering methods available', 'random inner inner initialization', 'discover complex clustering structures', 'convert converted local minimum', 'maybe simple english good', 'counts pull together counter', 'easily design generated model', 'directly specify similarity functions', 'two pairs objects would define', 'generally give different results', 'explicit definer similarity function', 'particular objective function like', 'certainly cluster terms based', 'model defines clustering bias', 'gradually group similar objects together', 'single link complete link', 'computer group similarity based', 'called k means clustering', 'two groups come together', 'highest similarity similarity pairs', 'many different methods', 'defined objective function', 'paradigmatic relation learning', 'everything specified except', 'binary tree eventually', 'recover different instruction', 'complex generative models', 'three popular methods', 'initial tentative classroom', 'complete link defines', 'link would mean', 'different clustering algorithms', 'two farthest appear', 'distinguish two strategies', 'custom methods would', 'completely completely better', 'process goes like', 'tentative clustering result', 'initial tentative clustering', 'within cluster sum', 'within cluster sum', 'cluster hosts entry', 'first single link', 'clustering result iteratively', 'two representative methods', 'similarity function calls', 'provider similarity function', 'hierarchical agglomerative clustering', 'agglomerative hierarchical clustering', 'average link defines', 'better clustering result', 'define objective function', 'father sister pair', 'group everything together', 'group everything together', 'customize clustering algorithm', 'similarity based classroom', 'two groups g1', 'distinguishes two ways', 'implicitly similarity function', 'similarity based clustering', 'similarity based clustering', 'hierarchical clustering algorithm', 'two groups together', 'whole data set', '00 text objects', 'using generative models', 'model based approaches', 'one potential disadvantage', 'means two groups', 'k means algorithm', 'k clusters based', 'computing group similarities', 'group groups together', 'model based approach', 'cluster objects together', 'get four clusters', 'given two groups', 'similarity based approaches', 'similarity based approaches', 'similarity based approaches', 'also different ways', 'tentative clustering centroids', 'given k clusters', 'precisely one cluster', 'individual object similarity', 'also use prior', 'local minimum', 'single link', 'clearly defined', 'called agglomerative', 'objective function', 'objective function', 'objective function', 'put one object', 'clustering bias', 'clustering bias', 'clustering methods']
lecture 35
['useful unsupervised general text mining technique', 'commonly used measures include purity', 'easily compare different methods based', 'give us various measures', 'would help us characterize', 'general procedure would look like', 'discover interesting clustering structures', 'question without knowing whether', 'best clustering result would', 'normalized mutual information', 'normalized mutual information', 'mutual information captures', 'must clearly specify', 'high level ideas', 'without clearly defining', 'would allow us', 'get good sense', 'commonly used measure', 'another possible measure', 'two different ways', 'explicitly define bias', 'method highly depends', 'different clustering method', 'ideal clustering result', 'model based approaches', 'strong clusters tend', 'human generated results', 'basically measures based', 'summarize text clustering', 'right generative model', 'also would like', 'clustering search results', 'desired clustering bias', 'desired clustering bias', 'humans would bring', 'explore text data', 'similarity based approaches', 'evaluate text clusters', 'evaluate text clusters', 'cluster clustering result', 'one must go', 'right similarity function', 'generate 100 clusters', 'best clustering results', 'also would create', 'system generated clusters', 'system generated clusters', 'system generated clusters', 'gold standard clusters', 'baseline system could', 'application specific question', 'also say sometimes', 'unsupervised algorithm', 'would like', 'procedure wise', 'would allow', 'measures whether', 'guide us', 'generated results', 'judgments based', 'text clustering', 'text clustering', 'particularly useful', 'particularly useful', 'two ways', 'quantitatively evaluate', 'many approaches', 'classroom methods', 'text content', 'clustering results', 'clustering results', 'clustering results', 'clustering bias', 'clustering bias', 'clustering bias', 'clustering bias', 'clustering bias', 'clustering bias', 'system results', 'model design', 'mixture model', 'gold standard', 'gold standard', 'gold standard', 'go back', 'f measure', 'text objects', 'text objects', 'text data', 'text data', 'cluster based', 'cluster based', 'ideal clusters', 'clustering system', 'clustering system', 'baseline system', 'baseline system', 'baseline system', 'baseline system', 'multiple ways']
lecture 36
['text categorization helps us enrich text representation', 'literature article categorizations another important task', 'categorize news generated every day', 'helpdesk email messages generally routed', 'search engine applications would want', 'build automatic text categorization system', 'often use text categorization techniques', 'infer certain desicion factors', 'news agencies would like', 'extra sensor data collected', 'text categorisation allows us', 'use text mining tool', 'non text data specifically', 'text categories important well', 'generally distinguish two kinds', 'help us distinguish spam', 'analyzing text data based', 'handle different people attempt', 'help us infer properties', 'also many applications like', 'also another important kind', 'k category categorisation task', 'would allow us', 'non native speakers', 'medical subject heading', 'various different kinds', 'provide 2 levels', 'email routing would', 'many examples like', 'big text data', 'yet another variation', 'use text categorization', 'use text categorization', 'text coding encoding', 'also many variants', 'text based prediction', 'text categorisation followed', 'text data mining', 'using text data', 'using text data', 'meaning keyword bag', 'yet another kind', 'text processing tasks', 'using binary categorization', 'handle different kinds', 'automatically email routing', 'text data indirectly', 'yet another application', 'semantic categories assigned', 'categorize products based', 'sentiment categories assigned', 'also add categories', 'new tax objects', 'text categorization problem', 'binary categorization problem', 'binary classification problem', 'category actually corresponds', 'author attribution might', 'sub categories etc', 'high level categories', 'categories form hierarchy', 'speakers another example', 'topic mining analysis', 'assign predefined categories', 'actually infer properties', 'also two categories', 'hierarchical category categorisation', 'multiple categorization tasks', 'general case would', 'labeled text objects', 'sentiment categories could', 'called author attribution', 'general assign categories', 'first text objects', 'non relevant documents', 'meaningful categories associated', 'terms characterize content', 'could first categorize', 'allow us', 'news categorization', 'tell us', 'two kinds', 'literature articles', 'data mining', 'using text', 'non spam', 'words representation', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'another variation', 'individual task', 'text categorization', 'text categorization']
lecture 37
['news article mentions word like game', 'logistical regression support vector machines', 'human experts must annotate datasets', 'matches game sports many times', 'means superficial features like keywords', 'even policy feature tags', 'sophisticated features like phrases', 'might use something similar', 'still require manual work', 'function behaves like based', 'learning processes optimization problem', 'strategy would work well', 'also combine multiple features', 'human experts also need', 'easily identify text data', 'becauses different rules would', 'combine multiple evidences', 'use human experts', 'even syntactic structures', 'label given data based', 'k nearest neighbors', 'following conditions hold', 'cannot handle uncertainties', 'sports three times', 'optimally combine features', 'human would assign', 'data looks like', 'learning process basically', 'would allow us', 'use machine learning', 'use machine learning', 'really completely automatic', 'naive bayes classifier', 'discriminative classifiers attempted', 'natural choice would', 'machine learning methods', 'quotation marks cause', 'nonlinear models might', 'main topic could', 'high level one', 'methods would rely', 'also classify accurately', 'marginally touching sports', 'called discriminative classifiers', 'called discriminative classifiers', 'machine learning problems', 'objective function tends', 'separating different categories', 'different object function', 'non linear combination', 'new text object', 'different methods tend', 'using machine learning', 'data point directly', 'still put automatic', 'classifier would take', 'features separate categories', 'supervised learning problems', 'often also called', 'human experts', 'data given label', 'distinguish different categories', 'generative classifiers try', 'manual work', 'news articles', 'easily use', 'learn soft rules', 'human would', 'things like', 'categories must', 'many methods', 'supervised learning', 'learning methods', 'might optimize', 'many variations', 'would need', 'also tend', 'use probabilities', 'generative classifiers', 'objective function', 'objective function', 'judged based', 'distinguish based', 'different variations', 'different topic', 'different depending', 'would lead', 'first model distribution', 'text object', 'text object', 'still decide', 'reliable take', 'linear combination', 'different problem', 'common choice', 'label given', 'surface features', 'discriminating features', 'basic features', 'also talk']
lecture 38
['also called idf weighting inverse document frequency weighting', 'naive bayes scoring function actually makes sense', 'background model would cause nonuniform smoothing', 'simply add small nonnegative constant delta', 'would help us preserve precision', 'also see another smoothing parameter mu', 'clearly see naive bayes classifier', 'strong connection close connection', 'classifier called logistical regression', 'actually adapt document clustering models', 'help achieve discriminative weighting', 'course human experts must', 'bayes rule allows us', 'maximum likelihood estimator indeed', 'would bring every word distribution', 'particular topic word distributions theta', 'also add k multiplied', 'also add k multiplied', 'use maximum likelihood estimator', 'added delta k times', 'simplified unigram language model', 'many text categorization tasks', 'seen also multiple times', 'use generative probabilistic models', 'seen naive bayes classifier', 'corresponding weight beta sub', 'idf weighting', 'l words represent represent', 'would help us make', 'generative probabilistic models', 'naive bayes classifier', 'naive bayes classifier', 'naive bayes classifier', 'words smaller across categories', 'naiyes bayes classifier', 'also several times', 'every word every category', 'see something really interesting', 'n sub one documents', 'delta approaches positive infinity', 'cause underflow problem', 'multiple topics represented', 'also generally true', 'trigram language model', 'bigram language model', 'actually quite effective', 'might indeed ignore', 'using bayes rule', 'using bayes rule', 'smoothing allows us', 'background language model', 'background language model', 'background language model', 'background language model', 'background language model', 'background language model', 'background language model', 'background language model', 'background language model', 'background language model', 'address data sparseness', 'use bayes rule', 'use generative models', 'use larger text data', 'using machine learning', 'assumption allows us', 'naiyes bayes categorization', 'actual generative model', 'mining word relations', 'feature vector f', 'constant beta zero', 'see n sub', 'left hand side', 'added pseudo counts', 'interesting special cases', 'feature support category one', 'mu approaches infinity', 'word helps contributing', 'bring prior knowledge', 'word distribution explains', 'word distribution characterizes', 'constant pseudo count', 'theta sub b', 'longer mixture model', 'also called', 'uniform pseudo counts', 'statistical estimation problem', 'avoid small probabilities', 'second probability depends', 'penalize common words', 'one possible way', 'sometimes multiple categories', 'two uniform distribution', 'training data available', 'generally take logarithm', 'supporting category one', 'total pseudo counts', 'get posterior becausw', 'training data set', 'document looks like']
lecture 39
['maximum likelihood estimator basically lets go find', 'also introduce another discriminative classifier called k nearest neighbors', 'although naive bayes classifier tries', 'documents first class theta 1', 'called k nearest neighbors', 'called k nearest neighbors', 'bringing additional data points', 'k nearest neighbor works', 'lets say theta 1', 'help us decide categories', 'lets say category 1', 'similarity function captures objects', 'well defined scoring function', 'actually use bayes rule', 'k nearest neighbors', 'function form looks like', 'general represent text data', 'using bayes rule', 'using bayes rule', 'using cross validation', 'solid field boxes', 'binary response variable', 'binary response variable', 'actually would assume explicitly', 'naive base classifier', 'cover discriminative approaches', 'naive bayes classifier', 'betavalues already known', 'actually far away', 'machine learning programs', 'machine learning algorithms', 'results might depend', 'standard optimization problem', 'assumed functional form', 'likelihood function would', 'seen science etc', 'actually could risk', 'including text categorization', 'actually three neighbors', 'would vary depending', 'would allow us', 'similarity function either', 'help us classify', 'follow similar distributions', 'help us estimate', 'general scoring function', 'course would depend', 'similarity function could', 'machine learning course', 'would allow many', 'data directly rather', 'training data set', 'logistical regression function', 'commonly used classifiers', 'actually important assumption', 'documents training documents', 'training set would', 'another possibility', 'another advantage', '1 minus probability', 'use one part', 'data objective since', 'take weighted sum', 'many possible ways', 'label given data', '1 given x', 'log odds ratio', 'given x directly', 'particular training case', 'naive bayes', 'naive bayes', 'naive bayes', 'naive bayes', 'discriminative approaches', 'basically feature vector', 'discriminative classifiers', 'discriminative classifiers', 'classifier would', 'category given document', 'actually help', 'value x sub', 'machine learning', 'machine learning', 'k examples', 'indeed k', 'looks like', 'explicitly requires', 'conditional likelihood', 'conditional likelihood', 'conditional likelihood', 'condition likelihood', 'useful classifier', 'second classifier', 'scoring function', 'scoring function', 'scoring function', 'parameter k', 'first think', 'objective function', 'training data']
lecture 40
['advanced machine learning techniques called domain adaptation', 'new machine learning methods like representation learning', 'semi supervised machine learning techniques', 'introduce yet another discriminative classifier called', 'features might help us distinguish categories', 'original high dimensional space corresponding', 'beta0 beta 1 beta 2', 'humans definitely play important role', 'could assume five star reviews', 'could give us another line', 'use supervised machine learning', 'high dimensional feature space', 'low dimensional space features', 'notations people usually use', 'high confidence classification results', 'k dimensional space instead', 'justice k values corresponding', 'really exploit unable data', 'maybe combine multiple methods', 'errors systematically across categories', 'low quality training examples', 'highly non linear classifier', 'require effective feature representation', 'supervised machine learning', 'actually help us reduce', 'texas domain cause words', 'help us design features', 'also mention negative opinions', 'really much general thing', 'five star reviews', 'high dimensional space', 'seen earlier actually uses', 'machine learning techniques', 'machine learning techniques', 'low dimensional structures', 'actually training label examples', 'would combine different methods', 'design effective feature set', 'classifier would also help', 'two dimensional space', 'lower dimensional space', 'topic space representation', 'topic define one dimension', 'also optimize different objects', 'minimize w transpose multiplied', 'new texture objects', 'goes beyond bag', 'another common scene', 'learning effective representation', 'support vector machine', 'support vector machine', 'corresponding wedding design', 'different learning refers', 'classifying new objects', 'original feature representation', 'complicated tasks like', 'require effective features', 'would give us', 'deep neural network', 'learning effective features', 'original word features', 'intermediate features embedded', 'compound features automatically', 'pseudo training examples', 'popular classification method', 'feature design tends', 'nearest neighbor way', 'naive bayes classifier', 'inducing intuitively makes', 'complex network effectively', 'call support vectors', 'unable text data', 'use another threshold', 'text processing tends', 'learn intermediate representations', 'help categorizing tweets', 'support vector machines', 'non 0 value', 'design new features', 'latent dimension features', 'introduced many methods', 'immediately effective classifier', 'design effective features', 'learn factor features', 'need domain knowledge', 'svm actually tries', 'sentiment categorizer meaning', 'real world variables', 'logistical regression classifier', 'involves human labor', 'one actually captures', 'different data sets', 'large value would see', 'example word count', 'also would like', 'machine learning', 'left hand side', 'makes 0 value', 'two public categories', 'particular point like']
lecture 41
['information retrieval researchers called cranfield evaluation methodology', 'occasionally let us spam email', 'information retrieval researchers', 'actual must multiple perspectives', 'desired ideal output generated', 'categorisation ground truth created', 'ground truth test collection', 'involve empirically defined problems', 'baseline would simply put', 'standard measure used everywhere', 'would give us 98', 'recall would tell us', 'test many different systems', 'arithmetic mean would still', 'recall tells us whether', 'called ground truth', 'classification error classification accuracy', 'two category categorization problem', 'would tell us', 'human system said yes', 'create test collection', 'would still give', 'called classification accuracy', 'different categorization mistakes', 'read many papers', 'systems categorization results', 'evaluating searching results', 'system says yes', 'system says yes', 'system says yes', 'system says yes', 'system says yes', 'system says yes', 'system says yes', 'system says yes', 'human says yes', 'becomes true positive', 'false positive fp', 'compare different systems', 'use classification accuracy', 'per document basis', 'already every document', 'parameter beta two', 'course would like', 'also another problem', 'also corrected vision', 'also two kinds', 'popular category valuation', 'meshes called recall', 'measure called f1', 'compare different methods', 'compare different methods', 'missed one assignment', 'less effective one', 'many different methods', 'method works better', 'true positive divided', 'sometimes also useful', 'evaluate categorisation results', 'document perspective based', 'human would think', '2 popular measures', 'characters decisions obviously', 'give us', 'category performs better', 'documents along documents', 'k categories denoted', 'spam filtering', 'classification accuracy', 'classification accuracy', 'actually indeed assigned', 'different perspectives', 'would like', 'use methodology', 'system output', 'letting us', 'allows us', 'taxable categorization', 'still expect', 'simple baseline', 'put together', 'system says', 'human says', 'human says', 'arithmetic mean', 'arithmetic mean', 'arithmetic mean', 'two kinds', 'would lead', 'would call', 'say yes', 'said yes', 'said yes', 'ends yes', 'test set', 'recall defined', 'legitimate email', 'legitimate email', 'legitimate email', 'actually useful']
lecture 42
['also variations like per document based evaluation', 'especially balanced tester set', 'particular customer service person', 'sometimes categorisation results might actually', 'whereas geometric mean would', 'measures must also reflect', 'micro versus macro averaging', 'precision p1 recall r1', 'general macro average tends', 'might get misleading results', 'per category evaluation', 'whole data set', 'two suggested readings', 'design measures appropriately', 'cause people tend', 'got different conclusions', 'different behavior depending', 'email messages might', 'weighted classification accuracy', 'following classification accuracy', 'optimizing ranking measures', 'maybe different ways', 'machine learning methods', 'also micro averaging', 'arithmetic mean would', 'use geometric mean', 'emphasize low values', 'different decision error', 'distinguishing relevant documents', 'binary categorization problem', 'ranking problem instead', 'compare different methods', 'human editors would', 'many different ways', 'search engine evaluation', 'application specific away', 'f value f1', 'consider arithmetic mean', 'high values would', 'commonly used measures', 'maybe better frame', 'useful would depend', 'category c one', 'news categorization results', 'see subtle differences', 'believe one method', 'method works better', 'like precision', 'sometimes ranking may', 'overall f score', 'aggregate different values', 'geometric mean', 'often indeed passed', 'evaluation measures', 'arithmetic mean', 'macro averaging', 'macro averaging', 'classification accuracy', 'micro averaging', 'micro averaging', 'categorization evaluation', 'different ways', 'person would', 'ranking evaluation', 'macro average', 'macro average', 'compute precision recall', 'micro average', 'actually compute', 'specific decision', 'introduced measures', 'sometimes categorisation', 'ranking accuracy', 'low values', 'categorisation decision', 'first evaluation', 'might reflect', 'high values', 'search engine', 'news articles', 'categorization problem', 'ranking problem', 'textual categorisation', 'text categorisation', 'text categorisation', 'different methods', 'different dimensions', 'different approaches', 'different angles', 'different angles', 'categorisation results', 'text categorization', 'categorization methods', 'particular application', 'particular application', 'ranking emails', 'might reveal', 'precision values', 'high precision', 'many cases']
lecture 43
['using natural language processing techniques', 'could help us better serve people', 'use text based prediction techniques', 'also differentiate distinguish different kinds', 'data driven social science research', 'deeper natural language processing', 'would help us better understand', 'person could also make observation', 'different background may also think', 'identify three major reasons', 'understanding obviously goes beyond', 'distinguish positive versus negative', 'using opinion mining techniques', 'text might also report opinions', 'policymakers may also want', 'person might think differently', 'second must also specify', 'help understand peoples preferences', 'reviewer might mention opinions', 'simplest opinion mining tasks', 'another persons opinion etc', 'deeply understand opinion sentiment', 'one person might comment', 'data like video data', 'basic opinion representation like', 'actually explicit opinion holder', 'also infer opinion sentiment', 'necessarily look like opinion', 'least three measurements', 'sometimes factual sentences like', 'help us optimize', 'identify one sentence opinion', 'key differentiating factor', 'help decision support', 'opinion tells us', 'real world objectively', '3 broad reasons', 'optimize recommender system', 'called voluntary survey', 'also complex text', 'taking text data', 'also would like', 'decision like buying', 'product search engine', 'one statement might', 'one phrase opinion', 'opinion would depend', 'actually unique advantage', 'observer might make', 'entire discourse context', 'like different time', 'happy versus sad', 'would define opinion', 'basic opinion representation', 'course adds value', 'sentiment analysis covering', 'someone else opinion', 'subjective statement describing', 'understanding consumers opinions', 'easily build applications', 'still quite difficult', 'often explicitly identified', 'holder clearly wish', 'generated text data', 'analyze opinions buried', 'also another target', 'identify opinion holder', 'identify opinion holder', 'formally define opinion', 'product manufacturers want', 'particular discourse context', 'better understand', 'example iphone 6', 'know peoples opinions', 'general also easy', 'one general kind', 'want opinion content', 'also help', 'usually aggregate opinions', 'two kinds', 'support research', 'offers us', 'helps us', 'also infer', 'see product reviews', 'make prediction', 'basic understanding', 'video data', 'also would', 'voluntary survey', 'second application', 'market research', 'another person', 'another person', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data']
lecture 44
['feature design actually affects categorization accuracy significantly', 'allows much larger search space', 'also learn word clusters empirically', 'naturally infrequent features tend', 'simple text categorization technique', 'named entities like people', 'ontology like word net', 'regular text categorization technique', 'derive simple world n', 'frequent versus infrequent features', 'may also generalize better', 'could use ordinal regression', 'also many meaningful features', 'representation actually would allow', 'parse tree based features', 'nlp enriches text representation', 'language pretty much', 'text categorization method', 'also mix n grams', 'patton discovery algorithms', 'natural language processing', 'long time ago', 'might still face', 'five might denote', 'opinionated text object', 'machine learning program', 'machine learning program', 'machine learning application', 'combine machine learning', 'applying machine learning', 'needs two kinds', 'go beyond polarity', 'especially also needed', 'word based representation', 'basic feature space', 'design seed features', 'word great might', '), different lengths', 'different feature space', 'really caused tradeoff', 'might represent concepts', 'might consider using', 'text processing tasks', 'especially polarity analysis', 'also different ways', 'sementically related words', 'paradigmatically related words', 'necessarily occur together', 'frequent pattern syntax', 'sentiment analysis clearly', 'speech tag n', 'using sentiment classification', 'frequent word set', 'would suggest positive', 'use domain knowledge', 'accuracy may', 'even speech act', 'enrich text representation', 'derive complex features', 'words might occur', 'may cause overfitting', 'let overfitting happen', 'well designed features', 'larger space', 'features cause overfitting', 'often good enough', 'noun could form', 'frequently used categories', 'text categorization', 'text categorization', 'text categorization', 'also consider part', 'feature design', 'machine learning', 'generalize well', 'actually often', 'regular n', 'feature space', 'simple sentence', 'parse tree', 'parse tree', 'feature learning', 'derive features', 'different lengths', 'different n', 'two ways', 'necessarily occur', 'many tasks', 'domain knowledge', 'allow us', 'occur also', 'word n', 'representation would', 'categorization errors', 'text mining', 'text data', 'text correctly', 'feature set', 'polarity analysis', 'polarity analysis']
lecture 45
['plus k minus one alpha values', 'standard two category categorization problem', '1 regular logistic regression classifiers', 'actually help us training data', 'binary logistic regression program', '1 independent logistical regression classifiers', 'typical sentiment classification problem', 'logistic regression classifiers indexed', 'would actually give us', 'also used offer subject', 'logistical regression program would', 'binary setting categorization problem', 'positive words generally suggest', 'introduce multiple binary classifiers', 'two category categorization problem', 'distinguish k versus others', '1 logistic regression classifiers', 'multi level rating prediction', 'ordinal logistic regression', 'ordinal logistic regression', 'ordinal logistic regression', 'ordinal logistic regression', 'ordinal logistic regression', 'help us choose', 'apply logistical regression', 'binary response variable', 'distinguish category 2', 'next class file', 'train categorisation program', 'would allow us', 'would allow us', 'help us decide', 'taking linear combination', 'use logistic regression', 'distinct alpha value', 'help us set', 'tells us whether', 'tell us whether', 'help us solve', 'logistical regression classifier', 'intuitively appealing assumption', 'k minus one', 'two positive benefit', 'formula would look', 'opinionated text document', 'rather rating 2', 'far fewer parameters', '1 given x', 'replace beta 0', 'good beta value', 'positive would make', 'many different approaches', 'general decision rule', 'actually assign ratings', 'general would make', 'logistic regression', 'binary categorization', 'logistical regression', '1 classifiers altogether', '0 means x', '1 means x', 'allow us', 'regular text', 'categorization problem', 'alpha parameter', 'actually dependent', 'logistical function', 'categorization technique', 'different alpha', 'sentiment analysis', 'apha subject', 'distinguish two', 'actually use', 'text document', 'training data', 'training data', 'two values', 'many classifiers', 'positive words', 'really independent', 'multiple levels', 'linear function', '1 classifiers', '1 classifiers', 'different classifiers', 'parameters would', 'second problem', 'problem set', 'problem set', 'optimal value', 'decide whether', 'criteria whether', 'also write', 'first problem', 'solution would', 'classifier two', 'parameter values', 'better values', 'beta values', 'beta values']
lecture 46
['word like amazing mentioned many times', 'likely would mean really cheaper prices', 'rank hotels along different dimensions', 'assumed market valued gaussian distribution', 'problem called latent aspect rating analysis', 'called latent rating regression', 'another multivariate gaussian distribution', 'multivariate gaussian prior distribution', 'mentioned many times', 'using seed words like location', 'continue discussing opinion mining', 'aspect level opinion summary', 'parameter values including betas', 'detailed understanding would reveal', 'latent aspect rating analysis', 'decompose overrating two ratings', 'would also allow us', 'also use topic models', 'also analyze reviewers preferences', 'observed overall rating given', 'use c sub w', 'press basic introduction', 'perform detailed analysis', 'reviewer really cares', 'others might care', 'denotes actually await', 'would allow us', 'reviewers may care', 'observer ratings condition', 'setup allows us', 'major aspects comment', 'covariance matrix sigma', 'use bayesian inference', 'w gives us', 'discussing different aspects', 'given five stars', 'gives us way', 'observed rating given', 'mine correlated words', 'compute interesting quantities', 'observed word frequencies', 'different aspects accurately', 'overall rating given', 'aspect specific sentiment', 'reviewer might give', 'decomposed aspect ratings', 'overall rating based', 'relative weights placed', 'document specific weights', 'actually latent', 'sentiment weights might', 'simply weighted combination', 'review documents denoted', 'different hotels', 'really cover', 'rating analysis', 'opinion mining', 'case likely', 'detailed understanding', 'detailed understanding', 'location using', 'another word', 'sentiment analysis', 'sentiment analysis', 'model different segment', 'obtain aspect segments', 'word might', 'reviewer cares', 'seed words', 'particular alpha value', 'normal distribution', 'normal distribution', 'normal distribution', 'allows us', 'allows us', 'opinion based', 'rating based', 'another aspect', 'overall rating', 'overall rating', 'overall rating', 'overall rating', 'overall rating', 'overall rating', 'overall rating', 'overall rating', 'word frequencies', 'maximizer observed', 'give us', 'give us', 'weighted combination', 'weight distribution', 'two stages', 'two stages', 'review documents', 'might still', 'might place', 'interesting variables', 'c supply', 'aspect rating']
lecture 47
['value versus location value versus room etc', 'highly weighted words versus', 'whole expensive hotels five stars', 'negatively lower weighted words', 'natural language processing techniques', 'bottom words also makes sense', 'also learn sentiment information directly', 'low overall ratings versus', 'rather query specific recommendation', 'analyzing users rating behavior', 'social media like tweets', 'precise whose inferred weights', 'others might score better', 'help us understand better', 'latent aspect rating analysis', 'non personalized recommendation results', 'mining latent user preferences', 'favor low price hotels', 'generative model like psa', 'really tell much difference', 'high level overall ratings', 'natural solution would', 'task sentiment analysis', 'mostly still reflecting', 'enriched feature representation', 'direct application would', 'problem using two stages', 'room condition tend', 'yet another application', 'end users better', 'dimensions like location', 'help us discover', 'would give us', 'standard techniques tend', 'like large screens', 'generator rated aspect', 'appreciating different features', 'next two papers', 'cutting edge topics', 'aspect rating analysis', 'latent regression model', 'also mother results', 'high overall ratings', 'top ten tends', 'like expensive hotels', 'review interesting patterns', 'top results generally', 'reach sentiment lexicon', 'different consumer groups', 'things like service', 'highest inferred value', 'text mining algorithms', 'overall rating condition', 'much higher price', 'analyzing review data', 'reviewers whose weights', 'top ten group', 'interesting preference information', 'dimensions like value', 'little regression model', 'interesting aspects commented', 'also another way', 'unified generative model', 'unified generative model', 'mp3 three reviews', 'reveal detailed opinions', 'word like long', 'give cheaper hotels', 'compare different hotels', 'text mining applications', 'average weights tend', 'compare different reviewers', 'put higher weights', 'social media', 'laptop etc', 'five stars', 'five stars', 'makes sense', 'could mean positive', 'two stages', 'users better', 'users better', 'room cleanliness', 'low group', 'directly used', 'also better', 'use topic model', 'low ratings', 'aspect level', 'bottom ten', 'techniques proposal', 'overall rating', 'overall rating', 'overall rating', 'overall rating', 'overall rating', 'much information', 'high scores', 'expensive hotels', 'expensive hotels']
lecture 48
['non text data also helps provide context', 'see text based prediction character serve', 'also add non text data directly', 'machine learning called active learning', 'text data like topic mining', 'help non text data mining', 'gives us two sets', 'also help us understand', 'next human also must', 'one sub task could', 'design high level features', 'help interpret patterns discovered', 'combine many text mining', 'contextual text mining techniques', 'course sometimes text alone', 'non text data together', 'see non text data', 'course help humans identify', 'general human would play', 'machine learning programs', 'really help improving', 'topic based indicators', 'qiaozhu mei dissertation', 'make decisions based', 'make decisions based', 'identify data points', 'non text data', 'non text data', 'non text data', 'non text data', 'non text data', 'non text data', 'non text data', 'non text data', 'non text data', 'help text mining', 'contextual text mining', 'contextual text mining', 'important real world variables', 'including topic mining', 'text mining algorithms', 'partition text data', 'original text data', 'interpreting text data', 'including text data', 'analyzing text data', 'text mining techniques', 'help provide predictors', 'provides limited view', 'improvement often leads', 'text data mining', 'mining text data', 'help us improve', 'text based prediction', 'text based prediction', 'additional data needs', 'human actually plays', 'would allow us', 'much better representation', 'text mining perspective', 'including human sensors', 'predictive model building', 'called pattern annotation', 'use text data', 'data mining loop', 'data mining loop', 'difference might suggest', 'content mining techniques', 'get sophisticated patterns', 'general text data', 'join mine text', 'real world variables', 'helps interpret', 'design effective predictors', 'human first would', 'non text', 'prediction problem set', 'helps discover', 'topic mining', 'topic mining', 'topic mining', 'directly related', 'directly characterize', 'context defined', 'also different', 'human could', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data']
lecture 49
['gaining increasing attention recently', 'almost allows partition text data', 'require contextual text mining', 'basically contextual text mining', 'combine non text data', 'indirect text context refers', 'course uses additional context', 'almost always available', 'contextual text mining', 'contextual text mining', 'contextual text mining', 'make opinion mining', 'derive sophisticated predictors', 'common research interests', 'interesting comparative analysis', '2012 presidential campaign', 'many interesting questions', 'data mining research', 'would allow us', 'would allow us', 'research topics published', 'broad analysis question', 'also reveal differences', 'additional information connected', 'research papers published', 'many interesting ways', 'many interesting ways', 'different conference names', 'text based prediction', 'often gives us', 'make topics associated', 'general provides meaning', 'comparing topics overtime', 'partition text data', 'making opinions connected', 'additional data related', 'also indirect context', 'rich context information', 'text context useful', 'compare different contexts', 'context would include', 'one social network', 'obtain additional context', 'compare sigir papers', 'include direct context', 'text mining', 'text often', 'allows us', 'allows us', 'interesting ways', 'interesting ways', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'partition data', 'partition data', 'data based', 'many applications', 'social network', 'social network', 'text yet', 'analyze text', 'topic mining', 'two contexts', 'presidential election', 'prediction problem', 'comparing topics', 'also partition', 'arbitrary ways', 'news data', 'different years', 'different years', 'different regions', 'sigir papers', 'direct context', 'useful knowledge', 'knowledge associated', 'conference venues', 'context data', 'two researchers', 'sudden changes', 'stock prices', 'stock prices', 'specific ones', 'separate unit', 'related data', 'one group', 'multiple kinds', 'issues mattered', 'gonna associate', 'goes beyond', 'generate even', 'enables discovery', 'discovery topics', 'complicated partitions', 'compare topics', 'also intersect']
lecture 50
['contextual text mining called contextual probabilistic latent semantic analysis', 'approach contextual probabilistic latent semantic analysis', 'introduce contextual probabilistic latent semantic analysis', 'word distributions actually tell us collection specific variations', 'estimate premise would naturally contain context variables', 'continue discussing contextual text mining', 'classical probabilistic model logic model', 'topic coverage might also vary according', 'another hurricane hurricane rita hit', 'major evaluation effort sponsored', 'explicitly add interesting context variables', 'enable cross collection comparison', 'perform comparative text mining', 'maybe particular topic like donation', 'makes 2 specific assumptions', 'another task introduced later', 'discover contextualized topics make', 'reveal spatial temporal patterns', 'different retrieval tasks though', 'contextual text mining', 'contextual text mining', 'contextual text mining', 'contextual text mining', 'gives us different views', 'might cover topics differently', 'technique would allow us', 'hurricane rita hit', 'top word top words', 'add context variables', 'topics given certain context', 'language modeling approach', 'would enable us', 'enterprise search tasks', 'high probability words shown', 'would allow us', 'yet another application', 'event impact analysis', 'topic coverage also depends', 'high level ideas', 'government response donation', 'assumption allows us', 'special temporal patterns', 'particularly sigir papers', 'two curves tracked', 'word like government', 'text retrieval conference', 'common theme indicating', 'general curves like', 'discover different variations', 'hit new orleans', 'tight little context', 'vector space model', 'comparing news articles', 'word distribution associated', 'probabilistic models', 'topics might represent', 'text given context', 'common topics shared', 'certain time period', 'research information retrieval', 'information retrieval research', 'different views associated', 'see clear dominance', 'next two cells', 'front block articles', 'first also choose', 'top one shows', 'time july 2005', 'common topic covered', 'common topics covered', 'common topics covered', 'different time periods', 'location like texas', 'bottom curve shows', 'still multiple topics', 'particular time period', 'different different contexts', 'see words like', 'language model paper', 'document specifically coverage', 'choose particular coverage', 'people might want', 'context variables', 'review common topics', 'boolean model etc', 'allow us', 'allow us', 'text would', 'one coverage distribution', 'word distributions', 'word distributions', 'word distributions', 'word distributions', 'word distributions', 'vary depending', 'topical collection', 'allows us', 'allows us', 'special patterns', 'maybe aid']
lecture 51
['social media content might also form social networks', 'model called network supervised topic model', 'continue discussing contextual text mining', 'network supervised topic model modeling', 'often use maximum likelihood estimator', 'parameters would give us useful information', 'regular topic models like plsa', 'optimize another function f', 'say two social networks', 'one single objective function', 'regularizer function called r', 'netplsa would give much', 'research articles might form', 'rich information network environment', 'even sub networks', 'must cover similar topics', 'text data given parameters', 'text data alone based', 'single optimization framework', 'one suggested reading', 'current topic models', 'people might follow', 'people might claim', 'solving optimization problem', 'share common distribution', 'optimization objective function', 'modified object function', 'ir information retrieval', 'also use four topics', 'likelihood objective function', 'information retrieval second', 'capturing different heuristics', 'collaboration network would', 'help us uncover', 'text collection c', 'second equation shows', 'similar model could', 'plsa log likelihood', 'plsa log likelihood', 'co occurring words', 'similar topic distribution', 'network induced regularizers', 'form geographical network', 'topic model plsa', 'social network context', 'tells us basically', 'office general approach', 'text data given', 'text data given', 'jointly analyzing text', 'collaboration network information', 'mine text data', 'simple constraints imposed', 'cover similar topics', 'cover similar topics', 'parameters generated denoted', 'generate represent words', 'similar topic distributions', 'network constraint perspective', 'two subnetworks let', 'actually quite powerful', 'model parameters lambda', 'network graph g', 'two adjacent nodes', 'collaboration network tend', 'related data together', 'part fairly familiar', 'general idea clearly', 'topics correspond well', 'social network', 'social network', 'social network', 'objective function', 'function f', 'would allow', 'likelihood function', 'topic model', 'topic model', 'called netplsa', 'regular results', 'using network context', 'topic mining', 'research articles', 'information together', 'guide us', 'allows us', 'plsa alone', 'model parameters', 'fairly familiar', 'big environment', 'network must', 'probabilistic model', 'generative model', 'function combines', 'imposed based', 'data mining', 'data mining', 'text mining', 'topic distributions', 'topic distributions']
lecture 52
['capture causal relation using somewhat past data', 'even using correlation measures like pearson correlation', 'indicate positive correlation w1and w3', 'effectively discover possibly causal topics based', 'continue discussing contextual text mining', 'help us infer new knowledge', 'dow jones industrial average', 'time series plus text data', 'top 3 words insignificant topics', 'non text data provide context', 'analyzing presidential election time series', 'called iterative causal topic model', 'topic models using time series', 'also many open challenges', 'combine non text data', 'iowa electronic market would', 'top ranked word list', 'new york times discovered', 'potentially discover causal topics', 'potentially discover causal topics', 'analyze word level correlation', 'non text time series', 'analyze text data together', 'texts semantically coherent topics', 'also get another subtopic', 'original topic topic one', '2000 presidential campaign election', 'time series context whether', 'iowa electronic market', 'apply regular topic modeling', 'new york times', 'new york times', 'new york times', 'contextual text mining', 'negatively correlated words w2', 'september 11 attack', 'september 11 attack', 'might provide insight', 'candi date causing', 'true causal relationship', 'true causal relations', 'text data together', 'mining text data', 'standard talking models', 'non text data', 'non text data', 'non text data', 'result help us', 'help us figure', 'social science study', 'also causal relation', 'auto regressive model', 'active research topic', 'crash actually happened', 'also support optimizig', 'also help interpret', 'iterative topic modeling', 'original topic one', 'original topic one', 'one simple solution', 'time series xt', 'time series feedback', 'external time series', 'external time series', 'external time series', 'external time series', 'external time series', 'stock market crash', 'still quite related', 'indeed quite related', 'pure topic models', 'companion text stream', 'pearson correlation', 'big data applications', 'statistically significant difference', 'necessarily mean anything', 'might discover w1', 'maximize light role', 'little bit discussion', 'indeed biased toward', 'actually quite simple', 'discover causal topics', 'causal topics ".', 'time series data', 'time series data', 'time series data', 'time series data', 'two suggested readings', 'regular topic model', 'presidential prediction market', 'text based prediction', 'text based prediction', 'granger causality test', 'granger causality test', 'companion news stream', 'companion news stream', 'use text mining', 'consider time lag', 'would mean x', 'frequently used measure']
lecture 53
['particularly logistical regression k nearest neighbor', 'particularly interesting area called deep learning', 'robust text mining technologies today tend', 'building practical text data application systems', 'two basic plan complementary relations', 'companion course called text retrieval', 'also help providing knowledge prominence', 'building efficient text mining systems', 'mostly used word based representations', 'big text data application system', 'particular help text mining systems', 'develop effective text analysis techniques', 'read frontier research papers', 'computing maximum likelihood estimator', 'text mining application systems', 'latent aspect rating analysis', 'help interpreting patterns later', 'help interpreting patterns discovered', 'practical text mining systems', 'harnessing big text data', 'category given text data', 'vector representations would allow', 'real world variables based', 'discover potentially paradigmatically relations', 'big text data applications', 'new techniques would lead', 'discover deeper knowledge buried', 'statistical learning techniques particularly', 'help non text data', 'text retrieval information retrieval', 'two important building blocks', 'many big data applications', 'information network mining techniques', 'analyze text information network', 'word similarity based evaluation', 'ordinal logistical regression', 'using non text data', 'maximum likelihood estimator', 'basic topic model plsa', 'method would allow us', 'course covered text mining', 'general data mining algorithms', 'using deep learning', 'text application system', 'given text reviews', 'interesting topical patterns', 'word association mining', 'introduced sentiment classification problem', 'big text data', 'supervised machine learning', 'reviewers latent weights', 'level takeaway messages', 'essential system component', 'discover paradigmatically relations', 'text based applications', 'text based applications', 'achieve better accuracy', 'potentially causal topics', 'text mining applications', 'invent new algorithms', 'text data mining', 'covers text retrieval', 'statistical machine learning', 'representing text data', 'non text data', 'non text data', 'share similar contexts', 'natural language processing', 'natural language processing', 'natural language processing', 'information theory concepts', 'contextual text mining', 'enriches text representation', 'enrich text representation', 'text based prediction', 'text based prediction', 'key takeaway messages', 'key takeaway messages', 'incorporate context variables', 'mine word associations', 'better text representation', 'relations might also', 'vector space model', 'word embedding technique', 'data mining techniques', 'probabilistic topic model', 'probabilistic topic model', 'analyze topic syntax', 'text analysis applications', 'many application areas', 'natural language used', 'another promising technique', 'also shown promise', 'discover new techniques', 'nowadays actually based', 'introduced naive bayes', 'courses would give', 'text data analysis', 'use social network', 'accurate knowledge discovery']
lecture 1
['video data might require computer vision', 'text retrieval also helps minimize human effort', 'network sensor would monitor network traffic', 'help us turn big text data', 'help people digest text data', 'distinguish two different results one', 'taught another separate mooc', 'generally need different algorithms', 'also develop special algorithms', 'geo sensor would sense', 'text mining supplies knowledge', 'two terms text mining', 'help us solve', 'two phrases literally', 'two different angles', 'cover specialized algorithms', 'relatively small amount', 'discussed various techniques', 'multimedia data like', 'understand video content', 'high level introduction', 'high quality information', 'high quality information', 'high quality information', 'text retrieval support', 'text retrieval refers', 'text mining systems', 'term text mining', 'called text mining', 'turning text data', 'treating text data', 'raw text data', 'raw text data', 'original text data', 'original text data', 'non text data', 'non text data', 'non text data', 'non text data', 'non text data', 'consuming text data', 'optimal decision making', 'data mining module', 'finding relevant information', 'two terms roughly', 'human effort', 'view text data', 'thermometer would watch', 'turn text data', 'mining text data', 'text analytics mean', 'term text analytics', 'relevant text data', 'giving actionable knowledge', 'called actionable knowledge', 'would also sense', 'data mining problems', 'network sensor', 'case text data', 'data mining problem', 'data mining problem', 'humans would express', 'often contain knowledge', 'high level', 'sense human', 'human sensors', 'generally would', 'text retrieval', 'text retrieval', 'text retrieval', 'text retrieval', 'text retrieval', 'text retrieval', 'text retrieval', 'text retrieval', 'mining algorithms', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'subjective sensor', 'really distinguish', 'really distinguish', 'different formats', 'information would', 'data mining', 'two ways', 'gives us', 'would observe', 'would facilitate', 'various forms', 'special kind', 'semantic content', 'might face', 'large amount', 'different kinds', 'different kinds', 'algorithmic view', 'concise information', 'mining emphasizes', 'effective mining', 'context mining', 'relational data', 'numerical data', 'data together', 'data together', 'data generated', 'text analytics', 'roughly corresponds', 'generally want', 'also see', 'also fine', 'also explain', 'actionable knowledge', 'actionable knowledge', 'actionable knowledge', 'actionable knowledge', 'actionable knowledge', 'actionable knowledge', 'actionable knowledge', 'general algorithms', 'better understand', 'general would', 'knowledge provenance', 'usually produced', 'subtle difference', 'subjective sensors', 'subjective sensors', 'subjective sensors', 'slide shows', 'shopping decision', 'search engines', 'really reliable', 'physical sensors', 'physical sensors', 'particular format', 'outcome could', 'much easier', 'minimize', 'longitude value', 'little bit', 'lattitude value', 'key concepts', 'hand emphasizes', 'go back', 'essential component', 'especially preferences', 'digital format', 'concise summary', 'clearly distinguished', 'battery life', 'also want', 'also needed', 'actually roughly', 'users would', 'also related', 'would report', 'network', 'real world', 'real world', 'real world', 'real world', 'real world', 'data observed', 'particular problem', 'decision problem', 'indeed needed', 'better choice', 'verify whether', 'particular kind', 'monitor', 'major opinions', 'location specification', 'basically taking', 'video', 'general problem', 'concise form', 'particularly useful', 'sensor', 'results', 'digest', 'two', 'two', 'terms', 'algorithms', 'us', 'us', 'would', 'would', 'retrieval', 'might', 'might', 'information', 'turn', 'turn', 'turn', 'mining', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'relevant', 'also', 'also', 'also', 'also', 'knowledge', 'knowledge', 'knowledge', 'knowledge', 'thermometer', 'sense', 'sense', 'sense', 'problems', 'mooc', 'mooc', 'express', 'decision', 'contain', 'world', 'world', 'analytics', 'analytics', 'analytics', 'analytics', 'problem', 'problem', 'problem', 'problem', 'problem', 'want', 'often', 'often', 'needed', 'case', 'case', 'better', 'general', 'general', 'verify', 'related', 'opinions', 'location', 'kinds', 'kinds', 'kind', 'kind', 'basically', 'actually', 'humans', 'humans', 'humans', 'humans', 'users', 'users', 'observed', 'observed', 'report', 'report', 'report', 'form', 'form', 'form', 'useful', 'useful', 'useful', 'useful', 'useful', 'way', 'way', 'utility', 'use', 'use', 'understanding', 'treat', 'treat', 'topics', 'topic', 'title', 'title', 'think', 'think', 'temperature', 'temperature', 'talk', 'taken', 'taken', 'take', 'take', 'take', 'speech', 'sometimes', 'something', 'solving', 'similarly', 'similarly', 'signal', 'signal', 'say', 'say', 'said', 'reviews', 'result', 'result', 'repeat', 'relation', 'relation', 'refer', 'reason', 'product', 'product', 'product', 'processor', 'process', 'probably', 'pre', 'positive', 'perspective', 'patterns', 'pattern', 'pattern', 'overview', 'overlap', 'output', 'output', 'number', 'next', 'necessarily', 'mostly', 'mind', 'means', 'meaning', 'make', 'make', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'looking', 'looking', 'looking', 'look', 'look', 'literature', 'let', 'let', 'let', 'let', 'lecture', 'laptop', 'know', 'interpretation', 'interpret', 'interesting', 'interchangeably', 'integrate', 'inside', 'input', 'important', 'important', 'hope', 'happening', 'going', 'going', 'going', 'going', 'going', 'give', 'framework', 'first', 'first', 'find', 'find', 'find', 'features', 'example', 'example', 'example', 'example', 'example', 'emphasize', 'distinction', 'discover', 'determine', 'define', 'dealing', 'dealing', 'dealing', 'course', 'course', 'course', 'course', 'course', 'course', 'consumer', 'concept', 'clear', 'chosen', 'change', 'categorical', 'cases', 'boundary', 'background', 'applicable', 'appealing', 'analogy', 'although', 'advantage', 'advantage', 'activities', 'actions', 'act', 'able', 'able']
lecture 2
['picture basically covered multiple types', 'help us make context sensitive analysis', 'historical stock price data would', 'different time might also pay attention', 'extracting high quality information', 'certainly also say something', 'partition text data based', 'human sensor would perceive', 'real world variable based', 'example entity relation graphs', 'actually provide interesting angles', 'many real world problems', 'form interesting comparison scenarios', 'text data would contain', 'might partition text data', 'often called predictive analytics', 'using knowledge representation language', 'text based prediction problems', 'include non text data', 'provide interesting angles', 'stock prices based', 'cover natural language processing', 'certain interesting variables', 'human observer would look', 'generating test data', 'different people would', 'useful context information', 'completing whatever tasks', 'different time periods', 'slide also serves', 'properties could include', 'pay attention', 'human would express', 'using text data', 'using text data', 'understanding text data', 'turn text data', 'text data generally', 'text data alone', 'require text data', 'original text data', 'generating text data', 'help us predict', 'text mining algorithms', 'real world variables', 'real world variables', 'real world variables', 'analyzing text data', 'analyze text data', 'text mining analytics', 'mining text data', 'also context associated', 'useful lexical knowledge', 'general picture would', 'could also use', 'could also use', 'mine word associations', 'fairly general landscape', 'observed world ".', 'non text data', 'general data mining', 'text mining problem', 'help us', 'stock prices', 'sensor would', 'human sensor', 'different angles', 'mining problems', 'different context', 'natural language', 'word associations', 'particular entity', 'human ...', 'discover something', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'supplying context', 'could analyze', 'real world', 'real world', 'real world', 'real world', 'text analysis', 'observed using', 'analyzing text', 'data mining', 'different language', 'time period', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'slide shows', 'different things', 'different languages', 'different aspects', 'biased also', 'also useful', 'also touch', 'also tend', 'also subject', 'also possible', 'also one', 'mining tasks', 'represent text', 'might generate', 'topic mining', 'opinion mining', 'mining knowledge', 'mining knowledge', 'joint mining', 'including mining', 'help prediction', 'world plus', 'observable world', 'various ways', 'useful way', 'useful techniques', 'summary could', 'subjective comments', 'still generated', 'social media', 'road map', 'really know', 'one way', 'objective way', 'news articles', 'mixed languages', 'main goal', 'looks like', 'left side', 'factual descriptions', 'decision making', 'also imagine', 'actionable knowledge', '1 type', 'prediction tasks', 'actually hope', 'mine knowledge', 'mine knowledge', 'mine knowledge', 'selectively cover', 'general way', 'observed world', 'observed world', 'observed world', 'problem definition', 'person could', 'analyzing content', 'analyze content', 'sentiment analysis', 'language usage', 'non textual', 'particular aspect', 'opinions expressed', 'mining content', 'one example', 'english ...', 'english ...', 'intermediate results', 'mostly looking', 'particular person', 'would', 'would', 'general topics', 'basically', 'context', 'might', 'might', 'make', 'make', 'analytics', 'variables', 'time', 'time', 'data', 'data', 'processing', 'help', 'help', 'different', 'comparison', 'called', 'also', 'also', 'analysis', 'analysis', 'text', 'text', 'text', 'text', 'actually', 'picture', 'picture', 'picture', 'mining', 'mining', 'language', 'language', 'language', 'world', 'world', 'world', 'world', 'world', 'world', 'world', 'useful', 'useful', 'use', 'use', 'properties', 'often', 'often', 'landscape', 'knowledge', 'knowledge', 'knowledge', 'form', 'form', 'express', 'mine', 'mine', 'general', 'general', 'general', 'general', 'general', 'cover', 'cover', 'cover', 'problem', 'problem', 'observed', 'observed', 'observed', 'associated', 'associated', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'prediction', 'prediction', 'prediction', 'prediction', 'prediction', 'prediction', 'usage', 'summary', 'opinions', 'observer', 'observer', 'observer', 'observer', 'observer', 'imagine', 'hope', 'generate', 'aspect', 'look', 'look', 'look', 'look', 'look', 'look', 'predict', 'predict', 'predict', 'predict', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'english', 'english', 'english', 'results', 'results', 'content', 'content', 'content', 'content', 'content', 'content', 'sentiment', 'sentiment', 'sentiment', 'looking', 'looking', 'looking', 'topics', 'topics', 'topics', 'topics', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'well', 'want', 'want', 'want', 'view', 'value', 'user', 'used', 'used', 'uncover', 'try', 'try', 'third', 'think', 'talk', 'talk', 'talk', 'taken', 'support', 'stocks', 'specifically', 'specifically', 'similarly', 'similar', 'show', 'sense', 'see', 'see', 'second', 'said', 'right', 'revert', 'result', 'result', 'rest', 'represented', 'regarded', 'regarded', 'reason', 'process', 'process', 'process', 'perspectives', 'perspective', 'perspective', 'perspective', 'patterns', 'outline', 'obviously', 'note', 'much', 'mood', 'mind', 'mind', 'mind', 'metadata', 'means', 'may', 'may', 'locations', 'location', 'kinds', 'kinds', 'join', 'interested', 'infer', 'infer', 'infer', 'indeed', 'important', 'important', 'important', 'helpful', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'get', 'focusing', 'focus', 'first', 'finally', 'finally', 'features', 'fact', 'except', 'exactly', 'everything', 'essence', 'especially', 'emphasize', 'distinguish', 'discussion', 'determines', 'detail', 'description', 'describe', 'describe', 'depends', 'course', 'course', 'course', 'course', 'course', 'course', 'course', 'course', 'course', 'concerned', 'combine', 'closely', 'changes', 'case', 'case', 'briefly', 'availability', 'authors', 'able', 'able']
lecture 3
['today particular statistical machine learning methods', 'art natural language processing techniques cannot', 'prepositional phrase attachment ambiguity meaning ...', 'boy b1 means b1 refers', 'natural language sentences precisely', 'another problem anaphora resolution', 'natural language content analysis', 'natural language content analysis', 'complete parse tree correctly', 'make human communications efficient', 'need common sense reasoning', 'understand natural language precisely', 'also another possible interpretation', 'speech tagging fairly well', 'precisely understanding natural language', 'cannot really fully understand', 'might also make inferences', 'called speech act analysis', 'statistical analysis methods', 'dog d1 means d1', 'natural language processing', 'natural language processing', 'natural language processing', 'natural language processing', 'say language processing', 'speech act analysis', 'quit smoking implies', 'probably achieve 90', 'annotate enough data', 'infer ... might', 'natural language first', 'syntactic ambiguity refers', 'still cannot solve', 'precisely 97 %.', 'using particular datasets', 'slides also shows', 'makes communication efficient', 'inferred meaning based', 'slide clearly shows', 'sentences general positive', 'also multiple meanings', 'word sense disambiguation', 'common sense knowledge', 'common sense knowledge', 'mine text data', 'got ambiguous part', 'natural language sentence', 'real world entities', 'reminding another person', 'another classic example', 'possibly different things', 'represent text data', 'word like design', 'word level ambiguity', 'text mining problem', 'john persuaded bill', 'person actually takes', 'numbers may need', 'mathematical sense etc', 'called syntactic parsing', 'language precisely', 'natural language', 'natural language', 'computer also needs', 'prepositional phrase', 'prepositional phrase', 'parse tree', 'computer would need', 'computer would actually', 'computational methods', 'generally difficult problem', 'particular tagging', 'generally cannot', 'cannot rely', 'speech tagging', 'speech tagging', 'computers today', 'fully understand', 'mathematical sense', 'make sense', 'ambiguity lies', 'another difficulty', 'another aspect', 'really understand', 'text mining', 'syntactical analysis', 'sentiment analysis', 'semantic analysis', 'semantic analysis', 'semantic analysis', 'really meaningful', 'pragmatic analysis', 'lexical analysis', 'data set', 'data set', 'noun phrase', 'multiple choices', 'formally represent', 'everything correctly', 'combinations may', 'another noun', 'whole problem', 'using symbols', 'speech tag', 'different interpretations', 'concepts using', 'also represented', 'also keep', 'also explains', 'syntactic categories', 'actually many', 'ordinary meaning', 'much meaning', 'computer needs', 'might even', 'boy might', 'sentence refers', 'three arguments', 'things like', 'studies obviously', 'specific examples', 'specialized cases', 'several steps', 'pre supposition', 'particularly extraction', 'partial parsing', 'parsing would', 'modifying saw', 'meeting would', 'mechanical approaces', 'man saw', 'main reason', 'main killer', 'limited domain', 'formal representation', 'first talk', 'every step', 'every step', 'connected together', 'certain way', 'bring back', 'big domain', 'basic concepts', 'artificial intelligence', 'interesting knowledge', 'additional knowledge', 'see also', 'different context', 'simplest part', 'well talking', 'syntactical categories', 'person met', 'indeed extract', 'even though', 'confused indeed', 'b1', 'place etc', 'computer would', 'computer would', 'simple example', 'use whatever', 'two ways', 'tells us', 'studies earlier', 'like ambiguities', 'anything perfectly', 'get meaning', 'playground would', 'phrases correct', 'simple sentence', 'natural', 'square root', 'better accuracy', 'language', 'processing', 'example dog', 'right choice', 'particular', 'd1', 'ambiguity', 'person saying', 'semantics yet', 'get scared', 'refers', 'means', 'means', 'sense', 'chasing action', 'possible', 'human', 'need', 'need', 'might', 'make', 'make', 'well', 'problem', 'problem', 'also', 'also', 'also', 'actually', 'word', 'word', 'text', 'text', 'understand', 'understand', 'understand', 'meaning', 'meaning', 'meaning', 'meaning', 'would', 'would', 'things', 'still', 'slide', 'parsing', 'numbers', 'like', 'level', 'john', 'generally', 'first', 'called', 'called', 'called', 'bill', '97', 'knowledge', 'knowledge', 'knowledge', 'understanding', 'understanding', 'understanding', 'part', 'part', 'person', 'person', 'person', 'person', 'person', 'makes', 'makes', 'infer', 'infer', 'indeed', 'even', 'entities', 'entities', 'categories', 'boy', 'boy', 'boy', 'boy', 'boy', 'boy', 'etc', 'etc', 'etc', 'computer', 'computer', 'computer', 'computer', 'computer', 'computer', 'computer', 'computer', 'computer', 'example', 'example', 'example', 'example', 'dog', 'dog', 'dog', 'dog', 'dog', 'use', 'us', 'two', 'tag', 'semantics', 'see', 'scared', 'right', 'perfectly', 'noun', 'noun', 'general', 'general', 'general', 'everything', 'earlier', 'context', 'choice', 'ambiguities', 'yet', 'yet', 'talking', 'talking', 'playground', 'playground', 'phrases', 'phrases', 'get', 'get', 'get', 'get', 'computers', 'computers', 'action', 'action', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'saying', 'saying', 'saying', 'root', 'root', 'root', 'chasing', 'chasing', 'chasing', 'accuracy', 'accuracy', 'accuracy', 'difficult', 'difficult', 'difficult', 'difficult', 'difficult', 'difficult', 'difficult', 'difficult', 'words', 'words', 'words', 'words', 'words', 'words', 'wanted', 'verb', 'verb', 'verb', 'used', 'used', 'tv', 'try', 'topic', 'think', 'think', 'therefore', 'terms', 'telescope', 'telescope', 'telescope', 'taken', 'take', 'take', 'structures', 'structures', 'structure', 'structure', 'state', 'space', 'someone', 'smoked', 'showed', 'show', 'segment', 'seeing', 'seeing', 'says', 'said', 'rule', 'result', 'requesting', 'request', 'relations', 'relation', 'referred', 'recognizing', 'read', 'reach', 'question', 'purpose', 'properly', 'product', 'problems', 'predicate', 'predic', 'point', 'plant', 'p1', 'outcome', 'order', 'order', 'order', 'order', 'order', 'one', 'omit', 'occurrence', 'moment', 'modify', 'mind', 'mean', 'mean', 'map', 'lot', 'lot', 'lot', 'lot', 'look', 'look', 'location', 'listed', 'lecture', 'learn', 'later', 'languages', 'know', 'know', 'know', 'know', 'know', 'know', 'interpreted', 'interpret', 'influence', 'imagine', 'however', 'help', 'hard', 'hard', 'hard', 'hard', 'hard', 'going', 'going', 'going', 'go', 'give', 'getting', 'generate', 'foundation', 'form', 'finally', 'figure', 'figure', 'feasible', 'far', 'factor', 'extent', 'explain', 'evaluation', 'especially', 'english', 'encode', 'easy', 'disambiguate', 'determines', 'determine', 'designed', 'decision', 'decide', 'connecting', 'concept', 'concept', 'chased', 'challenges', 'case', 'buy', 'attach', 'assume', 'assume', 'assume', 'aspects', 'analyze', 'although', 'algorithms', 'algorithms', 'able', 'ability', '.....', '.....', '."']
lecture 4
['deeper nlp requires common sense knowledge', 'typically would require human effort', 'somewhat different syntactic categories', 'one hundred percent correctly', 'get complete parsing correct', 'use supervised machine learning', 'deeper natural language analysis techniques', 'require much human effort', 'precise deep semantic analysis', 'use machine learning techniques', 'large scale text mining', 'deeper analysis techniques', 'use statistical nlp', 'general nlp tends', 'shallow nlp based', 'mine text data', 'analyzing text data', 'general statistical approaches', 'human effort', 'analyze text data', 'annotate text data', 'natural language', 'natural language', 'deeper understanding', 'first nlp', 'large scale', 'sense also', 'statistical analysis', 'text mining', 'text mining', 'deep understanding', 'use humans', 'shallow analysis', 'text data', 'text data', 'text data', 'general statistical', 'shallow techniques', 'generally based', 'various ways', 'take away', 'speech tagging', 'restaurant ".', 'parse depending', 'man saw', 'main points', 'limited domains', 'john owns', 'give us', 'statistical methods', 'useful techniques', 'generally combine', 'generally applicable', 'training examples', 'specific examples', 'bring humans', 'two kinds', 'sentence like', 'practical applications', 'main topic', 'important tasks', 'require', 'computers today', 'analysis', 'text', 'techniques', 'techniques', 'techniques', 'data', 'scale', 'shallow', 'general', 'general', 'analyze', 'generally', 'humans', 'examples', 'annotate', 'annotate', 'useful', 'two', 'topic', 'today', 'tasks', 'methods', 'like', 'computers', 'applications', 'also', 'also', 'working', 'well', 'versus', 'understand', 'understand', 'turned', 'turned', 'top', 'thus', 'telescope', 'task', 'summarized', 'summarize', 'state', 'since', 'robust', 'robust', 'rely', 'reason', 'precisely', 'precisely', 'practice', 'practically', 'part', 'obviously', 'needed', 'meaning', 'lot', 'lot', 'lot', 'loop', 'lecture', 'learn', 'inferences', 'highway', 'help', 'hard', 'going', 'foundation', 'follows', 'fix', 'feasible', 'far', 'fan', 'example', 'example', 'example', 'especially', 'easy', 'downside', 'done', 'difficult', 'difficult', 'difficult', 'define', 'cover', 'cover', 'course', 'course', 'course', 'context', 'category', 'boy', 'better', 'better', 'basis', 'basis', 'backbone', 'art', 'applied', 'applied', 'advantage', 'actually', 'actually', 'able', 'able', '"\'']
lecture 5
['sophisticated natural language processing techniques', 'would give us direct knowledge', 'original word sequence representation instead', 'text data analysis text mining', 'combine related words together', 'added yet another level', 'natural language processing', 'natural language processing', 'natural language processing', 'techniques would require', 'machine learning programs', 'actually give us', 'many different ways', 'many different ways', 'correcting grammar mistakes', 'would allow us', 'slightly less general', 'really analyze semantics', 'natural language text', 'shallow analysis based', 'even less robust', 'even inference rules', 'completely accurate representation', 'interesting analysis possibilities', 'representing text data', 'representing text data', 'representing text data', 'natural language sentence', 'even recognizing words', 'even syntactic analysis', 'representation less robust', 'might make mistakes', 'might make mistakes', 'might make mistakes', 'necessary deeper representation', 'also less robust', 'analyze text data', '... another level', 'identify words like', 'discuss text representation', 'natural language', 'allow us', 'special techniques', 'text mining', 'text mining', 'less accurate', 'word segmentation', 'word boundaries', 'inference rules', 'help us', 'make mistakes', 'representing text', 'representing text', 'representing text', 'shallow analysis', 'also related', 'representation would', 'text data', 'text data', 'text data', 'text data', 'text data', 'many applications', 'deeper analysis', 'deeper analysis', 'accurate analysis', 'syntactic structure', 'still necessary', 'another person', 'general way', 'general way', 'even harder', 'sentiment analysis', 'semantic analysis', 'semantic analysis', 'interesting analysis', 'interesting analysis', 'deep analysis', 'text representation', 'writing styles', 'whole collection', 'whole collection', 'speech tags', 'speech tags', 'speech tags', 'speech acts', 'semantic way', 'right types', 'right side', 'relation recognition', 'providing features', 'plus sign', 'picture shows', 'often needed', 'observer order', 'news articles', 'necessarily replace', 'little bit', 'little bit', 'knowledge graph', 'interesting opportunities', 'infer interesting', 'important role', 'human effort', 'human communication', 'frequent character', 'easily extract', 'derived facts', 'basic units', 'ascii symbols', 'arrow points', 'additional way', 'logical representation', 'knowledge representation', 'see text', 'represent text', 'represent text', 'english text', 'segmenting words', 'identifying words', 'frequent words', 'easily count', 'count easily', 'also tend', 'also say', 'words representation', 'interesting things', 'interesting things', 'generally see', 'identify words', 'frequent person', 'frequent nouns', 'form topics', 'recognize dog', 'recognize boy', 'useful representation', 'words opens', 'perhaps count', 'could go', 'always represent', 'also means', 'humans play', 'see mention', 'always use', 'always easy', 'computers may', 'would', 'example sequence', 'example sentence', 'mistakes', 'language', 'language', 'language', 'combine', 'data', 'data', 'make', 'text', 'text', 'text', 'text', 'even', 'even', 'analysis', 'analysis', 'robust', 'robust', 'yet', 'yet', 'might', 'might', 'might', 'knowledge', 'knowledge', 'knowledge', 'interesting', 'discuss', 'analyze', 'analyze', 'actually', '...', 'representation', 'representation', 'representation', 'representation', 'representation', 'representation', 'representation', 'representation', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'identify', 'identify', 'count', 'always', 'also', 'also', 'also', 'things', 'see', 'see', 'level', 'level', 'level', 'level', 'level', 'level', 'level', 'sequence', 'sequence', 'sequence', 'sequence', 'sequence', 'sequence', 'sequence', 'sequence', 'represent', 'represent', 'represent', 'person', 'person', 'perhaps', 'opens', 'nouns', 'mention', 'means', 'may', 'form', 'english', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'useful', 'useful', 'use', 'use', 'easy', 'easy', 'dog', 'dog', 'computers', 'computers', 'boy', 'boy', 'example', 'example', 'example', 'example', 'example', 'go', 'go', 'go', 'humans', 'humans', 'humans', 'humans', 'humans', 'work', 'whenever', 'verbs', 'using', 'used', 'used', 'unfortunately', 'unfortunately', 'unfortunately', 'try', 'true', 'trade', 'tolerate', 'time', 'time', 'thus', 'take', 'string', 'string', 'string', 'string', 'string', 'store', 'store', 'spaces', 'space', 'solving', 'since', 'sentences', 'sense', 'scenarios', 'saying', 'saying', 'result', 'request', 'represented', 'represented', 'rely', 'relations', 'relations', 'relations', 'reason', 'purpose', 'problems', 'predicates', 'powerful', 'positive', 'playground', 'playground', 'patterns', 'part', 'part', 'part', 'parsing', 'optimize', 'open', 'ok', 'obtain', 'obtain', 'obtain', 'note', 'negative', 'need', 'move', 'move', 'mind', 'mentioned', 'meant', 'meaning', 'makes', 'mainly', 'made', 'lot', 'lot', 'lot', 'loop', 'look', 'location', 'levels', 'let', 'lecture', 'lecture', 'languages', 'languages', 'knowing', 'kinds', 'kind', 'kind', 'kind', 'keep', 'interpreted', 'intention', 'intent', 'indicate', 'however', 'however', 'heard', 'guide', 'guide', 'google', 'going', 'going', 'generated', 'find', 'finally', 'extracted', 'extracted', 'etc', 'etc', 'etc', 'etc', 'errors', 'errors', 'entity', 'entities', 'entities', 'enriches', 'enables', 'effectively', 'document', 'desirable', 'course', 'course', 'correlation', 'consumed', 'computer', 'collaboration', 'closer', 'chinese', 'chasing', 'chasing', 'characters', 'characters', 'characters', 'characters', 'characters', 'characters', 'certainly', 'case', 'author', 'associated', 'approach', 'annotating', 'animal', 'add', 'add', 'add', 'actions', 'able', 'able', '1st']
lecture 6
['although knowing word boundaries might actually also help', 'might help us classify text objects', 'stylistic analysis generally requires syntactical representation', 'customer service people might want', 'would enable logic inference ofcourse', 'fragile natural language processing techniques', 'word based representation also powerful', 'business intelligence people might', 'using stream processing algorithms', 'covering techniques mainly based', 'real world energy entity', 'structure based feature features', 'different textual representation tends', 'important research topics today', 'gradually add additional representations', 'use graph mining algorithms', 'require much manual effort', 'analysis would enable applications', 'help biologists manage', 'syntactical structure representation', 'actually surprisingly powerful', 'also several advantages', 'word based representation', 'word based representation', 'word based representation', 'word based representation', 'also add ontology', 'also analysis capacities', 'add logic predicates', 'major topics covered', 'actually quite sufficient', 'word relation analysis', 'final column shows', 'deeper analysis results', 'word boundaries', 'major takeaway points', 'syntactic graph analysis', 'enable richer analysis', 'knowledge graph analysis', 'different categories corresponding', 'enable different analysis', 'people might', 'information network analysis', 'third column shows', 'first column shows', 'analyze syntactic graphs', 'text representation determines', 'intelligent knowledge assistant', 'partial structures extracted', 'adding syntactic structures', 'different authors want', 'understanding consumers opinions', 'interesting representation opportunities', 'represent text data', 'opinion related applications', 'text mining applications', 'natural language', 'support sophisticated applications', 'also use', 'discovering related words', 'biologists might', 'enabled analysis techniques', 'powerful analysis', 'manual effort', 'classify articles', 'mining algorithms', 'gradually add', 'different representations', 'also generate', 'actually written', 'actually track', 'generally need', 'real applications', 'different categories', 'would open', 'relation graphs', 'analysis techniques', 'text data', 'represent text', 'extracted information', 'support analysis', 'sentiment analysis', 'integrative analysis', 'syntactic structures', 'syntactic structures', 'syntactic structures', 'research problem', 'major complaints', 'important benefit', 'logical predicates', 'knowing', 'k authors', 'intelligent program', 'although', 'scientist want', 'logical representation', 'various applications', 'text recognition', 'string text', 'interesting applications', 'winning features', 'information extraction', 'topic analysis', 'enable lot', 'different levels', 'scattered knowledge', 'relevant knowledge', 'add entities', 'many applications', 'many applications', 'many applications', 'widely used', 'understanding functions', 'table summarizes', 'sophisticated approaches', 'second visualizes', 'scattered sources', 'researchers questioning', 'research literature', 'relevant facts', 'multiple ways', 'make inferences', 'make inferences', 'logical system', 'intended program', 'integrate everything', 'email messages', 'certain function', 'big advantage', 'basic units', 'apply directly', 'important level', 'representation accurate', 'techniques enabled', 'quite general', 'level representation', 'thesaurus discovery', 'necessarily need', 'meaning whether', 'competitors products', 'relatively robust', 'combined together', 'good example', 'would', 'analysis', 'enable', 'enable', 'using', 'require', 'much', 'mining', 'entity', 'want', 'want', 'representation', 'representation', 'representation', 'representation', 'representation', 'representation', 'representation', 'representation', 'text', 'text', 'text', 'applications', 'features', 'knowledge', 'structures', 'techniques', 'techniques', 'techniques', 'techniques', 'techniques', 'third', 'support', 'related', 'related', 'opinions', 'interesting', 'first', 'biologists', 'many', 'words', 'words', 'enabled', 'enabled', 'enabled', 'whether', 'topic', 'need', 'need', 'lot', 'levels', 'functions', 'discovery', 'approaches', 'accurate', 'products', 'products', 'literature', 'literature', 'entities', 'entities', 'robust', 'robust', 'robust', 'level', 'level', 'level', 'level', 'level', 'level', 'level', 'combined', 'combined', 'combined', 'combined', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'virtually', 'useful', 'top', 'thus', 'texting', 'take', 'summarize', 'strings', 'still', 'stick', 'sometimes', 'sometimes', 'semantics', 'seen', 'see', 'secondly', 'representing', 'rely', 'relations', 'recognize', 'read', 'processed', 'partly', 'particular', 'order', 'order', 'obviously', 'moving', 'means', 'makes', 'looking', 'look', 'look', 'let', 'know', 'know', 'know', 'kinds', 'kind', 'kind', 'kind', 'kind', 'kind', 'invented', 'interested', 'interested', 'hypothesis', 'humans', 'however', 'great', 'go', 'genes', 'genes', 'generality', 'gene', 'focuses', 'finally', 'finally', 'figure', 'figure', 'far', 'fact', 'extract', 'extent', 'explained', 'explained', 'examples', 'even', 'etc', 'effective', 'effective', 'done', 'customers', 'course', 'course', 'course', 'course', 'computer', 'compression', 'competing', 'communications', 'collection', 'classification', 'case', 'cannot', 'article', 'applied', 'application', 'application', 'application', 'application', 'application', 'applicable', 'answers', 'achieved', 'accurately', 'abundant']
lecture 7
['really help us predict whether text also occurs', 'sentence would generally help us predict', 'question would help us discover syntagmatic relations', 'syntagmatic relations would help us show', 'help us discover syntagmatic relations', 'whether meat also occurs indeed', 'many times two words occur together', 'knowing whether eats occurs', 'positive versus negative reviews', 'knowing whether eats occurred', 'introduce additional related words', 'often called query expansion', 'syntagmatic relation essentially captures', 'relatively low individual occurrences', 'two words around eats', 'whenever eats occurs', 'paradigmatically related words tend', 'become somewhat meaningless', 'suggest related queries', 'actually closely related', 'discover paradigmatic relation', 'words often either', 'similarity value would', 'feature word like', 'usually occur alone', 'user could navigate', 'many nlp tasks', 'larger expression based', 'learn syntagmatic relations', 'paradigmatic relation would', 'called syntagmatic relation', 'many different ways', 'use word associations', 'use word associations', 'really related', 'also helps explain', 'use related words', 'sentence would still', 'learn paradigmatic relations', 'two word relations', 'similar locations relative', 'discovering syntagmatic relation', 'cannot replace cat', 'capture basic relations', 'word association mining', 'mine word associations', 'discovering word associations', 'discovering paradigmatic relation', 'general replace one', 'words also tend', 'case clearly dog', 'taken away cat', 'would still', 'see eats occur', 'syntagmatic relations', 'syntagmatic relations', 'also even look', 'help', 'generally say', 'generally occur', 'high context similarity', 'call right context', 'syntagmatic relation', 'syntagmatic relation', 'syntagmatic relation', 'syntagmatic relation', 'syntagmatic relation', 'syntagmatic relation', 'syntagmatic relation', 'syntagmatic relation', 'paradigmatically relation', 'two relations', 'two relations', 'similar left context', 'sentences around', 'putting together', 'occur together', 'occur together', 'negative opinions', 'many applications', 'many applications', 'individual occurrences', 'word relations', 'computer would', 'dog would', 'taken away', 'taken away', 'describe relations', 'basically relations', 'word association', 'word associations', 'understanding positive', 'semantically related', 'see many', 'say also', 'related semantically', 'also occur', 'also done', 'paradigmatic relation', 'paradigmatic relation', 'paradigmatic relation', 'paradigmatic relation', 'paradigmatic relation', 'paradigmatic relation', 'paradigmatic relation', 'paradigmatic relation', 'paradigmatic relation', 'paradigmatic relation', 'two words', 'two words', 'two words', 'two words', 'two words', 'two words', 'two words', 'whether', 'mine associations', 'still get', 'quite basic', 'basic idea', 'paradigmatic relationship', 'verb eats', 'text retrieval', 'text retrieval', 'text data', 'text data', 'text data', 'similar locations', 'important question', 'world relations', 'one word', 'associations automatically', 'without affecting', 'topic map', 'take advantage', 'syntactic classes', 'simple sentences', 'occurring elements', 'noun phrase', 'iphone 6', 'information space', 'information space', 'improving accuracy', 'grammar learning', 'gonna assume', 'form classes', 'find information', 'extracted explicitly', 'discover', 'detailed opinions', 'data elements', 'corresponding sentences', 'component expressions', 'complex phrases', 'arbitrary sequences', 'also see', 'replace cat', 'words occur', 'words occur', 'words occur', 'words occur', 'assume words', 'right side', 'right side', 'extreme case', 'see eats', 'document even', 'different perspectives', 'syntactic class', 'semantic class', 'automatically construct', 'words tend', 'words tend', 'general problem', 'general ideas', 'general ideas', 'general ideas', 'valid sentence', 'valid sentence', 'valid sentence', 'would', 'summarize opinions', 'strongly associated', 'small sample', 'show', 'second kind', 'might improve', 'make sense', 'intuition behind', 'high co', 'following questions', 'correlated occurrences', 'co occurrences', 'another application', 'first case', 'sequence mining', 'sequence data', 'sit somewhere', 'natural language', 'left part', 'left context', 'left context', 'directly useful', 'convey meaning', 'context similarity', 'follow cat', 'similar context', 'similar context', 'phrases example', 'consider example', 'general context', 'general context', 'knowing', 'first look', 'first talk', 'see eat', 'see eat', 'see cat', 'predicting occurrence', 'called', 'relations', 'relations', 'relations', 'relations', 'related', 'related', 'low', 'essentially', 'also', 'also', 'also', 'also', 'also', 'two', 'associations', 'learn', 'eats', 'eats', 'eats', 'eats', 'eats', 'text', 'text', 'text', 'text', 'relation', 'relation', 'question', 'question', 'question', 'question', 'word', 'word', 'word', 'word', 'word', 'whenever', 'user', 'tasks', 'one', 'occurrences', 'occurrences', 'occur', 'occur', 'occur', 'occur', 'occur', 'occur', 'mining', 'mine', 'meat', 'meat', 'meat', 'high', 'explain', 'capture', 'call', 'based', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'discovering', 'discovering', 'discovering', 'discovering', 'similarity', 'similarity', 'similarity', 'left', 'left', 'right', 'right', 'case', 'case', 'query', 'query', 'query', 'even', 'even', 'different', 'different', 'tend', 'tend', 'tend', 'tend', 'tend', 'tend', 'similar', 'similar', 'similar', 'similar', 'similar', 'class', 'automatically', 'general', 'general', 'general', 'general', 'general', 'general', 'first', 'first', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'sentence', 'world', 'understanding', 'summarize', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'second', 'sample', 'predicting', 'part', 'might', 'make', 'intuition', 'following', 'correlated', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'co', 'co', 'associated', 'another', 'sequence', 'sequence', 'sequence', 'eat', 'eat', 'eat', 'look', 'look', 'look', 'look', 'look', 'look', 'useful', 'useful', 'talk', 'talk', 'sit', 'sit', 'meaning', 'meaning', 'language', 'language', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'cat', 'occurrence', 'occurrence', 'occurrence', 'computer', 'computer', 'computer', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'whereas', 'using', 'used', 'used', 'units', 'units', 'units', 'tuesday', 'thinking', 'think', 'think', 'think', 'think', 'techniques', 'synonyms', 'suggests', 'suggesting', 'substituted', 'structure', 'slide', 'similarly', 'similarly', 'similarly', 'search', 'rules', 'represent', 'product', 'probably', 'paragraph', 'outline', 'order', 'note', 'nodes', 'neighbors', 'need', 'monday', 'modify', 'methods', 'methods', 'means', 'means', 'means', 'means', 'makes', 'mainly', 'lot', 'looking', 'let', 'lecture', 'lecture', 'least', 'knowledge', 'knowledge', 'know', 'know', 'join', 'items', 'intuitively', 'intuitions', 'interested', 'interested', 'interested', 'increase', 'implemented', 'imagine', 'imagine', 'however', 'helpful', 'helpful', 'hand', 'gooing', 'going', 'going', 'going', 'going', 'going', 'go', 'go', 'generalized', 'generalized', 'fundamental', 'finally', 'finally', 'fact', 'explore', 'explore', 'else', 'effective', 'effective', 'edge', 'drive', 'drive', 'discovery', 'definition', 'definition', 'definitely', 'cover', 'cover', 'course', 'course', 'course', 'course', 'count', 'correlations', 'correlation', 'contrast', 'contrast', 'contrast', 'contexts', 'compute', 'complementary', 'compare', 'compare', 'combined', 'combined', 'combined', 'chance', 'car', 'car', 'browsing', 'bottom', 'battery', 'b', 'b', 'b', 'ask', 'ask', 'applied', 'animal', 'analysis', 'amp', 'amp', 'amp', 'able']
lecture 8
['gives us really semantically related words really give us', 'word based representation would actually give us interesting way', 'would give us loosely related paradigmatic relations', 'context may contain adjacent words like eats', 'might favor matching one frequent term', 'another word dog might give us', 'two probability distributions representing two contexts', 'dot product infact gives us', 'word association called paradigmatic relations', 'ate occurred three times etc', 'text retrieval techniques would realize', 'eats occured five times', 'ofcourse would allow us', 'flexibility also allows us', 'adjacent words like saturday', 'gives us one perspective', 'would give us', 'treats every word equally', 'discovering paradigmatically related words', 'see frequency vector representing', 'two randomly picked words', 'two randomly picked words', 'yi gives us', 'two words indeed occur', 'paradigmatic relations discovery', 'xi gives us', 'discovering paradigmatic relations', 'word like eats', 'word like eats', 'relatively low frequency', 'might indeed make', 'would actually pick', 'gives us', 'paradigmatic relation discovery', 'little bit questionable', 'defining one dimension', 'two contexts would', 'actually probability distribution', 'really surprising', 'two vectors actually', 'randomly picked word', 'context actually contains', 'word like dog', 'high dimensional space', 'another word like', 'two words picked', 'paradigmatically relation discovery', 'also assign weights', 'see words eats', 'naturally application specific', 'intuitively makes sense', 'allow us', 'words representation', 'eight words around', 'would work well', 'paradigmatic relations', 'vector space model', 'vector space model', 'vector space model', 'vector space model', 'paradigmatically related', 'similarity function actually', 'make sense right', 'one possible approach', 'one possible approach', 'formula little bit', 'two potential problems', 'see words like', 'pick another word', 'capture similarity based', 'dog based', 'much related', 'also different ways', 'would like', 'would like', 'contain cat', 'eowc expected overlap', 'share similar contexts', 'one term', 'dot product', 'dot product', 'seeing identical words', 'give', 'cat big cat', 'context based', 'word like', 'two words', 'one vector', 'two contexts', 'two contexts', 'two contexts', 'two contexts', 'two contexts', 'another perspective', 'text around', 'randomly pick', 'way', 'interesting', 'information retrieval', 'information retrieval', 'two questions', 'context left1 context', 'might want', 'might want', 'two vectors', 'two vectors', 'two vectors', 'one context', 'potential problems', 'one element', 'high value', 'different ways', 'another context', 'words immediately', 'remaining words', 'n words', '2 words', 'particular word', 'particular word', 'expected overlap', 'also find', 'also computed', 'also collect', 'work well', 'different contexts', 'word cat', 'word cat', 'word cat', 'word cat', 'word cat', 'query vector', 'document vector', 'document vector', 'award vector', 'another problem', 'cat etc', 'another case', 'capture words', 'see words', 'syntactical categories', 'strong evidence', 'strict criteria', 'similar positions', 'score higher', 'real data', 'pseudo document', 'pseudo document', 'normalized frequencies', 'normalized count', 'nice interpretation', 'n dimensions', 'modeling documents', 'intuitively prefer', 'imaginary document', 'higher score', 'exact formulas', 'distinct terms', 'different terms', 'different terms', 'corresponding elements', 'also see', 'word wi', 'know matching', 'also take', 'also analyze', 'similarity function', 'similarity function', 'occur right', 'particular kind', 'particular kind', 'occur frequently', 'viewing context', 'similar context', 'different context', 'context window8', 'context documents', 'would', 'would', 'vector d2', 'world cat', 'world cat', 'simply define', 'occurs everywhere', 'many approaches', 'different similarities', 'assess whether', 'similarity functions', 'main idea', 'general content', 'general combination', 'see overlap', 'identical pick', 'second problem', 'right1 context', 'context right1', 'overall sum', 'formally represent', 'used frequently', 'general context', 'term', 'picked', 'might', 'might', 'may', 'may', 'dog', 'ate', 'measuring similarity', 'define similarity', 'left context', 'similarity similarity', 'general idea', 'text', 'next lecture', 'discovering', 'exactly compute', 'model', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'first look', 'relation', 'perspective', 'also', 'etc', 'etc', 'contexts', 'contexts', 'contexts', 'vector', 'vector', 'vector', 'yi', 'similar', 'share', 'seeing', 'problems', 'occur', 'occur', 'naturally', 'intuitively', 'eowc', 'different', 'big', 'cat', 'cat', 'cat', 'cat', 'pick', 'pick', 'pick', 'matching', 'matching', 'matching', 'matching', 'matching', 'overlap', 'overlap', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'well', 'well', 'right', 'right', 'vectors', 'vectors', 'vectors', 'world', 'want', 'right1', 'left1', 'left1', 'kind', 'frequently', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'capture', 'capture', 'capture', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'approach', 'approach', 'approach', 'approach', 'approach', 'xi', 'xi', 'xi', 'wi', 'used', 'take', 'similarities', 'occurs', 'measuring', 'many', 'identical', 'identical', 'identical', 'identical', 'formula', 'formula', 'formula', 'exactly', 'element', 'define', 'define', 'assess', 'analyze', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'idea', 'idea', 'idea', 'general', 'general', 'general', 'general', 'general', 'general', 'problem', 'problem', 'problem', 'problem', 'next', 'next', 'lecture', 'lecture', 'd2', 'd2', 'sum', 'sum', 'sum', 'represent', 'represent', 'represent', 'left', 'left', 'left', 'first', 'first', 'first', 'first', 'case', 'case', 'case', 'case', 'look', 'look', 'look', 'look', 'look', 'compute', 'compute', 'compute', 'compute', 'compute', 'compute', 'compute', 'compute', 'window', 'window', 'whereas', 'weight', 'vocabulary', 'vocabulary', 'views', 'view', 'using', 'useful', 'useful', 'use', 'use', 'ultimately', 'tuesday', 'try', 'try', 'try', 'try', 'try', 'total', 'think', 'thing', 'test', 'talk', 'talk', 'taken', 'stay', 'specifically', 'sometimes', 'solve', 'small', 'similarly', 'similarly', 'shown', 'shared', 'sentences', 'semantics', 'search', 'saying', 'say', 'say', 'said', 'robust', 'rely', 'regarded', 'question', 'question', 'question', 'question', 'queries', 'purpose', 'products', 'placed', 'picking', 'note', 'non', 'namely', 'measure', 'measure', 'measure', 'measure', 'measure', 'means', 'means', 'match', 'match', 'match', 'lot', 'lot', 'lot', 'looking', 'likely', 'let', 'let', 'let', 'let', 'kinds', 'interpreted', 'interpreted', 'inside', 'illustrated', 'ignore', 'ideas', 'going', 'going', 'generally', 'focus', 'familiar', 'expect', 'example', 'example', 'example', 'even', 'equal', 'discover', 'developed', 'desirable', 'desirable', 'definition', 'defined', 'defined', 'd1', 'd1', 'd1', 'course', 'course', 'course', 'course', 'course', 'convert', 'convenient', 'contributes', 'confidence', 'computing', 'combine', 'check', 'chance', 'cases', 'carefully', 'call', 'call', 'call', 'call', 'call', 'bottom', 'basically', 'bag', 'bag', 'ask', 'answer', 'analytically', 'always', 'alright', 'addresses', 'address', 'address', 'adapt', '1']
lecture 9
['containing elements representing normalized bm 25 values', 'help us design effective similarity function', 'continue discussing paradigmatic relation discovery', 'weight would never exceed k', 'also another interesting transformation called', 'favors matching one frequent term', 'treats every word equally', 'method called expected overlap', 'generally non negetive number', 'highly similar word pairs', 'idf would give us', 'called idf term weighting', 'expected overlap account approach', 'using text retrieval models', 'bm 25 transformation', 'would help control', 'matching one frequent term', 'matching one frequent term', 'would give us', 'give us curve', 'paradigmatic relation mining', 'documents containing word', 'length formalization together', 'many common words like', 'discover paradigmatic relation', 'highly weighted terms', 'control length normalization', 'also makes sense', 'sub linear normalization', 'effective transformation function', 'high frequency counts', 'discover paradigmatic relations', 'non zero counts', 'highest weighted terms', 'apply idf weighting', 'introduced something else', 'bm 25', 'sub linear transformation', 'sub linear transformation', 'sub linear curve', 'would contribute equally', 'discovering paradigmatic relations', 'discovering paradigmatic relations', 'low value close', 'text retrieval techniques', 'inverse document frequency', 'inverse document frequency', 'discover syntagmatic relations', 'discover syntagmatic relations', 'really high counts', 'somewhat lower weights', 'little bit right', 'actually something similar', 'high frequency terms', 'high frequency terms', 'high frequency terms', 'original data set', 'share similar context', 'idf weighted vector', 'content word like', 'penalize popular terms', 'upper bound k', 'many different ways', 'bm25 retrieval model', 'average document length', 'frequently frequent term', 'matching one term', 'x /( x', 'two values', 'common word like', 'raw frequency count', 'lower weights depending', 'general would also', 'corresponding context documents', 'collected context documents', 'document frequency means', 'problems also occur', 'matching rare words', 'would likely see', 'approaches also represent', 'retrieval heuristics used', 'high counts', 'two candidate words', 'paradigmatic relations', 'paradigmatic relations', 'normalized count', 'frequent term', 'normalized weight', 'another parameter', 'normalization function', 'idf weighting', 'idf weighting', 'upper bound', 'syntagmatic relations', 'syntagmatic relations', 'linear transformation', 'linear transformation', 'linear transformation', 'similarity function', 'term frequency', 'frequent word', 'document length', 'transformation function', 'low idf', 'idf function', 'raw frequency', 'text retrieval', 'text retrieval', 'text retrieval', 'text retrieval', 'document frequency', 'document frequency', 'two relations', 'rare term', 'rare term', 'many ways', 'lower k', 'lower idfs', 'different ways', 'common terms', 'scoring function', 'scoring function', 'new function', 'logarithm function', 'terms would', 'rarest term', 'would still', 'would behave', 'idf stands', 'idf stands', 'term vector', 'term vector', 'term vector', 'data set', 'common word', 'looks like', 'looks like', 'also discuss', 'interesting transformations', 'generally form', 'reward matching', 'rare terms', 'lowest value', 'large value', 'higher value', 'pseudo document', 'document vectors', 'bm25 transformation', 'two extremes', 'two extremes', 'one possibility', 'terms based', 'distinct terms', 'rare word', 'rare word', 'document vector', 'document vector', 'yi probabilities', 'worth less', 'varying k', 'vary k', 'typically represented', 'total number', 'total number', 'strongly associated', 'strict constraint', 'stabilized interpretation', 'randomly pick', 'occurs everywhere', 'normalizing formula', 'may recall', 'main idea', 'k reaches', 'joint manner', 'higher weights', 'flat line', 'extra occurrence', 'assign weights', 'raw count', 'raw count', 'raw count', 'raw count', 'idf measure', 'also possible', 'vector d2', 'query vector', '01 vector', 'two problems', 'two problems', 'word occurs', 'word occurs', 'word distribution', 'particular word', 'candidate word', 'candidate word', 'candidate word', 'candidate word', 'average documents', 'tf transformation', 'one way', 'two heuristics', 'within zero', 'although zero', 'total count', 'word vector', 'simple way', 'reasonable way', 'new way', 'mapping function', 'also see', 'also see', 'two contexts', 'two contexts', 'general idea', 'commonly used', 'emphasizing matching', 'weight alone', 'maximum value', 'parameter k', 'zero count', 'context document', 'take logarithm', 'basically take', 'also use', 'also use', 'one extreme', 'taken earlier', 'old definition', 'occur frequently', 'next lecture', 'kinds controls', 'get penalized', "eats '.", 'clearly shows', 'context vectors', 'parameter b', 'parameter b', 'parameter b', 'problem set', 'context documents', 'effective', 'much emphasis', 'effectively solve', 'dot product', 'likely address', 'actually kind', 'second problem', 'second problem', 'whole collection', 'given collection', 'one case', 'possible words', 'introduce weight', 'take sum', 'relation', 'overlap', 'frequent', 'course measure', 'represent context', 'first problem', 'sum indicates', '1 parameter', 'similar', 'first look', 'discover', 'frequency', 'using', 'similarity', 'similarity', 'many', 'account', 'common', 'term', 'term', 'term', 'term', 'transformation', 'transformation', 'would', 'would', 'would', 'would', 'would', 'discovering', 'curve', 'idf', 'idf', 'like', 'like', 'also', 'also', 'also', 'interesting', 'interesting', 'matching', 'matching', 'matching', 'matching', 'matching', 'matching', 'retrieval', 'retrieval', 'retrieval', 'document', 'document', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'terms', 'terms', 'terms', 'terms', 'terms', 'weights', 'weights', 'ways', 'really', 'popular', 'occur', 'number', 'number', 'method', 'method', 'likely', 'k', 'k', 'k', 'k', 'introduced', 'depending', 'contribute', 'candidate', 'average', 'approaches', 'vector', 'vector', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'documents', 'documents', 'documents', 'documents', 'zero', 'zero', 'count', 'count', 'count', 'x', 'x', 'x', 'way', 'set', 'set', 'problems', 'problems', 'used', 'used', 'heuristics', 'heuristics', 'general', 'general', 'bm25', 'bm25', 'b', 'approach', 'approach', 'approach', 'actually', 'actually', 'weight', 'weight', 'weight', 'weight', 'weight', 'weight', 'second', 'possible', 'parameter', 'parameter', 'parameter', 'measure', 'means', 'means', 'contexts', 'take', 'take', 'problem', 'problem', 'problem', 'tf', 'see', 'see', 'see', 'see', 'see', 'right', 'right', 'right', 'represent', 'represent', 'represent', 'represent', 'penalized', 'maximum', 'lecture', 'kind', 'indicates', 'frequently', 'frequently', 'frequently', 'frequently', 'extreme', 'emphasizing', 'eats', 'earlier', 'definition', 'controls', 'clearly', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'sum', 'sum', 'sum', 'first', 'first', 'first', 'solve', 'solve', 'product', 'product', 'much', 'much', 'mapping', 'mapping', 'look', 'look', 'introduce', 'introduce', 'course', 'course', 'use', 'use', 'use', 'use', 'use', 'form', 'form', 'form', 'address', 'address', 'address', 'collection', 'collection', 'collection', 'collection', 'collection', 'collection', 'collection', '1', '1', '1', '1', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'xi', 'xi', 'whereas', 'well', 'well', 'want', 'w', 'useful', 'treat', 'transform', 'therefore', 'tend', 'talked', 'talked', 'talk', 'talk', 'talk', 'talk', 'talk', 'talk', 'taking', 'surprising', 'summarize', 'state', 'specifically', 'specifically', 'specifically', 'sorry', 'simulates', 'simulate', 'shown', 'shown', 'shown', 'show', 'seeing', 'search', 'say', 'say', 'say', 'saw', 'roughly', 'rewards', 'represents', 'representation', 'reflects', 'reflects', 'reason', 'puts', 'put', 'put', 'purpose', 'probability', 'probability', 'prevents', 'picking', 'normalize', 'naturally', 'namely', 'naive', 'modification', 'might', 'meaning', 'match', 'match', 'match', 'mapped', 'mapped', 'mapped', 'map', 'lot', 'lot', 'lot', 'log', 'leveraging', 'let', 'let', 'let', 'let', 'lengths', 'lengths', 'key', 'kept', 'interpreted', 'interestingly', 'instead', 'influence', 'indeed', 'improve', 'important', 'importance', 'implement', 'ignored', 'identical', 'however', 'heuristic', 'going', 'going', 'going', 'going', 'going', 'go', 'giving', 'frequencies', 'finally', 'far', 'fact', 'extent', 'exercise', 'example', 'example', 'even', 'ensure', 'end', 'emphasize', 'emphasize', 'effect', 'dominating', 'dominate', 'discussed', 'discovered', 'discovered', 'difference', 'detail', 'detail', 'denoted', 'defined', 'defined', 'defined', 'define', 'define', 'define', 'created', 'correlated', 'convert', 'contain', 'construct', 'constant', 'constant', 'constant', 'constant', 'considering', 'computed', 'compute', 'compute', 'compute', 'compare', 'collect', 'collect', 'choose', 'cat', 'brings', 'bi', 'bi', 'belief', 'bag', 'axis', 'assumed', 'associations', 'art', 'answers', 'amp', 'allow', 'affecting', 'affecting', 'advantage', 'added', 'add', 'adapt', 'achieve', 'able', '0', '0', '.....']
lecture 10
['random variable like x sub w', 'still predicted whether w', 'random variable x sub coin', 'function form looks like', 'head hotel equally likely', 'completely biased coin corresponds', 'function would show exactly', 'assume high entropy words', 'x sub w', 'question whenever eats occurs', 'equation looks like', 'equally likely taking', 'syntagmatic relation discovery', 'correlated co occurrences', 'random experiment like', 'completed biased coin', 'would generally find', 'function looks like', 'completely biased coin', 'completely biased coin', 'completely biased coin', 'completely biased coin', 'syntagmatic relations hold', 'discover syntagmatic relations', 'fair coin corresponds', 'clearly would expect', 'random variable inside', 'random variable equals', 'binary random variable', 'binary random variable', 'would force us', 'word association mining', 'another extreme case', 'see one word occurs', 'using coin tossing', 'random variable taking', 'coin always shows', 'measure called entropy', 'two probabilities would', 'also relatively easy', 'one quantitatively measure', 'might occur together', 'precisely one word', 'entropy reached maximum', 'three words shown', 'two values zero', 'particular word present', 'words also tend', 'world w', 'w denotes', 'random variable', 'random variable', 'random variable', 'random variable', 'random variable', 'random variable', 'random variable', 'random variable', 'random variable', 'random variable', 'completely fair', 'eats like', 'show eats', 'high entropy', 'coin shows', 'coin shows', 'coin shows', 'fair coin', 'fair coin', 'fair coin', 'occurs everywhere', 'prediction would', 'coin tossing', 'clearly depends', 'completely symmetric', 'two probabilities', 'two ends', 'two cases', 'two cases', 'tell us', 'gives us', 'always takes', 'measure introduced', 'function defined', 'three words', 'three words', 'entropy function', 'entropy function', 'entropy function', 'special cases', 'quantitative way', 'multiple values', 'might conclude', 'may also', 'mathematically proved', 'explicitly plugged', 'end point', 'easily generalized', 'discussed earlier', 'different distributions', 'continue talking', 'word prediction', 'always say', 'use entropy', 'higher entropy', 'entropy indicates', 'specific example', 'simple example', 'formally stated', 'formally defined', 'second one', 'either present', 'occur everywhere', 'possible values', 'negative sign', 'middle point', 'intuitively makes', 'information theory', 'right side', 'words occur', 'one case', 'prediction problem', 'entropy reaches', 'actually easier', 'lower value', 'different value', 'particular think', 'could think', 'interesting question', 'text segment', 'like', 'predict whereas', '2 right', 'problem right', 'occurs', 'would', 'random', 'coin', 'coin', 'variable', 'variable', '0 log', 'x', 'x', 'x', 'x', 'x', 'taking', 'relations', 'measure', 'function', 'function', 'function', 'values', 'tossing', 'shown', 'probabilities', 'particular', 'might', 'easy', 'also', 'also', 'word', 'word', 'word', 'word', 'word', 'word', 'words', 'words', 'words', 'words', 'words', 'words', 'entropy', 'entropy', 'entropy', 'entropy', 'entropy', 'entropy', 'entropy', 'eats', 'eats', 'eats', 'case', 'case', 'tend', 'tend', 'question', 'question', 'question', 'question', 'maximum', 'maximum', 'formally', 'example', 'present', 'present', 'present', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'occur', 'occur', 'occur', 'occur', 'symmetric', 'say', 'reaches', 'possible', 'negative', 'middle', 'makes', 'interesting', 'information', 'head', 'head', 'head', 'head', 'head', '2', 'right', 'right', 'right', 'right', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'zero', 'zero', 'zero', 'zero', 'zero', 'problem', 'problem', 'problem', 'problem', 'log', 'log', 'easier', 'easier', 'easier', 'value', 'value', 'value', 'value', 'value', 'value', 'value', 'value', 'think', 'think', 'think', 'think', 'think', 'think', 'think', 'think', 'think', 'segment', 'segment', 'segment', 'segment', 'segment', 'segment', 'predict', 'predict', 'predict', 'predict', 'predict', 'predict', 'predict', 'predict', '0', '0', '0', '0', '0', '0', '0', '0', '0', 'unicorn', 'unicorn', 'unicorn', 'unicorn', 'unicorn', 'undefined', 'uncertain', 'treat', 'time', 'thinking', 'therefore', 'terms', 'tends', 'talk', 'take', 'take', 'take', 'take', 'tail', 'tail', 'sum', 'sum', 'sum', 'sum', 'start', 'start', 'specifically', 'somewhere', 'sometimes', 'small', 'sentence', 'sentence', 'sentence', 'sentence', 'sense', 'semtence', 'scope', 'result', 'represent', 'rare', 'rare', 'randomness', 'randomness', 'randomness', 'quantify', 'purpose', 'product', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'predicting', 'plug', 'plotted', 'plot', 'particularly', 'paragraph', 'outcome', 'occurrence', 'note', 'need', 'naturally', 'moment', 'minimum', 'meet', 'meat', 'meat', 'meat', 'measures', 'means', 'means', 'means', 'means', 'lot', 'looking', 'look', 'look', 'look', 'look', 'look', 'let', 'let', 'let', 'let', 'left', 'left', 'lecture', 'lecture', 'large', 'kind', 'intuition', 'introduction', 'imagine', 'harder', 'harder', 'hard', 'going', 'going', 'give', 'general', 'general', 'general', 'frequency', 'follows', 'follows', 'follows', 'fish', 'fact', 'expand', 'equal', 'equal', 'entropies', 'entropies', 'entropies', 'dog', 'document', 'discovering', 'difficult', 'difficult', 'difficult', 'designing', 'denoted', 'definition', 'definition', 'define', 'curve', 'curve', 'course', 'context', 'context', 'connection', 'compute', 'close', 'clear', 'choice', 'cat', 'beyond', 'bet', 'basis', 'associated', 'associated', 'associated', 'asked', 'ask', 'ask', 'although', 'alright', 'accurately', 'absolutely', 'absent', 'absent', 'absent', 'absent', '5', '5', '1', '1', '1', '1', '1', '1', '1', '1', '1']
lecture 11
['eats would help us predict wether meat occurs', 'conditional entropy gives us directly one way', 'really tell us much', 'strongest k syntagmatic relations', 'really help us reduce', 'putting different scenarios together', 'discrete random variables x', 'help us reduce entropy', 'also help us predict', 'entropy conditional entropy would reach', 'comparable across different words', 'target word like w1', 'word like meat occurs', 'eats could reduce uncertainty', 'w1 given different words', 'always help us', 'minimum possible value would', 'conditional entropy would capture', 'help us predict', 'know whether meat occurs', 'smaller entropy means easier', 'help us mine', 'would allow us', 'potential syntagmatic relations', 'mining syntagmatic relations', 'discovering syntagmatic relations', 'discovering syntagmatic relations', 'capturing syntagmatic relations', 'helps us predict', 'helped us predict', 'syntagmatic relation discovery', 'random variable corresponding', 'intuitively makes sense', 'capture syntagmatic relations', 'target word w1', 'different upper bounds', 'different upper bounds', 'another random variable', 'strongly correlated words', 'gives us', 'known eats occurring', 'conditional entropy reaches', 'conditional entropy conditioned', 'cannot really compare', 'w1 given w2', 'w1 given w2', 'word association mining', 'minimum possible value', 'probabilities indicating whether', 'one word given', 'corresponding conditional probabilities', 'use conditional entropy', 'use conditional entropy', 'use mutual information', 'entropy conditional entropy', 'know eats occured', 'know eats occured', 'word like meat', 'particular word w1', 'w1 given w3', 'word occurs', 'special case listed', 'algorithm would look', 'knowing whether knowing', 'meat occurs', 'tells us', 'tells us', 'word given', 'upper bounds', 'words would', 'predict whether', 'word w1', 'variable x', 'predict w1', 'another word', 'would mean', 'would change', 'different scenario', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'words w2', 'reduce uncertainty', 'looks like', 'really related', 'whether eats', 'conditional entropies', 'conditional entropies', 'conditional entropies', 'know whether', 'know whether', 'upper bound', 'share w2', 'cannot hurt', 'conditional probabilities', 'conditional probabilities', 'smaller entropy', 'small entropy', 'original entropy', 'entropy function', 'entropy function', 'entropy function', 'two words', 'candidate words', 'candidate words', 'denotes eats', 'know eats', 'know eats', 'knowing eats', 'way', 'two pairs', 'top ranked', 'top candidates', 'stronger association', 'second term', 'entire collection', 'complete definition', 'also define', 'also ask', 'absolute value', 'using entropy', 'using entropy', 'know something', 'know nothing', 'also know', 'already know', 'another concept', 'eats zero', 'particular scenario', 'following equation', 'first look', 'fairly close', 'ascending order', 'first case', 'see eats', 'maximum value', 'comparable right', 'text segment', 'reduce', 'knowing presence', 'would', 'would', 'would', 'would', 'like', 'given', 'capture', 'word', 'word', 'w1', 'w1', 'w1', 'use', 'scenarios', 'predict', 'predict', 'predict', 'predict', 'predict', 'x', 'reach', 'reach', 'entropy', 'entropy', 'entropy', 'entropy', 'entropy', 'entropy', 'entropy', 'entropy', 'entropy', 'words', 'words', 'eats', 'eats', 'eats', 'eats', 'eats', 'eats', 'eats', 'eats', 'one', 'one', 'one', 'one', 'one', 'value', 'value', 'value', 'smaller', 'smaller', 'particular', 'minimum', 'minimum', 'minimum', 'mine', 'listed', 'compare', 'comparable', 'comparable', 'association', 'also', 'also', 'algorithm', 'know', 'know', 'know', 'know', 'know', 'probabilities', 'probabilities', 'probabilities', 'knowing', 'knowing', 'knowing', 'uncertainty', 'uncertainty', 'uncertainty', 'means', 'means', 'means', 'meat', 'meat', 'meat', 'meat', 'meat', 'meat', 'meat', 'meat', 'meat', 'meat', 'meat', 'meat', 'meat', 'meat', 'meat', 'meat', 'meat', 'w3', 'w3', 'using', 'information', 'information', 'zero', 'scenario', 'scenario', 'related', 'order', 'look', 'look', 'look', 'look', 'equation', 'concept', 'close', 'case', 'case', 'case', 'case', 'case', 'case', 'right', 'right', 'maximum', 'maximum', 'see', 'see', 'see', 'see', 'segment', 'segment', 'segment', 'segment', 'segment', 'segment', 'segment', 'presence', 'presence', 'presence', 'presence', 'presence', 'presence', 'presence', 'presence', 'presence', 'presence', 'whereas', 'well', 'well', 'want', 'want', 'w', 'w', 'useful', 'useful', 'uncertain', 'trying', 'trying', 'try', 'threshold', 'threshold', 'thought', 'think', 'think', 'think', 'think', 'think', 'think', 'think', 'think', 'talked', 'talk', 'take', 'take', 'suppose', 'suppose', 'suggests', 'stays', 'solve', 'slide', 'situation', 'sentence', 'sentence', 'seen', 'result', 'replace', 'questions', 'question', 'question', 'question', 'problem', 'problem', 'probability', 'present', 'present', 'prediction', 'prediction', 'prediction', 'prediction', 'precisely', 'overall', 'occurrence', 'occur', 'occur', 'number', 'note', 'note', 'need', 'need', 'namely', 'middle', 'meet', 'measure', 'measure', 'meaning', 'match', 'make', 'let', 'let', 'let', 'lecture', 'lecture', 'later', 'larger', 'kind', 'involvement', 'intuition', 'interesting', 'interesting', 'interested', 'increase', 'hope', 'hope', 'higher', 'hard', 'going', 'going', 'going', 'going', 'get', 'general', 'frame', 'follows', 'follows', 'find', 'favor', 'extent', 'explain', 'expect', 'expand', 'except', 'example', 'essentially', 'equal', 'enumerate', 'ensure', 'easy', 'earlier', 'discussion', 'discuss', 'course', 'course', 'course', 'course', 'continue', 'context', 'content', 'consider', 'condition', 'condition', 'condition', 'conclusion', 'compute', 'comperable', 'cases', 'called', 'basically', 'basically', 'basically', 'assume', 'anymore', 'answer', 'analysis', 'amp', 'amp', 'addressed', 'address', 'address', 'actually', 'achieve', 'absent', 'absence', 'absence', 'absence', 'absence', 'absence', 'absence', 'absence', 'absence', 'able', '1', '0', '0', '0', '0']
lecture 12
['rank different ys using conditional entropy would give', 'continue discussing syntagmatic relation discovery', 'discover strong syntagmatic relations globally', 'comparison would tell us whether', 'conditional entropy would never exceed', 'knowing one would allow us', 'really help us predict eats', 'possibly reduced conditional entropy', 'always help us potentially', 'comparable across different pairs', 'eats cause major information', 'first 2 probabilities corresponding', 'two conditional entropies h', 'mutual information allows us', 'syntagmatic relation discovery', 'discover syntagmatic relations', 'tell us anything', 'equations allow us', 'called mutual information denoted', 'syntagmatic relation mining', 'two probabilities representing presence', 'syntactic relation mining', 'help us predicting', 'two words occur together', 'compare different pairs', 'mutual information would', 'already saw two', '2 joint distributions', 'conditional entropy computed', 'two variables taking', 'four different scenarios', 'two scenarios depending', 'four possible scenarios', 'callback labeler divergance', 'two random variables', 'two random variables', 'two random variables', 'two random variables', 'two random variables', 'two random variables', 'introduce mutual information', 'high mutual information', 'computing mutual information', 'called mutual information', 'actual joint distribution', 'equations also follow', 'also non negative', 'using mutual information', 'additional constraints listed', 'joint actual observed', 'first word occurred', 'signal word actually', 'whenever eats occurs', 'mutual information reaches', 'mutual information measures', 'lower mutual information', 'expected joint distribution', 'weather second word', 'allows us', 'mutual information larger', 'higher mutual information', 'really comparable', 'already known right', 'mutual information question', 'two values 0', 'means knowing one', 'two probabilities shown', 'different pairs', 'first word occurs', 'would mean', 'would expect', 'similarly knowing eats', 'hurt us', 'words also tend', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'conditional entropy', 'random variables', 'two variables', 'two variables', 'two distributions', 'different values', 'two scenarios', 'called kl', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'joint distribution', 'occur together', 'different form', 'random variable', 'two cases', 'two probabilities', 'joint probabilities', 'give', 'constraints among', 'mutual info', 'information theory', 'information theory', 'information theory', 'means knowing', 'original entropy', 'original entropy', 'original entropy', 'original entropy', 'two words', 'two words', 'two words', 'four combinations', 'one means', 'joined distribution', 'expected distribution', 'second property', 'first word', 'first word', 'first word', 'first scenario', 'understand becausw', 'third property', 'strongly associated', 'last property', 'knowing one', 'interesting properties', 'independence assumption', 'helps measure', 'another term', 'another concept', 'another concept', '), measures', 'predicting x', 'choose one', 'three probabilities', 'probabilities involved', 'probabilities involved', 'probabilities involved', 'marginal probabilities', 'would', 'ranking based', 'ranking based', 'ranking based', 'second word', 'second word', 'second word', 'second word', 'second word', 'second scenario', 'also known', 'also interesting', 'entropy reduction', 'reaches 0', 'probabilities based', 'probabilities based', 'whereas words', 'seond word', 'another word', 'previous slide', 'many combinations', 'co occurrence', 'sum mainly', 'actually compute', 'indeed independent', 'indeed independent', 'knowing x', 'take care', 'simply looking', 'minimum zero', 'mathematically write', 'bottom part', 'actually equal', 'form shown', 'x obtained', 'x given', 'x given', 'x given', 'fix x', 'much reduction', 'also sum', 'might see', 'essentially see', 'easily see', 'also observed', 'also observed', 'easily calculate', 'available probability', 'words sum', 'help', 'easily compute', 'completely independent', 'completely independent', 'slide shows', 'co occurrences', 'different', 'relations', 'predict', 'comparison', 'probability captures', 'using', 'using', 'using', 'get exactly', 'case right', 'two', 'absence probability', 'variables', 'denoted', 'always', 'information', 'scenarios', 'entropy', 'entropy', 'entropy', 'entropy', 'first', 'distribution', 'knowing', 'knowing', 'knowing', 'knowing', 'values', 'tend', 'reaches', 'occurs', 'occurs', 'measures', 'lower', 'listed', 'known', 'h', 'h', 'expected', 'eats', 'eats', 'eats', 'eats', 'eats', 'eats', 'eats', 'divergance', 'one', 'one', 'one', 'one', 'one', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'based', 'occur', 'occur', 'occur', 'also', 'also', 'also', 'also', 'also', 'also', 'shown', 'shown', '0', '0', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'slide', 'similarly', 'similarly', 'scenario', 'larger', 'larger', 'higher', 'higher', 'form', 'completely', 'combinations', 'co', 'sum', 'sum', 'observed', 'observed', 'observed', 'observed', 'independent', 'independent', 'independent', 'zero', 'take', 'simply', 'shows', 'right', 'right', 'right', 'right', 'question', 'question', 'question', 'presence', 'presence', 'presence', 'presence', 'presence', 'occurrences', 'mathematically', 'get', 'captures', 'bottom', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'reduction', 'reduction', 'reduction', 'see', 'see', 'see', 'see', 'see', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'calculate', 'calculate', 'exactly', 'exactly', 'exactly', 'compute', 'compute', 'compute', 'compute', 'compute', 'compute', 'case', 'case', 'case', 'absence', 'absence', 'absence', 'equal', 'equal', 'equal', 'equal', 'equal', 'well', 'well', 'w1', 'verify', 'verified', 'value', 'value', 'useful', 'used', 'use', 'use', 'think', 'therefore', 'talked', 'talk', 'symmetrical', 'symmetric', 'symmetric', 'sums', 'sums', 'summing', 'specifically', 'specifically', 'specifically', 'simplify', 'sense', 'segment', 'segment', 'seen', 'seen', 'says', 'rest', 'represented', 'related', 'reasoning', 'quantity', 'product', 'problem', 'present', 'present', 'picking', 'particular', 'particular', 'order', 'order', 'often', 'obtain', 'observe', 'observe', 'numerator', 'numerator', 'note', 'normally', 'normalize', 'none', 'need', 'namely', 'meats', 'meats', 'maximum', 'maximum', 'makes', 'make', 'lot', 'look', 'look', 'look', 'look', 'let', 'let', 'let', 'let', 'let', 'lecture', 'lecture', 'largest', 'know', 'know', 'know', 'know', 'know', 'know', 'join', 'intuition', 'intuition', 'interpreted', 'interestingly', 'interested', 'inside', 'hard', 'going', 'going', 'going', 'going', 'going', 'going', 'general', 'general', 'function', 'function', 'framed', 'formula', 'formula', 'formula', 'fixed', 'fixed', 'finally', 'finally', 'finally', 'example', 'example', 'example', 'examine', 'equation', 'equation', 'equation', 'equation', 'equation', 'either', 'either', 'easy', 'easy', 'easy', 'easy', 'easy', 'easier', 'diversions', 'divergences', 'divergence', 'difference', 'denominator', 'denominator', 'defined', 'defined', 'corpus', 'constraint', 'consideration', 'computes', 'computation', 'computation', 'calculation', 'calculated', 'calculated', 'boxes', 'boxes', 'basically', 'basically', 'assume', 'association', 'ask', 'amp', 'amp', 'add', 'absent', '1', '1', '1', '1', '1']
lecture 13
['select medical relations using random walks', 'pseudo segments would contribute additional counts', 'random variable x conditional entropy', 'would give us another way', 'would give us different kinds', 'introduced multiple statistical approaches', 'using various statistical measures', 'provides yet another way', 'syntactic medical relation discovery', 'least non zero counter', 'counting co occurrences using', 'seen like medical relations', 'also use different ways', 'one single pseudo segment', 'two words occur together', 'would give us', 'would give us', 'actually discover interesting relations', 'using different contexts', 'using bm 25', 'contain world w one', 'non composition compositional', 'pure statistical approaches', 'strongest cinematical relationship', 'helmet human effort', 'commonly used technique', 'discover different flavors', 'discover lexical atoms', 'maximum likelihood estimate', 'discover syntagmatic relations', 'discover syntagmatic relations', 'four different combinations', 'word association mining', 'four pseudo segments', 'paradigmatically relation discovery', 'two basic associations', 'say mutual information', 'zero counts sometimes', 'two pseudo segments', 'syntagmatic relation discovery', 'potentially also suggest', 'one word occurs', 'least one count', 'discovering syntagmatic relations', 'longer text article', 'technique called smoothing', 'give us', 'text data mining', 'zero probability probability', 'use mutual information', 'compute mutual information', 'example hot dog', 'syntagmatic relations', 'would lead', 'would believe', 'world graphs', 'unified way', 'principled way', 'interesting variations', 'best way', 'pseudo segments', 'pseudo segments', 'relation discovery', 'different pairs', 'allows us', 'pseudo segment', 'discovery associations', 'mutual information', 'mutual information', 'mutual information', 'mutual information', 'zero probability', 'zero probability', 'second counter', 'w one', 'third one', 'paradigmatic relations', 'potentially possible', 'actually one', 'two words', 'word like', 'information theory', 'information retrieval', 'three counts', 'three counts', 'text window', 'empirical counts', 'entropy reduction', 'entropy reduction', 'two ones', 'cinematic relation', 'word occurring', 'word 2', 'whole part', 'values computer', 'three concepts', 'three concepts', 'third account', 'term waiting', 'small sample', 'small constant', 'right side', 'right side', 'recommended readings', 'quite relevant', 'mostly becausw', 'meshes uncertainly', 'measuring correlations', 'maybe paragraphs', 'first take', 'computer similarity', 'candidate word', 'already discussed', 'achieve waiting', '05 comes', 'words occur', 'words occur', 'contain w2', 'total number', 'total number', 'total number', 'total number', 'total number', 'actually comes', 'actual segments', 'weighted 1', 'weighted 1', 'weighted 1', 'estimate probabilities', 'simply normalize', 'simply normalize', 'data maybe', 'words occurred', 'words around', 'also represent', 'small problem', 'simply need', 'select', 'new paper', 'knowing eggs', 'called paradigmatic', 'basically concludes', 'observed accounts', 'many ones', 'still want', 'fairly general', 'context similarity', 'words based', 'relations', 'observed data', 'kinds', 'blue chip', 'second column', 'define context', 'approaches', 'one', 'discover', 'discover', 'zero', 'yet', 'measures', 'counting', 'like', 'contain', 'text', 'occur', 'counts', 'counts', 'counts', 'probability', 'entropy', 'entropy', 'also', 'also', 'relation', 'relation', 'relation', 'x', 'x', 'x', 'word', 'word', 'w', 'w', 'use', 'use', 'use', 'smoothing', 'seen', 'seen', 'occurrences', 'occurrences', 'hot', 'example', 'dog', 'combinations', 'called', 'associations', 'article', 'total', 'actually', 'actually', 'actually', 'segments', 'segments', 'segments', 'segments', 'segments', 'segments', 'segments', 'segments', 'segments', '1', 'segment', 'segment', 'segment', 'segment', 'second', 'normalize', 'data', 'data', 'data', 'compute', 'compute', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'w2', 'problem', 'paradigmatic', 'paradigmatic', 'paper', 'pairs', 'need', 'knowing', 'define', 'chip', 'blue', 'basically', 'based', 'allows', 'ones', 'ones', 'ones', 'observed', 'observed', 'observed', 'discovering', 'discovering', 'discovering', 'discovering', 'count', 'count', 'count', 'count', 'want', 'want', 'represent', 'represent', 'general', 'general', 'context', 'context', 'context', 'context', 'context', 'probabilities', 'probabilities', 'probabilities', 'column', 'column', 'column', 'zeros', 'worse', 'well', 'well', 'weights', 'visible', 'useful', 'useful', 'units', 'understand', 'topic', 'topic', 'top', 'time', 'time', 'terms', 'techniques', 'support', 'summarize', 'summarize', 'sum', 'spend', 'specifically', 'slide', 'similarly', 'similarly', 'showing', 'sentence', 'see', 'see', 'see', 'see', 'see', 'really', 'rank', 'pretend', 'possibility', 'point', 'phrases', 'phrases', 'phrases', 'perform', 'particular', 'particular', 'particular', 'ok', 'observe', 'note', 'none', 'narrow', 'n', 'matches', 'look', 'locations', 'listed', 'let', 'let', 'lectures', 'language', 'know', 'know', 'join', 'items', 'introduce', 'introduce', 'introduce', 'indicator', 'indicates', 'imagine', 'illustrated', 'hypothesizes', 'going', 'given', 'get', 'get', 'generally', 'follows', 'five', 'explain', 'events', 'event', 'event', 'estimating', 'estimating', 'entities', 'due', 'documents', 'discussion', 'discussion', 'discovered', 'discounts', 'detail', 'denominator', 'denominator', 'defined', 'course', 'course', 'comfortable', 'combined', 'combination', 'columns', 'columns', 'collection', 'collect', 'cluster', 'chapter', 'cat', 'cases', 'cases', 'case', 'case', 'case', 'candidates', 'book', 'award', 'available', 'arrow', 'applied', 'applied', 'applications', 'applications', 'applications', 'analysis', 'amp', 'address', 'added', 'added', 'added', 'add', 'add', '5', '4th', '4', '4', '4', '1st', '1st']
lecture 14
['covering topic theta sub j', 'help us analyze patterns', 'generally two different tasks', 'product like iphone 6', 'would involve first discovering', 'also covers topic k', 'five years ago', '2012 presidential election', 'tells us something', 'topic generally provide', 'language namely discovery', 'paradigmatic relations relations', 'mining another kind', 'theta sub k', 'also negative reviews', 'generally also need', 'etc ., right', 'twitter users talking', 'theater sub one', 'main idea discussed', 'case k topics', 'also would like', 'major topics debated', 'involves discovering topics', 'define theta sub', 'current research topics', 'n text documents', 'cover topic 1', 'covered topic 2', 'discover k topics', 'covered mining knowledge', 'covered mining knowledge', 'iphone 6', 'would like', 'exactly theta', 'involves discovery', 'discovering topics', 'topic k', 'define theta', 'syntagmatic relations', 'topic 1', 'π sub', 'π sub', 'often need', 'document covers', 'output would', 'topic 2', 'research articles', 'require discovery', 'topics k', 'k topics', 'k topics', 'topic mining', 'topic mining', 'topic mining', 'mining task', 'data mining', 'data mining', 'content mining', 'people like', 'major topics', 'main topics', 'different ways', 'different type', 'different granularities', 'different granularities', 'different applications', 'view topic', 'trending topic', 'article topic', 'research topics', 'word associations', 'values indicate', 'task definition', 'task definition', 'small portion', 'second task', 'roughly speaking', 'probabilities sum', 'nba sports', 'meta data', 'many applications', 'international events', 'fading away', 'documents cover', 'digital library', 'completely defined', 'automatically suggest', 'mining topics', 'discover whether', 'discover knowledge', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text connection', 'text article', 'different locations', 'different locations', 'document 2', 'one might', 'topics overtime', 'topics outside', 'context variables', 'context variables', 'discover topics', 'text etc', 'time associated', 'similar looking', 'positive opinions', 'document one', 'topics might', 'might know', 'might know', 'discover denoted', 'also interested', 'useful techniques', 'topics perhaps', 'topics obviously', 'formally define', 'might see', 'tasks', 'sub', 'sub', 'product', 'analyze', 'would', 'j', 'j', 'different', 'also', 'also', 'also', 'also', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'something', 'one', 'knowledge', 'knowledge', 'documents', 'discussed', 'cover', 'discover', 'discover', 'covered', 'covered', 'covered', 'covered', 'text', 'text', 'text', 'text', 'first', 'first', 'first', 'etc', 'etc', 'might', 'might', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'talking', 'talking', 'locations', 'know', 'define', 'define', 'define', 'context', 'perhaps', 'people', 'opinions', 'obviously', 'looking', 'formally', 'denoted', 'associated', 'document', 'document', 'document', 'document', 'see', 'see', 'techniques', 'techniques', 'techniques', 'interested', 'interested', 'interested', 'interested', 'world', 'world', 'world', 'well', 'want', 'want', 'use', 'understand', 'trying', 'today', 'today', 'think', 'think', 'theme', 'texts', 'talk', 'talk', 'talk', 'talk', 'talk', 'talk', 'talk', 'subtasks', 'subject', 'starting', 'specify', 'sources', 'set', 'sentence', 'roadmap', 'question', 'produced', 'problem', 'problem', 'probability', 'person', 'past', 'paragraph', 'ok', 'number', 'number', 'number', 'number', 'non', 'next', 'motivation', 'motivation', 'may', 'make', 'lot', 'lot', 'lot', 'look', 'look', 'literatures', 'literature', 'let', 'let', 'lectures', 'lecture', 'lecture', 'lecture', 'lecture', 'knowing', 'knowing', 'knowing', 'knowing', 'knowing', 'insights', 'input', 'input', 'indeed', 'important', 'hand', 'going', 'going', 'going', 'going', 'going', 'going', 'generate', 'general', 'general', 'follows', 'figure', 'extent', 'extent', 'extent', 'extent', 'expected', 'examples', 'example', 'example', 'example', 'example', 'example', 'example', 'easy', 'dislike', 'discussion', 'discuss', 'discovered', 'di', 'description', 'denote', 'denote', 'coverage', 'conversation', 'concept', 'comparison', 'collection', 'collection', 'call', 'c', 'authors', 'assume', 'analyzing', 'analyzing', 'analysis', 'analysis', 'analysis', 'actually', 'able', 'able']
lecture 15
['sometimes called maximal marginal relevance ranking', 'easily count related terms toward contributing', 'extent document 1 covers sports', 'star might actually suggest perhaps', 'cannot really describe complicated topics', 'many related words like basketball', 'might also discover document 2', 'basketball star versus star', 'mine k topical terms', 'get k topical terms', 'favor title words becauses', 'could also favor hashtags', 'course many different ways', 'highest scored terms would', 'simply picking k terms', 'topic coverage pi sub', 'might favor title words', 'term frequency idf stands', 'term travel actually occurred', 'occurred four times', 'k topical terms', 'k topical terms', 'lacks expressive power', 'domain specific heuristics', 'highest scored terms', 'also likely effective', 'also high score', 'travel occurred twice', 'nba basketball game', 'favor representative terms', 'word sense ambiguation', 'obtain candidate terms', 'word like star', 'might also mean', 'document would add', 'using actual datasets', 'inverse document frequency', 'simply count occurrences', 'particular approach could', 'count related words', 'become candidate topics', 'specialized topic would', 'gradually take terms', 'general would like', 'consider related words', 'defined mostly based', 'terms like sports', 'word sports actually', 'offer one way', 'scoring function would', 'basketball star', 'estimated coverage would', 'use pure statistics', 'symbol general topics', 'estimated also zero', 'simple example illustrates', 'term science also', 'many terms', 'document 1', 'favor terms', 'would like', 'would like', 'complicated topics', 'might also', 'many things', 'would mean', 'naturally hashtags', 'idf weighting', 'highest scores', 'get balance', 'topical term', 'candidate terms', 'different topics', 'closely related', 'might require', 'might encounter', 'formula would', 'cannot represent', 'simply count', 'related term', 'also raised', 'also consider', 'sports might', 'tf stands', 'avoid picking', 'actually ambiguous', 'functional terms', 'might want', 'might want', 'candidate term', 'also want', 'also want', 'words belong', 'general terms', 'zero count', 'scoring function', 'scoring function', 'scoring function', 'scoring function', 'scoring function', 'word associations', 'two tasks', 'statistical methods', 'simplest solution', 'probabilistic modeling', 'natural way', 'main restriction', 'greedy algorithm', 'fairly frequently', 'empirical evaluation', 'best way', 'authors tend', 'allow us', 'particular problem', 'simple example', 'example illustrates', 'simply use', 'representing topics', 'discover topics', 'denote topics', 'word sports', 'sports certainly', 'one way', 'cover sports', 'simply normalize', 'next term', 'also need', 'describe topics', 'topic simply', 'meaning terms', 'text document', 'text data', 'remove redundancy', 'redundancy removal', 'even though', 'even synonyms', 'one term', 'one term', 'one term', 'vocabulary coverage', 'coverage probability', 'characterize coverage', 'topic travel', 'top mining', 'task definition', 'ranking', 'next lecture', 'list based', 'first thought', 'earlier lecture', 'topic theta', 'topic mining', 'topic mining', 'travel etc', 'probably means', 'good candidates', 'one phrase', 'first term', 'semantically similar', 'initial idea', 'one approach', 'next let', 'exactly define', 'non zero', 'general problems', 'star', 'frequent term', 'title', 'suggest', 'favor', 'actually', 'solving problem', 'second problem', 'related', 'related', 'first think', 'already picked', 'might', 'like', 'frequency', 'also', 'also', 'terms', 'terms', 'terms', 'terms', 'terms', 'terms', 'terms', 'terms', 'count', 'count', 'count', 'estimated', 'simply', 'travel', 'travel', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'word', 'word', 'word', 'way', 'statistics', 'game', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'discover', 'discover', 'discover', 'defined', 'consider', 'based', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'function', 'function', 'describe', 'describe', 'describe', 'describe', 'sports', 'sports', 'sports', 'sports', 'sports', 'sports', 'sports', 'sports', 'one', 'one', 'one', 'term', 'term', 'term', 'term', 'term', 'term', 'term', 'term', 'term', 'term', 'zero', 'zero', 'zero', 'want', 'want', 'using', 'using', 'text', 'take', 'take', 'redundancy', 'general', 'general', 'general', 'general', 'general', 'even', 'use', 'use', 'use', 'course', 'course', 'course', 'course', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'top', 'tf', 'task', 'solving', 'second', 'science', 'science', 'science', 'problems', 'phrase', 'normalize', 'non', 'list', 'lecture', 'lecture', 'first', 'first', 'first', 'avoid', 'ambiguous', 'already', 'problem', 'problem', 'problem', 'problem', 'approach', 'approach', 'approach', 'approach', 'approach', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'think', 'think', 'picked', 'picked', 'means', 'means', 'good', 'good', 'frequent', 'frequent', 'similar', 'similar', 'similar', 'meaning', 'meaning', 'meaning', 'idea', 'idea', 'idea', 'represent', 'represent', 'represent', 'represent', 'need', 'need', 'need', 'need', 'let', 'let', 'let', 'let', 'etc', 'etc', 'etc', 'etc', 'define', 'define', 'define', 'define', 'worse', 'works', 'well', 'well', 'well', 'tweets', 'try', 'treating', 'thresholding', 'talking', 'talked', 'talk', 'talk', 'talk', 'suppose', 'solve', 'slide', 'sky', 'sky', 'situation', 'seen', 'see', 'see', 'say', 'retrieval', 'respect', 'represented', 'represented', 'regarded', 'question', 'question', 'picture', 'pick', 'phrases', 'penalize', 'parse', 'ok', 'ok', 'ok', 'occur', 'occur', 'occur', 'obviously', 'news', 'measure', 'lot', 'looking', 'look', 'look', 'leverage', 'lectures', 'language', 'know', 'j', 'invented', 'intuitively', 'incomplete', 'imagine', 'ideas', 'however', 'hard', 'gotten', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'go', 'given', 'found', 'forms', 'find', 'finally', 'finally', 'figure', 'examine', 'estimate', 'estimate', 'estimate', 'english', 'distribution', 'discussed', 'discovery', 'discovered', 'desirable', 'desirable', 'designed', 'design', 'design', 'design', 'design', 'dealing', 'deal', 'counts', 'counts', 'content', 'content', 'content', 'content', 'content', 'content', 'considering', 'compute', 'collection', 'collection', 'collection', 'collection', 'collection', 'collect', 'collect', 'cause', 'case', 'case', 'case', 'basically', 'ask', 'article', 'article', 'apply', 'applied', 'anyway', 'analyze', 'analyze', 'analysis', 'analysis', 'analysis', 'always', 'able', '30', '2nd', '12', '",']
lecture 16
['likely parameter values lambda star given', 'capital lambda actually consists', 'introduce semantically related words', 'describe fairly complicated topics', 'introduce probabilistic topic models', 'general involve many words', 'problem called generative model', 'data points higher probabilities', 'see top words like', 'k topics would sum', 'distinguish subtle differences', 'also see k theta_i', 'precisely topic mining problem', 'solve text mining problems', 'specific parameter values', 'discover various kinds', 'still see one word', 'many different ways', 'reasonably high probabilities', 'hotel etc ."', 'probabilistic topic models', 'mine k topics', 'non zero probability', 'non zero probability', 'one interesting question', 'one dimensional variable', 'actually samples drawn', 'non zero probabilities', 'model subtle variations', 'model subtle differences', 'model several differences', 'much smaller probabilities', 'using statistical modeling', 'also allows us', 'uses multiple words', 'tax mining problem', 'also generally assume', 'related words together', 'high probability words', 'related terms necessary', 'text data c', 'discover different knowledge', 'describe complicated topics', 'would give us', 'output would consist', 'travel topic 0', 'data mining algorithm', 'would also give', 'words must sum', 'probabilistic topic model', 'probability obviously depends', 'using multiple words', 'already see n', 'top mining', 'particular data set', 'parameter values', 'parameter values', 'parameter values', 'parameter values', 'allows us', 'higher probabilities', 'also introduce', 'different values', 'topics k', 'much related', 'zero probabilities', 'data points', 'would like', 'would like', 'probability values', 'complicated topic', 'text mining', 'text mining', 'multiple topics', 'introduce weights', 'probabilistic model', 'probabilistic model', 'lambda star', 'lambda star', 'must sum', 'different probabilities', 'generative model', 'generative model', 'related terms', 'related terms', 'also want', 'also occurred', 'different topics', 'topic mining', 'topic mining', 'n sets', 'n documents', 'words like', 'also see', 'also see', 'k parameters', 'slide given', 'actually related', 'would maximize', 'would allow', 'units would', 'relevant would', 'three problems', 'three problems', 'probability threshold', 'probability mass', 'maximum probability', 'highest probability', 'highest probability', 'small probabilities', 'small probabilities', 'particular model', 'many parameters', 'many parameters', 'particular topic', 'x axis', 'two constraints', 'theoretical speaking', 'text articles', 'simply assume', 'oversimplification obviously', 'namely represented', 'mentioned earlier', 'knowledge discovered', 'heuristic program', 'fuzzy manner', 'formally represented', 'expressive power', 'even though', 'discovered knowledge', 'continue talking', 'concentrated entire', 'clearly specify', 'basically degenerates', 'basic units', 'axis shows', 'assigns weights', 'assign weights', 'added refinement', 'word ambiguity', 'ambiguous word', 'two topics', 'topics represented', 'three topics', 'exact topics', 'see later', 'pi values', 'also illustrate', 'one phrase', 'necessary generated', 'sample words', 'probability distribution', 'one word', 'one word', 'one word', 'travel ",', 'text data', 'text data', 'text data', 'simple representation', 'old representation', 'observed data', 'improved representation', 'generate data', 'fitted data', 'data observe', 'actual data', 'problem definition', 'computation problem', 'see words', 'would treat', 'problems intuitively', 'underlying topic', 'topic outside', 'world distribution', 'distribution represents', 'word distribution', 'word distribution', 'word distribution', 'word distribution', 'word distribution', 'word distribution', 'word distribution', 'word distribution', 'word distribution', 'respective distributions', 'principle way', 'new way', 'new way', 'basic idea', 'word distributions', 'word distributions', 'would address', 'model travel', 'means even', 'makes sense', 'generation process', 'coverage numbers', 'another constraint', 'topic representation', 'second problem', 'describe topic', '1st design', 'model topic', 'one term', 'general way', 'general idea', 'general idea', 'general idea', 'special case', 'namely pis', 'parameters note', 'following parameters', 'first pretend', 'seen earlier', 'data set', 'data set', 'topic coverage', 'topic coverage', 'second constraint', 'new science', 'mean sports', 'use words', 'given', 'say shared', 'vocabulary set', 'vocabulary set', 'parameters based', 'first design', 'introduce', 'still', 'precisely', 'star', 'star', 'related', 'also', 'discover', 'would', 'would', 'problems', 'probability', 'probability', 'lambda', 'lambda', 'lambda', 'lambda', 'lambda', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'text', 'text', 'text', 'terms', 'sum', 'sum', 'sum', 'obviously', 'knowledge', 'etc', 'depends', 'assume', 'algorithm', 'word', 'word', 'word', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'one', 'one', 'one', 'one', 'one', 'one', 'actually', 'actually', 'actually', 'actually', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'travel', 'travel', 'representation', 'give', 'give', 'give', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'general', 'general', 'general', 'general', 'describe', 'describe', 'describe', 'describe', 'describe', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'second', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'already', 'already', 'theta_i', 'theta_i', 'theta_i', 'theta_i', 'solve', 'solve', 'solve', 'solve', 'distributions', 'distributions', 'way', 'way', 'way', 'using', 'using', 'using', 'using', 'using', 'idea', 'idea', 'idea', 'treat', 'term', 'shared', 'sense', 'means', 'intuitively', 'illustrate', 'generation', 'coverage', 'coverage', 'coverage', 'constraint', 'constraint', 'based', 'vocabulary', 'vocabulary', 'vocabulary', 'set', 'set', 'set', 'set', 'set', 'set', 'set', 'set', 'set', 'set', 'output', 'output', 'output', 'output', 'design', 'design', 'design', 'say', 'say', 'pis', 'pis', 'pi', 'pi', 'generated', 'generated', 'case', 'case', 'address', 'address', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'first', 'first', 'first', 'first', 'first', 'slide', 'slide', 'slide', 'use', 'use', 'use', 'use', 'seen', 'seen', 'seen', 'seen', 'science', 'science', 'science', 'science', 'science', 'sports', 'sports', 'sports', 'sports', 'sports', 'sports', 'sports', 'sports', 'write', 'whereas', 'well', 'well', 'well', 'varying', 'varies', 'value', 'unit', 'turns', 'try', 'trip', 'treated', 'total', 'third', 'think', 'theta', 'tend', 'telescope', 'task', 'talked', 'talk', 'take', 'summarize', 'suffices', 'split', 'spend', 'spaceship', 'solving', 'solving', 'slightly', 'since', 'similarly', 'similarly', 'similar', 'side', 'show', 'show', 'setting', 'semantics', 'semantics', 'semantics', 'seeing', 'scientist', 'right', 'representing', 'representing', 'represent', 'refined', 'recover', 'play', 'picture', 'picture', 'pi_ij', 'pi_i', 'outcome', 'others', 'others', 'order', 'occured', 'occur', 'number', 'number', 'number', 'need', 'need', 'need', 'might', 'might', 'meaning', 'meaning', 'may', 'marked', 'lot', 'look', 'let', 'let', 'lectures', 'lectures', 'lecture', 'lecture', 'leave', 'lack', 'know', 'know', 'know', 'know', 'introduced', 'interested', 'interested', 'interested', 'interested', 'interested', 'inside', 'input', 'input', 'input', 'infer', 'infer', 'indeed', 'imagine', 'imagine', 'illustrated', 'ij', 'hypothesize', 'hope', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'get', 'genomics', 'game', 'forms', 'football', 'flight', 'fit', 'fit', 'find', 'finally', 'figure', 'fact', 'exist', 'exercise', 'except', 'example', 'example', 'exactly', 'estimate', 'estimate', 'embed', 'done', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'discussed', 'discovering', 'disambiguate', 'disambiguate', 'dim', 'determines', 'depending', 'denoted', 'denoted', 'define', 'decode', 'coverages', 'cover', 'course', 'course', 'course', 'course', 'course', 'course', 'course', 'control', 'compute', 'collection', 'collection', 'collection', 'change', 'cases', 'bring', 'black', 'behavior', 'basketball', 'basis', 'attraction', 'assumption', 'approach', 'analysis', 'analysis', 'analysis', 'analysis', 'analysis', 'allowed', 'advantage', 'adjust', 'adjust', 'adjust', 'addresses', '05']
lecture 17
['estimate somehow even slightly', 'cover probabilistic topic models', 'cover probabilistic topic models', 'must specify probability values', 'positive ", might get', 'text mining paper publishing', 'called maximum likelihood estimate', 'food nutrition paper would', 'text mining paper would', 'text data looks like', 'positive ", sometimes', 'might get wednesday often', 'observed text 10 times', 'maximum likelihood estimate', 'words like eigenvalue might', 'observe certain kind', 'involve topic analysis', 'unigram language models', 'statistical language models', 'statistical language models', 'show two examples', 'non grammatical sentence', 'sample drawn according', 'similarly another sentence', 'text mining paper', 'text mining paper', 'might often get', 'meaningful text documents', 'unigram language model', 'statistical language model', 'statistical language model', 'simplest language model', 'view text data', 'second distribution show', 'relatively large probability', 'like many others', 'model generative model', 'might also get', 'sample words according', 'text mining context', 'observed text data', 'relatively high probability', 'likely language model', 'relatively small probability', 'actually characterize topic', 'word sequences might', 'looks like', 'looks like', 'probabilistic mechanism', 'many times', 'might get', 'even though', 'different topic', 'text mining', 'relatively higher', 'often occur', 'best estimate', 'text documents', 'language model', 'language model', 'language model', 'maximum probability', 'text data', 'text data', 'observed data', 'observed data', 'observed data', 'data observed', 'non zero', 'data points', 'many tasks', 'sequence like', 'words might', 'general models', 'different text', 'might give', 'sample sequences', 'w_n would', 'today often', 'guess text', 'wednesday ".', 'wednesday ".', 'unlikely coherent', 'two problems', 'top conference', 'speech recognition', 'special cases', 'special cases', 'somewhat smaller', 'quite sufficient', 'possible sequences', 'particular document', 'numbers together', 'know exactly', 'gonna draw', 'fake numbers', 'estimation process', 'different sequences', 'different context', 'clearly indicates', 'simple model', 'sample words', 'sample words', 'n probabilities', 'n probabilities', 'higher probabilities', 'higher probabilities', 'estimated probabilities', 'different probabilities', 'reasonable guess', 'would give', 'also call', 'words w_1', 'n words', 'individual words', 'drawing words', 'different words', 'see text', 'probably wednesday', 'may care', 'zero probability', 'smaller probability', 'higher probability', 'different probability', 'word sequences', 'word independently', 'word distributions', 'many parameters', 'significantly simplify', 'sampling process', 'independence assumption', 'guess probably', 'estimation problem', 'essentially generate', 'context dependent', 'small probability', 'small probability', 'small probability', 'particular distribution', 'distribution clearly', 'model given', 'generating text', 'every sequence', 'today wednesday', 'today wednesday', 'words may', 'generated independently', 'diet etc', 'best guess', 'best guess', 'also compute', 'simply assume', 'see words', 'first talk', 'paper', 'probability distribution', 'probability distribution', 'word distribution', 'total number', '100 words', 'models', 'topic', 'first one', 'word order', 'means given', 'mining', 'mining', 'get', 'get', 'specify', 'nutrition', 'might', 'might', 'might', 'food', '10', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'sample', 'would', 'would', 'would', 'observed', 'observed', 'wednesday', 'wednesday', 'similarly', 'sequences', 'others', 'high', 'eigenvalue', 'eigenvalue', 'context', 'called', 'called', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'probabilities', 'small', 'small', 'guess', 'also', 'also', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'probably', 'may', 'likely', 'likely', 'give', 'first', 'characterize', 'characterize', 'actually', 'actually', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'word', 'word', 'word', 'word', 'word', 'word', 'total', 'talk', 'simplify', 'see', 'see', 'sampling', 'problems', 'problem', 'number', 'given', 'given', 'generate', 'dependent', 'compute', 'best', 'best', 'best', 'assumption', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'sequence', 'sequence', 'sequence', 'today', 'today', 'today', 'today', 'today', 'parameters', 'parameters', 'generated', 'generated', 'general', 'general', 'etc', 'etc', 'order', 'order', 'order', 'means', 'means', 'means', 'generating', 'generating', 'generating', 'assume', 'assume', 'assume', 'assume', '100', '100', '100', '100', 'one', 'one', 'one', 'one', 'one', 'way', 'vocabulary', 'used', 'turns', 'turns', 'try', 'time', 'think', 'think', 'think', 'tend', 'sum', 'suggests', 'specifies', 'specifically', 'slide', 'signals', 'shown', 'shown', 'shown', 'shown', 'showed', 'sense', 'say', 'right', 'right', 'regarded', 'reason', 'question', 'question', 'question', 'question', 'query', 'query', 'product', 'product', 'practice', 'overview', 'overview', 'ok', 'occasionally', 'obviously', 'observing', 'observing', 'observd', 'multiply', 'moment', 'moment', 'meaning', 'mean', 'make', 'look', 'let', 'let', 'let', 'let', 'lecture', 'lecture', 'kinds', 'kinds', 'intuitively', 'interested', 'interested', 'indeed', 'including', 'impossible', 'impossible', 'ignore', 'healthy', 'health', 'hand', 'guesses', 'guessed', 'going', 'going', 'going', 'gives', 'generally', 'five', 'figure', 'fact', 'example', 'example', 'example', 'example', 'eventually', 'enumerate', 'effective', 'discussion', 'depends', 'define', 'course', 'course', 'counts', 'change', 'case', 'case', 'case', 'case', 'case', 'case', 'bottom', 'basically', 'basically', 'based', 'assuming', 'assuming', 'association', 'ask', 'ask', 'answer', 'although', 'abstract', '001', '00001', '0', '0', '...']
lecture 18
['non zero probability would take away probability mass', 'estimate intuitively also makes sense', 'maximum likelihood estimator would give', 'gives us also one point', 'basically would go back', 'contain another word related', 'bias towards certain values', 'gives us another way', 'distribution would allow us', 'data also tells us', 'parameter values would maximize', 'called maximum likelihood estimate', 'word zero probability', 'would tell us', 'likely parameter value according', 'bayesian reasoning would', 'x given theta reach', 'maximum likelihood estimator', 'unseen words may', 'maximum likehood estimate', 'prior tells us', 'function reach maximum', 'unigram language model', 'simplest language model', 'trust data entirely', 'one conditional probability', 'maximum likelihood estimate', 'maximum likelihood estimate', 'maximum likelihood estimate', 'maximum likelihood estimate', 'observed 100 words', 'posterior probability combines', 'one dimension value', 'maximum likelihood estimation', 'interesting point estimates', 'using bayes rule', 'two different ways', 'zero probability', 'actually would look', 'two conditional probabilities', 'basically probability distribution', 'use bayesian estimation', 'noninformative prior meaning', 'observed evidence x', 'function maximum value', 'allow us', 'would maximize', 'one way', 'give us', 'data sample x', 'would return', 'tell us', 'also called', 'theta given x', 'x given theta', 'maximum probability', 'would say', 'allows us', 'word distribution', 'f according', 'certain thetas', 'also talked', 'conditional probability', 'conditional probability', 'estimate may', 'language model', 'parameter values', 'parameter values', 'observed words', 'posterior probability', 'posterior probability', 'posterior probability', 'bayesian estimation', 'bayesian estimation', 'bayesian estimation', 'bayesian estimation', 'take', 'also look', 'higher probability', 'higher probability', 'higher likelihood', 'map estimate', 'map estimate', 'observed evidence', 'things related', 'two probabilities', 'two probabilities', 'parameter value', 'high probabilities', 'bayes rule', 'bayes rule', 'bayes rule', 'data likelihood', 'data likelihood', 'data likelihood', 'generative model', 'likelihood value', 'general estimate', 'likelihood function', 'likelihood function', 'posterior mean', 'posterior distribution', 'posterior distribution', 'posterior distribution', 'posterior distribution', 'posterior distribution', 'data point', 'data point', 'values like', 'possible values', 'derived values', 'x values', 'x values', 'updating formula', 'two variables', 'two preferences', 'showed f', 'particular set', 'making inferences', 'little bit', 'interesting quantities', 'indeed true', 'full explanation', 'free lunch', 'brief introduction', 'bayesian inference', 'bayesian inference', 'bayesian inference', 'bayesian inference', 'arg max', 'arg max', 'arg max', 'x encodes', 'particular x', 'posterior mode', 'posterior mode', 'x given', 'x given', 'given x', 'text mining', 'text mining', 'treat data', 'data well', 'data points', 'data points', 'general illustration', 'expected value', 'expected value', 'knowledge ideally', 'function f', 'basically defined', 'observed data', 'observed data', 'uncertain variable', 'interesting variable', 'best explain', 'best explain', 'data given', 'data given', 'posterior belief', 'text data', 'text data', 'theta values', 'theta values', 'theta values', 'theta values', 'theta values', 'theta values', 'likely value', 'point represents', 'informative prior', 'topic mining', 'reasonable sometimes', 'might depend', 'mainly going', 'general knowledge', 'must define', 'define best', 'theta given', 'good compromise', 'assume f', 'prior mode', 'estimating parameters', 'prior knowledge', 'prior knowledge', 'prior knowledge', 'special case', 'prior belief', 'prior belief', 'prior belief', 'makes', 'estimator', 'also', 'one', 'gives', 'gives', 'maximum', 'maximum', 'maximum', 'give', 'parameters p', 'parameter', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'likelihood', 'likelihood', 'likelihood', 'estimate', 'estimate', 'estimate', 'estimate', 'related', 'maximize', 'called', 'point', 'point', 'model', 'basically', 'basically', 'values', 'values', 'values', 'use', 'two', 'two', 'sample', 'rule', 'meaning', 'interesting', 'distribution', 'distribution', 'distribution', 'different', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'observed', 'observed', 'observed', 'given', 'given', 'given', 'given', 'given', 'given', 'text', 'likely', 'likely', 'likely', 'inference', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'value', 'value', 'value', 'value', 'value', 'value', 'value', 'value', 'mode', 'general', 'knowledge', 'knowledge', 'function', 'function', 'function', 'function', 'function', 'variable', 'explain', 'actually', 'actually', 'belief', 'belief', 'belief', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'topic', 'sometimes', 'say', 'might', 'look', 'look', 'look', 'look', 'going', 'defined', 'define', 'define', 'define', 'compromise', 'compromise', 'assume', 'assume', 'represents', 'represents', 'represents', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'p', 'p', 'p', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'write', 'want', 'want', 'want', 'want', 'useful', 'useful', 'useful', 'used', 'update', 'uniform', 'try', 'topics', 'think', 'think', 'therefore', 'terms', 'terms', 'talk', 'talk', 'summarize', 'specifically', 'sorry', 'somewhere', 'solve', 'small', 'small', 'small', 'simplification', 'shown', 'show', 'seeks', 'see', 'see', 'see', 'see', 'scope', 'reliable', 'regarded', 'reached', 'rather', 'proportional', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'preference', 'preference', 'posteriori', 'places', 'outside', 'otherwise', 'others', 'others', 'others', 'others', 'order', 'order', 'optimal', 'optimal', 'ok', 'ok', 'often', 'often', 'obviously', 'observing', 'observe', 'observe', 'need', 'necessarily', 'much', 'mentioned', 'means', 'means', 'means', 'means', 'means', 'maximizing', 'made', 'let', 'let', 'let', 'let', 'let', 'later', 'know', 'kind', 'introduced', 'introduced', 'interested', 'interested', 'interested', 'interested', 'inject', 'influence', 'inferring', 'infer', 'infer', 'infer', 'incorporate', 'includes', 'impose', 'important', 'hypothesis', 'hypothesis', 'giving', 'get', 'get', 'formally', 'fit', 'first', 'find', 'expression', 'example', 'especially', 'equal', 'detour', 'determined', 'determine', 'detail', 'course', 'course', 'course', 'course', 'concept', 'computing', 'compute', 'compute', 'combine', 'combine', 'combination', 'choose', 'characterize', 'biased', 'believe', 'believe', 'believe', 'believe', 'based', 'argument', 'argument', 'argmax', 'always', 'already', 'address', 'accurate']
lecture 19
['continue discussing probabilistic models', 'topic well like text', 'high probability words tend', 'easily see theta sub', 'maximum likelihood estimate problem', 'use lagrange multiplier approach', 'often see later also', 'likelihood function look like', 'use probabilistic models', 'probabilistic topic models', 'maximum likelihood estimator', 'maximum likelihood estimator', 'maximum likelihood estimator', 'topic 100 %.', 'often use later', 'text mining paper', 'introduce lagrange multiplier', 'much probability mass', 'also would like', 'since lagrange multiplier', 'cases later also', 'whole document given', 'constrained maximization problem', 'w sub one', 'unigram language model', 'also see various', 'somehow get rid', 'use theta sub', 'use theta sub', 'might also notice', 'log likelihood instead', 'estimate one topic', 'also see later', 'general topic models', 'function would combine', 'high probability words', 'would look like', 'unconstrained optimizing problem', 'often functional words', 'word w sub', 'mining one topic', 'analyzing one document', 'closed form formula', 'estimated parameters would', 'optimal point would', 'one document also', 'probabilities must sum', 'solve optimization problem', 'w sub', 'estimate would', 'looks like', 'theta sub', 'often needed', 'often easy', 'lagrange function', 'lagrange function', 'x sub', 'constrained optimization', 'also see', 'mining etc', 'mining algorithm', 'get rid', 'also rewrite', 'original likelihood', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'one topic', 'one topic', 'optimization problem', 'optimization problem', 'estimated parameters', 'functional words', 'optimal solution', 'optimal solution', 'optimal setting', 'would need', 'would lead', 'topic representation', 'topic modeling', 'single topic', 'must sum', 'one question', 'theta values', 'might see', 'mathematical problem', 'useful form', 'useful approach', 'simply taken', 'simple setup', 'repeated occurrences', 'repeated occurrences', 'really characterizing', 'really characterized', 'previous slide', 'pay attention', 'particular angle', 'part comes', 'numerical algorithms', 'necessary condition', 'may recall', 'get familiar', 'externally mentioned', 'corresponding term', 'continued discussion', 'complicated cases', 'brief introduction', 'another term', 'analytical solution', 'allows us', 'allow us', 'additional parameter', 'objective function', 'objective function', 'library function', 'would sum', 'document would', 'many parameters', 'one way', 'one document', 'words might', 'unique words', 'observed words', 'many words', 'common words', 'common words', 'common words', 'model would', 'general result', 'different positions', 'different form', 'word one', 'sum makes', 'sum instead', 'negative sum', 'particular way', 'single document', 'document length', 'document covers', 'logarithm function', 'word might', 'word distribution', 'simplest case', 'simple case', 'simple case', 'normalized count', 'normalize count', 'specific model', 'generative model', 'count function', 'different words', 'sufficient though', 'really related', 'partial derivative', 'mathematical convenience', 'would maximize', 'please take', 'right parameters', 'previous line', 'main goal', 'data representation', 'data point', 'content words', 'word probabilities', 'usually interested', 'logarithm transformation', 'zero count', 'obtained something', 'take derivative', 'next lecture', 'later', 'text', 'use', 'use', 'also', 'also', 'also', 'likelihood', 'would', 'would', 'topic', 'topic', 'topic', 'topic', 'one', 'one', 'one', 'theta', 'theta', 'theta', 'see', 'see', 'see', 'see', 'see', 'problem', 'problem', 'problem', 'problem', 'point', 'optimizing', 'might', 'introduce', 'instead', 'get', 'form', 'approach', 'approach', 'function', 'function', 'function', 'function', 'probability', 'probability', 'probability', 'probability', 'probability', 'parameters', 'parameters', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'general', 'general', 'different', 'look', 'look', 'look', 'look', 'look', 'sum', 'sum', 'sum', 'sum', 'way', 'since', 'since', 'logarithm', 'formula', 'formula', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'word', 'word', 'word', 'word', 'word', 'probabilities', 'probabilities', 'probabilities', 'case', 'case', 'count', 'count', 'count', 'model', 'model', 'model', 'model', 'model', 'zero', 'transformation', 'though', 'something', 'right', 'related', 'obtained', 'next', 'derivative', 'derivative', 'convenience', 'content', 'complicated', 'take', 'take', 'take', 'solve', 'solve', 'solve', 'solve', 'solve', 'line', 'line', 'lecture', 'lecture', 'goal', 'goal', 'data', 'data', 'data', 'data', 'data', 'maximize', 'maximize', 'maximize', 'interested', 'interested', 'interested', 'interested', 'writing', 'write', 'want', 'want', 'want', 'want', 'vocabulary', 'vocabulary', 'vocabulary', 'vocabulary', 'using', 'turns', 'turn', 'trying', 'topics', 'top', 'thinking', 'thinking', 'think', 'thetas', 'therefore', 'talk', 'talk', 'steps', 'start', 'sometimes', 'similarly', 'shown', 'set', 'set', 'sequence', 'sense', 'scope', 'rewritten', 'respect', 'purely', 'purely', 'product', 'product', 'product', 'product', 'product', 'product', 'procedure', 'possible', 'plug', 'perspective', 'perspective', 'perspective', 'output', 'output', 'occur', 'obviously', 'number', 'notation', 'multiplied', 'means', 'means', 'maximizing', 'math', 'looking', 'longer', 'longer', 'likely', 'let', 'let', 'let', 'let', 'lambda', 'lambda', 'lambda', 'lambda', 'knowledge', 'knowledge', 'know', 'know', 'kind', 'k', 'intuitive', 'intuitive', 'intuition', 'inside', 'input', 'independence', 'imagine', 'imagine', 'ideal', 'idea', 'hope', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'give', 'generation', 'generating', 'generating', 'formally', 'followed', 'finding', 'find', 'find', 'find', 'find', 'express', 'example', 'eventually', 'estimating', 'equal', 'english', 'end', 'end', 'encodes', 'easier', 'done', 'discovering', 'discover', 'discover', 'discover', 'disappear', 'design', 'denotes', 'denoted', 'denote', 'denote', 'coverage', 'course', 'course', 'counts', 'counts', 'counts', 'copied', 'construct', 'constraints', 'constraints', 'constraint', 'constraint', 'collection', 'closely', 'change', 'capture', 'calculus', 'beyond', 'become', 'become', 'basically', 'assumed', 'assume', 'assigning', 'always', 'actually', 'achieved', 'able', '0']
lecture 20
['might bias towards using one topic', 'continue discussing probabilistic topic models', 'mission would allow us', 'two unigram language models', 'two unigram language models', 'maximum likelihood estimator later', 'background word distribution denoted', 'unigram language models', 'unigram language models', 'component like theta sub', 'also consider two ways', 'maximum likelihood estimator', 'called background topic model', 'usually good exercise', 'likelihood function look like', 'particular word distribution multiplied', 'theta sub b', 'theta sub b', 'theta sub b', 'theta sub b', 'events must happen', 'estimated language model', 'occurring often later', 'second part accounts', 'assigned high probabilities', 'assign high probabilities', 'maximum likelihood estimate', 'generating multiple ways', 'using another distribution', 'consider 2 cases', 'particular component distribution', 'following generative model', 'estimator obviously would', 'topic must sum', 'topic must sum', 'two distributions together', 'something quite general', 'topic word distribution', 'topic word distribution', 'topic word distribution', 'background word distribution', 'different unique words', 'one natural way', 'two word distributions', 'complicated mixture model', 'common words would', 'topic models', 'observed text data', 'one distribution right', 'still 1 document', 'generating text data', 'actually general form', 'background topic', 'topic theta', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'mixture models', 'two ways', 'give us', 'different ways', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'might recall', 'something different', 'quite different', 'two distributions', 'two distributions', 'two distributions', 'two distributions', 'two distributions', 'would happen', '2 cases', 'background words', 'component model', 'component model', 'component model', 'two topics', 'two terms', 'two terms', 'two components', 'two cases', 'mixed together', 'components together', 'complicated model', 'unique words', 'first must', 'model multiplied', 'generative model', 'generative model', 'one distribution', 'one distribution', 'one distribution', 'one distribution', 'would mean', 'might want', 'two kinds', 'two kinds', 'uncertainty associated', 'text ",', 'special cases', 'small modification', 'simple w', 'often useful', 'mixing weights', 'make sure', 'make sure', 'give example', 'get rid', 'get rid', 'easily verify', 'common words', 'common words', 'common words', 'common words', 'collectively called', 'calculation earlier', 'basic idea', 'word distribution', 'word distribution', 'words like', 'another way', 'text data', 'text data', 'text data', 'data point', 'assuming one', 'previous model', 'model index', 'mixed model', 'generation model', 'convenient form', 'word would', 'different way', 'general description', 'fair coin', 'coin shows', 'coin based', 'word w', 'word probabilities', 'specific word', 'still generating', 'next question', 'generating text', 'basic question', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'one model', 'one model', 'would maximize', 'observing text', 'actually observing', 'actually observing', 'actually observing', 'general form', 'one word', 'whole box', 'really understood', 'interesting knowledge', 'complete understanding', 'parameters given', 'many parameters', 'another probability', 'one document', 'seen earlier', 'previous set', '1st decide', 'course also', 'content words', 'special case', 'special case', 'model parameters', 'probability would', 'still think', 'still see', 'set one', 'sum instead', 'inside sum', 'first sum', 'document exactly', 'basically flip', 'first case', 'using', 'using', 'might', 'topic', 'topic', 'topic', 'topic', 'later', 'consider', 'background', 'likelihood', 'two', 'multiplied', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'particular', 'particular', 'like', 'like', 'like', 'would', 'would', 'would', 'text', 'text', 'still', 'second', 'probabilities', 'probabilities', 'observed', 'look', 'look', 'cases', 'called', 'called', 'actually', '1', 'data', 'data', 'data', 'mixture', 'mixture', 'mixture', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'words', 'words', 'words', 'words', 'words', 'form', 'form', 'also', 'also', 'also', 'general', 'general', 'general', 'general', 'coin', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'question', 'obviously', 'obviously', 'kinds', 'generating', 'generating', 'generating', 'generating', 'generating', 'estimate', 'estimate', 'sum', 'sum', 'sum', 'sum', 'sum', 'sum', 'way', 'way', 'way', 'way', 'observing', 'observing', 'observing', 'document', 'document', 'document', 'document', 'useful', 'understanding', 'right', 'right', 'right', 'really', 'maximize', 'interesting', 'instead', 'inside', 'flip', 'first', 'first', 'first', 'exactly', 'course', 'content', 'box', 'parameters', 'parameters', 'parameters', 'parameters', 'want', 'want', 'set', 'set', 'set', 'set', 'seen', 'seen', 'decide', 'decide', 'basically', 'basically', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'think', 'think', 'think', 'think', 'think', 'see', 'see', 'see', 'see', 'see', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'writing', 'write', 'work', 'well', 'well', 'well', 'well', 'vocabulary', 'view', 'usual', 'used', 'use', 'use', 'use', 'use', 'use', 'use', 'use', 'use', 'use', 'use', 'understand', 'treated', 'treat', 'top', 'time', 'therefore', 'term', 'target', 'talking', 'talked', 'talked', 'summarize', 'summarize', 'specifically', 'sorry', 'solve', 'solve', 'slide', 'since', 'similarly', 'similar', 'similar', 'showed', 'show', 'sense', 'selecting', 'selecting', 'selecting', 'selecting', 'seeing', 'say', 'say', 'say', 'sampled', 'sampled', 'said', 'representing', 'represented', 'product', 'product', 'product', 'product', 'product', 'product', 'process', 'problem', 'problem', 'problem', 'positions', 'path', 'parameter', 'otherwise', 'order', 'order', 'order', 'observe', 'observe', 'nothing', 'note', 'namely', 'means', 'mathematically', 'lot', 'looks', 'let', 'let', 'let', 'lecture', 'lecture', 'lambda', 'know', 'introduce', 'introduce', 'interested', 'intended', 'indeed', 'illustration', 'illustration', 'however', 'head', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'generated', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'frequent', 'find', 'fact', 'fact', 'expression', 'explained', 'explain', 'eventually', 'equal', 'end', 'due', 'discover', 'discover', 'dimmer', 'difference', 'determines', 'determined', 'denote', 'denote', 'degenerate', 'define', 'coverage', 'coverage', 'cover', 'convince', 'controlled', 'constraints', 'computing', 'computing', 'chosen', 'choosing', 'choosing', 'choosing', 'choosing', 'choice', 'characterize', 'bring', 'basis', 'attract', 'assumed', 'allows', 'account', '5', '0', '0', '0', '".']
lecture 21
['continue discussing probabilistic topic models', 'text data actually contain two kinds', 'unknown topic world distribution', 'use maximum likelihood estimator', 'use maximum likelihood estimator', 'background model gives probability', 'two probabilities must sum', 'maximum likelihood estimator', 'flip fair coin', 'two component models', 'high probability background words', 'bet high probabilities', 'assign high probabilities', 'assign smaller probabilities', 'topic word distribution', 'topic word distribution', 'different exact probabilities', 'simple algebra problems', 'adjust theta sub', 'mixture model except', 'mixture model estimation', 'common words like', 'two words text', 'two words text', 'simple algebra question', 'background word distribution', 'two probabilities equal', 'precisely two words', 'one distribution assigns', 'simple case like', 'smaller probability given', 'two models', 'text point one', 'smaller probabilities', 'unknown variables', 'observed data', 'two probabilities', 'two probabilities', 'two words', 'different words', 'background distribution', 'two ways', 'two variables', 'two variables', 'two variables', 'two variables', 'two sides', 'common words', 'common words', 'simple expression', 'extremely simple', 'actually end', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'background words', 'background words', 'background words', 'background words', 'high probability', 'high probability', 'word distribution', 'word distribution', 'word distribution', 'one distribution', 'maximum value', 'likelihood function', 'likelihood function', 'must sum', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'background part', 'background model', 'background model', 'background model', 'background model', 'background model', 'content words', 'two distributions', 'unclear whether', 'somewhat larger', 'somewhat larger', 'probalistic model', 'probalistic model', 'point nine', 'point nine', 'observed patterns', 'naive oversimplification', 'much larger', 'much easier', 'model heuristically', 'might feel', 'interesting behaviors', 'instead put', 'gain advantage', 'away etc', '5 multiplied', '2 cases', 'simple case', 'text given', 'point one', 'generating text', 'actual text', 'one word', 'interesting question', 'well supported', 'explained well', 'objective function', 'also means', 'would discourage', 'order distribution', 'probability mass', 'probability mass', 'probability assigned', 'low probability', 'powerful way', 'observed document', 'must make', 'kinds', 'already fixed', 'text would', 'special case', 'case corresponds', 'interesting behavior', 'one kind', 'observing text', 'mathematical fact', 'indeed encourage', 'already know', 'particular need', 'also competing', 'would tend', 'small probability', 'formula intuitively', 'data', 'actually', 'general behavior', 'maximum', 'would set', 'also assume', 'see obviously', 'first look', 'likelihood', 'two', 'probabilities', 'probabilities', 'distribution', 'like', 'algebra', 'must', 'background', 'background', 'words', 'words', 'words', 'use', 'use', 'use', 'use', 'sum', 'sum', 'point', 'model', 'model', 'model', 'model', 'given', 'assigns', 'word', 'word', 'word', 'word', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'one', 'one', 'one', 'question', 'question', 'well', 'precisely', 'precisely', 'function', 'function', 'also', 'also', 'would', 'would', 'would', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'way', 'value', 'tend', 'small', 'obviously', 'observing', 'kind', 'intuitively', 'general', 'formula', 'fixed', 'first', 'equal', 'equal', 'equal', 'end', 'document', 'distributions', 'competing', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'behavior', 'behavior', 'behavior', 'know', 'know', 'indeed', 'indeed', 'fact', 'fact', 'particular', 'particular', 'particular', 'make', 'make', 'make', 'set', 'set', 'set', 'set', 'see', 'see', 'see', 'see', 'see', 'look', 'look', 'look', 'look', 'look', 'order', 'order', 'order', 'order', 'order', 'order', 'assume', 'assume', 'assume', 'assume', 'assume', 'assume', 'assume', 'assume', 'yes', 'write', 'write', 'weak', 'want', 'via', 'values', 'values', 'using', 'using', 'useful', 'useful', 'useful', 'understand', 'understand', 'understand', 'understand', 'understand', 'turns', 'try', 'think', 'things', 'target', 'talk', 'specifically', 'someway', 'solve', 'solve', 'solution', 'simplify', 'similarly', 'sense', 'seen', 'seeing', 'right', 'product', 'product', 'problem', 'problem', 'problem', 'plug', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'others', 'optimize', 'opposite', 'ok', 'obvious', 'notice', 'note', 'naturally', 'motivation', 'moment', 'meaning', 'mean', 'maximize', 'maximize', 'maximize', 'maximize', 'maximize', 'looking', 'let', 'let', 'let', 'let', 'lecture', 'lecture', 'known', 'known', 'knowledge', 'intuition', 'interested', 'interested', 'interaction', 'inside', 'important', 'imagine', 'idea', 'hope', 'hope', 'hope', 'higher', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'goal', 'give', 'generalizable', 'furthermore', 'form', 'follows', 'factoring', 'factored', 'factor', 'factor', 'factor', 'exercise', 'examining', 'examine', 'examine', 'exactly', 'exactly', 'estimate', 'equation', 'equation', 'embed', 'easy', 'easy', 'dictated', 'designed', 'decide', 'customizing', 'customizing', 'course', 'could', 'constraint', 'constraint', 'constraint', 'constraint', 'constant', 'consider', 'competition', 'competition', 'compensate', 'collaborating', 'clearly', 'choosing', 'choosing', 'choose', 'cannot', 'basically', 'basically', 'balanced', 'balance', 'avoid', 'assumed', 'assumed', 'answer', 'another', 'although', 'allocate', 'accounts', 'account', 'able', '1']
lecture 22
['1st every component component model attempts', 'basically means one model must', 'unknown distribution theta sub', 'different component models tend', 'high frequency words tend', 'high frequency words generally', 'assign somewhat higher probability', 'must take away', 'prior would allow us', 'theta sub b', 'contributed one turn', 'effectively get rid', 'likelihood function look like', 'get higher probabilities', 'fixing one component', 'repeated many times', 'bet high probabilities', 'assign high probabilities', 'two component models', 'high frequency words', 'overall result would', 'specialized mixture model', 'maximum likelihood estimator', 'maximum likelihood estimator', 'background language model', 'relatively small impact', 'gives us 0', 'collaboratively maximize likelihood', 'becomes less important', 'zero prior probability', 'would make sense', 'background word distribution', 'theta sub', 'two word document', 'component models', 'component models', 'high probabilities', 'one distribution', 'higher values', 'actually assign', 'two words', 'different words', 'component regulates', 'would allow', 'would allow', 'give us', 'somewhat regulated', 'regularised somewhat', 'different form', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'effectively excludes', 'model parameters', 'background model', 'high probability', 'high probability', 'would make', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'make room', 'frequent words', 'background words', 'one word', 'objective function', 'new function', 'would happen', 'would happen', 'small probability', 'start adding', 'obviously changes', 'normalized frequencies', 'might want', 'little bit', 'estimation problem', 'discovered topic', 'bayesian estimation', 'another occurrence', 'many terms', 'impact would', 'choosing one', 'positive impact', 'important behavior', 'another behavior', 'another behavior', 'less important', 'general phenomenon', 'also encourages', 'add many', 'additional terms', 'additional terms', 'even larger', 'even larger', 'smaller probability', 'probability mess', 'probability mass', 'slight decrease', 'original solution', 'equally likely', 'english documents', 'avoid competition', 'additional occurrences', 'general behavior', 'would know', '5 would', 'special case', 'data point', 'data frequencies', 'also talk', 'also makes', 'larger probability', 'larger probability', 'word occurs', 'optimal solution', 'large collection', 'frequency', 'basically', 'also interesting', 'give text', 'component', 'component', 'component', 'coefficient 0', 'case let', 'one', 'like', 'distribution', 'model', 'model', 'model', 'probabilities', 'estimator', 'likelihood', 'likelihood', 'words', 'words', 'words', 'function', 'would', 'would', 'would', 'sense', 'prior', 'prior', 'prior', 'means', 'means', 'means', 'maximize', 'important', 'background', 'background', 'impact', 'impact', 'behavior', 'less', 'less', 'give', 'general', 'also', 'also', 'word', 'word', 'word', 'word', 'word', 'word', 'terms', 'terms', 'look', 'look', 'look', 'look', 'larger', 'larger', 'larger', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'talk', 'solution', 'solution', 'optimal', 'occurs', 'occurrences', 'makes', 'likely', 'large', 'documents', 'document', 'document', 'document', 'decrease', 'competition', 'collection', 'coefficient', 'data', 'data', 'data', 'case', 'case', 'case', 'let', 'let', 'know', 'know', 'interesting', 'interesting', 'add', 'add', '5', '5', '0', '0', '0', '0', '0', '0', '0', '0', 'text', 'text', 'text', 'choosing', 'choosing', 'choosing', 'whereas', 'well', 'well', 'waste', 'video', 'value', 'using', 'using', 'use', 'understanding', 'try', 'think', 'think', 'think', 'think', 'think', 'think', 'term', 'term', 'term', 'talked', 'surprise', 'summarize', 'sum', 'started', 'since', 'show', 'seems', 'seeing', 'see', 'see', 'see', 'see', 'second', 'scenario', 'scenario', 'say', 'right', 'right', 'right', 'response', 'respond', 'reduced', 'recall', 'question', 'question', 'question', 'prime', 'picture', 'picture', 'pause', 'particular', 'particular', 'order', 'ok', 'observe', 'number', 'multiply', 'multiply', 'multiply', 'moment', 'moment', 'maximizing', 'longer', 'later', 'issue', 'intuitively', 'intuitions', 'influenced', 'indeed', 'increasing', 'increased', 'increase', 'increase', 'increase', 'increase', 'incentive', 'impossible', 'imposing', 'imagine', 'going', 'going', 'game', 'front', 'forth', 'formula', 'favor', 'far', 'fact', 'fact', 'expect', 'example', 'example', 'exactly', 'estimated', 'estimate', 'efficiently', 'distributions', 'discussed', 'discussed', 'discussed', 'discriminative', 'consistent', 'consistent', 'collaboration', 'collaborate', 'chosen', 'chosen', 'change', 'change', 'change', 'change', 'change', 'capture', 'assuming', 'already', 'affected', 'added', 'added', 'account', '9', '9', '9', '3rd', '1', '1', '1', '1']
lecture 23
['initialized parameter values would allow us', 'bayes formula provides provides us', 'two things must happen first', 'parameter values somewhat randomly', 'prediction essentially helped us', 'typical bayesian inference situation', 'consider word like text', 'two component mixture model', 'actually know tentative probabilities', 'background theta sub b', 'unknown topic word distribution', 'single word distribution problem', 'one topic word distribution', 'like many others', 'almost 100 %.', 'use bayes rule', 'apply bayes rule', 'word distribution theta sub', 'probabilistic topic models', 'actually go back', 'maximum likelihood estimator', 'expectation maximization algorithm', 'theta sub b', 'theta sub b', 'maybe text could', 'topic theta sub', 'latent variable z', 'one group would', 'word w sub', 'longer mixture model', 'text would depend', 'maximum likelihood estimate', 'maximum likelihood estimate', 'much higher probability', 'topic word distribution', 'topic word distribution', 'world distribution would', 'guess must also', 'might think well', 'different prior possible', 'word higher probability', 'background word distribution', 'bayes rule', 'allows us', 'bayesian inference', 'gives us', 'z values', 'thing unknown', 'mixture models', 'group would', 'higher likelihood', 'probability values', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'theta sub', 'inference process', 'two probabilities', 'two probabilities', 'mixture model', 'mixture model', 'mixture model', 'would mean', 'would affect', 'actually know', 'two together', 'two priors', 'two groups', 'word probabilities', 'em algorithm', 'em algorithm', 'em algorithm', 'word counts', 'choose word', 'background model', 'background model', 'background model', 'background model', 'word distribution', 'two distributions', 'two distributions', 'would update', 'useful algorithms', 'still possible', 'similar form', 'really know', 'pseudo document', 'principled way', 'familiar scenario', 'fairly straightforward', 'extremely simple', 'denote whether', 'complete specification', 'another product', 'also introduced', 'also called', 'already know', '1st initialize', 'word text', 'word text', 'word text', 'word text', 'word text', 'distribution precisely', 'generating text', 'cases text', 'observed document', 'data likelihood', 'already observe', 'observed text', 'likelihood right', 'think text', 'word given', 'also need', 'say text', 'selection probability', '0 given', 'best guess', 'text data', 'strong prior', 'called prior', 'equally likely', 'equally likely', 'say well', 'basic idea', 'background words', 'observed evidence', 'observed evidence', 'guess text', 'simply take', 'even observe', 'small probability', 'small probability', 'topic', 'simply normalize', 'must', 'imagine perhaps', 'fact making', 'blue words', 'probably see', 'one', 'case let', 'would', 'would', 'actually', 'separate words', 'two', 'theta', 'likelihood', 'probabilities', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'model', 'model', 'world', 'problem', 'problem', 'possible', 'know', 'know', 'estimate', 'estimate', 'background', 'background', 'background', 'also', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'observed', 'z', 'z', 'think', 'think', 'small', 'simply', 'say', 'observe', 'distributions', 'well', 'well', 'well', 'update', 'take', 'probably', 'perhaps', 'need', 'making', 'imagine', 'gives', 'even', 'data', 'data', 'blue', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'given', 'given', 'given', 'evidence', 'evidence', 'evidence', 'guess', 'guess', 'guess', 'guess', 'guess', 'guess', 'guess', 'guess', 'right', 'right', 'normalize', 'normalize', 'fact', 'fact', 'case', 'case', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'likely', 'likely', 'likely', 'likely', 'likely', 'separate', 'separate', 'separate', 'idea', 'idea', 'idea', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'let', 'let', 'let', 'let', 'see', 'see', 'see', 'see', 'see', 'zero', 'work', 'want', 'wait', 'using', 'used', 'used', 'used', 'used', 'used', 'understanding', 'typically', 'try', 'try', 'texture', 'tend', 'sure', 'suppose', 'step', 'start', 'specifically', 'specifically', 'solve', 'solid', 'similarly', 'shown', 'shown', 'separating', 'selected', 'secondly', 'saying', 'regarded', 'realize', 'question', 'quantify', 'probabilistically', 'prime', 'practice', 'particular', 'part', 'parameters', 'parameters', 'parameters', 'order', 'multiply', 'moment', 'moment', 'mining', 'means', 'means', 'maximize', 'maximize', 'look', 'lecture', 'lecture', 'lecture', 'known', 'known', 'known', 'known', 'known', 'known', 'known', 'kind', 'kind', 'intuitively', 'introduce', 'interested', 'interested', 'infer', 'infer', 'infer', 'indeed', 'however', 'high', 'high', 'hand', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'get', 'generated', 'generated', 'generated', 'generated', 'generated', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'flipping', 'family', 'factor', 'explained', 'explained', 'example', 'etc', 'estimating', 'equivalently', 'equal', 'end', 'end', 'easy', 'discussion', 'dictated', 'crucial', 'continue', 'computing', 'computing', 'compute', 'compare', 'compare', 'combine', 'combine', 'combine', 'color', 'coin', 'clustering', 'choosing', 'call', 'call', 'belief', 'basically', 'assumed', 'assume', 'assume', 'application', 'although', 'although', 'alright', 'affected', 'adjust', 'adjust', 'able']
lecture 24
['invoke another spec step called', 'initialized values would allow us', 'binary variable would indicate whether', 'covered background versus content words', 'e step would give us', 'really variable z hidden variable', 'invoke e step followed', 'e step 2 gas', 'topic word distributions theta sub', 'parameters would allow us', 'initialization says uniform distribution', 'additional information like z', 'sure whether taxes', 'might wonder whether', 'theater super b', 'point three three', 'help us solve', 'know exactly whether', 'log like code', 'hill climbing algorithm', 'parameter values randomly', 'often randomly set', 'e step based', 'assigns high probabilities', 'covered background words', 'less likely probably', 'hidden variable values', 'would gradually improve', 'common words like', 'allocated counts toward', 'use bayes rule', 'would actually happen', 'discriminating world distribution', 'text mining algorithm', 'high probability like', 'different guest probabilities', 'word distribution right', 'give us', 'simply take advantage', 'binary variable', 'somewhat improved estimate', 'inferred word split', 'called e', 'background world distribution', 'additional information', 'right data counts', 'hidden variable', 'hidden variable', 'hidden variable', 'theta sub', 'help us', 'help us', 'uniform distribution', 'would happen', 'two distributions', 'latent variable', 'distribution like', 'initialization stage', 'cedar sub', 'e step', 'e step', 'e step', 'another generation', 'another generation', 'algorithm would', 'would work', 'would lead', 'z values', 'z values', 'z values', 'z values', 'probabilities would', 'inferred values', 'z etc', 'z attached', 'last rule', 'word distribution', 'word distribution', 'using e', 'counts toward', 'hidden variables', 'hidden variables', 'z value', 'guess says', 'take advantage', 'log likelihood', 'em algorithm', 'em algorithm', 'em algorithm', 'em algorithm', 'em algorithm', 'social values', 'random values', 'possible values', 'initial values', 'status update', 'simply normalize', 'relevant statistics', 'primary goal', 'practice mate', 'negative becausw', 'may need', 'main goal', 'local maximum', 'last column', 'last column', 'iteratively improve', 'full council', 'explain later', 'expectation maximization', 'even though', 'em algorithms', 'basically captured', '4 must', '100 %.', 'topic distribution', 'word counts', 'one distribution', 'background model', 'background model', 'equal probabilities', 'different probabilities', 'different probabilities', 'different words', '4 words', 'improved setting', 'step first', 'soft split', 'equally likely', 'data accounts', 'also interesting', 'also illustrate', 'new generation', 'new generation', 'new generation', 'new generation', 'new generation', 'two models', 'take logarithm', 'negative value', 'likelihood function', 'improved parameters', 'word given', 'one text', 'old one', 'probabilities using', 'em formulas', 'one use', 'specific case', 'initial estimate', 'also useful', 'explained well', 'group words', 'really', 'indicate', 'first iteration', 'content', 'also see', 'also see', 'general idea', 'would', 'would', 'would', 'e', 'theta', 'like', 'distribution', 'right', 'step', 'step', 'step', 'step', 'step', 'step', 'step', 'algorithm', 'word', 'word', 'word', 'word', 'values', 'values', 'values', 'values', 'text', 'set', 'improve', 'different', 'actually', 'background', 'background', 'background', 'background', 'background', 'background', 'probabilities', 'probabilities', 'probabilities', 'generation', 'counts', 'counts', 'counts', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'new', 'world', 'world', 'world', 'use', 'use', 'split', 'split', 'likely', 'likely', 'data', 'data', 'also', 'also', 'variables', 'value', 'using', 'two', 'take', 'take', 'take', 'likelihood', 'know', 'know', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'one', 'one', 'one', 'useful', 'setting', 'normalize', 'iteration', 'idea', 'given', 'formulas', 'first', 'first', 'probability', 'probability', 'probability', 'probability', 'probability', 'group', 'group', 'general', 'general', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'see', 'see', 'see', 'see', 'see', 'see', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'case', 'case', 'case', 'well', 'well', 'well', 'well', 'guess', 'guess', 'guess', 'guess', 'zero', 'zero', 'video', 'used', 'used', 'used', 'try', 'together', 'think', 'talked', 'superscripts', 'sunday', 'start', 'slide', 'showed', 'show', 'separate', 'say', 'said', 'said', 'revise', 'repeat', 'repeat', 'reaching', 'rather', 'product', 'problem', 'predicting', 'predict', 'picture', 'percentage', 'pause', 'order', 'ok', 'occurrence', 'observe', 'note', 'note', 'note', 'normalized', 'n', 'n', 'multiplied', 'multiplication', 'means', 'mainly', 'made', 'lot', 'look', 'let', 'let', 'known', 'kind', 'introduce', 'interested', 'interested', 'interested', 'initialize', 'initialize', 'infer', 'indicates', 'indeed', 'increasing', 'including', 'improvement', 'imagine', 'hope', 'hardware', 'happened', 'hand', 'hand', 'guesses', 'guarantee', 'going', 'going', 'going', 'going', 'going', 'going', 'go', 'get', 'get', 'get', 'get', 'generated', 'generated', 'generate', 'forth', 'formula', 'follows', 'extent', 'extent', 'example', 'example', 'example', 'example', 'essence', 'easily', 'done', 'document', 'digest', 'difference', 'course', 'course', 'course', 'count', 'count', 'compute', 'computation', 'compare', 'compare', 'come', 'come', 'come', 'come', 'come', 'collect', 'clustering', 'cause', 'call', 'bridge', 'believed', 'believed', 'believed', 'believed', 'believe', 'becomes', 'average', 'augment', 'assumed', 'assume', 'adjust', 'adjust', 'adjust', 'add', '1', '1', '1', '1', '0', '0', '0']
lecture 25
['original lighter holder curve', 'original like role function', 'original like record function', 'stable converged parameter values', 'good initial starting point', 'really covered yet', 'higher like recorder', 'estimated parameter values', 'like roller function', 'latent variable values', 'many local maxima', 'thought data augmentation', 'actual global maximum', 'useful hidden variables', 'simple mixture model', 'cannot easily find', 'parameter value given', 'current gas unless', 'hill climb algorithm', 'initial points', 'starting point', 'mixture model', 'hidden variables', 'hidden variable', 'predicting values', 'possible values', 'stable value', 'hill climbing', 'mixture models', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'local maximum', 'local maximum', 'local maximum', 'current generation', 'current gas', 'current gas', 'x dimension', 'two things', 'two steps', 'two distributions', 'random guess', 'numerical optimization', 'numerical algorithm', 'next guest', 'next gas', 'multiple times', 'maximum regular', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'lower bound', 'events probabilistically', 'even though', 'em algorithm', 'em algorithm', 'em algorithm', 'em algorithm', 'done probabilistically', 'different points', 'detailed explanation', 'climbing algorithm', 'augmented data', 'analytical solution', 'always better', 'allows us', 'would use', 'would mean', 'would make', 'would exploit', 'would depend', 'two would', 'generally would', 'new point', 'another point', 'end step', 'e step', 'e step', 'general algorithm', 'top two', 'say exactly', 'much easier', 'helps determine', 'difficult problem', 'basically part', 'already know', 'probability distribution', 'little converge', 'general idea', 'world counts', 'move parameters', 'like', 'parameters also', 'parameter', 'definitely improve', 'function', 'curve', 'maximum', 'gas', 'data', 'algorithm', 'would', 'point', 'point', 'point', 'likelihood', 'step', 'general', 'find', 'find', 'value', 'value', 'value', 'world', 'top', 'say', 'problem', 'parameters', 'parameters', 'move', 'know', 'idea', 'easier', 'determine', 'definitely', 'counts', 'climb', 'climb', 'climb', 'basically', 'distribution', 'distribution', 'also', 'also', 'converge', 'converge', 'converge', 'improve', 'improve', 'improve', 'improve', 'improve', 'improve', 'works', 'well', 'way', 'way', 'way', 'way', 'used', 'try', 'theoretically', 'terms', 'summarize', 'stuck', 'start', 'start', 'start', 'start', 'start', 'split', 'split', 'somewhere', 'simplify', 'showed', 'set', 'set', 'see', 'see', 'see', 'see', 'satisfied', 'roughly', 'right', 'right', 'right', 'resolve', 'require', 'repeat', 'reason', 'reaching', 'reached', 'proved', 'properties', 'order', 'order', 'optimize', 'optimal', 'one', 'one', 'one', 'note', 'necessary', 'mri', 'moving', 'means', 'maximizing', 'maximize', 'maximize', 'maximize', 'maximise', 'map', 'made', 'likely', 'let', 'left', 'lecture', 'knowledge', 'kinds', 'introduce', 'insured', 'inequalities', 'increase', 'improving', 'improved', 'imagine', 'illustration', 'illustration', 'hope', 'hope', 'happened', 'hand', 'guaranteed', 'guaranteed', 'gradually', 'gone', 'going', 'going', 'generate', 'fix', 'fit', 'first', 'figure', 'example', 'example', 'estimation', 'estimate', 'estimate', 'estimate', 'equal', 'empirically', 'eml', 'eml', 'direct', 'course', 'convert', 'computing', 'computed', 'computed', 'compute', 'causes', 'cause', 'case', 'case', 'case', 'axis', 'augmenting', 'actually', 'achieve']
lecture 26
['lambda sub b versus non background', 'probability 1 minus lambda sub b', 'one minus lambda b gives us', 'topic category characterization seedies hci', 'introduce probabilistic latent semantic analysis', 'probabilistic latent semantic analysis', 'see lambda sub b', 'find applied math problem', 'lambda sub b', 'topical theta sub k', 'topic theta sub k', 'constrained optimization problem like', 'see two interesting kinds', 'first topic popular like', 'two things must happen', 'two component mixture model', 'two things happen', 'also see background words', 'many useful applications', 'problem many times', 'w given sit', 'basic topic model', 'maximum likelihood estimator', 'complete likelihood function', 'basic topic models', 'background language model', 'would also allow', 'useful topic models', 'analyze text data', 'one topic besides', 'would believe exist', 'two way switch', 'first line shows', 'mining multiple topics', 'mine multiple topics', 'example government response', 'many unknown parameters', 'likelihood function shown', 'make another decision', 'lambda multiplied', 'question would help', 'text data right', 'topic analysis', 'topic coverage distribution', 'two component', 'probabilistic modeling', 'optimization problem', 'tell us', 'set us', 'two kinds', 'two kinds', 'two kinds', 'like holder', 'like hold', 'mixture model', 'another topic', 'mixture models', 'topical model', 'government response', 'government response', 'useful exercise', 'topical would', 'background topical', 'background help', 'many premise', 'likelihood function', 'likelihood function', 'topic models', 'k among', 'key way', 'background model', 'background model', 'background model', 'background model', 'two topics', 'words must', 'unknown value', 'unknown primers', 'k distributions', 'two constraints', 'k topics', 'k topics', 'k topics', 'text data', 'text data', 'also ask', 'would want', 'topic coverage', 'background words', 'many variables', 'vocabulary set', 'usual thing', 'unknown parameters', 'set empirically', 'really key', 'parameter estimation', 'p lsa', 'often called', 'new orlean', 'make sure', 'hurricane katrina', 'formal definition', 'familiar form', 'different possibilities', 'different design', 'different constraint', 'already introduced', 'first examine', 'multiple topics', 'using one', 'choose one', 'parameters right', 'multiway switch', 'important parameters', 'important parameters', '141 distribution', 'first one', 'sample article', 'little bit', 'little bit', 'important formula', 'important formula', 'blog article', 'awarded distributions', 'awarded distributions', 'second topic', 'topics behind', 'sample topics', 'different topics', 'actually choose', 'previous slide', 'decide whether', 'cover precisely', 'closer look', 'also assume', 'already said', 'whole collection', 'use words', 'large collection', 'first look', 'water distribution', 'water distribution', 'three terms', 'really understand', 'earlier lecture', 'document covers', 'anna document', 'seen earlier', 'next line', 'world w', 'world w', 'text documents', 'sum involves', 'actually obtained', 'actually choosing', 'first let', 'word distributions', 'things', 'use plsa', 'problem', 'problem', 'topic', 'topic', 'topic', 'topic', 'topic', 'must', 'model', 'multiple', 'models', 'also', 'also', 'would', 'would', 'background', 'background', 'background', 'background', 'background', 'see', 'see', 'see', 'see', 'see', 'words', 'words', 'w', 'shown', 'right', 'maximum', 'make', 'line', 'function', 'function', 'exist', 'example', 'decision', 'first', 'first', 'first', 'first', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'switch', 'switch', 'parameters', 'parameters', 'parameters', 'coverage', 'coverage', 'distribution', 'distribution', 'distribution', 'text', 'text', 'text', 'text', 'text', 'water', 'question', 'question', 'little', 'formula', 'distributions', 'distributions', 'article', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'actually', 'actually', 'variables', 'slide', 'said', 'precisely', 'obtained', 'look', 'look', 'documents', 'decide', 'constraints', 'choosing', 'use', 'use', 'use', 'collection', 'collection', 'collection', 'word', 'word', 'understand', 'understand', 'terms', 'terms', 'lecture', 'lecture', 'document', 'document', 'document', 'document', 'assume', 'assume', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'seen', 'seen', 'seen', 'second', 'second', 'second', 'next', 'next', 'next', 'world', 'world', 'world', 'world', 'world', 'world', 'world', 'world', 'sum', 'sum', 'sum', 'sum', 'plsa', 'plsa', 'plsa', 'plsa', 'let', 'let', 'let', 'let', 'worry', 'values', 'used', 'understanding', 'typically', 'trying', 'try', 'together', 'think', 'think', 'tests', 'tasks', 'talk', 'talk', 'take', 'switches', 'summarization', 'submit', 'stress', 'still', 'specifically', 'someone', 'solutions', 'simply', 'similarly', 'similarly', 'similar', 'similar', 'showed', 'show', 'serious', 'separate', 'sentences', 'segmentation', 'segment', 'seeing', 'say', 'rest', 'rest', 'represents', 'recall', 'put', 'public', 'product', 'probabilities', 'point', 'plug', 'pies', 'pies', 'pie', 'pie', 'picture', 'picture', 'percentage', 'output', 'output', 'output', 'otherwise', 'ok', 'ok', 'observing', 'observing', 'observing', 'observing', 'number', 'notice', 'need', 'need', 'naturally', 'mixed', 'made', 'lot', 'lot', 'log', 'known', 'knowing', 'know', 'know', 'know', 'kind', 'interested', 'input', 'indeed', 'indeed', 'illustrated', 'illustrate', 'ideas', 'idea', 'hope', 'happens', 'going', 'going', 'going', 'going', 'goal', 'generating', 'generating', 'generated', 'generated', 'generate', 'generate', 'generate', 'generate', 'generally', 'general', 'followed', 'flooding', 'flooding', 'flooding', 'flipping', 'flip', 'figure', 'figure', 'figure', 'figure', 'figure', 'figure', 'figure', 'figure', 'extent', 'excuse', 'exactly', 'etc', 'etc', 'estimate', 'estimate', 'estimate', 'essentially', 'end', 'donation', 'donation', 'discussion', 'discover', 'difference', 'difference', 'detail', 'detail', 'decode', 'decided', 'criticism', 'covering', 'course', 'contains', 'consequences', 'components', 'complicated', 'complicated', 'compare', 'color', 'coin', 'coin', 'clustering', 'city', 'city', 'city', 'chosen', 'choice', 'characterized', 'characterize', 'cause', 'cases', 'cases', 'calculate', 'cake', 'basically', 'available', 'asked', 'added', 'accounting', 'account', '2nd']
lecture 27
['hidden variable z sub dw indicates whether word', 'k unigram language models representing k topics', 'actually helps us better understand', 'coverage parameters -- pis --', 'keep counting various events', 'predetermined background language model', 'know whether likelihood converges', 'accumulated counts various counts', 'help discover discriminating topics', 'hidden variable z', 'generate computed expected count', 'gives us different distributions', 'maximum likelihood estimator based', 'k plus one values', 'case plsa allows us', 'inferred z values', 'hidden variable values', 'equation allows us', 'background language model', 'maximum likelihood estimator', 'discover topical knowledge', 'topic theta sub j', 'e step formulas essentially', 'also aggregate topics covered', 'maximum regular estimated', 'would allow us', 'discover two things', 'unknown parameters randomly', 'slightly different form', 'keep count counter', 'say stop right', 'would give us', 'topic theta sub', 'topic theta sub', 'topic theta sub', 'topic theta sub', 'many useful applications', 'hidden variable', 'potentially different pis', 'remember pis indicate', 'particular time period', 'actually general phenomenon', 'e step result', 'e step formulas', 'also cluster terms', 'higher probability words', 'allocated word counts', 'theta sub', 'theta sub', 'theta sub', 'split words among', 'plsa model', 'word whether', 'likelihood converges', 'k topics', 'tells us', 'tells us', 'tells us', 'time period', 'keep track', 'help attract', 'hidden variables', 'mixture model', 'estimated based', 'two values', 'previous likelihood', 'might give', 'current likelihood', 'also added', '1 distributions', 'right counts', 'relevant counts', 'allocated counts', 'word distributions', 'word distributions', 'different ways', 'renormalize based', 'particular parameter', 'event based', 'discounted count', 'discounted count', 'world multiplied', 'topics written', 'term clusters', 'temporal chains', 'representing', 'intentionally leave', 'first initialize', 'em algorithms', 'detailed characterization', 'compute likelihood', 'common terms', 'change much', 'bayes rule', 'basically assessing', 'b denoting', 'particular topic', 'particular topic', 'e step', 'e step', 'e step', 'estimate parameters', 'different documents', 'normalized among', 'actually normalizing', 'actually depends', 'randomly normalize', 'topic indicator', 'topic cluster', 'topic cluster', 'word belongs', 'whether', 'generating word', 'different guess', 'em algorithm', 'em algorithm', 'em algorithm', 'em algorithm', 'em algorithm', 'particular author', 'initialization step', 'text data', 'normalize among', 'documents associated', 'cluster documents', 'augmented data', 'one cluster', 'bottom one', 'bottom one', 'predicted probability', 'may recall', 'using exactly', 'simply normalize', 'highest pi', 'coverage probability', 'k', 'k', 'k', 'word w', 'take advantage', 'concise way', 'see basically', 'similarly documents', 'word distribution', 'word distribution', 'word distribution', 'parameters', 'formulas', 'values', 'use document', 'likelihood', 'essentially', 'also', 'aggregate', 'counts', 'counts', 'would', 'pis', 'pis', 'pis', 'actually', 'actually', 'e', 'e', 'different', 'different', 'based', 'count', 'count', 'two', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'terms', 'split', 'right', 'indicate', 'generate', 'generate', 'covered', 'covered', 'cluster', 'background', 'background', 'background', 'allocated', 'coverage', 'coverage', 'coverage', 'coverage', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'algorithm', 'algorithm', 'step', 'step', 'step', 'step', 'step', 'step', 'step', 'step', 'step', 'words', 'words', 'words', 'words', 'useful', 'useful', 'general', 'general', 'documents', 'documents', 'data', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'w', 'use', 'similarly', 'recall', 'probability', 'probability', 'probability', 'probability', 'probability', 'normalizing', 'normalized', 'guess', 'depends', 'compute', 'author', 'case', 'case', 'case', 'case', 'case', 'case', 'j', 'j', 'j', 'j', 'j', 'j', 'j', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'variables', 'variables', 'using', 'using', 'pi', 'pi', 'normalize', 'normalize', 'normalize', 'normalize', 'normalize', 'normalize', 'way', 'way', 'way', 'take', 'take', 'take', 'see', 'see', 'see', 'see', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'well', 'well', 'typically', 'together', 'tied', 'think', 'think', 'thetas', 'technique', 'summarize', 'sum', 'specifically', 'sorry', 'show', 'selecting', 'seen', 'said', 'represented', 'repeat', 'regarded', 'regarded', 'quantity', 'quantities', 'put', 'proportional', 'proportion', 'probabilities', 'presenting', 'prediction', 'prediction', 'predicting', 'predicting', 'predict', 'predict', 'phenomena', 'part', 'obtain', 'observing', 'observing', 'note', 'normalizer', 'normalization', 'normalization', 'namely', 'multiply', 'make', 'lot', 'look', 'look', 'likely', 'introduced', 'introduce', 'interested', 'interested', 'index', 'improve', 'implementation', 'implementation', 'implement', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'go', 'get', 'get', 'get', 'get', 'generated', 'generated', 'fact', 'extent', 'extent', 'extent', 'explained', 'explained', 'exercise', 'example', 'example', 'etc', 'end', 'enable', 'detail', 'desirable', 'denote', 'contrast', 'constraint', 'computation', 'completing', 'comparison', 'compare', 'come', 'collecting', 'collect', 'collect', 'collect', 'clustered', 'characterize', 'cases', 'belonging', 'begin', 'augment', 'assign', 'assign', 'assess', 'analysis', 'already', 'allocate', 'affect', 'addition']
lecture 28
['generative model fully generated model', 'dirichlet prior dirichlet distribution based', 'high pseudocounts similar life would', 'two specific extreme cases', 'would assign high probabilities', 'strongly favor certain kind', 'using maximum likelihood estimator', 'prior called conjugate prior', 'tags topic tags assigned', 'also called map estimate', 'make one distribution fixed', 'latent dirichlet allocation', 'latent dirichlet allocation', 'latent dirichlet allocation', 'generated using topics corresponding', 'background language model', 'background language model', 'model setting would', 'assign high probabilities', 'tags already assigned', 'cover two things', 'maximum likelihood estimator', 'two functional forms', 'also high pseudocounts', 'mean conjugate prior', 'would allow us', 'world distribution right', 'use map estimator', 'contain one distribution', 'similar em algorithm', 'force particular choice', 'choose topic one', 'contain one topic', 'say supplier says', 'additional pseudo data', 'precisely 1 background', 'also would listen', 'use prior together', 'smallest parameters reflect', 'estimate word distributions', 'user may also', 'analyze text data', 'add mu multiplied', 'posterior distribution probability', 'force one distribution', 'see retrieval models', 'see example later', 'user controlled plsa', 'assigned tags', 'map estimator', 'would mean', 'would make', 'strongly suggests', 'use maximum', 'background distribution', 'also controlled', 'would affect', 'topics corresponding', 'em algorithm', 'em algorithm', 'certain number', 'favor set', 'user generated', 'information retrieval', 'functional form', 'would happen', 'would happen', 'would happen', 'also may', 'specific example', 'computed using', 'prior says', 'prior right', 'may recall', 'may know', 'one dominate', 'certain aspects', 'many pseudocounts', 'also use', 'battery life', 'battery life', 'battery life', 'battery life', 'total sum', 'theoretically speaking', 'special form', 'pi value', 'even force', 'entirely focused', 'entirely concentrated', 'deal works', 'continue talking', 'changes happened', 'cannot attract', 'bayesian inference', 'analyzing reviews', 'additional counts', 'additional knowledge', 'posteriori estimate', 'one way', 'prior distribution', 'set one', 'parameter mu', 'also need', 'also listens', 'also impose', 'conjugated prior', 'battery obviously', 'may want', 'generated document', 'topic models', 'topic coverage', 'third topic', 'particularly interested', 'particularly interested', 'first let', 'elegant way', 'connected counts', 'best way', 'basically convert', '0 pseudocounts', 'set somewhere', 'w given', 'user might', 'standard plsa', 'sense indicates', 'positive infinity', 'non zero', 'might expect', 'including zero', 'artificially inflated', 'apply plsa', '0 according', 'estimate parameters', 'fit data', 'prior preferences', 'prior preferences', 'set mu', 'set mu', 'basically let', 'work well', 'example use', 'prior knowledge', 'prior knowledge', 'prior knowledge', 'practice mu', 'blindly listen', 'document must', 'reasonable way', 'particular words', 'tag could', 'based', 'two', 'likelihood', 'similar', 'similar', 'map', 'kind', 'generated', 'generated', 'cases', 'assigned', 'using', 'using', 'make', 'life', 'probabilities', 'probabilities', 'also', 'also', 'one', 'one', 'one', 'one', 'pseudocounts', 'pseudocounts', 'pseudocounts', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'user', 'user', 'reflect', 'precisely', 'particular', 'models', 'force', 'force', 'analyze', 'allow', '1', 'use', 'use', 'use', 'estimate', 'estimate', 'estimate', 'estimate', 'mu', 'mu', 'mu', 'listen', 'listen', 'happen', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'battery', 'battery', 'way', 'way', 'topics', 'topics', 'topics', 'topics', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'see', 'see', 'see', 'see', 'preferences', 'let', 'interested', 'distributions', 'distributions', 'counts', 'basically', 'add', 'add', 'set', 'set', 'set', 'parameters', 'parameters', 'parameters', 'knowledge', 'knowledge', 'knowledge', 'zero', 'zero', 'want', 'tag', 'sense', 'reasonable', 'practice', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'need', 'might', 'might', 'listens', 'infinity', 'impose', 'given', 'could', 'blindly', 'aspects', 'artificially', '0', '0', 'example', 'example', 'example', 'example', 'example', 'example', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'well', 'well', 'say', 'say', 'say', 'say', 'say', 'say', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'useful', 'useful', 'used', 'used', 'turns', 'treated', 'think', 'think', 'talk', 'talk', 'suppose', 'strong', 'strength', 'step', 'step', 'sometimes', 'seen', 'second', 'said', 'result', 'remove', 'question', 'priors', 'priors', 'price', 'prevent', 'preference', 'precise', 'plan', 'plan', 'pis', 'picture', 'participate', 'p', 'opinions', 'needs', 'needed', 'much', 'modifying', 'modification', 'memory', 'maximize', 'looking', 'look', 'like', 'led', 'lecture', 'lecture', 'lecture', 'lda', 'lda', 'lda', 'laptop', 'laptop', 'lambda', 'kinds', 'intuitively', 'insight', 'inject', 'influence', 'influence', 'incorporated', 'incorporate', 'impossible', 'impossible', 'guide', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'give', 'give', 'get', 'generating', 'generate', 'formula', 'fact', 'fact', 'extensions', 'extend', 'extend', 'expectations', 'exactly', 'essentially', 'equivalent', 'enforce', 'end', 'encoded', 'encode', 'encode', 'effect', 'effect', 'documents', 'difference', 'development', 'denoted', 'denominator', 'definitely', 'defined', 'define', 'covered', 'covered', 'course', 'course', 'course', 'counting', 'convenient', 'constraints', 'constraints', 'constraints', 'consequences', 'computes', 'computation', 'combined', 'combine', 'combination', 'collect', 'changed', 'cause', 'case', 'case', 'case', 'case', 'case', 'believe', 'analysis', 'adding', 'added', 'accuracy', '2']
lecture 29
['use algorithm called collapsed gibbs sampling', 'might favor generating skewed coverage', 'best basis test setup', 'really represents global maximum', 'would work equally well', 'approach maximum likelihood estimator', '2 dirichlet distributions respectively', 'training data looks like', 'lda formula would add', 'dirichlet distribution tell us', 'work around though', 'good local maximum', 'many local maximum', 'maximum likelihood estimated', 'general principal way', 'could possibly draw', 'local maxima problem', 'modeling future data', 'explaining future data', 'relatively uniform distribution', 'n values corresponding', 'force certain choices', 'really generating model', 'probabilistic topic models', 'probabilistic topic models', 'also output proportions', 'also often adequate', 'use different toolkits', 'take tax data', 'topic modeling problem', 'k parameters corresponding', 'estimated using exactly', 'multiple word distributions', 'basic topic model', 'basic topic model', 'another dirichlet distribution', 'use bayesian inference', 'many parameters exactly', 'topic word distributions', 'topic word distributions', 'many fewer parameters', 'might favor', 'give similar performance', 'algorithm looks', 'lda versus plsa', 'tell us', 'training data', 'training data', 'would worry', 'would overfit', 'k values', 'dirichlet distributions', 'em algorithm', 'might think', 'might find', 'training documents', 'n words', 'multiple times', 'elegant way', 'draw one', 'topic coverage', 'topic coverage', 'dirichlet distribution', 'dirichlet distribution', 'dirichlet distribution', 'dirichlet distribution', 'dirichlet distribution', 'word distributions', 'word distributions', 'word distributions', 'gives us', 'gives us', 'many methods', 'models provide', 'bayesian version', 'appealing models', 'another distribution', 'topic models', 'topic models', 'also find', 'use phrases', 'necessary problem', 'likelihood functions', 'likelihood function', 'likelihood function', 'likelihood function', 'likelihood function', 'bayesian inference', 'word distribution', 'special distribution', 'dirichlet prior', 'many parameters', 'many parameters', 'many parameters', 'word inside', 'whole collection', 'third one', 'suggested readings', 'previous slide', 'nice review', 'intuitively suggest', 'generation process', 'full treatment', 'empirical comparison', 'different ways', 'core assumption', 'close connection', 'brief idea', 'already seen', 'vector also', 'topic multiplied', 'also important', 'important formula', 'another vector', 'future document', 'fewer parameters', 'fewer parameters', 'distributions free', 'allow us', 'many applications', 'might see', 'posterior inference', 'inference part', 'mixture model', 'generative model', 'topic model', 'much fewer', 'text mining', 'text mining', 'specify prior', 'one vector', 'one vector', 'longer parameters', 'lda formula', 'another integral', 'also see', 'also means', 'various tasks', 'topics covered', 'perform similarly', 'key topics', 'imposing priors', 'free launch', 'different extensions', 'depth introduction', 'computed based', 'automatically label', 'analyzing topics', '2nd paper', 'model parameters', 'lda improves', 'lda formulas', 'introduce lda', 'derive lda', 'plsa intended', 'plsa component', 'possible pi', 'particular choice', 'drawing pi', 'components added', 'always interested', 'similar things', 'background model', 'cannot compute', 'cannot compute', 'new document', 'first equation', 'plsa model', 'beta parameters', 'theoretically lda', 'make plsa', 'generalize plsa', 'explain plsa', 'parameters alpha', 'next let', 'algorithm', 'example pis', 'see plsa', 'data', 'generating', 'generating', 'coverage', 'distributions', 'dirichlet', 'exactly', 'use', 'use', 'use', 'use', 'problem', 'likelihood', 'formula', 'distribution', 'distribution', 'distribution', 'word', 'word', 'word', 'output', 'often', 'force', 'different', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'inference', 'inference', 'model', 'model', 'model', 'model', 'model', 'model', 'vector', 'vector', 'using', 'using', 'take', 'take', 'prior', 'mining', 'important', 'give', 'give', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'topics', 'topics', 'topics', 'theoretically', 'tasks', 'similarly', 'paper', 'next', 'much', 'make', 'label', 'imposing', 'generalize', 'free', 'free', 'extensions', 'explain', 'example', 'depth', 'based', 'background', 'applications', 'allow', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'lda', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'pi', 'pi', 'pi', 'pi', 'interested', 'interested', 'integral', 'integral', 'choice', 'choice', 'added', 'added', 'similar', 'similar', 'similar', 'similar', 'similar', 'similar', 'similar', 'similar', 'compute', 'compute', 'compute', 'compute', 'compute', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'means', 'means', 'means', 'let', 'let', 'let', 'first', 'first', 'first', 'document', 'document', 'document', 'document', 'document', 'document', 'beta', 'beta', 'beta', 'alpha', 'alpha', 'alpha', 'alpha', 'pis', 'pis', 'pis', 'pis', 'pis', 'pis', 'world', 'whereas', 'want', 'want', 'vocabulary', 'vectors', 'vectors', 'variables', 'unfortunately', 'understand', 'understand', 'uncertainties', 'top', 'time', 'time', 'tied', 'theta', 'theta', 'theta', 'texts', 'terms', 'tend', 'techniques', 'talking', 'talk', 'talk', 'talk', 'summarize', 'sum', 'sum', 'stressed', 'start', 'spend', 'specifically', 'something', 'something', 'simplicity', 'simplicity', 'shown', 'shown', 'set', 'set', 'sense', 'secondly', 'scope', 'sample', 'right', 'right', 'rest', 'resort', 'represented', 'remove', 'rely', 'regularized', 'read', 'proposed', 'prone', 'product', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probabilities', 'price', 'present', 'precisely', 'practice', 'practice', 'possibilities', 'pies', 'picture', 'parameter', 'papers', 'overfitting', 'others', 'order', 'observing', 'note', 'needed', 'need', 'namely', 'motivate', 'making', 'lot', 'looking', 'likely', 'likely', 'led', 'least', 'know', 'know', 'intractable', 'integrals', 'instead', 'instead', 'input', 'influence', 'influence', 'improve', 'impose', 'impose', 'illustrate', 'however', 'however', 'however', 'heuristic', 'hard', 'govern', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'goal', 'go', 'getting', 'generate', 'generally', 'generality', 'form', 'flexible', 'fixed', 'fitting', 'fitting', 'fit', 'fact', 'fact', 'face', 'face', 'extension', 'extension', 'extend', 'essentially', 'essentially', 'especially', 'end', 'end', 'enables', 'easy', 'dropped', 'drawn', 'drawn', 'drawn', 'drawn', 'discussion', 'discussion', 'difference', 'difference', 'detail', 'depending', 'deficiency', 'course', 'course', 'course', 'course', 'course', 'copied', 'convenience', 'controlling', 'controlled', 'controlled', 'consider', 'conclusion', 'computation', 'complicated', 'complicated', 'complicated', 'complexity', 'complex', 'choosing', 'choose', 'characterized', 'characterize', 'change', 'change', 'cases', 'case', 'case', 'case', 'care', 'beyond', 'betas', 'basically', 'basically', 'available', 'asked', 'approximate', 'alphas', 'alphas', 'algorithmically', 'actually', 'actually', 'achieve', 'account', 'account']
lecture 30
['also cluster fairly large text law gets', 'becausw clusters likely represent different senses', 'link similar text objects together', 'might group authors together based', 'mean text objects may contain', 'help us remove redundancy', 'helps provide additional discrimination', 'may also use text clustering', 'group similar texture objects together', 'also clearly tells us', 'clustering help us achieve', 'may also use text', 'also cluster articles written', 'particularly exploratory text analysis', 'group similar objects together', 'would give us cluster', 'might get different clusters', 'get different clustering results', 'large text objects', 'two words like car', 'really see text objects', 'clustering problem well defined', 'well defined clustering problem', 'might also represent', 'induce additional features', 'would give us', 'discover interesting clusters', '2 dimensional space', 'represent text data', 'course texture objects', 'text clustering interesting', 'combining text clustering', 'text mining algorithms', 'natural groups well', 'best clustering result', 'group similar objects', 'might also feel', 'two words similar', 'cluster documents together', 'discover natural structures', 'must use perspective', 'whole text collection', 'removing duplicated documents', 'sometimes also want', 'feature value would', 'cluster email messages', 'general many applications', 'cluster search results', 'see overall structure', 'user must define', 'might cluster websites', 'text clusters', 'see another example', 'grouped together', 'text clustering', 'text clustering', 'text clustering', 'text clustering', 'text clustering', 'text clustering', 'text clustering', 'text objects', 'texture classification', 'group objects', 'well defined', 'well defined', 'objects based', 'text mining', 'clustering results', 'clearly defined', 'similar objects', 'three clusters', 'three clusters', 'major clusters', 'looks like', 'email messages', 'clustering result', 'search results', 'also seen', 'text object', 'two objects', 'still group', 'different ways', 'different levels', 'text data', 'text data', 'text data', 'grouped based', 'particularly useful', 'natural structures', 'different way', 'different way', 'objects might', 'clustering problem', 'results returned', 'overall content', 'many examples', 'future value', 'duplicated content', 'clustering bias', 'text segments', 'denote objects', 'see clearly', 'search engine', 'articles published', 'another application', 'really depends', 'really depends', 'customers based', 'clustering actually', 'might think', 'might agree', 'sophisticated mining', 'cluster documents', 'problem lies', 'data mining', 'world distribution', 'transportation tool', 'time period', 'specific ones', 'published papers', 'previous slide', 'previous lectures', 'multiple documents', 'much disagreement', 'major topics', 'major complaints', 'major complaints', 'main point', 'high probabilities', 'following lectures', 'discuss later', 'complete coverage', 'basic questions', '1st slide', 'flexible way', 'actually composed', 'actually combined', 'might extract', 'another example', 'topic mining', 'also', 'together', 'typical scenario', 'texts segments', 'physical properties', 'lecture organ', 'cause otherwise', 'ambiguous word', 'topic models', 'topic model', 'one unit', 'first one', 'without perspective', 'cluster terms', 'representative document', 'concrete example', 'assessing similarity', 'use', 'useful technique', 'case documents', 'general technique', 'mean', 'get', 'define similarity', 'group', 'group', 'different', 'may', 'may', 'may', 'example passages', 'would', 'words', 'give', 'similar', 'similar', 'similar', 'similar', 'two', 'achieve', 'clustering', 'clustering', 'clustering', 'well', 'well', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'articles', 'clearly', 'might', 'might', 'might', 'might', 'might', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'see', 'see', 'see', 'problem', 'problem', 'problem', 'websites', 'want', 'user', 'structures', 'structure', 'feel', 'feature', 'documents', 'documents', 'car', 'car', 'analysis', 'analysis', 'data', 'data', 'way', 'actually', 'useful', 'sometimes', 'sometimes', 'depends', 'collection', 'collection', 'general', 'general', 'general', 'define', 'define', 'define', 'typical', 'technique', 'technique', 'segments', 'segments', 'physical', 'passages', 'lecture', 'extract', 'customers', 'cause', 'ambiguous', 'topic', 'topic', 'topic', 'topic', 'one', 'one', 'one', 'perspective', 'perspective', 'perspective', 'perspective', 'perspective', 'perspective', 'similarity', 'similarity', 'similarity', 'similarity', 'seen', 'seen', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'document', 'document', 'terms', 'terms', 'terms', 'terms', 'case', 'case', 'case', 'case', 'case', 'case', 'zero', 'whether', 'website', 'using', 'used', 'used', 'used', 'units', 'understand', 'understand', 'uncertainty', 'turns', 'treat', 'treat', 'theme', 'techniques', 'talk', 'take', 'subtopics', 'start', 'specify', 'sizes', 'size', 'size', 'similarly', 'shapes', 'shapes', 'shapes', 'shapes', 'set', 'sentences', 'sentences', 'sense', 'sense', 'sense', 'sense', 'seeing', 'say', 'say', 'say', 'saw', 'right', 'regenerate', 'query', 'query', 'particular', 'order', 'order', 'order', 'maybe', 'maybe', 'make', 'lot', 'lot', 'lot', 'look', 'look', 'look', 'look', 'look', 'look', 'look', 'look', 'look', 'look', 'look', 'locations', 'literature', 'linking', 'let', 'let', 'let', 'let', 'let', 'learned', 'interested', 'interested', 'important', 'important', 'imagine', 'idea', 'however', 'horse', 'horse', 'horse', 'hope', 'hierarchy', 'hierarchy', 'hard', 'got', 'got', 'got', 'going', 'goal', 'goal', 'goal', 'getting', 'getting', 'generally', 'furthermore', 'functionally', 'find', 'fact', 'evaluate', 'evaluate', 'etc', 'emails', 'depending', 'depending', 'defining', 'create', 'create', 'courses', 'concept', 'clustered', 'clustered', 'call', 'browsing', 'brcause', 'author', 'author', 'ask', 'ask', 'ambiguity', 'able']
lecture 31
['also discuss similarity based approaches', 'basically tells us document di', 'detailed coverage distributions pi ij', 'estimating one word distribution based', 'naturally design alternative mixture model', 'topic mining problem using topic models', 'document covers precisely one topic', 'also mentioned multiple times', 'possibly cover multiple topics', 'introduce generative probabilistic models', 'continue discussing text clustering', 'likelihood function looks like', 'two component mixture model', 'generative model would adopt', 'precisely one topic instead', 'using precisely one topic', 'generative probabilistic models', 'covers different topic', 'cover one topic', 'multiple topics covered', 'allowed multiple topics', 'word distribution based', 'covers one topic', 'precisely one topic', 'precisely one topic', 'precisely one topic', 'non zero probabilities', 'formula looks like', 'made multiple times', 'text collection c', 'generating probabilistic models', 'word x sub', 'force every document', 'two different distributions', 'precisely one distribution', 'share k topics', 'potential different decision', 'precisely one document', 'cluster assignment decisions', 'document must share', 'probabilistic generating model', 'theta sub two', 'output two things', 'multiple distribution could', 'pi ij', 'would allow us', 'documents share topics', 'four different words', 'use precise one', 'k word distributions', 'particular data point', 'assume one topic', 'longer mixture model', 'covering text clustering', 'theta two right', 'one particular distribution', 'text clustering problem', 'topics also independent', 'topic coverage', 'document clustering model', 'document clustering problem', 'word first make', 'multiple distributions', 'mixture models', 'choosing one distribution', 'detailed view', 'generative models', 'give us', 'likelihood function', 'likelihood function', 'topic models', 'two models', 'c sub', 'document covers', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'also different', 'one topic', 'one topic', 'one topic', 'design model', 'generative model', 'generative model', 'generative model', 'generative model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'topic model', 'different distribution', 'topic modeling', 'formula looks', 'data point', 'first one', 'text clustering', 'text clustering', 'text clustering', 'text clustering', 'text clustering', 'text clustering', 'text clustering', 'text clustering', 'text clustering', 'two problems', 'two probabilities', 'two kinds', 'two components', 'theta two', 'theta one', 'theta one', 'previous one', 'one box', 'indicate one', 'choose one', 'topics k', 'k topics', 'k topics', 'one distribution', 'one distribution', 'obviously also', 'also visualized', 'also copying', 'first word', 'topics denoted', 'fewer topics', 'many decisions', 'document must', 'k components', 'two clusters', 'word independently', 'word independently', 'sample word', 'topic definition', 'particular distribution', 'particular distribution', 'particular distribution', 'particular distribution', 'vocabulary v', 'still uncertain', 'still recall', 'still recall', 'simplified situation', 'overall plan', 'one cluster', 'next step', 'main requirement', 'em algorithm', 'could potentially', 'clustering problem', 'causes confusion', 'one document', 'one document', 'also assume', 'word distribution', 'word distribution', 'two differences', 'main problem', 'looks similar', 'whereas text', 'generating model', 'k clusters', 'use one', 'two cases', 'discussed earlier', 'discussed earlier', 'document clustering', 'document clustering', 'document clustering', 'document clustering', 'also consider', 'many ways', 'cluster identity', 'whole document', 'whole document', 'entire document', 'document could', 'model cannot', 'first look', 'n documents', 'n documents', 'n documents', 'first make', 'clustering structure', 'second distribution', 'second distribution', 'decision regarding', 'decision regarding', 'decision regarding', 'also useful', 'mining', 'main difference', 'main difference', 'key difference', 'also think', 'actually inside', 'first take', '1st make', 'first think', 'world distribution', 'product outside', 'precisely', 'previous lecture', 'likely generated', 'generated independently', 'sum corresponds', 'seen earlier', 'special case', 'going back', 'naturally', 'discuss', 'approaches', 'general document', 'product inside', 'models', 'generate document', 'design', 'topic', 'topic', 'topic', 'instead', 'distributions', 'distributions', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'basically', 'basically', 'also', 'also', 'also', 'topics', 'topics', 'topics', 'topics', 'using', 'using', 'using', 'using', 'using', 'particular', 'model', 'model', 'model', 'model', 'model', 'k', 'clustering', 'clustering', 'clustering', 'clustering', 'word', 'word', 'word', 'word', 'word', 'word', 'theta', 'theta', 'right', 'probabilities', 'data', 'covering', 'could', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'would', 'would', 'would', 'would', 'formula', 'formula', 'earlier', 'assume', 'assume', 'generating', 'generating', 'generating', 'generating', 'second', 'output', 'output', 'many', 'longer', 'longer', 'independent', 'independent', 'clusters', 'cluster', 'cluster', 'cluster', 'allow', 'allow', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'documents', 'documents', 'documents', 'documents', 'decision', 'decision', 'decision', 'decision', 'decision', 'world', 'whereas', 'structure', 'look', 'inside', 'inside', 'differences', 'difference', 'difference', 'difference', 'definition', 'consider', 'cannot', 'make', 'make', 'make', 'make', 'make', 'make', 'use', 'use', 'use', 'use', 'use', 'useful', 'useful', 'take', 'take', 'seen', 'seen', 'made', 'made', 'made', 'made', 'made', 'cases', 'cases', 'think', 'think', 'think', 'think', 'think', 'product', 'product', 'product', 'product', 'product', 'choosing', 'choosing', 'choosing', 'choosing', 'choosing', 'choosing', 'general', 'general', 'general', 'general', 'similar', 'similar', 'similar', 'similar', 'similar', 'lecture', 'lecture', 'lecture', 'lecture', 'lecture', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'sum', 'sum', 'sum', 'sum', 'sum', 'sum', 'sum', 'sum', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'write', 'well', 'way', 'want', 'violating', 'variations', 'value', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'uncertainty', 'true', 'top', 'top', 'top', 'together', 'time', 'talked', 'talked', 'talk', 'talk', 'stick', 'stay', 'stay', 'stay', 'stay', 'stay', 'solve', 'solve', 'slightly', 'slide', 'slide', 'slide', 'slide', 'similarly', 'shown', 'show', 'show', 'set', 'seeing', 'seeing', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'say', 'revisit', 'revisit', 'recover', 'reason', 'really', 'realize', 'realize', 'question', 'purpose', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'preferences', 'possibilities', 'plsa', 'picture', 'partitioning', 'parameters', 'p', 'observing', 'observing', 'observing', 'observing', 'observing', 'observing', 'observe', 'number', 'number', 'number', 'notice', 'need', 'multiplied', 'modify', 'means', 'means', 'means', 'means', 'means', 'means', 'means', 'map', 'let', 'let', 'later', 'later', 'know', 'kind', 'interesting', 'input', 'infer', 'indeed', 'indeed', 'importantly', 'illustrated', 'ideas', 'however', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'hope', 'group', 'get', 'generator', 'form', 'expand', 'example', 'example', 'everyone', 'estimate', 'estimate', 'estimate', 'equal', 'ensure', 'embed', 'distributing', 'detail', 'designing', 'desicion', 'decided', 'course', 'course', 'corresponding', 'correspond', 'contribute', 'connections', 'connection', 'connection', 'compare', 'clearly', 'clearly', 'ci', 'ci', 'chosen', 'choice', 'choice', 'choice', 'choice', 'choice', 'choice', 'changed', 'change', 'certainty', 'cause', 'cause', 'assumption', 'assumption', 'assumption', 'assuming', 'assumed', 'although', '1']
lecture 32
['particularly generative probabilistic models', 'k unigram language models', 'k unigram language models', 'tax capture text clustering', 'documents n documents denoted', 'model basically would make', 'two component mixture model', 'generative probabilistic models', 'maximum likelihood estimator', 'x sub j', 'would allow us', 'little bit different', 'likelihood function would', 'potentially different choice', 'generic choice probability', 'use unique word', 'change allows us', 'word distributions corresponding', 'topic model presentation', 'intuitively makes sense', 'choose theta based', 'document actually depends', 'likely cluster would', 'document dependent probability', 'text clustering', 'two distributions', 'likelihood function', 'topic choice', 'k distributions', 'makes sense', 'general presentation', 'documents lets', 'basically still', 'mixture model', 'mixture model', 'tells us', 'tell us', 'k clusters', 'word distribution', 'topic model', 'topic model', 'top terms', 'steps using', 'second line', 'particular position', 'parameter estimation', 'may recall', 'helps summarize', 'generated model', 'generally higher', 'general notion', 'following assumption', 'estimation formulas', 'distributed generator', 'different model', 'continued discussion', 'continue talking', 'c sub', 'use likelihood', 'cluster corresponding', 'likelihood together', 'posterior probability', 'posterior probability', 'highest probability', 'highest probability', 'high probability', 'change also', 'also recover', 'also consistent', 'also available', 'standard way', 'first choose', 'choose one', 'better way', 'document clustering', 'document clustering', 'document clustering', 'simply use', 'terms like', 'product instead', 'key problem', 'following formula', 'easily solve', 'base formula', 'particular document', 'document using', 'assigning document', 'small cluster', 'assigning clusters', 'allocate clusters', 'see later', 'see later', 'might think', 'seen earlier', 'topic theta', 'actually get', 'similar parameters', 'large cluster', 'large cluster', 'lets look', 'actually compute', 'first think', 'take one', 'large p', 'theta given', 'would', 'cluster size', 'k', 'documents', 'documents', 'mixture', 'us', 'clustering', 'topic', 'sub', 'sense', 'model', 'model', 'model', 'model', 'model', 'intuitively', 'distributions', 'different', 'different', 'dependent', 'choose', 'change', 'probability', 'probability', 'probability', 'probability', 'also', 'actually', 'actually', 'large', 'way', 'one', 'first', 'use', 'use', 'use', 'use', 'together', 'product', 'problem', 'likely', 'likely', 'likely', 'like', 'given', 'get', 'formula', 'formula', 'easily', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'clusters', 'clusters', 'clusters', 'clusters', 'think', 'think', 'think', 'see', 'see', 'see', 'take', 'take', 'still', 'still', 'size', 'size', 'seen', 'seen', 'distribution', 'distribution', 'compute', 'compute', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'parameters', 'parameters', 'parameters', 'p', 'p', 'p', 'look', 'look', 'look', 'look', 'written', 'words', 'words', 'words', 'words', 'w', 'vocabulary', 'values', 'used', 'used', 'used', 'used', 'used', 'unlike', 'topics', 'things', 'thetas', 'thetas', 'thetas', 'talk', 'talk', 'slide', 'slide', 'situation', 'show', 'set', 'selecting', 'seeing', 'right', 'right', 'result', 'represents', 'repeated', 'realize', 'range', 'question', 'properties', 'probabilities', 'probabilities', 'probabilities', 'prior', 'prior', 'precisely', 'plsa', 'note', 'note', 'note', 'notation', 'notation', 'notation', 'next', 'need', 'need', 'means', 'means', 'means', 'means', 'matches', 'let', 'let', 'lecture', 'lecture', 'lecture', 'larger', 'lack', 'interpreted', 'interesting', 'information', 'infer', 'indicating', 'indicate', 'include', 'important', 'however', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'goal', 'gives', 'generation', 'generating', 'generating', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'generalize', 'generalize', 'functioning', 'follow', 'find', 'favor', 'familiar', 'example', 'estimate', 'estimate', 'estimate', 'estimate', 'equation', 'discuss', 'design', 'denote', 'data', 'data', 'course', 'content', 'content', 'consider', 'collection', 'closely', 'class', 'changed', 'chance', 'cases', 'case', 'case', 'case', 'case', 'byproduct', 'bottom', 'best', 'assume', 'assigned', 'assign', 'assign', 'approach', 'although', 'add', 'add', 'according', 'able']
lecture 33
['compute another average district called theater bar', 'k representing k different distributions', 'hidden variable must tell us', 'using multiple unigram language models', 'first theater subway must', 'tax given still awhile', 'two things must happen', '1 hidden variable attached', 'estimation involves two kinds', 'likelihood function looks like', 'value variable could take', 'used 1 hidden variable', 'word distribution actually generates', 'sometimes also use logarithm', 'hidden variable zd', 'unigram language model', 'unigram language model', 'help preserve precision', 'capture different patterns', 'apply bayes rule', 'single language model', 'maximum likelihood estimator', 'would allow us', 'estimated model pamateter', 'harder clusters mainly', 'z sub dj', 'z probability z probabilities', 'generative probabilistic models', 'generative probabilistic models', 'achieve different goals', 'use bayes rule', 'cannot use logarithms', 'namely two occurrences', 'new mixture model', 'em algorithm starts', 'special normalization technique', 'hidden variable', 'maximum liklihood estimate', 'average word distribution', 'k probabilities sum', 'theta 2 would', 'particular topic theta', 'intuitively would give', 'take two values', 'word given theta', 'words must sum', 'four words together', 'many small probabilities', 'consider one document', 'avoid underflow problem', 'k topics', 'hidden variables', 'looks like', 'selecting theta one', 'two distributions', 'take average', 'likelihood converges', 'gives us', 'gives us', 'z values', 'mixture models', 'must use', 'topic models', 'topic models', 'z equals', 'maximum lag', 'council would', 'two occurrences', 'two occurrences', 'two occurrences', 'two clusters', 'two clusters', 'average distribution', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'theta sub', 'probabilistic assignment', 'probabilistic assignment', 'must sum', 'em algorithm', 'em algorithm', 'em algorithm', 'topic model', 'topic model', 'first topic', 'first topic', '2 given', 'top model', 'model slightly', 'using topic', 'two solutions', 'two possibilities', 'first thing', 'first choosing', 'also compute', 'numerical values', 'initializer values', 'disjoint clusters', 'avoid underflow', 'whole value', 'theta 2', 'theta 2', 'theta 2', 'small value', 'also shows', 'easily compute', 'topic capitalization', 'second topic', 'two topics', 'theta 1', 'theta 1', 'theta 1', 'theta 1', 'theta 1', 'theta 1', '101 1', 'particular distribution', 'four words', 'document must', 'total account', 'slight variation', 'randomly initialized', 'pull together', 'pull together', 'observe anything', 'may recall', 'little bit', 'generation assumption', 'eml works', 'e steps', 'district', '01 second', 'word distribution', 'generating models', 'generating models', 'two words', 'would know', 'many words', 'many words', 'many probabilities', 'key topics', 'overall value', 'sometimes useful', 'first look', 'also shown', 'look like', 'water distribution', 'theta one', 'text data', 'send one', 'world probabilities', 'also important', 'e step', 'e step', 'e step', 'e step', 'e step', 'used exact', 'text clustering', 'text clustering', 'text clustering', 'raw counts', 'fractional counts', 'potential problem', 'partition documents', 'documents say', 'simple example', 'continued discussion', 'posterior probability', 'high probability', 'term cluster', 'cluster corresponding', 'clustering documents', 'simply normalize', 'original numerator', 'parameters based', 'whole document', 'see d1', 'equally likely', '000 one', 'different', 'average', 'document clustering', 'probability distribution', 'must', 'one right', 'distribution p', 'z', 'distributions', 'involves', 'actually', 'word', 'word', 'two', 'sometimes', 'using', 'using', 'like', 'value', 'given', 'given', 'also', 'compute', 'compute', 'compute', 'compute', '1', '1', '1', 'together', 'theta', 'theta', 'theta', 'theta', 'technique', 'take', 'take', 'take', 'small', 'normalization', 'intuitively', 'give', 'achieve', 'use', 'use', 'use', 'use', 'use', 'clustering', 'underflow', 'underflow', 'topics', 'sum', 'sum', 'sum', 'sum', 'sum', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'distribution', 'text', 'text', 'text', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'shown', 'selecting', 'selecting', 'selecting', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'overall', '000', 'step', 'step', 'step', 'step', 'step', 'step', 'used', 'used', 'used', 'used', 'used', 'used', 'used', 'counts', 'counts', 'counts', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'documents', 'documents', 'documents', 'documents', 'documents', 'useful', 'useful', 'look', 'look', 'look', 'look', 'important', 'important', 'example', 'example', 'discussion', 'discussion', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'right', 'right', 'right', 'know', 'know', 'know', 'generating', 'generating', 'generating', 'generating', 'generating', 'generating', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'numerator', 'numerator', 'numerator', 'numerator', 'normalize', 'normalize', 'normalize', 'normalize', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'p', 'p', 'p', 'p', 'p', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'whereas', 'well', 'well', 'well', 'want', 'want', 'verify', 'unlikely', 'understood', 'understand', 'times', 'thus', 'think', 'think', 'terms', 'talk', 'summarize', 'specifically', 'specifically', 'sorry', 'solving', 'solve', 'solve', 'solve', 'slide', 'situations', 'since', 'similar', 'similar', 'similar', 'silouan', 'showed', 'show', 'setup', 'selected', 'seen', 'seen', 'scylla', 'said', 'said', 'represented', 'repeat', 'remember', 'relist', 'range', 'quantities', 'put', 'pulled', 'proportional', 'proportional', 'proportional', 'product', 'product', 'product', 'product', 'product', 'precisely', 'precise', 'power', 'plug', 'plsa', 'plsa', 'plsa', 'output', 'output', 'order', 'order', 'order', 'occur', 'obtain', 'observed', 'observation', 'numerators', 'note', 'note', 'normalizes', 'normalizes', 'normalizers', 'normalizer', 'normalizer', 'normalizer', 'normalizer', 'normalized', 'normalized', 'need', 'need', 'need', 'need', 'naturally', 'much', 'mining', 'mining', 'medical', 'manageable', 'make', 'make', 'magnitude', 'made', 'made', 'long', 'log', 'let', 'let', 'let', 'let', 'let', 'let', 'lecture', 'lecture', 'lecture', 'knowledge', 'knowledge', 'kind', 'kind', 'kind', 'inverted', 'intuition', 'introduce', 'instead', 'initialization', 'infer', 'infer', 'infer', 'infer', 'infer', 'indicate', 'indeed', 'imagine', 'health', 'happened', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'get', 'generator', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'generate', 'general', 'formulas', 'force', 'finish', 'expression', 'exactly', 'evidence', 'evidence', 'etc', 'equal', 'effective', 'easy', 'divide', 'discount', 'detail', 'denominators', 'denominator', 'denominator', 'denominator', 'denominator', 'denominator', 'define', 'd2', 'convert', 'constraints', 'constraint', 'constraint', 'constraint', 'constraint', 'computing', 'computer', 'computation', 'computation', 'comperable', 'collect', 'class', 'changing', 'changing', 'changed', 'cause', 'cause', 'cases', 'cases', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'byproduct', 'basically', 'basically', 'basically', 'basically', 'assume', 'applications', 'adapt', '6', '5', '4', '100']
lecture 34
['selecting kate randomly selected vectors', 'assuming similarity function defined onto objects', 'many different clustering methods available', 'random inner inner initialization', 'discover complex clustering structures', 'convert converted local minimum', 'maybe simple english good', 'counts pull together counter', 'easily design generated model', 'directly specify similarity functions', 'two pairs objects would define', 'generally give different results', 'explicit definer similarity function', 'particular objective function like', 'certainly cluster terms based', 'model defines clustering bias', 'gradually group similar objects together', 'single link complete link', 'computer group similarity based', 'called k means clustering', 'two groups come together', 'highest similarity similarity pairs', 'many different methods', 'defined objective function', 'paradigmatic relation learning', 'everything specified except', 'binary tree eventually', 'recover different instruction', 'complex generative models', 'three popular methods', 'initial tentative classroom', 'complete link defines', 'link would mean', 'different clustering algorithms', 'two farthest appear', 'distinguish two strategies', 'custom methods would', 'completely completely better', 'process goes like', 'tentative clustering result', 'initial tentative clustering', 'within cluster sum', 'within cluster sum', 'cluster hosts entry', 'first single link', 'clustering result iteratively', 'two representative methods', 'similarity function calls', 'provider similarity function', 'hierarchical agglomerative clustering', 'agglomerative hierarchical clustering', 'average link defines', 'better clustering result', 'define objective function', 'father sister pair', 'group everything together', 'group everything together', 'customize clustering algorithm', 'similarity based classroom', 'two groups g1', 'distinguishes two ways', 'implicitly similarity function', 'similarity based clustering', 'similarity based clustering', 'hierarchical clustering algorithm', 'two groups together', 'whole data set', '00 text objects', 'using generative models', 'model based approaches', 'one potential disadvantage', 'means two groups', 'k means algorithm', 'k clusters based', 'computing group similarities', 'group groups together', 'model based approach', 'cluster objects together', 'get four clusters', 'given two groups', 'similarity based approaches', 'similarity based approaches', 'similarity based approaches', 'also different ways', 'tentative clustering centroids', 'given k clusters', 'precisely one cluster', 'individual object similarity', 'also use prior', 'local minimum', 'single link', 'clearly defined', 'called agglomerative', 'objective function', 'objective function', 'objective function', 'put one object', 'clustering bias', 'clustering bias', 'clustering methods', 'hierarchical clustering', 'generative models', 'different model', 'tentative clustering', 'complete link', 'different groups', 'different groups', 'k means', 'k means', 'good relationship', 'give us', 'explicit definition', 'different ways', 'object function', 'similarity function', 'similarity function', 'similarity function', 'generative model', 'implicitly define', 'likelihood function', 'highest similarity', 'highest similarity', 'different variations', 'different kind', 'every link', 'similarity based', 'putting together', 'bring together', 'would bring', 'generated model', 'gradually group', 'gradually group', 'gradually group', 'text clustering', 'flat clustering', 'clustering program', 'two groups', 'two groups', 'two groups', 'two groups', 'two groups', 'two groups', 'two groups', 'two groups', 'two groups', 'two groups', 'two groups', 'two groups', 'two groups', 'two groups', 'two groups', 'adjusted based', 'two objects', 'two objects', 'two objects', 'two objects', 'average link', 'average link', 'hierarchical clusters', 'clusters based', 'individual objects', 'also based', 'also based', 'k clusters', 'mixture model', 'mixture model', 'mixture model', 'mixture model', 'say come', 'complete linker', 'cluster two', 'particular view', 'first discussed', 'explicitly specify', 'two steps', 'two leaders', 'clustering structure', 'structure based', 'gradually partitioning', 'two clusters', 'text objects', 'objects allocated', 'individual decision', 'individual decision', 'allocated objects', 'group similarity', 'group similarity', 'text object', 'one disadvantage', 'intragroup similarity', 'intergroup similarity', 'high similarity', 'high similarity', 'singling algorithm', 'em algorithm', 'em algorithm', 'em algorithm', 'em algorithm', 'em algorithm', '1 cluster', 'group decision', 'larger groups', 'text content', 'tax representations', 'stopping criterions', 'sense pessimistic', 'principle consider', 'primary destinations', 'optimistic view', 'opposite situation', 'much easier', 'might remind', 'iteratively improve', 'iteratively improve', 'high level', 'high level', 'gives us', 'general generated', 'full coverage', 'far away', 'explicitly provide', 'every time', 'empirically evaluate', 'easy way', 'data point', 'data point', 'consider probabilities', 'allows us', 'multiple clusters', 'loose clusters', 'whole process', 'people would', 'also talked', 'also shown', 'also maximizing', 'also initialize', 'also form', 'centroid based', 'takes average', 'every pair', 'course based', 'whole hierarchy', 'general strategy', 'general idea', 'general idea', 'general basis', 'form larger', 'better estimate', 'larger clusters', 'probabilistic allocations', 'optimally partitioning', 'first let', 'data points', 'data points', 'similar object', 'use potentially', 'current vector', 'allocate vector', 'similarity measure', 'measure similarity', 'term clusters', 'term clusters', 'term clusters', 'whole class', 'squares converges', 'repeat eastep', 'progressively construct', 'often leads', 'often leads', 'might need', 'may recall', 'mainly differ', 'loose classes', 'central eastside', '2 steps', 'tight clusters', 'smaller clusters', 'document clusters', 'essentially similar', 'make sure', 'make comparison', 'closest repair', 'another example', 'also need', 'well connected', 'close orders', 'also similar', 'general goals', 'e step', 'closest pair', 'term vector', 'probabilistic location', 'next one', 'one example', 'vectors', 'process actually', 'hierarchy depending', 'say exactly', 'pairs', 'pairs', 'together', 'together', 'together', 'would', 'clustering', 'clustering', 'clustering', 'methods', 'methods', 'based', 'terms', 'tentative', 'means', 'maybe', 'k', 'generally', 'counts', 'model', 'define', 'define', 'complete', 'specify', 'particular', 'given', 'two', 'two', 'ways', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'objects', 'generated', 'better', 'object', 'approaches', 'approaches', 'approaches', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'similarity', 'algorithm', 'algorithm', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'group', 'group', 'group', 'group', 'group', 'group', 'group', 'group', 'using', 'text', 'similarities', 'get', 'data', 'approach', 'clusters', 'clusters', 'clusters', 'clusters', 'also', 'also', 'also', 'also', 'average', 'average', 'pair', 'pair', 'larger', 'general', 'general', 'structure', 'process', 'process', 'process', 'probabilistic', 'points', 'partitioning', 'improve', 'centroids', 'centroids', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'vector', 'vector', 'use', 'use', 'use', 'term', 'term', 'term', 'tight', 'squares', 'smaller', 'put', 'put', 'put', 'often', 'often', 'next', 'need', 'need', 'may', 'mainly', 'location', 'hierarchy', 'hierarchy', 'goals', 'exactly', 'estimate', 'eastep', 'document', 'depending', 'construct', 'classes', 'central', '2', 'similar', 'similar', 'similar', 'similar', 'similar', 'similar', 'similar', 'similar', 'similar', 'similar', 'similar', 'similar', 'say', 'say', 'say', 'measure', 'measure', 'measure', 'make', 'make', 'make', 'example', 'example', 'example', 'closest', 'closest', 'closest', 'well', 'well', 'let', 'let', 'close', 'close', 'actually', 'actually', 'people', 'people', 'people', 'class', 'class', 'class', 'centroid', 'centroid', 'centroid', 'step', 'step', 'step', 'step', 'course', 'course', 'course', 'course', 'whether', 'want', 'want', 'useful', 'useful', 'used', 'used', 'used', 'unlikely', 'understand', 'try', 'treat', 'tradeoff', 'topic', 'top', 'times', 'threshold', 'threshold', 'think', 'think', 'think', 'think', 'think', 'theoretically', 'talking', 'talk', 'talk', 'talk', 'taking', 'take', 'take', 'summarize', 'subset', 'stop', 'started', 'start', 'start', 'start', 'start', 'split', 'specifically', 'sometimes', 'sometimes', 'sometimes', 'show', 'sentence', 'sensitive', 'sensitive', 'seen', 'see', 'see', 'see', 'scope', 'said', 'rules', 'review', 'representing', 'representation', 'represent', 'represent', 'repeated', 'recovery', 'reason', 'realize', 'reached', 'rather', 'question', 'profession', 'probability', 'predators', 'practice', 'pick', 'person', 'party', 'partition', 'parties', 'parameters', 'parameter', 'outliers', 'outliers', 'outliers', 'optimizing', 'optimized', 'number', 'note', 'normalize', 'nature', 'moment', 'minimize', 'minimize', 'method', 'members', 'maximize', 'looking', 'look', 'look', 'look', 'long', 'long', 'long', 'likeable', 'lecture', 'lecture', 'know', 'know', 'insured', 'instep', 'insensitive', 'inject', 'inject', 'inferring', 'induce', 'indeed', 'improved', 'implement', 'illustrating', 'illustrate', 'however', 'however', 'harder', 'hard', 'hand', 'hand', 'guaranteed', 'guarantee', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'giving', 'generate', 'generate', 'generate', 'generate', 'g2', 'flexible', 'fact', 'expected', 'expect', 'even', 'even', 'ensure', 'ensure', 'ensure', 'eml', 'either', 'driven', 'done', 'divisive', 'divide', 'distribution', 'distribution', 'distance', 'discussion', 'discussion', 'direct', 'differences', 'differences', 'difference', 'difference', 'difference', 'detail', 'depend', 'cut', 'cut', 'cut', 'cover', 'count', 'count', 'could', 'could', 'converge', 'control', 'control', 'contrast', 'continue', 'continue', 'context', 'compute', 'compute', 'compute', 'compute', 'comfortable', 'collection', 'collection', 'coherent', 'clear', 'clear', 'choose', 'choice', 'choice', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'call', 'built', 'browsing', 'bottom', 'bottom', 'beyond', 'best', 'belonging', 'becausw', 'basically', 'assignment', 'assign', 'assess', 'applications', 'application', 'application', 'anne', 'anne', 'analogy', 'always', 'aim', 'aggressively', 'agency', 'adjusting', 'adjust', 'adjust', 'achieving', 'achieved', '70']
lecture 35
['useful unsupervised general text mining technique', 'commonly used measures include purity', 'easily compare different methods based', 'give us various measures', 'would help us characterize', 'general procedure would look like', 'discover interesting clustering structures', 'question without knowing whether', 'best clustering result would', 'normalized mutual information', 'normalized mutual information', 'mutual information captures', 'must clearly specify', 'high level ideas', 'without clearly defining', 'would allow us', 'get good sense', 'commonly used measure', 'another possible measure', 'two different ways', 'explicitly define bias', 'method highly depends', 'different clustering method', 'ideal clustering result', 'model based approaches', 'strong clusters tend', 'human generated results', 'basically measures based', 'summarize text clustering', 'right generative model', 'also would like', 'clustering search results', 'desired clustering bias', 'desired clustering bias', 'humans would bring', 'explore text data', 'similarity based approaches', 'evaluate text clusters', 'evaluate text clusters', 'cluster clustering result', 'one must go', 'right similarity function', 'generate 100 clusters', 'best clustering results', 'also would create', 'system generated clusters', 'system generated clusters', 'system generated clusters', 'gold standard clusters', 'baseline system could', 'application specific question', 'also say sometimes', 'unsupervised algorithm', 'would like', 'procedure wise', 'would allow', 'measures whether', 'guide us', 'generated results', 'judgments based', 'text clustering', 'text clustering', 'particularly useful', 'particularly useful', 'two ways', 'quantitatively evaluate', 'many approaches', 'classroom methods', 'text content', 'clustering results', 'clustering results', 'clustering results', 'clustering bias', 'clustering bias', 'clustering bias', 'clustering bias', 'clustering bias', 'clustering bias', 'system results', 'model design', 'mixture model', 'gold standard', 'gold standard', 'gold standard', 'go back', 'f measure', 'text objects', 'text objects', 'text data', 'text data', 'cluster based', 'cluster based', 'ideal clusters', 'clustering system', 'clustering system', 'baseline system', 'baseline system', 'baseline system', 'baseline system', 'multiple ways', 'components would', 'clustering algorithm', 'method works', 'method works', 'specific applications', 'text cluster', 'current system', 'also measured', 'two objects', 'good number', 'vice versa', 'thorough discussion', 'test set', 'test set', 'test set', 'sometimes desirable', 'similar depending', 'second kind', 'performance measure', 'overall picture', 'new component', 'multiple perspectives', 'multiple perspectives', 'multiple angles', 'interface design', 'following question', 'first step', 'explicit assessment', 'essentially inject', 'directl valuation', 'captured appropriately', 'assessed assessed', 'always set', 'second application', 'particular application', 'particular application', 'particular application', 'intended application', 'intended application', 'implied application', 'right number', 'training data', 'similar objects', 'data worse', 'best number', 'better understand', 'better idea', 'assess whether', 'intended applications', 'optimal number', 'optimal number', 'data better', 'cluster labels', 'significantly improve', 'performance figures', 'often used', 'may see', 'done either', 'difficult problem', 'actually see', '2nd way', 'add clustering', 'general fit', 'well defined', 'indirect evaluation', 'indirect evaluation', 'indirect evaluation', 'evaluation purpose', 'direct evaluation', 'also want', 'automatically determine', 'ask humans', 'data well', 'use application', 'using humans', 'suggested reading', 'often needed', 'measures', 'would', 'would', 'useful', 'like', 'without', 'methods', 'structures', 'generated', 'whether', 'right', 'clustering', 'clustering', 'clustering', 'clustering', 'clustering', 'method', 'best', 'best', 'system', 'system', 'also', 'also', 'general', 'general', 'general', 'general', 'general', 'general', 'used', 'used', 'used', 'used', 'sometimes', 'say', 'question', 'question', 'question', 'look', 'look', 'look', 'generate', 'could', 'compare', 'compare', 'compare', 'basically', 'clusters', 'clusters', 'clusters', 'clusters', 'clusters', 'clusters', 'clusters', 'clusters', 'clusters', 'clusters', 'application', 'application', 'application', 'objects', 'humans', 'humans', 'data', 'data', 'data', 'better', 'similarity', 'similarity', 'similarity', 'similarity', 'create', 'create', 'applications', 'number', 'number', 'number', 'number', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'cluster', 'way', 'suggested', 'see', 'see', 'reading', 'problem', 'performance', 'performance', 'one', 'one', 'one', 'often', 'often', 'needed', 'improve', 'done', 'components', 'assess', 'ask', 'well', 'well', 'well', 'evaluation', 'evaluation', 'evaluation', 'evaluation', 'evaluation', 'evaluation', 'evaluation', 'evaluation', 'use', 'use', 'fit', 'fit', 'determine', 'determine', 'using', 'using', 'using', 'add', 'add', 'add', 'want', 'want', 'want', 'want', 'work', 'watch', 'vary', 'usefulness', 'trying', 'think', 'think', 'terms', 'talked', 'talk', 'take', 'specified', 'something', 'slide', 'sizes', 'situations', 'show', 'shapes', 'shapes', 'select', 'scope', 'rather', 'quantifying', 'quantify', 'quantify', 'quality', 'quality', 'qualify', 'produce', 'probability', 'predict', 'precisely', 'perspective', 'perspective', 'perspective', 'partition', 'order', 'obviously', 'obtaining', 'object', 'object', 'need', 'mind', 'might', 'meaningful', 'matter', 'lot', 'lot', 'lecture', 'know', 'know', 'keep', 'introduced', 'indirectly', 'impossible', 'imposed', 'important', 'important', 'identity', 'ideally', 'hope', 'hope', 'got', 'going', 'going', 'going', 'going', 'given', 'fitness', 'fitness', 'finally', 'finally', 'far', 'explain', 'experiment', 'example', 'exactly', 'evaluations', 'end', 'effectiveness', 'discussed', 'discuss', 'directly', 'dictated', 'dependent', 'depend', 'deciding', 'deal', 'crucial', 'course', 'course', 'counts', 'correlation', 'contribution', 'contribution', 'consists', 'closeness', 'closeness', 'close', 'case', 'case', 'case', 'case', 'case', 'case', 'care', 'call', 'calculated', 'beyond', 'beginning', 'appropriate', 'answer', 'answer', 'answer', 'able', 'able', '0']
lecture 36
['text categorization helps us enrich text representation', 'literature article categorizations another important task', 'categorize news generated every day', 'helpdesk email messages generally routed', 'search engine applications would want', 'build automatic text categorization system', 'often use text categorization techniques', 'infer certain desicion factors', 'news agencies would like', 'extra sensor data collected', 'text categorisation allows us', 'use text mining tool', 'non text data specifically', 'text categories important well', 'generally distinguish two kinds', 'help us distinguish spam', 'analyzing text data based', 'handle different people attempt', 'help us infer properties', 'also many applications like', 'also another important kind', 'k category categorisation task', 'would allow us', 'non native speakers', 'medical subject heading', 'various different kinds', 'provide 2 levels', 'email routing would', 'many examples like', 'big text data', 'yet another variation', 'use text categorization', 'use text categorization', 'text coding encoding', 'also many variants', 'text based prediction', 'text categorisation followed', 'text data mining', 'using text data', 'using text data', 'meaning keyword bag', 'yet another kind', 'text processing tasks', 'using binary categorization', 'handle different kinds', 'automatically email routing', 'text data indirectly', 'yet another application', 'semantic categories assigned', 'categorize products based', 'sentiment categories assigned', 'also add categories', 'new tax objects', 'text categorization problem', 'binary categorization problem', 'binary classification problem', 'category actually corresponds', 'author attribution might', 'sub categories etc', 'high level categories', 'categories form hierarchy', 'speakers another example', 'topic mining analysis', 'assign predefined categories', 'actually infer properties', 'also two categories', 'hierarchical category categorisation', 'multiple categorization tasks', 'general case would', 'labeled text objects', 'sentiment categories could', 'called author attribution', 'general assign categories', 'first text objects', 'non relevant documents', 'meaningful categories associated', 'terms characterize content', 'could first categorize', 'allow us', 'news categorization', 'tell us', 'two kinds', 'literature articles', 'data mining', 'using text', 'non spam', 'words representation', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'another variation', 'individual task', 'text categorization', 'text categorization', 'text categorization', 'text categorization', 'text categorization', 'text categorization', 'text categorization', 'important technique', 'important technique', 'coding text', 'binary categorization', 'binary categorization', 'help routing', 'text categorisation', 'hierarchical categorization', 'might want', 'human sensor', 'always use', 'text analysis', 'different categories', 'text objects', 'text objects', 'sentiment categorization', 'author attribution', 'infer properties', 'categorization problem', 'multiple people', 'opinion mining', 'email detection', 'different folders', 'categorization tasks', 'help categorize', 'another application', 'sentiment analysis', 'multiple levels', 'two categories', 'two categories', 'two categories', 'semantic categories', 'many examples', 'manually assign', 'text object', 'text object', 'category categorization', 'sentiment categories', 'another case', 'tax objects', 'joint categorization', 'joint categorization', 'automatically sort', 'often also', 'right people', 'predefined categories', 'predefined categories', 'sense based', 'politician based', 'topical categorisation', 'texture categorisation', 'flat categorisation', 'categorisation system', 'categorisation results', 'categorisation results', 'spam filter', 'spam filter', 'inferring properties', 'application spam', 'problem formulation', 'might collect', 'many cases', 'tax object', 'predefined topics', 'second kind', 'multiple topics', 'multiple folders', 'actually performed', 'actually connected', 'topic categories', 'known categories', 'known categories', 'internal categories', 'external categories', 'general categorize', 'categorize restaurants', 'routing emails', 'text content', 'another example', 'another example', 'another example', 'another example', 'often together', 'often assume', 'external category', 'category separately', 'first categorize', 'whether object', 'topical hierarchy', 'sometimes classification', 'small number', 'slide shows', 'several applications', 'possibly forming', 'political speech', 'party affiliation', 'particular query', 'overall plan', 'much harder', 'miss seeing', 'mesh stands', 'mesh annotations', 'meaningful connection', 'information retrieval', 'information network', 'individual review', 'indirectly useful', 'improve accuracy', 'general way', 'facilitate aggregation', 'distinguishing spams', 'direct characterization', 'concise way', 'biomedical domain', 'also related', 'also related', 'observer based', 'training examples', 'training examples', 'specific examples', 'specific examples', 'partly also', 'also think', 'also imagine', 'simplest case', 'filtering right', 'tax content', 'relevant documents', 'also vary', 'categorize content', 'legitimate emails', 'directly connected', 'often used', 'often interested', 'remotely related', 'particular person', 'discovering knowledge', 'discover knowledge', 'discover knowledge', 'several reasons', 'real world', 'real world', 'make predictions', 'learn patterns', 'easily aggregate', 'corresponding reviews', 'controller vocabulary', 'characterize content', 'always useful', 'right person', 'right person', 'authors ages', 'authors affiliations', 'obvious entities', 'entities associated', 'corresponding entities', 'representation', 'generally', 'training set', 'incoming request', 'entity associated', 'already showed', 'task', 'overall opinions', 'views positive', 'one application', 'directly useful', 'would', 'important', 'use', 'messages', 'distinguish', 'example discovery', 'email', 'product reviews', 'want', 'text', 'text', 'text', 'text', 'text', 'help', 'categorization', 'categorization', 'k', 'infer', 'infer', 'based', 'categorisation', 'properties', 'also', 'also', 'problem', 'could', 'objects', 'tasks', 'author', 'kind', 'kind', 'actually', 'categories', 'categories', 'categories', 'categories', 'categories', 'categories', 'categories', 'categorize', 'categorize', 'categorize', 'often', 'often', 'category', 'category', 'category', 'topic', 'system', 'system', 'system', 'relevant', 'meaning', 'level', 'labeled', 'hierarchy', 'handle', 'handle', 'handle', 'general', 'general', 'etc', 'characterize', 'called', 'associated', 'applications', 'applications', 'applications', 'applications', 'applications', 'application', 'examples', 'examples', 'first', 'first', 'first', 'right', 'case', 'case', 'documents', 'documents', 'emails', 'directly', 'related', 'related', 'person', 'person', 'knowledge', 'knowledge', 'world', 'world', 'vocabulary', 'useful', 'useful', 'useful', 'used', 'think', 'sometimes', 'reviews', 'reviews', 'reasons', 'partly', 'observer', 'make', 'learn', 'interested', 'imagine', 'discovery', 'content', 'content', 'content', 'content', 'content', 'content', 'content', 'connection', 'cases', 'articles', 'aggregate', 'authors', 'authors', 'authors', 'entities', 'entities', 'entities', 'entities', 'entities', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'vary', 'vary', 'set', 'set', 'request', 'request', 'entity', 'entity', 'already', 'already', 'product', 'product', 'product', 'opinions', 'opinions', 'opinions', 'positive', 'positive', 'positive', 'positive', 'one', 'one', 'one', 'one', 'written', 'written', 'without', 'variables', 'values', 'units', 'understanding', 'tweets', 'tune', 'try', 'treat', 'things', 'talk', 'talk', 'talk', 'tagged', 'stuided', 'sorting', 'sorry', 'simple', 'shown', 'shown', 'sentiments', 'sentence', 'seen', 'secondly', 'secondly', 'route', 'restaurant', 'restaurant', 'requests', 'represented', 'recognize', 'provides', 'progressively', 'produce', 'produce', 'processed', 'problems', 'predict', 'predict', 'possibilities', 'picture', 'perform', 'passage', 'part', 'ontology', 'neutral', 'negative', 'negative', 'negative', 'negative', 'negative', 'nature', 'mentioned', 'means', 'means', 'may', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'look', 'long', 'long', 'leverage', 'lecture', 'lecture', 'lecture', 'language', 'know', 'interesting', 'inside', 'humans', 'hope', 'helping', 'happens', 'going', 'going', 'going', 'given', 'fundamental', 'follows', 'folder', 'finally', 'finally', 'fact', 'evaluate', 'especially', 'done', 'done', 'document', 'determine', 'detail', 'dependents', 'defined', 'defined', 'creates', 'covering', 'connect', 'common', 'common', 'collections', 'clustering', 'clearly', 'classify', 'classify', 'clasify', 'cause', 'categorizing', 'basically', 'availability', 'associate', 'analyzed', 'analytics', 'analytics', 'among', 'achieve', '70', '30']
lecture 37
['news article mentions word like game', 'logistical regression support vector machines', 'human experts must annotate datasets', 'matches game sports many times', 'means superficial features like keywords', 'even policy feature tags', 'sophisticated features like phrases', 'might use something similar', 'still require manual work', 'function behaves like based', 'learning processes optimization problem', 'strategy would work well', 'also combine multiple features', 'human experts also need', 'easily identify text data', 'becauses different rules would', 'combine multiple evidences', 'use human experts', 'even syntactic structures', 'label given data based', 'k nearest neighbors', 'following conditions hold', 'cannot handle uncertainties', 'sports three times', 'optimally combine features', 'human would assign', 'data looks like', 'learning process basically', 'would allow us', 'use machine learning', 'use machine learning', 'really completely automatic', 'naive bayes classifier', 'discriminative classifiers attempted', 'natural choice would', 'machine learning methods', 'quotation marks cause', 'nonlinear models might', 'main topic could', 'high level one', 'methods would rely', 'also classify accurately', 'marginally touching sports', 'called discriminative classifiers', 'called discriminative classifiers', 'machine learning problems', 'objective function tends', 'separating different categories', 'different object function', 'non linear combination', 'new text object', 'different methods tend', 'using machine learning', 'data point directly', 'still put automatic', 'classifier would take', 'features separate categories', 'supervised learning problems', 'often also called', 'human experts', 'data given label', 'distinguish different categories', 'generative classifiers try', 'manual work', 'news articles', 'easily use', 'learn soft rules', 'human would', 'things like', 'categories must', 'many methods', 'supervised learning', 'learning methods', 'might optimize', 'many variations', 'would need', 'also tend', 'use probabilities', 'generative classifiers', 'objective function', 'objective function', 'judged based', 'distinguish based', 'different variations', 'different topic', 'different depending', 'would lead', 'first model distribution', 'text object', 'text object', 'still decide', 'reliable take', 'linear combination', 'different problem', 'common choice', 'label given', 'surface features', 'discriminating features', 'basic features', 'also talk', 'also imagine', 'loss function', 'cost function', 'soft rules', 'used directly', 'train classifier', 'directly tackle', 'directly measure', 'category based', 'category based', 'particular label', 'category accurately', 'training data', 'training data', 'training data', 'training data', 'training data', 'training data', 'training data', 'training data', 'training data', 'well defined', 'weighted matter', 'text objects', 'text objects', 'text articles', 'sufficient knowledge', 'several problems', 'one disadvantage', 'often used', 'often tide', 'labor intensive', 'indirectly captures', 'future access', 'face uncertainty', 'domain knowledge', 'design carefully', 'classifiers try', 'another topic', 'text categorization', 'text categorization', 'text categorization', 'text categorization', 'label distribution', 'categorization problems', 'generated given', 'clear rules', 'used using', 'also figure', 'categorization problem', 'x given', 'right category', 'particular category', 'like', 'categorization may', 'word', 'two kinds', 'deterministically decide', 'clearly decide', 'actually likelihood', 'rules may', 'label x', 'general setting', 'general goal', 'two ways', 'special vocabulary', 'important thing', 'examples include', 'topical categorisation', 'potentially provide', 'conditional probability', 'conditional probability', 'conditional probability', 'base rule', 'training errors', 'one example', 'means', 'keywords', 'likely category', 'category labels', 'minimize errors', 'say exactly', 'learning', 'would', 'would', 'feature', 'called', 'features', 'features', 'features', 'features', 'also', 'also', 'function', 'need', 'methods', 'given', 'automatic', 'classifiers', 'classifiers', 'classifier', 'data', 'data', 'data', 'data', 'label', 'label', 'well', 'well', 'topic', 'text', 'text', 'text', 'put', 'problems', 'problems', 'one', 'often', 'models', 'distinguish', 'combination', 'cause', 'training', 'sports', 'sports', 'sports', 'sports', 'sports', 'sports', 'categorization', 'categories', 'categories', 'categories', 'categories', 'problem', 'problem', 'problem', 'rules', 'rules', 'rules', 'rules', 'rules', 'using', 'using', 'try', 'try', 'distribution', 'distribution', 'conditional', 'may', 'category', 'category', 'category', 'category', 'category', 'probability', 'probability', 'probability', 'minimize', 'learn', 'learn', 'learn', 'kinds', 'generated', 'first', 'first', 'first', 'exactly', 'decide', 'decide', 'decide', 'actually', 'general', 'general', 'general', 'x', 'x', 'x', 'x', 'ways', 'ways', 'vocabulary', 'vocabulary', 'model', 'model', 'model', 'model', 'model', 'likely', 'likely', 'labels', 'labels', 'important', 'important', 'figure', 'figure', 'examples', 'examples', 'errors', 'errors', 'errors', 'errors', 'say', 'say', 'say', 'rule', 'rule', 'rule', 'provide', 'provide', 'provide', 'categorisation', 'categorisation', 'categorisation', 'example', 'example', 'example', 'example', 'example', 'words', 'words', 'whatever', 'weights', 'weights', 'want', 'vary', 'vary', 'values', 'value', 'value', 'value', 'useful', 'ultimately', 'turns', 'tradeoffs', 'thus', 'tell', 'sure', 'sure', 'start', 'specifically', 'specifically', 'sorry', 'sorry', 'sometimes', 'sometimes', 'solved', 'simulate', 'simple', 'setup', 'set', 'set', 'separation', 'seen', 'see', 'secondly', 'secondly', 'secondly', 'scratch', 'scale', 'saying', 'robustness', 'results', 'results', 'requires', 'represent', 'reflect', 'recognize', 'receive', 'punctuations', 'provisions', 'provided', 'product', 'prediction', 'predict', 'powerful', 'person', 'pattern', 'output', 'output', 'output', 'order', 'optimized', 'occurrences', 'occur', 'obviously', 'next', 'needed', 'method', 'method', 'mention', 'measuring', 'map', 'map', 'make', 'lot', 'looking', 'look', 'lectures', 'lecture', 'lecture', 'known', 'kind', 'judgement', 'join', 'join', 'instead', 'input', 'input', 'inconsistent', 'idea', 'humans', 'however', 'hope', 'help', 'hard', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'generate', 'games', 'finally', 'factored', 'estimate', 'effective', 'effective', 'easy', 'domains', 'documents', 'document', 'document', 'discuss', 'deterministic', 'determine', 'detail', 'designing', 'cover', 'course', 'course', 'course', 'correct', 'correct', 'contradictory', 'concern', 'computers', 'computer', 'computer', 'compute', 'compute', 'complex', 'combining', 'clue', 'classes', 'case', 'case', 'case', 'case', 'basketball', 'basis', 'attempts', 'assigned', 'approaches', 'approaches', 'approach', 'approach', 'approach', 'applying', 'applied', 'applied', 'amp', 'allows', 'alleviated', 'able', '1st', '100', '1']
lecture 38
['also called idf weighting inverse document frequency weighting', 'naive bayes scoring function actually makes sense', 'background model would cause nonuniform smoothing', 'simply add small nonnegative constant delta', 'would help us preserve precision', 'also see another smoothing parameter mu', 'clearly see naive bayes classifier', 'strong connection close connection', 'classifier called logistical regression', 'actually adapt document clustering models', 'help achieve discriminative weighting', 'course human experts must', 'bayes rule allows us', 'maximum likelihood estimator indeed', 'would bring every word distribution', 'particular topic word distributions theta', 'also add k multiplied', 'also add k multiplied', 'use maximum likelihood estimator', 'added delta k times', 'simplified unigram language model', 'many text categorization tasks', 'seen also multiple times', 'use generative probabilistic models', 'seen naive bayes classifier', 'corresponding weight beta sub', 'idf weighting', 'l words represent represent', 'would help us make', 'generative probabilistic models', 'naive bayes classifier', 'naive bayes classifier', 'naive bayes classifier', 'words smaller across categories', 'naiyes bayes classifier', 'also several times', 'every word every category', 'see something really interesting', 'n sub one documents', 'delta approaches positive infinity', 'cause underflow problem', 'multiple topics represented', 'also generally true', 'trigram language model', 'bigram language model', 'actually quite effective', 'might indeed ignore', 'using bayes rule', 'using bayes rule', 'smoothing allows us', 'background language model', 'background language model', 'background language model', 'background language model', 'background language model', 'background language model', 'background language model', 'background language model', 'background language model', 'background language model', 'address data sparseness', 'use bayes rule', 'use generative models', 'use larger text data', 'using machine learning', 'assumption allows us', 'naiyes bayes categorization', 'actual generative model', 'mining word relations', 'feature vector f', 'constant beta zero', 'see n sub', 'left hand side', 'added pseudo counts', 'interesting special cases', 'feature support category one', 'mu approaches infinity', 'word helps contributing', 'bring prior knowledge', 'word distribution explains', 'word distribution characterizes', 'constant pseudo count', 'theta sub b', 'longer mixture model', 'also called', 'uniform pseudo counts', 'statistical estimation problem', 'avoid small probabilities', 'second probability depends', 'penalize common words', 'one possible way', 'sometimes multiple categories', 'two uniform distribution', 'training data available', 'generally take logarithm', 'supporting category one', 'total pseudo counts', 'get posterior becausw', 'training data set', 'document looks like', 'say every word', 'naive bayes', 'naive bayes', 'naive bayes', 'prior knowledge says', 'original estimate based', 'one word distribution', 'training data right', 'example document length', 'observed training data', 'observed training data', 'bayes rule', 'language models', 'generative models', 'actually represents category', 'mu multiplied', 'represents indeed category', 'known categories assigned', 'one data set', 'another issue', 'highest posterior probability', 'smoothing parameter', 'get higher probability', 'topic word distribution', 'estimated probability would', 'actually achieve', 'see clearly', 'sense irrelevant', 'background model', 'background model', 'background model', 'background model', 'add mu', 'also dealing', 'makes categorization', 'keyword bayes', 'also see', 'tells us', 'tells us', 'give us', 'would help', 'beta sub', 'mixture model', 'logistic regression', 'k categories', 'k categories', 'special cases', 'scoring function', 'scoring function', 'scoring function', 'scoring function', 'scoring function', 'pseudocounts added', 'every word', 'every word', 'beta zero', 'prior probability ratio', 'also non', 'training data', 'training data', 'training data', 'naive assumption', 'negative constant', 'general classifier', 'add delta', 'pseudo counts', 'pseudo counts', 'every category', 'every category', 'discriminative approaches', 'discriminative approaches', 'discriminative approaches', 'several parts', 'actually use', 'actually one', 'training dataset', 'discriminative classifiers', 'actual counts', 'hf sub', 'take logarithm', 'helps make', 'add pseudocounts', 'add pseudocounts', 'would happen', 'would happen', 'would happen', 'prior knowledge', 'second line', 'really depend', 'two distributions', 'two distributions', 'word distribution', 'word distribution', 'word distribution', 'simply count', 't2 represents', 'infinity amount', 'infinity amount', 'would like', 'smaller pseudocounts', 'conditional approaches', 'approaches later', 'actually set', 'would make', 'second part', 'second part', 'text clustering', 'text clustering', 'text clustering', 'one p', 'word distributions', 'word distributions', 'word distributions', 'word distributions', 'word distributions', 'like delta', 'text categorization', 'text categorization', 'text categorization', 'text categorization', 'text categorization', 'text categorization', 'two things', 'two problems', 'two kinds', 'two kinds', 'two kinds', 'two kinds', 'somewhere else', 'small amount', 'simply normalize', 'medical science', 'may notice', 'may notice', 'good exercise', 'extent observing', 'discussed earlier', 'directly adapted', 'appropriate settings', 'data set', 'likelihood part', 'actually log', 'estimation problem', 'small probabilities', 'small probabilities', 'documents represented', 'control smoothing', 'higher level', 'category would', 'formula actually', 'larger delta', 'common words', 'common words', 'training documents', 'training documents', 'training documents', 'make sure', 'make sure', 'make sure', 'make sure', 'make sure', 'make sure', 'word serves', 'word ok', 'highest probability', 'theta one', 'theta one', 'theta one', 'one way', 'two categories', 'two categories', 'two categories', 'two categories', 'predefined categories', 'predefined categories', 'different categories', 'different categories', 'different categories', 'difference would', 'course would', 'seen text', 'feature value', 'words would', 'theta 2', 'theta 2', 'theta 2', 'called', 'count would', 'count would', 'posterior probability', 'posterior probability', 'posterior probability', 'posterior probability', 'even use', 'represents category', 'weight would', 'world given', 'whole set', 'slide shows', 'previous slide', 'extra number', 'even though', 'empirically set', 'certain number', 'basically based', 't1 represents', 'word like', 'highest score', 'highest score', 'two probabilities', 'special case', 'often sufficient', 'often face', 'zero probability', 'zero probability', 'zero probability', 'zero probability', 'probability based', 'conditional probability', 'one category', 'category one', 'category one', 'category one', 'category one', 'category one', 'category one', 'category one', 'category one', 'specific category', 'considering category', 'category two', 'category two', 'category slightly', 'category based', 'document independently', 'document belongs', 'say one', 'one cluster', 'estimate would', 'third reason', 'multiply value', 'main difference', 'large collection', 'important technique', 'go back', 'following intuition', 'essentially removes', '1 cluster', 'obvious delta', 'words may', 'rare words', 'really know', 'higher probability', 'higher probability', 'higher prior', 'zero count', 'observed frequently', 'score based', 'score based', 'generated using', 'document clustering', 'document clustering', 'classify documents', 'higher according', 'precisely like', 'sense', 'makes', 'next lecture', 'easily understand', 'accurately representing', 'high probabilities', 'high probabilities', 'words make', 'highest product', 'smoothing formula', 'see documents', 'generated independently', 'generated independently', 'simplest case', 'simple case', 'categorization problem', 'topic given', 'font size', 'also', 'also', 'ratio basically', 'already know', 'typically assume', 'prior probability', 'question boils', 'negative evidence', 'probability ratio', 'probability ratio', 'ann let', 'word occurrences', 'feature weights', 'naive', 'sports category', 'first part', 'cluster document', 'assign document', 'take sum', 'constant', 'bring', 'general form', 'general form', 'often write', 'probability proportional', 'prior information', 'category bias', 'probability estimate', 'help', 'help', 'put together', 'background', 'mu', 'simply', 'collect evidence', 'words tend', 'best guess', 'words sum', 'document well', 'general problem', 'understand whether', 'interesting', 'work well', 'generate documents', 'case fi', 'likely occur', 'model', 'model', 'model', 'model', 'model', 'model', 'something', 'simplified', 'represent', 'represent', 'positive', 'close', 'add', 'add', 'add', 'actually', 'actually', 'actually', 'actually', 'distribution', 'distribution', 'distribution', 'infinity', 'indeed', 'indeed', 'uniform', 'sub', 'sub', 'sub', 'smaller', 'get', 'generally', 'data', 'data', 'data', 'data', 'would', 'would', 'would', 'would', 'would', 'would', 'small', 'small', 'small', 'really', 'distributions', 'using', 'using', 'represents', 'likelihood', 'likelihood', 'likelihood', 'delta', 'delta', 'delta', 'delta', 'delta', 'delta', 'text', 'text', 'posterior', 'approaches', 'approaches', 'use', 'use', 'use', 'use', 'use', 'use', 'use', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'zero', 'two', 'total', 'support', 'support', 'says', 'longer', 'function', 'function', 'function', 'function', 'function', 'becausw', 'based', 'available', 'counts', 'counts', 'counts', 'counts', 'clustering', 'clustering', 'clustering', 'clustering', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'smoothing', 'smoothing', 'smoothing', 'smoothing', 'smoothing', 'smoothing', 'smoothing', 'smoothing', 'smoothing', 'smoothing', 'smoothing', 'higher', 'higher', 'categorization', 'categorization', 'categorization', 'categorization', 'categorization', 'like', 'like', 'take', 'take', 'pseudocounts', 'part', 'make', 'make', 'make', 'make', 'make', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'word', 'categories', 'categories', 'categories', 'categories', 'categories', 'categories', 'categories', 'categories', 'categories', 'feature', 'feature', 'feature', 'feature', 'feature', 'way', 'way', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'theta', 'amount', 'sometimes', 'sometimes', 'slide', 'set', 'set', 'set', 'set', 'set', 'set', 'number', 'negative', 'larger', 'larger', 'larger', 'larger', 'high', 'hand', 'hand', 'given', 'form', 'example', 'example', 'even', 'estimated', 'estimated', 'basically', 'seen', 'seen', 'seen', 'seen', 'seen', 'seen', 'seen', 'seen', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'probabilities', 'often', 'often', 'assumption', 'assumption', 'assumption', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'prior', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'ratio', 'ratio', 'ratio', 'ratio', 'ratio', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'work', 'together', 't1', 'sports', 'say', 'say', 'say', 'say', 'right', 'right', 'right', 'reason', 'precisely', 'occur', 'non', 'multiply', 'log', 'important', 'go', 'following', 'evidence', 'evidence', 'essentially', 'difference', 'difference', 'course', 'course', 'course', 'course', 'course', 'course', 'collection', 'cluster', 'cluster', 'cluster', 'best', 'assign', 'according', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'count', 'count', 'count', 'count', 'count', 'count', 'count', 'count', 'observed', 'observed', 'observed', 'observed', 'observed', 'observed', 'observed', 'observed', 'weight', 'weight', 'weight', 'weight', 'weight', 'weight', 'weight', 'score', 'score', 'score', 'score', 'score', 'formula', 'formula', 'formula', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'problem', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'write', 'write', 'whether', 'whether', 'weights', 'weights', 'understand', 'understand', 'understand', 'understand', 'put', 'put', 'proportional', 'proportional', 'occurrences', 'occurrences', 'obvious', 'obvious', 'lecture', 'lecture', 'information', 'information', 'fi', 'fi', 'bias', 'bias', 'accurately', 'accurately', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'generated', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'tend', 'tend', 'tend', 'size', 'size', 'size', 'normalize', 'normalize', 'normalize', 'known', 'known', 'known', 'known', 'known', 'known', 'known', 'generate', 'generate', 'generate', 'first', 'first', 'first', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'collect', 'collect', 'collect', 'know', 'know', 'know', 'know', 'know', 'know', 'know', 'sum', 'sum', 'sum', 'sum', 'sum', 'sum', 'sum', 'sum', 'guess', 'guess', 'guess', 'guess', 'assume', 'assume', 'assume', 'assume', 'question', 'question', 'question', 'question', 'question', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'let', 'let', 'let', 'let', 'let', 'let', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'likely', 'xi', 'want', 'want', 'want', 'want', 'want', 'want', 'want', 'want', 'want', 'w', 'vocabulary', 'vocabulary', 'vocabulary', 'video', 'vary', 'values', 'useful', 'useful', 'useful', 'useful', 'used', 'used', 'used', 'used', 'update', 'understandable', 'uncertainty', 'try', 'top', 'time', 'time', 'time', 'thought', 'thought', 'think', 'think', 'think', 'think', 'think', 'think', 'think', 'think', 'think', 'think', 'think', 'therefore', 'therefore', 'therefore', 'therefore', 'terms', 'talked', 'talk', 'talk', 'talk', 'talk', 'symbols', 'suppose', 'suppose', 'suppose', 'state', 'spend', 'spend', 'specifically', 'sorry', 'sorry', 'solve', 'solve', 'solve', 'solve', 'solve', 'solve', 'slides', 'situations', 'simplify', 'similarity', 'similar', 'similar', 'shown', 'separately', 'rigorously', 'remember', 'rely', 'rely', 'relevant', 'related', 'related', 'related', 'related', 'receive', 'realize', 'reality', 'primarily', 'pretend', 'popular', 'popular', 'picture', 'pick', 'perspective', 'perhaps', 'pause', 'patterns', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'order', 'order', 'obviously', 'observe', 'observe', 'observe', 'observe', 'notation', 'normalizer', 'need', 'nature', 'naturally', 'namely', 'namely', 'much', 'mostly', 'moment', 'moment', 'moment', 'mixing', 'means', 'means', 'means', 'means', 'means', 'means', 'meaning', 'maximize', 'maximize', 'maximize', 'marked', 'made', 'made', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'looking', 'look', 'look', 'look', 'learned', 'kind', 'kind', 'kind', 'involves', 'intuitively', 'introduced', 'introduce', 'inside', 'inject', 'influenced', 'influence', 'influence', 'inference', 'indicates', 'independence', 'including', 'imagine', 'imagine', 'idea', 'idea', 'idea', 'idea', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'goal', 'generating', 'generating', 'generalized', 'generalization', 'follows', 'find', 'find', 'find', 'find', 'finally', 'figure', 'features', 'features', 'features', 'features', 'features', 'features', 'favor', 'favor', 'fact', 'fact', 'faced', 'explain', 'explain', 'expect', 'except', 'events', 'event', 'event', 'etc', 'estimating', 'estimating', 'equivalent', 'effectively', 'effect', 'documentary', 'distinction', 'disappear', 'differences', 'dictated', 'details', 'dependent', 'denotes', 'denoted', 'denote', 'denote', 'denote', 'denominator', 'denominator', 'defined', 'decompose', 'decision', 'decision', 'decision', 'deciding', 'deciding', 'decide', 'could', 'could', 'controls', 'content', 'content', 'contain', 'contain', 'constraints', 'constraints', 'constraint', 'consider', 'consider', 'consequences', 'connected', 'compute', 'compute', 'comparing', 'come', 'clusters', 'clusters', 'classified', 'classification', 'classification', 'choose', 'choose', 'choose', 'choose', 'changed', 'change', 'change', 'call', 'c', 'bottom', 'betas', 'belong', 'belief', 'becomes', 'basis', 'basis', 'assuming', 'approach', 'applying', 'appear', 'answer', 'answer', 'allocate', 'aggregate', 'aggregate', 'addition', 'adding', 'accurate', 'accurate', 'accommodate', 'accommodate', '0', '0', '0']
lecture 39
['maximum likelihood estimator basically lets go find', 'also introduce another discriminative classifier called k nearest neighbors', 'although naive bayes classifier tries', 'documents first class theta 1', 'called k nearest neighbors', 'called k nearest neighbors', 'bringing additional data points', 'k nearest neighbor works', 'lets say theta 1', 'help us decide categories', 'lets say category 1', 'similarity function captures objects', 'well defined scoring function', 'actually use bayes rule', 'k nearest neighbors', 'function form looks like', 'general represent text data', 'using bayes rule', 'using bayes rule', 'using cross validation', 'solid field boxes', 'binary response variable', 'binary response variable', 'actually would assume explicitly', 'naive base classifier', 'cover discriminative approaches', 'naive bayes classifier', 'betavalues already known', 'actually far away', 'machine learning programs', 'machine learning algorithms', 'results might depend', 'standard optimization problem', 'assumed functional form', 'likelihood function would', 'seen science etc', 'actually could risk', 'including text categorization', 'actually three neighbors', 'would vary depending', 'would allow us', 'similarity function either', 'help us classify', 'follow similar distributions', 'help us estimate', 'general scoring function', 'course would depend', 'similarity function could', 'machine learning course', 'would allow many', 'data directly rather', 'training data set', 'logistical regression function', 'commonly used classifiers', 'actually important assumption', 'documents training documents', 'training set would', 'another possibility', 'another advantage', '1 minus probability', 'use one part', 'data objective since', 'take weighted sum', 'many possible ways', 'label given data', '1 given x', 'log odds ratio', 'given x directly', 'particular training case', 'naive bayes', 'naive bayes', 'naive bayes', 'naive bayes', 'discriminative approaches', 'basically feature vector', 'discriminative classifiers', 'discriminative classifiers', 'classifier would', 'category given document', 'actually help', 'value x sub', 'machine learning', 'machine learning', 'k examples', 'indeed k', 'looks like', 'explicitly requires', 'conditional likelihood', 'conditional likelihood', 'conditional likelihood', 'condition likelihood', 'useful classifier', 'second classifier', 'scoring function', 'scoring function', 'scoring function', 'parameter k', 'first think', 'objective function', 'training data', 'training data', 'training data', 'training data', 'training data', 'functional form', 'text data', 'similarity function', 'similarity function', 'similarity function', 'similarity function', 'similarity function', 'similarity function', 'nn works', 'many ways', 'function form', 'function form', 'weighted combination', 'two distributions', 'gives us', 'gives us', 'called', 'given data', 'logistical function', 'theta given', 'particular form', 'text categorization', 'text categorization', 'text categorization', 'text categorization', 'future data', 'data instances', 'theta 2', 'training documents', 'define similarity', 'also written', 'also solved', 'also changed', 'logistical regression', 'logistical regression', 'logistical regression', 'training instances', 'training examples', 'works well', 'known categories', 'actually express', 'assumption would', 'category theta', 'important parameter', 'many times', 'many times', 'training set', 'neighbors around', 'getting neighbors', 'four neighbors', 'four neighbors', 'logistic regression', 'logistic regression', 'logistic regression', 'logistic regression', 'logistic regression', 'also look', 'word given', 'labels given', 'even given', 'particular category', 'two parts', 'thorough introduction', 'quite useful', 'potentially advantages', 'potential problems', 'nice properties', 'next step', 'may recall', 'log odds', 'locally smoothed', 'linear combination', 'linear combination', 'largely determined', 'known label', 'key point', 'good opportunity', 'give us', 'getting rid', 'feature weights', 'distinguishing objects', 'different form', 'different form', 'decision may', 'continue talking', 'closest neighbor', 'closest neighbor', 'closely tied', 'closely related', 'clearly beyond', 'box filled', 'basic introduction', 'also precisely', 'general idea', 'general idea', 'also similar', 'one minus', 'either one', 'categorization problem', 'two categories', 'two categories', 'two categories', 'also estimate', 'set empirically', 'also illustrates', 'key assumption', 'key assumption', 'feature values', 'feature values', 'feature values', 'feature values', 'beta values', 'beta values', 'beta values', 'beta values', 'category given', 'category given', 'directly estimate', 'precise idea', 'basic idea', 'text object', 'text object', 'text object', 'text object', 'category based', 'used fi', 'particular xi', 'neighbors right', 'might use', 'look like', 'label given', 'actually regarded', 'would make', 'different categories', 'different categories', 'total number', 'total number', 'feature vector', 'conditional distribution', 'conditional distribution', 'introduce', 'introduce', 'given x', 'given x', 'given x', 'given x', 'given x', 'really similar', 'training cases', 'seen sports', 'right side', 'beta value', 'rough estimate', 'different category', 'different category', 'would model', 'category look', 'word count', 'votes based', 'simply count', 'previous slide', 'previous slide', 'parameter values', 'parameter values', 'newtons method', 'new object', 'intuitively imagine', 'current object', 'current object', 'close neighbor', 'classification tasks', 'given document', 'topic one', 'one neighbor', 'popular category', 'probability ratios', 'probability ratio', 'modeled probability', 'conditional probability', 'conditional probability', 'conditional probability', 'conditional probability', 'conditional probability', 'precisely similar', 'different way', 'x vector', 'special case', 'entire region', 'easily see', 'category one', 'category one', 'k', 'k', 'word features', 'effective features', 'popular way', 'larger neighborhood', 'basically assume', 'observed x', 'modeling x', 'document right', 'model set', 'see basically', 'eventually rewrite', 'although', 'one hand', 'basically try', 'classifier', 'classifier', 'classifier', 'basically going', 'region r', 'us', 'represent', 'data', 'theta', 'theta', '1', '1', '1', '1', '1', 'would', 'would', 'would', 'could', 'also', 'also', 'also', 'allow', 'function', 'function', 'function', 'function', 'directly', 'form', 'actually', 'actually', 'actually', 'actually', 'actually', 'many', 'general', 'general', 'neighbors', 'neighbors', 'neighbors', 'neighbors', 'neighbors', 'neighbors', 'neighbors', 'neighbors', 'neighbors', 'neighbors', 'documents', 'documents', 'documents', 'documents', 'given', 'given', 'given', 'sum', 'sub', 'ratio', 'rather', 'objects', 'objects', 'neighbor', 'neighbor', 'might', 'like', 'like', 'find', 'find', 'find', 'find', 'find', 'feature', 'depending', 'approaches', 'categories', 'categories', 'categories', 'set', 'set', 'set', 'values', 'values', 'assumption', 'assumption', 'idea', 'classifiers', 'classifiers', 'different', 'different', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'used', 'used', 'use', 'use', 'use', 'use', 'use', 'say', 'say', 'say', 'say', 'say', 'say', 'problem', 'problem', 'look', 'basically', 'basically', 'basically', 'basically', 'basically', 'basically', 'basically', 'basically', 'basically', 'basically', 'vector', 'vector', 'vector', 'take', 'take', 'precisely', 'popular', 'parameter', 'parameter', 'number', 'label', 'label', 'label', 'distribution', 'course', 'course', 'course', 'course', 'classify', 'classify', 'similar', 'similar', 'similar', 'similar', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'value', 'value', 'value', 'seen', 'seen', 'seen', 'right', 'right', 'well', 'well', 'well', 'well', 'well', 'estimate', 'estimate', 'estimate', 'estimate', 'estimate', 'xi', 'way', 'way', 'votes', 'slide', 'slide', 'rewrite', 'regarded', 'possible', 'possible', 'possible', 'observed', 'object', 'object', 'object', 'object', 'object', 'object', 'object', 'modeling', 'method', 'log', 'log', 'log', 'log', 'intuitively', 'illustrates', 'hand', 'give', 'eventually', 'count', 'count', 'close', 'classification', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'document', 'document', 'document', 'document', 'document', 'document', 'assume', 'assume', 'assume', 'assume', 'assume', 'assume', 'assume', 'assume', 'assume', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'probability', 'think', 'think', 'r', 'r', 'make', 'make', 'cases', 'cases', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'since', 'since', 'since', 'since', 'since', 'since', 'since', 'see', 'see', 'see', 'see', 'see', 'see', 'region', 'region', 'region', 'region', 'region', 'region', 'features', 'features', 'features', 'features', 'features', 'features', 'features', 'features', 'features', 'try', 'try', 'try', 'try', 'try', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'neighborhood', 'neighborhood', 'neighborhood', 'neighborhood', 'neighborhood', 'neighborhood', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'zero', 'zero', 'zero', 'zero', 'zero', 'xs', 'work', 'words', 'words', 'whether', 'whether', 'want', 'want', 'want', 'want', 'want', 'want', 'want', 'want', 'want', 'want', 'vote', 'vote', 'vote', 'vote', 'typically', 'true', 'transformation', 'topics', 'terms', 'talk', 'taking', 'taking', 'suppose', 'suppose', 'suppose', 'specifically', 'specifically', 'solve', 'small', 'signals', 'signals', 'shown', 'show', 'separate', 'sense', 'scope', 'saying', 'saying', 'roughly', 'reliable', 'red', 'reality', 'read', 'reached', 'range', 'question', 'put', 'product', 'proceed', 'proceed', 'predictors', 'prediction', 'plugging', 'plug', 'pink', 'performance', 'parameters', 'parameters', 'pair', 'p', 'order', 'order', 'optimize', 'optimize', 'optimize', 'opposed', 'ok', 'ok', 'ok', 'often', 'obviously', 'observing', 'observe', 'numerator', 'notice', 'note', 'note', 'notation', 'notation', 'normalize', 'need', 'need', 'need', 'necessary', 'much', 'moment', 'minimize', 'methods', 'mentioned', 'mentioned', 'meant', 'means', 'means', 'means', 'means', 'maximizing', 'maximize', 'maximize', 'maximize', 'maximize', 'maximize', 'mathematically', 'mapped', 'makes', 'made', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'logarithm', 'likely', 'let', 'let', 'let', 'let', 'let', 'let', 'let', 'let', 'let', 'let', 'lecture', 'lecture', 'lecture', 'know', 'know', 'know', 'knn', 'knn', 'kind', 'kind', 'keep', 'intuition', 'introduced', 'interested', 'interested', 'interested', 'interested', 'instance', 'insights', 'input', 'inject', 'influence', 'indirectly', 'include', 'include', 'improved', 'illustrate', 'high', 'helpful', 'happen', 'goal', 'goal', 'get', 'get', 'get', 'get', 'get', 'generation', 'generation', 'generation', 'found', 'found', 'found', 'found', 'formula', 'formula', 'following', 'finding', 'finding', 'falls', 'fact', 'fact', 'explain', 'example', 'estimating', 'equivalent', 'end', 'emphasize', 'draw', 'distances', 'distance', 'discussion', 'differently', 'diamonds', 'diamonds', 'design', 'design', 'depends', 'dependent', 'dependency', 'dependency', 'dependency', 'denoted', 'denote', 'denominator', 'corresponding', 'controlled', 'considering', 'conclude', 'compute', 'compute', 'compute', 'compute', 'compute', 'common', 'common', 'colored', 'closed', 'classifying', 'classifying', 'choose', 'center', 'capture', 'c', 'best', 'benefit', 'assuming', 'assign', 'assign', 'assign', 'assess', 'approach', 'approach', 'approach', 'approach', 'approach', 'amp', 'among', 'adjust', 'accurate', 'accurate', 'according', '0', '0', '0', '0', '0']
lecture 40
['advanced machine learning techniques called domain adaptation', 'new machine learning methods like representation learning', 'semi supervised machine learning techniques', 'introduce yet another discriminative classifier called', 'features might help us distinguish categories', 'original high dimensional space corresponding', 'beta0 beta 1 beta 2', 'humans definitely play important role', 'could assume five star reviews', 'could give us another line', 'use supervised machine learning', 'high dimensional feature space', 'low dimensional space features', 'notations people usually use', 'high confidence classification results', 'k dimensional space instead', 'justice k values corresponding', 'really exploit unable data', 'maybe combine multiple methods', 'errors systematically across categories', 'low quality training examples', 'highly non linear classifier', 'require effective feature representation', 'supervised machine learning', 'actually help us reduce', 'texas domain cause words', 'help us design features', 'also mention negative opinions', 'really much general thing', 'five star reviews', 'high dimensional space', 'seen earlier actually uses', 'machine learning techniques', 'machine learning techniques', 'low dimensional structures', 'actually training label examples', 'would combine different methods', 'design effective feature set', 'classifier would also help', 'two dimensional space', 'lower dimensional space', 'topic space representation', 'topic define one dimension', 'also optimize different objects', 'minimize w transpose multiplied', 'new texture objects', 'goes beyond bag', 'another common scene', 'learning effective representation', 'support vector machine', 'support vector machine', 'corresponding wedding design', 'different learning refers', 'classifying new objects', 'original feature representation', 'complicated tasks like', 'require effective features', 'would give us', 'deep neural network', 'learning effective features', 'original word features', 'intermediate features embedded', 'compound features automatically', 'pseudo training examples', 'popular classification method', 'feature design tends', 'nearest neighbor way', 'naive bayes classifier', 'inducing intuitively makes', 'complex network effectively', 'call support vectors', 'unable text data', 'use another threshold', 'text processing tends', 'learn intermediate representations', 'help categorizing tweets', 'support vector machines', 'non 0 value', 'design new features', 'latent dimension features', 'introduced many methods', 'immediately effective classifier', 'design effective features', 'learn factor features', 'need domain knowledge', 'svm actually tries', 'sentiment categorizer meaning', 'real world variables', 'logistical regression classifier', 'involves human labor', 'one actually captures', 'different data sets', 'large value would see', 'example word count', 'also would like', 'machine learning', 'left hand side', 'makes 0 value', 'two public categories', 'particular point like', 'could also use', 'true training examples', 'compare different methods', 'text categorisation methods', 'one important advantage', 'identical x one', 'also using locates', 'feature vector x', 'many different ways', 'many different ways', 'original object function', 'often another way', 'general also achievable', 'completely separate bowl', 'actually wait set', 'combine label data', 'denote beta zero', 'rather negative one', 'w vector multiplied', 'achieve good performance', 'positive training examples', 'data points actually', 'unlabeled text documents', 'related training data', 'simple optimization problem', 'improve categorization method', 'classifier value must', 'text categorization problem', 'techniques called', 'still learn something', 'high quality', 'one side would', 'choose threshold one', 'training data set', 'performance might also', 'discriminative methods', 'want every instance', 'two vectors together', 'domain knowledge', 'two different instances', 'center separate line', 'right hand side', 'give us', 'give us', 'basically optimization problem', 'ann one reason', 'training domain', 'learning representations', 'data points may', 'zero point corresponds', 'parameter values b', 'deep learning', 'deep learning', 'deep learning', 'new representation', 'high values', 'learning features', 'secondary role', 'original 1', 'feature representation', 'feature representation', 'people would', 'corresponding label', 'feature design', 'distinguish positive', 'transfer learning', 'supervise learning', 'allowed us', 'would like', 'new world', 'vector beta', 'many tasks', 'training examples', 'training examples', 'training examples', 'beta two', 'also techniques', 'season 2', 'k topics', 'categorisation results', 'another way', 'another way', 'design features', 'support vectors', 'support vectors', 'support vectors', 'use beta', 'beta one', 'help annotate', 'negative opinions', 'feature vector', 'feature vector', 'effective features', 'discriminative classifiers', 'multiple lines', 'multiresolution representation', 'excellent representation', 'category 2', 'actually combine', 'topic models', 'topic models', 'categorize reviews', 'point like', 'involves data', 'another idea', 'really talked', 'really matter', 'really affect', 'humans invention', 'basically like', 'quite effective', 'feature selection', 'could imagine', 'learn categories', 'actually trainer', 'actually reliable', 'general method', 'allow us', 'particular problem', 'logistic regression', 'eyes must', 'dimension reduction', 'completely reliable', 'training data', 'training data', 'training data', 'training data', 'training data', 'training data', 'training data', 'training data', 'category tends', 'text data', 'text data', 'x vector', 'unlabeled data', 'unlabeled data', 'unlabeled data', 'one wait', 'side would', 'side would', 'text categorization', 'text categorization', 'new words', 'original constraint', 'known label', 'actually set', 'two values', 'two values', 'often much', 'another criteria', 'category 1', 'actually applied', 'actually applied', 'training laid', 'training categorisation', 'two categories', 'two categories', 'two categories', 'two categories', 'two categories', 'two categories', 'two categories', 'different way', 'different version', 'different signs', 'different kinds', 'different distributions', 'text object', 'text localization', 'text content', 'text concisely', 'sentiment analysis', 'parameter values', 'finding values', 'easily combine', 'another kind', 'see transpose', 'features provided', 'performance might', 'might wonder', 'future data', 'enable data', 'empirical data', 'categories defined', 'classifying topics', 'would happen', 'would happen', 'would happen', 'optimization problem', 'optimization problem', 'optimization algorithm', 'categorization problem', 'categorization problem', 'case actually', 'really optimize', 'new technique', 'training set', 'two sides', 'two domains', 'two classes', 'also plenty', 'also pick', 'also increase', 'also define', 'also combined', 'also changed', 'also becausw', 'also add', 'classification error', 'training points', 'linear osfm', 'linear classifier', 'actually find', 'data point', 'data instance', 'data instance', 'also use', 'various ways', 'unified way', 'typical biclustering', 'symbol approaches', 'suggestion reading', 'standard algorithms', 'speech recognition', 'speech recognition', 'specific problem', 'specific classifier', 'related problem', 'reason advance', 'quadratic function', 'previous formulation', 'porters define', 'overall ratings', 'often denote', 'metrics factorization', 'mathematical convenience', 'linearly separable', 'go back', 'go back', 'generous sufficient', 'generative models', 'generally hard', 'generalization capacity', 'extra variables', 'em algorithm', 'easily modify', 'easily affect', 'dot product', 'dominated mostly', 'cross validation', 'continued discussion', 'confusion matrix', 'conditional likelihood', 'computer vision', 'computer vision', 'clear winner', 'characterize content', 'categorisation task', 'application specific', 'added something', 'training errors', 'training errors', 'training errors', 'sometimes training', 'data set', 'data points', 'data points', 'data points', 'data points', 'future value', 'function value', 'f value', 'f value', 'data may', 'falsely use', 'season one', 'one side', 'also lines', 'category two', 'category two', 'simple case', 'simple case', 'training error', 'training error', 'general rule', 'onstar negative', 'negative 11', 'data instances', 'file w', 'daughter lines', 'bad lines', 'negative value', 'category ones', 'category cause', 'negative one', 'negative one', 'negative one', 'negative one', 'basically ensuring', 'different problems', 'different line', 'different coefficient', 'use w', 'closest points', 'text clustering', 'large value', 'vector form', 'objective function', 'objective function', 'objective function', 'easily applied', 'would determine', 'category one', 'category one', 'also minimize', 'also minimize', 'small value', 'small value', 'different class', 'news might', 'might need', 'specific instances', 'second goal', 'quadratic program', 'quadratic program', 'performing similarly', 'perform similarly', 'mainly affected', 'mainly affected', 'main goal', 'classified correctly', 'classified accurately', 'basic idea', 'basic idea', 'basic idea', 'linear constraints', 'linear constraints', 'linear constraints', 'optimization first', 'would allow', 'two boundaries', 'useful features', 'also verify', 'also maximize', 'error allowed', 'allowed error', 'simplest case', 'say x', 'make might', 'also separate', 'different mistakes', 'linear separator', 'linear separator', 'linear separator', 'linear separator', 'linear separate', 'linear separate', 'yet', 'three parameters', 'specific classifiers', 'somehow align', 'representing content', 'previous slide', 'next question', 'natural question', 'line separates', 'line defined', 'increase x1', 'document cannot', 'decision boundary', 'convenient way', 'classify documents', 'biased constant', 'better connected', 'higher value', 'classify value', 'nia separate', 'one question', 'also say', 'take care', 'even imagine', 'called', 'called', 'rain example', 'particularly useful', 'want lying', 'one class', 'one class', 'positive value', 'also think', 'say compare', 'provide replenishing', 'parameter c', 'make sure', 'instance xi', 'gonna say', 'error analysis', 'error analysis', 'one constraint', 'positive one', 'also see', 'set kci', 'basically working', 'soft margin', 'largest margin', 'one ci', 'aim weights', 'also shown', 'mixture model', 'could', 'well defined', 'already know', 'fact use', 'basically want', 'stay right', 'small number', 'constraints easy', 'weights w', 'weights w', 'margin basically', 'lines best', 'although w', 'useful words', 'techniques', 'techniques', 'play', 'maximize sorry', 'corresponding', 'bias constant', 'simply take', 'methods', 'methods', 'methods', 'help', 'help', 'help', 'like', 'still want', 'course sometimes', 'representation', 'new', 'design', 'maximize margin', 'reviews', 'multiplied', 'introduce', 'introduce', 'humans', 'simply look', 'effective', 'effective', 'feature', 'give', 'maximizing margin', 'actually', 'uses', 'reduce', 'dimension', 'classification', '1', 'would', 'would', 'learn', 'label', 'much', 'hand', '0', 'different', 'different', 'different', 'different', 'text', 'vector', 'vector', 'values', 'combine', 'combine', 'combine', 'features', 'features', 'features', 'features', 'features', 'features', 'features', 'features', 'features', 'features', 'features', 'x', 'might', 'might', 'might', 'important', 'important', 'important', 'data', 'data', 'categories', 'categories', 'categories', 'categories', 'optimization', 'categorization', 'two', 'two', 'also', 'also', 'also', 'also', 'also', 'also', 'also', 'also', 'linear', 'ways', 'ways', 'way', 'way', 'variables', 'using', 'tweets', 'true', 'svm', 'side', 'side', 'side', 'related', 'reason', 'problem', 'problem', 'problem', 'performance', 'performance', 'parameter', 'often', 'object', 'meaning', 'lower', 'intuitively', 'introduced', 'instead', 'instead', 'human', 'function', 'effectively', 'documents', 'denote', 'define', 'define', 'corresponds', 'compare', 'classifier', 'classifier', 'classifier', 'classifier', 'classifier', 'classifier', 'classifier', 'classifier', 'classifier', 'classifier', 'classifier', 'classifier', 'cause', 'cause', 'categorisation', 'ann', 'value', 'value', 'value', 'value', 'value', 'value', 'use', 'use', 'use', 'use', 'use', 'use', 'use', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'general', 'general', 'general', 'general', 'negative', 'negative', 'negative', 'negative', 'negative', 'point', 'point', 'point', 'w', 'w', 'w', 'w', 'w', 'lines', 'instance', 'instance', 'category', 'category', 'zero', 'zero', 'zero', 'large', 'large', 'large', 'set', 'set', 'set', 'set', 'set', 'set', 'set', 'basically', 'basically', 'basically', 'points', 'points', 'points', 'points', 'points', 'small', 'objective', 'may', 'may', 'happen', 'applied', 'analysis', 'topics', 'together', 'together', 'sometimes', 'similarly', 'program', 'instances', 'instances', 'instances', 'improve', 'improve', 'idea', 'idea', 'good', 'good', 'goal', 'errors', 'errors', 'errors', 'errors', 'errors', 'classified', 'assume', 'assume', 'assume', 'assume', 'assume', 'affected', 'words', 'words', 'words', 'words', 'words', 'minimize', 'minimize', 'minimize', 'minimize', 'minimize', 'right', 'right', 'right', 'error', 'error', 'error', 'error', 'error', 'constraints', 'constraints', 'constraints', 'case', 'case', 'case', 'xi', 'x1', 'working', 'verify', 'still', 'still', 'still', 'still', 'stay', 'sorry', 'slide', 'simply', 'simply', 'seen', 'seen', 'seen', 'seen', 'seen', 'representing', 'question', 'question', 'question', 'problems', 'parameters', 'news', 'need', 'need', 'need', 'need', 'maximize', 'maximize', 'maximize', 'line', 'line', 'line', 'line', 'line', 'line', 'line', 'line', 'line', 'line', 'line', 'line', 'line', 'kci', 'higher', 'form', 'first', 'find', 'easy', 'document', 'convenient', 'constant', 'constant', 'coefficient', 'clustering', 'classify', 'classify', 'classifiers', 'classifiers', 'boundary', 'boundaries', 'bias', 'better', 'align', 'added', 'separator', 'separator', 'separator', 'separator', 'separator', 'separate', 'separate', 'separate', 'separate', 'separate', 'separate', 'separate', 'separate', 'separate', 'separate', 'separate', 'separate', 'optimize', 'optimize', 'optimize', 'optimize', 'optimize', 'optimize', 'optimize', 'class', 'class', 'class', 'class', 'take', 'take', 'take', 'imagine', 'imagine', 'imagine', 'constraint', 'constraint', 'constraint', 'choose', 'choose', 'choose', 'choose', 'b', 'b', 'b', 'b', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'useful', 'useful', 'useful', 'useful', 'useful', 'example', 'example', 'example', 'example', 'example', 'example', 'want', 'want', 'want', 'want', 'want', 'want', 'want', 'want', 'want', 'want', 'technique', 'technique', 'say', 'say', 'say', 'say', 'say', 'say', 'say', 'say', 'provide', 'provide', 'number', 'number', 'maximizing', 'maximizing', 'make', 'make', 'make', 'make', 'determine', 'determine', 'criteria', 'criteria', 'ci', 'ci', 'c', 'c', 'margin', 'margin', 'margin', 'margin', 'margin', 'margin', 'margin', 'margin', 'margin', 'margin', 'margin', 'margin', 'margin', 'margin', 'think', 'think', 'think', 'mistakes', 'mistakes', 'mistakes', 'kind', 'kind', 'kind', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'weights', 'weights', 'weights', 'weights', 'weights', 'weights', 'weights', 'weights', 'weights', 'weights', 'weights', 'model', 'model', 'model', 'model', 'fact', 'fact', 'fact', 'fact', 'best', 'best', 'best', 'best', 'although', 'although', 'although', 'although', 'allow', 'allow', 'allow', 'allow', 'allow', 'allow', 'allow', 'allow', 'allow', 'well', 'well', 'well', 'well', 'well', 'course', 'course', 'course', 'course', 'course', 'look', 'look', 'look', 'look', 'look', 'look', 'know', 'know', 'know', 'know', 'know', 'know', 'shown', 'shown', 'shown', 'shown', 'shown', 'shown', 'shown', 'yi', 'yi', 'x2', 'x2', 'x2', 'vm', 'vm', 'vm', 'visualized', 'valuable', 'uvm', 'used', 'used', 'used', 'used', 'unfortunately', 'understand', 'turns', 'turns', 'trying', 'trying', 'try', 'try', 'try', 'try', 'train', 'train', 'tradeoff', 'time', 'tide', 'tide', 'tend', 'tend', 'talk', 'sven', 'summarize', 'sum', 'sum', 'squares', 'specifically', 'sparsity', 'solving', 'solve', 'solve', 'solve', 'slides', 'similar', 'similar', 'similar', 'similar', 'similar', 'signals', 'sign', 'sign', 'sign', 'sign', 'shows', 'show', 'setup', 'settled', 'sense', 'select', 'seek', 'satisfy', 'said', 'robustness', 'robust', 'result', 'represented', 'represented', 'represented', 'represented', 'represent', 'relationship', 'region', 'regarding', 'recognize', 'recently', 'recall', 'ready', 'readings', 'purely', 'pros', 'properties', 'promising', 'promise', 'principle', 'prevent', 'predictor', 'practice', 'powerful', 'p', 'overfit', 'otherwise', 'otherwise', 'order', 'order', 'order', 'order', 'optimized', 'ok', 'ok', 'obviously', 'obtained', 'obtain', 'obtain', 'notation', 'nice', 'neighbors', 'needs', 'naturally', 'multiply', 'multiplication', 'moment', 'modification', 'mistake', 'minimizing', 'minimized', 'middle', 'mentioned', 'means', 'means', 'means', 'means', 'means', 'means', 'means', 'means', 'means', 'maximization', 'margins', 'margins', 'mapped', 'magnitude', 'made', 'lot', 'lot', 'lot', 'lot', 'long', 'leverage', 'let', 'let', 'let', 'let', 'let', 'let', 'let', 'let', 'let', 'let', 'let', 'less', 'lecture', 'lecture', 'lda', 'larger', 'labeling', 'labeled', 'labeled', 'job', 'job', 'invented', 'internet', 'interesting', 'insights', 'insights', 'inequality', 'indicate', 'incidences', 'illustrated', 'identify', 'hyperplane', 'heading', 'happens', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'getting', 'get', 'get', 'get', 'generate', 'gamma', 'functions', 'functions', 'follow', 'finally', 'figure', 'expression', 'expression', 'expression', 'expression', 'expect', 'examine', 'examine', 'examine', 'examine', 'exactly', 'evaluate', 'essentially', 'especially', 'especially', 'equivalent', 'equivalent', 'equal', 'equal', 'ensure', 'ensure', 'elements', 'effectiveness', 'ecfmg', 'distance', 'discussed', 'direction', 'diamonds', 'developed', 'determining', 'determined', 'determined', 'determined', 'details', 'critical', 'covered', 'correct', 'control', 'control', 'consistent', 'cons', 'confused', 'confused', 'computers', 'communication', 'combining', 'combinations', 'coefficients', 'close', 'clauses', 'circles', 'chosen', 'choosing', 'choice', 'choice', 'change', 'certainly', 'cassie', 'cassie', 'cases', 'cases', 'carefully', 'careful', 'capturing', 'borrow', 'beta2', 'beta1', 'available', 'assumption', 'assumption', 'associated', 'ask', 'approach', 'apparently', 'amp', 'amp', 'amp', 'amp', 'among', 'affects', 'accommodate', '01']
lecture 41
['information retrieval researchers called cranfield evaluation methodology', 'occasionally let us spam email', 'information retrieval researchers', 'actual must multiple perspectives', 'desired ideal output generated', 'categorisation ground truth created', 'ground truth test collection', 'involve empirically defined problems', 'baseline would simply put', 'standard measure used everywhere', 'would give us 98', 'recall would tell us', 'test many different systems', 'arithmetic mean would still', 'recall tells us whether', 'called ground truth', 'classification error classification accuracy', 'two category categorization problem', 'would tell us', 'human system said yes', 'create test collection', 'would still give', 'called classification accuracy', 'different categorization mistakes', 'read many papers', 'systems categorization results', 'evaluating searching results', 'system says yes', 'system says yes', 'system says yes', 'system says yes', 'system says yes', 'system says yes', 'system says yes', 'system says yes', 'human says yes', 'becomes true positive', 'false positive fp', 'compare different systems', 'use classification accuracy', 'per document basis', 'already every document', 'parameter beta two', 'course would like', 'also another problem', 'also corrected vision', 'also two kinds', 'popular category valuation', 'meshes called recall', 'measure called f1', 'compare different methods', 'compare different methods', 'missed one assignment', 'less effective one', 'many different methods', 'method works better', 'true positive divided', 'sometimes also useful', 'evaluate categorisation results', 'document perspective based', 'human would think', '2 popular measures', 'characters decisions obviously', 'give us', 'category performs better', 'documents along documents', 'k categories denoted', 'spam filtering', 'classification accuracy', 'classification accuracy', 'actually indeed assigned', 'different perspectives', 'would like', 'use methodology', 'system output', 'letting us', 'allows us', 'taxable categorization', 'still expect', 'simple baseline', 'put together', 'system says', 'human says', 'human says', 'arithmetic mean', 'arithmetic mean', 'arithmetic mean', 'two kinds', 'would lead', 'would call', 'say yes', 'said yes', 'said yes', 'ends yes', 'test set', 'recall defined', 'legitimate email', 'legitimate email', 'legitimate email', 'actually useful', 'actually performs', 'true positive', 'virtually used', 'often used', 'also evaluate', 'systems decisions', 'desired categories', 'gives us', 'called precision', 'different methods', 'actually positive', 'another type', 'harmonic mean', 'different perspective', 'different measures', 'different costs', 'different causes', 'two types', 'would happen', 'true negatives', 'false negatives', 'false negative', 'human said', 'human confirmed', 'taxi categorisation', 'detailed evaluation', 'desirable also', 'also turn', 'also proposed', 'also controlled', 'different ways', 'different ways', 'undesirable property', 'true positives', 'true positives', 'texture catalyzation', 'specific application', 'six days', 'reasonably high', 'reasonable ff1', 'phone numbers', 'particular application', 'often done', 'nothing appears', 'imbalanced tests', 'high cost', 'even tried', 'even though', 'detailed view', 'cost variation', 'controlled experiments', 'certain characteristic', 'basic idea', '1 type', '041 value', 'category two', 'two values', 'perform better', 'combine results', 'better understanding', 'better characterize', 'particular method', 'effective method', 'four combinations', 'four combinations', 'four combinations', 'actually assigned', 'equal weight', 'best way', 'best way', 'simplified measure', 'convenient measure', 'decisions equally', 'corrective decisions', 'old categories', 'minority categories', 'particular category', 'major category', 'apa category', 'desicion errors', 'analyze errors', 'one number', 'shows clearly', 'means looking', 'good result', 'empirical tasks', 'classes tend', 'also interested', 'total number', 'total number', 'equal number', 'short documents', 'weather precision', 'define precision', 'combine precision', 'help humans', 'general thoughts', 'one measure', 'category one', 'category one', 'methodology', 'serious mistake', 'plus sign', 'often ok', 'characterize performance', 'decisions right', 'relative difference', 'previous case', 'extreme case', 'divisions right', 'called', 'basically answer', 'n documents', 'indeed correct', 'errors might', 'first measure', 'decision errors', 'correct decisions', 'one example', 'n minus', 'first missing', 'would', 'humans decision', 'higher similarity', 'problem', 'collection', 'general cases', 'might see', 'accuracy', '98', 'different', 'two', 'positive', 'popular', 'system', 'system', 'system', 'system', 'system', 'system', 'human', 'human', 'compare', 'compare', 'categorisation', 'categorisation', 'methods', 'methods', 'many', 'many', 'many', 'evaluation', 'evaluation', 'evaluation', 'evaluation', 'also', 'also', 'use', 'sometimes', 'said', 'said', 'problems', 'problems', 'perspective', 'meshes', 'measures', 'k', 'f1', 'effective', 'divided', 'denoted', 'course', 'corrected', 'beta', 'assignment', '2', 'results', 'results', 'results', 'results', 'better', 'better', 'better', 'number', 'method', 'method', 'whether', 'whether', 'whether', 'recall', 'recall', 'recall', 'recall', 'recall', 'recall', 'recall', 'recall', 'one', 'one', 'one', 'one', 'one', 'indeed', 'indeed', 'error', 'error', 'error', 'combinations', 'ways', 'way', 'total', 'right', 'positives', 'n', 'equal', 'measure', 'measure', 'measure', 'measure', 'measure', 'measure', 'measure', 'measure', 'decisions', 'decisions', 'decisions', 'decisions', 'decisions', 'categories', 'categories', 'categories', 'categories', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'category', 'errors', 'errors', 'errors', 'values', 'tasks', 'set', 'obviously', 'obviously', 'obviously', 'missing', 'might', 'might', 'means', 'higher', 'happen', 'good', 'gives', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'document', 'divisions', 'clearly', 'classes', 'assigned', 'assigned', 'assigned', 'assigned', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'documents', 'precision', 'precision', 'precision', 'precision', 'precision', 'humans', 'humans', 'humans', 'general', 'general', 'general', 'first', 'first', 'first', 'decision', 'decision', 'decision', 'think', 'think', 'think', 'think', 'think', 'serious', 'serious', 'plus', 'plus', 'performance', 'performance', 'ok', 'ok', 'interested', 'interested', 'correct', 'correct', 'correct', 'correct', 'similarity', 'similarity', 'similarity', 'minus', 'minus', 'minus', 'difference', 'difference', 'difference', 'cases', 'cases', 'cases', 'example', 'example', 'example', 'example', 'example', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'basically', 'basically', 'basically', 'basically', 'basically', 'basically', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'wonder', 'well', 'well', 'well', 'weaknesses', 'want', 'want', 'using', 'using', 'understand', 'understand', 'understand', 'treated', 'tradeoff', 'thought', 'tends', 'talked', 'take', 'tagged', 'sum', 'subset', 'strengths', 'solving', 'slide', 'skew', 'situation', 'similarly', 'similarly', 'similar', 'show', 'separate', 'search', 'saw', 'retrieved', 'retrieved', 'represents', 'reduce', 'reason', 'reality', 'reality', 'range', 'questions', 'question', 'question', 'query', 'quantify', 'provides', 'prefer', 'possibilities', 'pluses', 'perfect', 'percentage', 'percentage', 'pattern', 'pair', 'overlooked', 'others', 'others', 'others', 'others', 'others', 'others', 'order', 'obvious', 'nos', 'normalize', 'multiplied', 'model', 'miss', 'mesh', 'measured', 'maybe', 'may', 'made', 'low', 'look', 'look', 'look', 'look', 'look', 'look', 'look', 'long', 'lecture', 'later', 'knowing', 'knowing', 'know', 'know', 'know', 'kind', 'kind', 'kind', 'issue', 'introduce', 'introduce', 'interesting', 'instead', 'instances', 'instances', 'instances', 'instances', 'insight', 'indicate', 'incorrectly', 'incorrect', 'incorrect', 'inbox', 'improving', 'important', 'important', 'important', 'important', 'important', 'imagine', 'ideally', 'however', 'got', 'going', 'going', 'going', 'going', 'going', 'going', 'get', 'get', 'get', 'generally', 'folder', 'etc', 'equivalently', 'ensure', 'easy', 'dn', 'differences', 'developed', 'detail', 'detail', 'detail', 'despair', 'depending', 'depending', 'd1', 'count', 'could', 'could', 'could', 'correlated', 'correctly', 'consider', 'compute', 'component', 'come', 'come', 'combining', 'combining', 'class', 'ck', 'cause', 'categorize', 'c1', 'bias', 'bias', 'beneficial', 'become', 'balanced', 'ask', 'appearing', 'ann', 'always', 'alright', 'aggregate', 'address', 'address', 'account', '1960s', '19']
lecture 42
['also variations like per document based evaluation', 'especially balanced tester set', 'particular customer service person', 'sometimes categorisation results might actually', 'whereas geometric mean would', 'measures must also reflect', 'micro versus macro averaging', 'precision p1 recall r1', 'general macro average tends', 'might get misleading results', 'per category evaluation', 'whole data set', 'two suggested readings', 'design measures appropriately', 'cause people tend', 'got different conclusions', 'different behavior depending', 'email messages might', 'weighted classification accuracy', 'following classification accuracy', 'optimizing ranking measures', 'maybe different ways', 'machine learning methods', 'also micro averaging', 'arithmetic mean would', 'use geometric mean', 'emphasize low values', 'different decision error', 'distinguishing relevant documents', 'binary categorization problem', 'ranking problem instead', 'compare different methods', 'human editors would', 'many different ways', 'search engine evaluation', 'application specific away', 'f value f1', 'consider arithmetic mean', 'high values would', 'commonly used measures', 'maybe better frame', 'useful would depend', 'category c one', 'news categorization results', 'see subtle differences', 'believe one method', 'method works better', 'like precision', 'sometimes ranking may', 'overall f score', 'aggregate different values', 'geometric mean', 'often indeed passed', 'evaluation measures', 'arithmetic mean', 'macro averaging', 'macro averaging', 'classification accuracy', 'micro averaging', 'micro averaging', 'categorization evaluation', 'different ways', 'person would', 'ranking evaluation', 'macro average', 'macro average', 'compute precision recall', 'micro average', 'actually compute', 'specific decision', 'introduced measures', 'sometimes categorisation', 'ranking accuracy', 'low values', 'categorisation decision', 'first evaluation', 'might reflect', 'high values', 'search engine', 'news articles', 'categorization problem', 'ranking problem', 'textual categorisation', 'text categorisation', 'text categorisation', 'different methods', 'different dimensions', 'different approaches', 'different angles', 'different angles', 'categorisation results', 'text categorization', 'categorization methods', 'particular application', 'particular application', 'ranking emails', 'might reveal', 'precision values', 'high precision', 'many cases', 'many cases', 'overall precision', 'overall precision', 'overall precision', 'overall precision', 'f score', 'f score', 'perspectives would', 'method might', 'different cost', 'different cost', 'one problem', 'might see', 'commonly used', 'commonly used', 'various purposes', 'variations', 'tentatively categorized', 'take average', 'take average', 'research papers', 'ranked list', 'ranked list', 'ranked list', 'question related', 'obtain insights', 'multiple aspects', 'intended use', 'help prioritizing', 'help desk', 'false positive', 'contingency table', 'computer precision', 'basically computing', 'based', 'different documents', 'ranking perspective', 'right person', 'many perspectives', 'results sequentially', 'suitable one', 'check one', 'category 2', 'category 1st', 'ranking documents', 'high recall', 'want people', 'better formulated', 'would need', 'spam filtering', 'spam emails', 'really spam', 'overall recall', 'multiple perspectives', 'commonly reported', 'many applications', 'spam category', 'verify whether', 'verify whether', 'excellent discussion', 'continued discussion', 'f scores', 'different categories', 'precision recall', 'true positive', 'relative comparison', 'ranking task', 'pull together', 'best way', 'always good', 'generally useful', 'generally need', 'introduced earlier', 'decisions equally', 'basically similar', 'right one', 'users perspective', 'provides insight', 'also', 'also', 'also', 'understanding performance', 'evaluation', 'would', 'would', 'sometimes', 'sometimes', 'reflect', 'might', 'might', 'might', 'values', 'document', 'document', 'document', 'document', 'problem', 'f', 'use', 'search', 'score', 'precision', 'precision', 'precision', 'passed', 'methods', 'methods', 'indeed', 'human', 'general', 'general', 'frame', 'f1', 'especially', 'especially', 'differences', 'results', 'results', 'results', 'results', 'results', 'results', 'results', 'one', 'one', 'one', 'category', 'category', 'category', 'method', 'method', 'method', 'better', 'better', 'better', 'used', 'used', 'spam', 'see', 'see', 'perspectives', 'get', 'get', 'get', 'whether', 'take', 'perspective', 'often', 'often', 'may', 'may', 'discussion', 'cost', 'application', 'application', 'application', 'recall', 'recall', 'recall', 'recall', 'recall', 'recall', 'recall', 'recall', 'documents', 'documents', 'documents', 'documents', 'way', 'users', 'understanding', 'true', 'together', 'scores', 'reported', 'provides', 'insight', 'consider', 'consider', 'consider', 'comparison', 'applications', 'always', 'useful', 'useful', 'useful', 'useful', 'useful', 'need', 'need', 'need', 'compute', 'compute', 'compute', 'compute', 'compute', 'compute', 'want', 'want', 'similar', 'similar', 'performance', 'performance', 'earlier', 'earlier', 'decisions', 'decisions', 'right', 'right', 'right', 'right', 'right', 'categories', 'categories', 'categories', 'aggregate', 'aggregate', 'aggregate', 'aggregate', 'aggregate', 'aggregate', 'aggregate', 'task', 'task', 'task', 'task', 'well', 'weakness', 'weak', 'utility', 'utility', 'utility', 'using', 'user', 'user', 'typically', 'treat', 'tradeoffs', 'title', 'think', 'think', 'terms', 'tasks', 'talked', 'talk', 'system', 'system', 'summarize', 'summarize', 'strength', 'situations', 'situation', 'similarly', 'seen', 'second', 'said', 'routed', 'response', 'reflects', 'rank', 'rank', 'query', 'quality', 'quality', 'pulled', 'processed', 'prioritized', 'possible', 'performances', 'paper', 'others', 'needed', 'misled', 'mentioned', 'measure', 'look', 'look', 'look', 'lecture', 'lecture', 'later', 'know', 'instances', 'informative', 'indicated', 'improving', 'improving', 'important', 'important', 'important', 'important', 'important', 'important', 'important', 'important', 'ideally', 'humans', 'humans', 'handling', 'going', 'going', 'going', 'give', 'generate', 'generate', 'framed', 'find', 'finally', 'fill', 'fact', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'examining', 'examine', 'examine', 'exactly', 'evaluated', 'evaluated', 'evaluated', 'evaluate', 'evaluate', 'evaluate', 'etc', 'editing', 'done', 'dominated', 'discovery', 'difference', 'difference', 'diagnosis', 'desirable', 'designed', 'course', 'course', 'counting', 'could', 'correct', 'contrast', 'contrast', 'contrast', 'confidence', 'computations', 'completed', 'common', 'combine', 'combination', 'choosing', 'characterize', 'chapters', 'categorizations', 'case', 'case', 'case', 'case', 'case', 'careful', 'called', 'book', 'beneficial', 'associate', 'associate', 'associate', 'aspect', 'appropriate', 'appropriate', 'analysis', 'aggregation', 'aggregation', 'aggregation', 'affected']
lecture 43
['using natural language processing techniques', 'could help us better serve people', 'use text based prediction techniques', 'also differentiate distinguish different kinds', 'data driven social science research', 'deeper natural language processing', 'would help us better understand', 'person could also make observation', 'different background may also think', 'identify three major reasons', 'understanding obviously goes beyond', 'distinguish positive versus negative', 'using opinion mining techniques', 'text might also report opinions', 'policymakers may also want', 'person might think differently', 'second must also specify', 'help understand peoples preferences', 'reviewer might mention opinions', 'simplest opinion mining tasks', 'another persons opinion etc', 'deeply understand opinion sentiment', 'one person might comment', 'data like video data', 'basic opinion representation like', 'actually explicit opinion holder', 'also infer opinion sentiment', 'necessarily look like opinion', 'least three measurements', 'sometimes factual sentences like', 'help us optimize', 'identify one sentence opinion', 'key differentiating factor', 'help decision support', 'opinion tells us', 'real world objectively', '3 broad reasons', 'optimize recommender system', 'called voluntary survey', 'also complex text', 'taking text data', 'also would like', 'decision like buying', 'product search engine', 'one statement might', 'one phrase opinion', 'opinion would depend', 'actually unique advantage', 'observer might make', 'entire discourse context', 'like different time', 'happy versus sad', 'would define opinion', 'basic opinion representation', 'course adds value', 'sentiment analysis covering', 'someone else opinion', 'subjective statement describing', 'understanding consumers opinions', 'easily build applications', 'still quite difficult', 'often explicitly identified', 'holder clearly wish', 'generated text data', 'analyze opinions buried', 'also another target', 'identify opinion holder', 'identify opinion holder', 'formally define opinion', 'product manufacturers want', 'particular discourse context', 'better understand', 'example iphone 6', 'know peoples opinions', 'general also easy', 'one general kind', 'want opinion content', 'also help', 'usually aggregate opinions', 'two kinds', 'support research', 'offers us', 'helps us', 'also infer', 'see product reviews', 'make prediction', 'basic understanding', 'video data', 'also would', 'voluntary survey', 'second application', 'market research', 'another person', 'another person', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'opinion like', 'peoples opinions', 'factual statement', 'factual statement', 'might want', 'might say', 'also identify', 'statement one', 'explicit target', 'different ways', 'different ways', 'different ways', 'different locations', 'define opinion', 'social networks', 'social media', 'social media', 'could also', 'elements must', 'video recorder', 'background topic', 'text mining', 'negative sentiment', 'one hour', 'holder could', 'observed world', 'necessarily extract', 'another complication', 'often also', 'examples also', 'also want', 'also simple', 'also often', 'longer text', 'extra data', 'data generated', 'particular person', 'keyword person', 'one target', 'different kind', 'opinion positive', 'lets think', 'opinion holder', 'opinion holder', 'opinion holder', 'opinion holder', 'opinion holder', 'opinion holder', 'opinion holder', 'opinion holder', 'opinion holder', 'opinion holder', 'different context', 'analyzing sentiment', 'implicit holder', 'opinion representation', 'opinion mining', 'opinion mining', 'opinion mining', 'opinion mining', 'opinion mining', 'opinion mining', 'objective statement', 'objective statement', 'also vary', 'also vary', 'also vary', 'subjective sentences', 'sentiment analysis', 'negative opinions', 'reader reviews', 'additional understanding', 'words like', 'people think', 'people think', 'person thinks', 'another context', 'whose opinion', 'opinion representations', 'opinion held', 'subjective statement', 'one attribute', 'winning features', 'winning features', 'whole country', 'whole article', 'questioning answering', 'prove whether', 'practical viewpoint', 'pay attention', 'particular policy', 'new policy', 'new england', 'much harder', 'making inferences', 'main difference', 'last longer', 'information easily', 'great opportunity', 'following lectures', 'even verify', 'emotion dimension', 'directly related', 'computationally solve', 'computational perspective', 'computation perspective', 'business intelligence', 'always needed', 'review text', 'opinion content', 'opinion content', 'general opinion', 'person believes', 'opinion target', 'opinion target', 'opinion target', 'opinion target', 'opinion target', 'product reviews', 'product reviews', 'product reviews', 'product reviews', 'inferred opinions', 'indirect opinions', 'indirect opinions', 'keyword subjective', 'general harder', 'implicit target', 'often look', 'relatively easy', 'fairly easy', 'analysis shows', 'sometimes opinion', 'particular product', 'opinion context', 'understand people', 'people tend', 'target content', 'third kind', 'thinks implies', 'public information', 'prediction task', 'multiple elements', 'easily done', 'simple context', 'opinion expressed', 'opinion expressed', 'targeted advertising', 'start talking', 'proved wrong', 'prove wrong', 'nice screen', 'namely knowledge', 'manual surveys', 'little bit', 'hurricane sandy', 'either true', 'discussed earlier', '1 entity', 'help', 'extract opinions', 'subjective sensors', 'content tends', 'using', 'may', 'interesting variations', 'interesting variations', 'phone ran', 'although mostly', 'already known', 'opinions need', 'product review', 'product review', 'rich opinions', 'others opinions', 'competitive products', 'simple example', 'good example', 'proved right', 'best battery', 'people need', 'first look', 'really expressed', 'might', 'might', 'author loves', 'like', 'like', 'make', 'background', 'actually', 'one', 'would', 'would', 'tasks', 'specify', 'preferences', 'prediction', 'obviously', 'comment', 'also', 'also', 'also', 'also', 'also', 'also', 'also', 'text', 'data', 'person', 'person', 'think', 'think', 'sentiment', 'sentiment', 'identify', 'identify', 'identify', 'sentences', 'report', 'report', 'positive', 'optimize', 'negative', 'could', 'could', 'could', 'could', 'could', 'representation', 'representation', 'mining', 'mining', 'understanding', 'understanding', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'opinion', 'want', 'want', 'want', 'want', 'understand', 'understand', 'understand', 'understand', 'understand', 'understand', 'understand', 'time', 'someone', 'sentence', 'sentence', 'sad', 'reviewer', 'reviewer', 'quite', 'particular', 'often', 'observer', 'iphone', 'identified', 'happy', 'generated', 'formally', 'etc', 'etc', 'difficult', 'buying', 'broad', 'analyze', 'aggregate', 'advantage', 'opinions', 'opinions', 'opinions', 'opinions', 'opinions', 'opinions', 'opinions', 'opinions', 'opinions', 'opinions', 'opinions', 'subjective', 'subjective', 'general', 'general', 'general', 'content', 'content', 'target', 'target', 'target', 'look', 'look', 'look', 'easy', 'easy', 'analysis', 'analysis', 'product', 'product', 'product', 'product', 'product', 'product', 'product', 'vary', 'people', 'people', 'people', 'people', 'people', 'people', 'people', 'people', 'variations', 'usually', 'usually', 'thinks', 'sometimes', 'sometimes', 'sometimes', 'sometimes', 'kind', 'kind', 'kind', 'interesting', 'information', 'elements', 'clearly', 'clearly', 'applications', 'applications', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'wrong', 'wrong', 'tends', 'surveys', 'start', 'sensors', 'see', 'see', 'see', 'screen', 'review', 'review', 'review', 'really', 'loves', 'little', 'knowledge', 'hurricane', 'extract', 'extract', 'entity', 'either', 'discussed', 'believes', 'attribute', 'advertising', 'expressed', 'expressed', 'expressed', 'expressed', 'need', 'need', 'need', 'words', 'words', 'rich', 'rich', 'phone', 'phone', 'others', 'others', 'mostly', 'mostly', 'done', 'done', 'author', 'author', 'already', 'already', 'products', 'products', 'products', 'know', 'know', 'know', 'know', 'know', 'know', 'know', 'course', 'course', 'course', 'course', 'course', 'course', 'course', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'right', 'right', 'right', 'right', 'first', 'first', 'first', 'first', 'task', 'task', 'task', 'task', 'task', 'battery', 'battery', 'battery', 'battery', 'battery', 'battery', 'battery', 'written', 'worst', 'work', 'whatever', 'well', 'well', 'way', 'vote', 'variation', 'variation', 'useful', 'useful', 'useful', 'useful', 'useful', 'useful', 'useful', 'used', 'unlike', 'turns', 'turns', 'thinking', 'terms', 'talked', 'talk', 'talk', 'talk', 'take', 'take', 'systematically', 'surface', 'suggest', 'study', 'study', 'statements', 'statements', 'specific', 'something', 'something', 'something', 'something', 'something', 'something', 'situation', 'set', 'service', 'said', 'reviewed', 'regarded', 'regarded', 'reflects', 'reason', 'questions', 'questions', 'quality', 'problem', 'problem', 'problem', 'posted', 'perhaps', 'painting', 'painting', 'order', 'order', 'obtaining', 'obtained', 'obtain', 'observers', 'notice', 'next', 'news', 'news', 'neutral', 'needs', 'motivation', 'mine', 'mine', 'means', 'mean', 'mainly', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'leverage', 'let', 'let', 'let', 'let', 'let', 'let', 'lecture', 'lecture', 'lecture', 'laptop', 'laptop', 'interpreted', 'interested', 'interested', 'interested', 'instead', 'input', 'individual', 'indicates', 'indicated', 'indeed', 'include', 'include', 'improve', 'improve', 'important', 'important', 'ideally', 'humans', 'humans', 'humans', 'humans', 'humans', 'highlighted', 'hard', 'happening', 'happened', 'group', 'group', 'governor', 'going', 'going', 'going', 'get', 'get', 'get', 'generate', 'generate', 'gave', 'gain', 'furthermore', 'friend', 'friend', 'forms', 'finally', 'fill', 'feeling', 'feeling', 'feeling', 'false', 'fact', 'extracting', 'extracted', 'extracted', 'express', 'examine', 'exactly', 'enrich', 'directed', 'devices', 'designing', 'defined', 'defined', 'decisions', 'decide', 'culture', 'correct', 'correct', 'contrast', 'contrast', 'contrast', 'contrast', 'contrast', 'connecticut', 'concept', 'computer', 'compared', 'company', 'committee', 'commenting', 'clear', 'check', 'characterize', 'cause', 'cause', 'cause', 'cases', 'case', 'case', 'case', 'case', 'case', 'case', 'benefit', 'believe', 'behavior', 'bad', 'assess', 'answer', 'analyzed', 'advantages', 'accuracy', '3rd', '2015', '1938']
lecture 44
['feature design actually affects categorization accuracy significantly', 'allows much larger search space', 'also learn word clusters empirically', 'naturally infrequent features tend', 'simple text categorization technique', 'named entities like people', 'ontology like word net', 'regular text categorization technique', 'derive simple world n', 'frequent versus infrequent features', 'may also generalize better', 'could use ordinal regression', 'also many meaningful features', 'representation actually would allow', 'parse tree based features', 'nlp enriches text representation', 'language pretty much', 'text categorization method', 'also mix n grams', 'patton discovery algorithms', 'natural language processing', 'long time ago', 'might still face', 'five might denote', 'opinionated text object', 'machine learning program', 'machine learning program', 'machine learning application', 'combine machine learning', 'applying machine learning', 'needs two kinds', 'go beyond polarity', 'especially also needed', 'word based representation', 'basic feature space', 'design seed features', 'word great might', '), different lengths', 'different feature space', 'really caused tradeoff', 'might represent concepts', 'might consider using', 'text processing tasks', 'especially polarity analysis', 'also different ways', 'sementically related words', 'paradigmatically related words', 'necessarily occur together', 'frequent pattern syntax', 'sentiment analysis clearly', 'speech tag n', 'using sentiment classification', 'frequent word set', 'would suggest positive', 'use domain knowledge', 'accuracy may', 'even speech act', 'enrich text representation', 'derive complex features', 'words might occur', 'may cause overfitting', 'let overfitting happen', 'well designed features', 'larger space', 'features cause overfitting', 'often good enough', 'noun could form', 'frequently used categories', 'text categorization', 'text categorization', 'text categorization', 'also consider part', 'feature design', 'machine learning', 'generalize well', 'actually often', 'regular n', 'feature space', 'simple sentence', 'parse tree', 'parse tree', 'feature learning', 'derive features', 'different lengths', 'different n', 'two ways', 'necessarily occur', 'many tasks', 'domain knowledge', 'allow us', 'occur also', 'word n', 'representation would', 'categorization errors', 'text mining', 'text data', 'text correctly', 'feature set', 'polarity analysis', 'polarity analysis', 'syntactic like', 'sentence like', 'sentiment tag', 'might see', 'parse trees', 'correct form', 'sometimes also', 'also robust', 'also means', 'also define', 'longer n', 'common n', 'character n', 'character n', 'hybrid feature', 'feature validation', 'feature construction', 'would revise', 'opinion representation', 'cause overfitting', 'cause overfitting', 'training set', 'frequent subtrees', 'closely together', 'speech tags', 'speech tags', 'speech tags', 'speech tags', 'speech tagging', 'sentiment analysis', 'sentiment analysis', 'error analysis', 'error analysis', 'emotion analysis', 'emotion analysis', 'sentiment classification', 'sentiment classification', 'sentiment classification', 'sentiment classification', 'sentiment classification', 'sentiment classification', 'wanted features', 'unique features', 'unique features', 'sophisticated features', 'possible features', 'possible features', 'new features', 'features become', 'designing features', 'designing features', 'complicated features', 'additional features', 'sentiment tagging', 'sentiment label', 'n grams', 'training carefully', 'test data', 'robust way', 'patterns provide', 'numerical ratings', 'mining associations', 'mainly need', 'main challenge', 'high coverage', 'future data', 'easily pick', 'common challenge', 'already known', '1 character', 'also know', 'word classes', 'misspelled word', 'opinion sentiment', 'classification task', 'complex features', 'opinion target', 'opinion holder', 'opinion holder', 'could become', 'task maybe', 'categorisation task', 'good together', 'often see', 'spelling errors', 'recognition errors', 'uni grams', 'grams representations', 'specificity requires', 'something else', 'simplest one', 'one maybe', 'exhaustivity means', 'enriched information', 'distinguish categories', 'discrete categories', 'another lecture', 'actually', 'effective features', 'discriminating features', 'discriminative features', 'understanding opinion', 'order among', 'design', 'special case', 'sometimes useful', 'general way', 'important part', 'important part', 'mixed grams', 'search', 'clusters', 'like', 'representation', 'representation', 'nlp', 'mix', 'may', 'entities', 'enriches', 'also', 'also', 'also', 'also', 'word', 'word', 'word', 'feature', 'feature', 'would', 'frequent', 'frequent', 'cause', 'overfitting', 'overfitting', 'features', 'features', 'features', 'features', 'features', 'features', 'features', 'features', 'sentiment', 'use', 'use', 'use', 'tradeoff', 'tasks', 'represent', 'pattern', 'often', 'noun', 'designed', 'consider', 'consider', 'opinion', 'could', 'could', 'could', 'could', 'task', 'complex', 'complex', 'used', 'used', 'see', 'positive', 'positive', 'let', 'let', 'important', 'even', 'even', 'enrich', 'enrich', 'errors', 'errors', 'grams', 'grams', 'grams', 'grams', 'grams', 'grams', 'grams', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'understanding', 'specificity', 'something', 'one', 'one', 'mixed', 'misspelled', 'lecture', 'know', 'information', 'exhaustivity', 'classes', 'categories', 'categories', 'categories', 'categories', 'categories', 'part', 'part', 'part', 'part', 'part', 'part', 'good', 'good', 'good', 'good', 'good', 'good', 'effective', 'effective', 'discriminating', 'discriminating', 'order', 'order', 'order', 'discriminative', 'discriminative', 'discriminative', 'useful', 'useful', 'useful', 'useful', 'useful', 'case', 'case', 'case', 'case', 'case', 'general', 'general', 'general', 'general', 'general', 'general', 'general', 'work', 'web', 'want', 'want', 'unlikely', 'unit', 'typically', 'turns', 'true', 'trade', 'think', 'thesaurus', 'talked', 'talk', 'talk', 'take', 'take', 'surpised', 'suppose', 'supplement', 'sufficient', 'start', 'specifically', 'specifically', 'specific', 'six', 'shown', 'showed', 'sequence', 'sequence', 'sense', 'semantic', 'select', 'seen', 'says', 'said', 'sad', 'right', 'reviews', 'review', 'require', 'rely', 'recover', 'recognized', 'problem', 'problem', 'problem', 'problem', 'problem', 'place', 'perhaps', 'paths', 'particular', 'particular', 'pair', 'output', 'otherwise', 'optimize', 'occurs', 'obviously', 'obviously', 'next', 'next', 'next', 'neutral', 'negative', 'negative', 'mostly', 'moment', 'matched', 'match', 'match', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'look', 'locations', 'likely', 'lead', 'later', 'kind', 'iterate', 'input', 'independent', 'improvements', 'improvement', 'humans', 'humans', 'help', 'happy', 'generate', 'generally', 'generally', 'furthermore', 'formed', 'follows', 'followed', 'first', 'feeling', 'fearful', 'expect', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'essentially', 'elements', 'documents', 'disgusted', 'discuss', 'difficult', 'desirable', 'defined', 'decide', 'course', 'course', 'course', 'course', 'context', 'content', 'contain', 'construct', 'communication', 'cluster', 'clear', 'classifier', 'characters', 'characters', 'characterize', 'characterize', 'careful', 'bigram', 'assume', 'art', 'appropriate', 'applied', 'angry', 'analyzed', 'analyze', 'ambiguous', 'advantages', 'adjective', 'accurate', 'accurate', 'accurate']
lecture 45
['plus k minus one alpha values', 'standard two category categorization problem', '1 regular logistic regression classifiers', 'actually help us training data', 'binary logistic regression program', '1 independent logistical regression classifiers', 'typical sentiment classification problem', 'logistic regression classifiers indexed', 'would actually give us', 'also used offer subject', 'logistical regression program would', 'binary setting categorization problem', 'positive words generally suggest', 'introduce multiple binary classifiers', 'two category categorization problem', 'distinguish k versus others', '1 logistic regression classifiers', 'multi level rating prediction', 'ordinal logistic regression', 'ordinal logistic regression', 'ordinal logistic regression', 'ordinal logistic regression', 'ordinal logistic regression', 'help us choose', 'apply logistical regression', 'binary response variable', 'distinguish category 2', 'next class file', 'train categorisation program', 'would allow us', 'would allow us', 'help us decide', 'taking linear combination', 'use logistic regression', 'distinct alpha value', 'help us set', 'tells us whether', 'tell us whether', 'help us solve', 'logistical regression classifier', 'intuitively appealing assumption', 'k minus one', 'two positive benefit', 'formula would look', 'opinionated text document', 'rather rating 2', 'far fewer parameters', '1 given x', 'replace beta 0', 'good beta value', 'positive would make', 'many different approaches', 'general decision rule', 'actually assign ratings', 'general would make', 'logistic regression', 'binary categorization', 'logistical regression', '1 classifiers altogether', '0 means x', '1 means x', 'allow us', 'regular text', 'categorization problem', 'alpha parameter', 'actually dependent', 'logistical function', 'categorization technique', 'different alpha', 'sentiment analysis', 'apha subject', 'distinguish two', 'actually use', 'text document', 'training data', 'training data', 'two values', 'many classifiers', 'positive words', 'really independent', 'multiple levels', 'linear function', '1 classifiers', '1 classifiers', 'different classifiers', 'parameters would', 'second problem', 'problem set', 'problem set', 'optimal value', 'decide whether', 'criteria whether', 'also write', 'first problem', 'solution would', 'classifier two', 'parameter values', 'better values', 'beta values', 'beta values', 'beta values', 'beta values', 'beta values', 'multiple categories', 'simply make', 'scoring function', 'scoring function', 'previous slide', 'predicted probabilities', 'output already', 'new instance', 'make decisions', 'keep invoking', 'interesting exercise', 'direct application', 'complex model', 'beta parameter', 'alot alot', 'rating prediction', 'rating prediction', 'two problems', '1 }.', '1 premise', '1 ).', 'distinguish k', 'distinguish k', 'also solve', 'different levels', 'different levels', 'feature value', 'predict whether', 'different set', 'many parameters', 'many parameters', 'many parameters', 'many parameters', 'feature values', 'rating higher', 'higher rating', 'higher rating', 'discrete rating', 'corresponding rating', 'one set', 'total number', 'simpler way', 'real number', 'optimal way', 'also ask', 'many premises', 'parameters significantly', 'optimal parameters', 'beta parameters', 'beta parameters', 'values correspond', '1 parameters', 'take advantage', 'say yes', 'interesting question', 'classifier sequentially', 'another classifier', 'also end', 'text object', 'predict rating', 'one index', 'one classifier', 'classifier one', 'may recall', 'features weighted', 'features altogether', 'add order', 'rating k', 'main idea', 'key idea', 'means rating', 'ratings lower', 'might want', 'general need', 'course needed', 'probability according', 'particular range', 'reading level', 'least k', 'first think', 'first let', 'different j', 'k categories', 'basically k', 'would', 'independent', 'two', 'classifiers', 'classifiers', 'classifiers', 'classifiers', 'classifiers', 'x', 'training', 'problem', 'problem', 'problem', 'problem', 'words', 'positive', 'positive', 'distinguish', '0', 'prediction', 'value', 'data', 'data', 'whether', 'also', 'values', 'values', 'values', 'train', 'make', 'make', 'look', 'intuitively', 'general', 'general', 'decision', 'decide', 'beta', 'beta', 'assign', 'altogether', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', 'different', 'different', 'set', 'set', 'one', 'one', 'one', 'one', 'one', 'one', 'one', 'level', 'level', 'level', 'use', 'use', 'solve', 'solve', 'ratings', 'ratings', 'first', 'rating', 'rating', 'rating', 'rating', 'rating', 'rating', 'rating', 'rating', 'rating', 'way', 'predict', 'number', 'feature', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'k', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'means', 'means', 'means', 'means', 'means', 'means', 'take', 'say', 'question', 'premises', 'lower', 'least', 'index', 'correspond', 'classifier', 'classifier', 'classifier', 'classifier', 'classifier', 'classifier', 'classifier', 'classifier', 'ask', 'categories', 'categories', 'categories', 'think', 'think', 'solution', 'solution', 'reading', 'reading', 'problems', 'problems', 'order', 'order', 'may', 'may', 'let', 'let', 'features', 'features', 'features', 'features', 'end', 'end', 'idea', 'idea', 'idea', 'idea', 'idea', 'want', 'want', 'want', 'basically', 'basically', 'basically', 'object', 'object', 'object', 'object', 'need', 'need', 'need', 'need', 'course', 'course', 'course', 'course', 'range', 'range', 'range', 'range', 'range', 'probability', 'probability', 'probability', 'probability', 'probability', 'j', 'j', 'j', 'j', 'j', 'j', 'zero', 'y_j', 'well', 'well', 'well', 'weights', 'video', 'using', 'using', 'unfortunately', 'tying', 'turns', 'try', 'treating', 'trained', 'together', 'tied', 'tie', 'tie', 'thus', 'thus', 'talk', 'suppose', 'sum', 'strategy', 'straightforward', 'specifically', 'specifically', 'specifically', 'specifically', 'solving', 'similar', 'similar', 'shown', 'shown', 'show', 'shared', 'share', 'separately', 'seen', 'seen', 'seen', 'seeing', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'score', 'score', 'score', 'score', 'score', 'says', 'right', 'right', 'rest', 'rest', 'represented', 'representation', 'relates', 'reduce', 'reasonable', 'ranges', 'predictors', 'precisely', 'plan', 'pause', 'notation', 'negative', 'negative', 'negative', 'n', 'n', 'moment', 'method', 'lot', 'lot', 'looking', 'longer', 'log', 'likely', 'lecture', 'larger', 'larger', 'larger', 'large', 'invoke', 'invoke', 'intuition', 'input', 'influence', 'indicate', 'improvement', 'however', 'hit', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'getting', 'generate', 'follows', 'figure', 'falls', 'factor', 'fact', 'except', 'example', 'exactly', 'even', 'equivalent', 'equations', 'equation', 'equal', 'equal', 'equal', 'equal', 'encodes', 'desicion', 'depends', 'dependency', 'count', 'corresponds', 'corresponds', 'corresponds', 'consistent', 'consider', 'consequence', 'compared', 'classify', 'classify', 'case', 'capture', 'called', 'bracket', 'bottom', 'b_i', 'assumed', 'assumed', 'assume', 'assume', 'assume', 'approach', 'approach', 'approach', 'anyway', 'answer', 'alpha_j', 'able', '5', '5', '5', '5']
lecture 46
['word like amazing mentioned many times', 'likely would mean really cheaper prices', 'rank hotels along different dimensions', 'assumed market valued gaussian distribution', 'problem called latent aspect rating analysis', 'called latent rating regression', 'another multivariate gaussian distribution', 'multivariate gaussian prior distribution', 'mentioned many times', 'using seed words like location', 'continue discussing opinion mining', 'aspect level opinion summary', 'parameter values including betas', 'detailed understanding would reveal', 'latent aspect rating analysis', 'decompose overrating two ratings', 'would also allow us', 'also use topic models', 'also analyze reviewers preferences', 'observed overall rating given', 'use c sub w', 'press basic introduction', 'perform detailed analysis', 'reviewer really cares', 'others might care', 'denotes actually await', 'would allow us', 'reviewers may care', 'observer ratings condition', 'setup allows us', 'major aspects comment', 'covariance matrix sigma', 'use bayesian inference', 'w gives us', 'discussing different aspects', 'given five stars', 'gives us way', 'observed rating given', 'mine correlated words', 'compute interesting quantities', 'observed word frequencies', 'different aspects accurately', 'overall rating given', 'aspect specific sentiment', 'reviewer might give', 'decomposed aspect ratings', 'overall rating based', 'relative weights placed', 'document specific weights', 'actually latent', 'sentiment weights might', 'simply weighted combination', 'review documents denoted', 'different hotels', 'really cover', 'rating analysis', 'opinion mining', 'case likely', 'detailed understanding', 'detailed understanding', 'location using', 'another word', 'sentiment analysis', 'sentiment analysis', 'model different segment', 'obtain aspect segments', 'word might', 'reviewer cares', 'seed words', 'particular alpha value', 'normal distribution', 'normal distribution', 'normal distribution', 'allows us', 'allows us', 'opinion based', 'rating based', 'another aspect', 'overall rating', 'overall rating', 'overall rating', 'overall rating', 'overall rating', 'overall rating', 'overall rating', 'overall rating', 'word frequencies', 'maximizer observed', 'give us', 'give us', 'weighted combination', 'weight distribution', 'two stages', 'two stages', 'review documents', 'might still', 'might place', 'interesting variables', 'c supply', 'aspect rating', 'aspect rating', 'aspect rating', 'aspect rating', 'aspect rating', 'abstract rating', 'case like', 'different perspectives', 'different aspects', 'different aspects', 'different aspects', 'different aspects', 'different aspects', 'different aspects', 'also useful', 'also unclear', 'also r', 'also need', 'also mention', 'also important', 'overall ratings', 'overall ratings', 'overall ratings', 'overall ratings', 'overall ratings', 'would increase', 'would enable', 'would embed', 'counter would', 'five star', 'five star', 'five star', 'overall reading', 'observed words', 'reviewers view', 'reviewers opinions', 'specifica sentiment', 'sentiment wait', 'positive sentiment', 'negative sentiment', 'sub id', 'r sub', 'r sub', 'r sub', 'r sub', 'alpha sub', 'alpha sub', 'aspect ratings', 'aspect ratings', 'aspect ratings', 'aspect ratings', 'aspect ratings', 'aspect ratings', 'aspect ratings', 'aspect ratings', 'aspect ratings', 'aspect ratings', 'decomposer ratings', 'would obtain', 'useful variables', 'understand better', 'three things', 'review articles', 'prediction happens', 'pre segmented', 'personalized recommendation', 'paper cited', 'observed information', 'mean denoted', 'formal way', 'easily compute', 'clear whether', 'alpha values', 'alpha values', 'advanced topics', 'actual reading', '2 computer', 'sentiment weights', 'sentiment weights', 'sentiment weights', 'sentiment weights', 'sentiment weights', 'weighted average', 'weighted average', 'weighted average', 'weighted average', 'beta sub', 'beta sub', 'two reviews', 'aspect weights', 'aspect weights', 'aspect weights', 'aspect weights', 'beta values', 'second step', 'second stage', 'second line', 'necessary generating', 'sentiment weight', 'world w', 'mule vector', 'also reviews', 'unified model', 'generation probability', 'conditional probability', 'conditional probability', 'negative weight', 'high weight', '04 words', 'valuable rooms', 'technique developed', 'segmentation stage', 'next question', 'left side', 'first stage', 'first stage', 'first draw', 'combined together', 'collectively denote', 'bi product', 'also want', 'also get', 'weighted sum', 'beta vector', 'would see', 'typical case', 'aspect segment', 'respective reviews', 'generating model', 'generating model', 'alpha value', 'room conditioning', 'right side', 'often see', 'important parameters', 'maximum likelihood', 'like', 'like', 'text data', 'word', 'opinion', 'topic', 'prior', 'parameter', 'mean', 'mean', 'given', 'given', 'decompose', 'rating', 'rating', 'course later', 'different', 'different', 'also', 'also', 'would', 'would', 'would', 'would', 'would', 'w', 'w', 'reviewers', 'reviewers', 'sub', 'ratings', 'ratings', 'ratings', 'aspect', 'aspect', 'aspect', 'aspect', 'aspect', 'aspect', 'aspect', 'aspect', 'aspect', 'aspect', 'way', 'values', 'values', 'sigma', 'review', 'problem', 'problem', 'problem', 'problem', 'placed', 'give', 'frequencies', 'document', 'compute', 'aspects', 'aspects', 'aspects', 'alpha', 'use', 'use', 'use', 'use', 'use', 'use', 'weights', 'weights', 'weights', 'weights', 'weights', 'weights', 'location', 'location', 'location', 'location', 'beta', 'average', 'assumed', 'assumed', 'assumed', 'assumed', 'second', 'preferences', 'preferences', 'preferences', 'obtain', 'obtain', 'generating', 'vector', 'simply', 'simply', 'particular', 'particular', 'model', 'model', 'model', 'model', 'probability', 'probability', 'weight', 'weight', 'weight', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'world', 'together', 'technique', 'supply', 'segmentation', 'rooms', 'question', 'product', 'maximum', 'likelihood', 'left', 'first', 'first', 'first', 'denoted', 'denoted', 'denoted', 'denoted', 'denote', 'data', 'case', 'case', 'case', 'case', 'segments', 'segments', 'segments', 'segments', 'value', 'value', 'value', 'value', 'value', 'value', 'segment', 'segment', 'segment', 'segment', 'segment', 'segment', 'reviews', 'reviews', 'reviews', 'reviews', 'reviews', 'text', 'text', 'sum', 'sum', 'room', 'room', 'right', 'right', 'later', 'later', 'information', 'information', 'want', 'want', 'want', 'want', 'get', 'get', 'get', 'get', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'course', 'course', 'course', 'course', 'course', 'course', 'course', 'course', 'course', 'zero', 'yeah', 'wnd', 'vocabulary', 'variance', 'usual', 'users', 'used', 'uncover', 'time', 'thought', 'think', 'therefore', 'task', 'task', 'task', 'talking', 'talking', 'talk', 'take', 'take', 'take', 'take', 'take', 'take', 'square', 'spectrum', 'specifically', 'specifically', 'solve', 'solve', 'shown', 'shown', 'shown', 'show', 'settings', 'set', 'set', 'set', 'set', 'set', 'set', 'service', 'service', 'service', 'service', 'service', 'service', 'service', 'service', 'seen', 'said', 'said', 'said', 'rs', 'retrieve', 'results', 'relevant', 'reason', 'read', 'ranking', 'products', 'price', 'price', 'premise', 'premise', 'predicting', 'predict', 'predict', 'predict', 'predict', 'predict', 'precisely', 'precisely', 'posteriori', 'plsa', 'part', 'output', 'order', 'order', 'order', 'opposed', 'one', 'one', 'occurring', 'occurred', 'occur', 'obviously', 'note', 'nice', 'neither', 'multiply', 'mu', 'motivation', 'modeling', 'means', 'maximize', 'maximize', 'make', 'made', 'lot', 'lot', 'lot', 'lot', 'look', 'listed', 'lecture', 'lecture', 'late', 'lambda', 'know', 'kind', 'k', 'introduce', 'interpret', 'internet', 'interested', 'interested', 'interest', 'input', 'input', 'infer', 'infer', 'indexed', 'hotel', 'hotel', 'hotel', 'hotel', 'hotel', 'hotel', 'hope', 'hope', 'hope', 'good', 'good', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'generate', 'generate', 'generate', 'generate', 'generate', 'general', 'general', 'general', 'formally', 'follows', 'follow', 'figure', 'far', 'factor', 'expensive', 'example', 'example', 'example', 'example', 'etc', 'estimate', 'estimate', 'drawn', 'done', 'discussion', 'discover', 'details', 'detail', 'depends', 'delta', 'decrease', 'counts', 'counts', 'count', 'count', 'computing', 'compare', 'compare', 'cause', 'cases', 'cases', 'basically', 'back', 'award', 'assumption', 'assumption', 'assume', 'assume', 'assume', 'assume', 'arts', 'applications', 'applications', 'anyway', 'always', 'adjust', 'accounts', 'according', 'able', '9', '3rd', '3']
lecture 47
['value versus location value versus room etc', 'highly weighted words versus', 'whole expensive hotels five stars', 'negatively lower weighted words', 'natural language processing techniques', 'bottom words also makes sense', 'also learn sentiment information directly', 'low overall ratings versus', 'rather query specific recommendation', 'analyzing users rating behavior', 'social media like tweets', 'precise whose inferred weights', 'others might score better', 'help us understand better', 'latent aspect rating analysis', 'non personalized recommendation results', 'mining latent user preferences', 'favor low price hotels', 'generative model like psa', 'really tell much difference', 'high level overall ratings', 'natural solution would', 'task sentiment analysis', 'mostly still reflecting', 'enriched feature representation', 'direct application would', 'problem using two stages', 'room condition tend', 'yet another application', 'end users better', 'dimensions like location', 'help us discover', 'would give us', 'standard techniques tend', 'like large screens', 'generator rated aspect', 'appreciating different features', 'next two papers', 'cutting edge topics', 'aspect rating analysis', 'latent regression model', 'also mother results', 'high overall ratings', 'top ten tends', 'like expensive hotels', 'review interesting patterns', 'top results generally', 'reach sentiment lexicon', 'different consumer groups', 'things like service', 'highest inferred value', 'text mining algorithms', 'overall rating condition', 'much higher price', 'analyzing review data', 'reviewers whose weights', 'top ten group', 'interesting preference information', 'dimensions like value', 'little regression model', 'interesting aspects commented', 'also another way', 'unified generative model', 'unified generative model', 'mp3 three reviews', 'reveal detailed opinions', 'word like long', 'give cheaper hotels', 'compare different hotels', 'text mining applications', 'average weights tend', 'compare different reviewers', 'put higher weights', 'social media', 'laptop etc', 'five stars', 'five stars', 'makes sense', 'could mean positive', 'two stages', 'users better', 'users better', 'room cleanliness', 'low group', 'directly used', 'also better', 'use topic model', 'low ratings', 'aspect level', 'bottom ten', 'techniques proposal', 'overall rating', 'overall rating', 'overall rating', 'overall rating', 'overall rating', 'much information', 'high scores', 'expensive hotels', 'expensive hotels', 'rating regression', 'top words', 'sentiment weights', 'sentiment waits', 'sentiment polarities', 'also interesting', 'overall ratings', 'overall ratings', 'overall ratings', 'higher ratings', 'top ten', 'top ten', 'opinion mining', 'opinion mining', 'one might', 'aspect rating', 'another dimension', 'much cheaper', 'inferred weights', 'inferred weights', 'inferred weights', 'reveal differences', 'large amount', 'interesting results', 'generative models', 'generative models', 'generative models', 'generative models', 'detailed understanding', 'also possible', 'also important', 'also allows', 'allow us', 'different groups', 'different groups', 'bottom time', 'ordinal regression', 'average price', 'average price', 'text mining', 'aspect ratings', 'aspect ratings', 'three cities', 'review test', 'preference weights', 'personalized ranking', 'original review', 'people might', 'two reviewers', 'value dimension', 'different dimensions', 'different dimensions', 'different dimensions', 'unified model', 'unified model', 'generative model', 'also model', 'simple results', 'interesting applications', 'bottom 10', 'three hotels', 'decomposed ratings', 'regression model', 'aspect weights', 'aspect ratio', 'different locations', 'different cities', 'four dimensions', 'cheaper hotels', 'cheaper hotels', 'cheaper hotels', 'reviews like', 'useful information', 'review text', 'review text', 'interesting way', 'reviewers tend', 'usually done', 'suggested readings', 'small books', 'second one', 'reviewer cares', 'reference site', 'really emphasize', 'rating decomposition', 'practical applications', 'people tend', 'overrating based', 'opinion targets', 'opinion target', 'opinion holders', 'opinion holder', 'lowest ratio', 'left side', 'important topic', 'highest ratios', 'higher prices', 'ground truth', 'go beyond', 'consumers trained', 'computer based', 'cell phones', 'battery life', 'average weights', 'also need', 'almost computer', 'two reviews', 'different aspects', 'different aspects', 'different aspects', 'different aspects', 'different aspects', 'different reviewers', 'similar preference', 'also shown', 'indirect way', 'topic model', 'topic model', 'generating model', 'word long', 'results show', 'positive sentence', 'good service', 'course also', 'text categorization', 'reviewers weights', 'reviewers cared', 'decompose ratings', 'next generation', 'rebooting time', 'actually meaningful', 'particular aspect', 'location', 'different context', 'problem formulation', 'first one', 'first 2', 'word distributions', 'useful becausw', 'tag comments', 'show whether', 'right side', 'really get', 'negative sentences', 'less compared', 'know whether', 'evaluating result', 'difficult challenge', 'average prices', 'always accurate', '2nd result', 'excellent reviews', 'table shows', 'shows 90', 'infer whether', 'infer wait', 'continued discussion', 'aspects discovered', 'problem using', 'cheap hotel', 'use reviewers', 'see whether', 'product laptop', 'product reviews', 'useful technique', 'recommendation', 'first predict', 'actually say', 'like', 'like', 'see clearly', 'information', 'words', 'words', 'words', 'user', 'understand', 'preferences', 'others', 'difference', 'also', 'price', 'much', 'tend', 'personalized', 'give', 'condition', 'compare', 'value', 'value', 'value', 'value', 'value', 'value', 'value', 'value', 'top', 'sense', 'sense', 'results', 'results', 'interesting', 'ratings', 'ratings', 'aspect', 'aspect', 'different', 'dimensions', 'word', 'weights', 'weights', 'weights', 'weights', 'weights', 'topics', 'topic', 'really', 'really', 'mean', 'discover', 'could', 'average', 'applications', 'hotels', 'hotels', 'hotels', 'hotels', 'hotels', 'hotels', 'hotels', 'hotels', 'hotels', 'hotels', 'way', 'way', 'learn', 'learn', 'learn', 'learn', 'aspects', 'aspects', 'aspects', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'service', 'service', 'query', 'query', 'query', 'positive', 'positive', 'long', 'long', 'text', 'text', 'text', 'text', 'text', 'reviewers', 'reviewers', 'reviewers', 'reviewers', 'reviewers', 'reviewers', 'using', 'using', 'using', 'using', 'time', 'put', 'put', 'product', 'people', 'opinions', 'opinions', 'lexicon', 'lexicon', 'actually', 'problem', 'problem', 'problem', 'problem', 'problem', 'first', 'first', 'useful', 'useful', 'useful', 'use', 'use', 'use', 'use', 'technique', 'tag', 'similar', 'show', 'show', 'right', 'result', 'result', 'ratios', 'prices', 'prices', 'particular', 'negative', 'need', 'laptop', 'laptop', 'know', 'get', 'distributions', 'discovered', 'difficult', 'decomposition', 'decompose', 'context', 'compared', 'clearly', 'cleanliness', 'allow', 'accurate', '10', 'data', 'data', 'data', 'data', 'reviews', 'reviews', 'reviews', 'reviews', 'reviews', 'reviews', 'reviews', 'reviews', 'reviews', 'reviews', 'reviews', 'shows', 'shows', 'shows', 'shows', 'shown', 'shown', 'predict', 'predict', 'infer', 'infer', 'infer', 'infer', 'discussion', 'discussion', 'say', 'say', 'say', 'hotel', 'hotel', 'hotel', 'course', 'course', 'course', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'generation', 'generation', 'generation', 'generation', 'whereas', 'well', 'weight', 'weight', 'variations', 'validating', 'validate', 'validate', 'uncover', 'trends', 'trend', 'toptenreviews', 'tended', 'talked', 'talk', 'surprising', 'support', 'supervised', 'summary', 'summarize', 'summaries', 'suggests', 'something', 'solving', 'solving', 'solving', 'solving', 'solve', 'solve', 'since', 'serve', 'segmentation', 'seal', 'rooms', 'remember', 'recommend', 'recommend', 'read', 'read', 'random', 'provides', 'proposed', 'products', 'prediction', 'powerful', 'potentially', 'poses', 'plug', 'plans', 'part', 'order', 'obtain', 'news', 'modeling', 'measure', 'means', 'means', 'means', 'means', 'means', 'means', 'may', 'many', 'manufacturers', 'mainly', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'look', 'look', 'look', 'look', 'look', 'lightweight', 'letting', 'letting', 'letting', 'let', 'lecture', 'laxing', 'lara', 'knowledge', 'kind', 'kind', 'kind', 'kind', 'internet', 'integrated', 'informative', 'indeed', 'implicit', 'humans', 'hard', 'hard', 'hand', 'going', 'given', 'getting', 'generate', 'general', 'general', 'general', 'focused', 'fitting', 'find', 'find', 'find', 'finally', 'feedback', 'favored', 'favored', 'favored', 'fact', 'expected', 'example', 'example', 'example', 'example', 'example', 'even', 'even', 'even', 'evaluation', 'evaluated', 'entity', 'entities', 'enough', 'emphasis', 'emphasis', 'embed', 'easy', 'earlier', 'drawn', 'discuss', 'discuss', 'dictated', 'develop', 'details', 'detail', 'design', 'design', 'described', 'decomposing', 'correct', 'consider', 'component', 'comparison', 'comment', 'combine', 'column', 'cleared', 'clear', 'cause', 'categories', 'cases', 'case', 'care', 'care', 'care', 'care', 'care', 'care', 'care', 'calls', 'call', 'byproduct', 'build', 'believe', 'bad', 'available', 'assuming', 'assumed', 'assumed', 'assume', 'approaches', 'applying', 'applied', 'apology', 'analyze', 'ambiguous', 'added', 'accurately']
lecture 48
['non text data also helps provide context', 'see text based prediction character serve', 'also add non text data directly', 'machine learning called active learning', 'text data like topic mining', 'help non text data mining', 'gives us two sets', 'also help us understand', 'next human also must', 'one sub task could', 'design high level features', 'help interpret patterns discovered', 'combine many text mining', 'contextual text mining techniques', 'course sometimes text alone', 'non text data together', 'see non text data', 'course help humans identify', 'general human would play', 'machine learning programs', 'really help improving', 'topic based indicators', 'qiaozhu mei dissertation', 'make decisions based', 'make decisions based', 'identify data points', 'non text data', 'non text data', 'non text data', 'non text data', 'non text data', 'non text data', 'non text data', 'non text data', 'non text data', 'help text mining', 'contextual text mining', 'contextual text mining', 'important real world variables', 'including topic mining', 'text mining algorithms', 'partition text data', 'original text data', 'interpreting text data', 'including text data', 'analyzing text data', 'text mining techniques', 'help provide predictors', 'provides limited view', 'improvement often leads', 'text data mining', 'mining text data', 'help us improve', 'text based prediction', 'text based prediction', 'additional data needs', 'human actually plays', 'would allow us', 'much better representation', 'text mining perspective', 'including human sensors', 'predictive model building', 'called pattern annotation', 'use text data', 'data mining loop', 'data mining loop', 'difference might suggest', 'content mining techniques', 'get sophisticated patterns', 'general text data', 'join mine text', 'real world variables', 'helps interpret', 'design effective predictors', 'human first would', 'non text', 'prediction problem set', 'helps discover', 'topic mining', 'topic mining', 'topic mining', 'directly related', 'directly characterize', 'context defined', 'also different', 'human could', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'data mining', 'data mining', 'data mining', 'data mining', 'text together', 'prediction based', 'sub tasks', 'one perspective', 'also know', 'human preferences', 'human behavior', 'data together', 'additional data', 'additional data', 'also involved', 'opinion mining', 'joint mining', 'sometimes already', 'make predictions', 'text analysis', 'real world', 'real world', 'intermediate representation', 'allows us', 'allows us', 'analysis techniques', 'new data', 'data acquisition', 'big data', 'text content', 'predictive model', 'predictive model', 'predictive model', 'predictive model', 'join analysis', 'sometimes machines', 'variables would', 'pattern annotation', 'data analysis', 'next lectures', 'mine text', 'would enlarge', 'put together', 'put together', 'unified framework', 'taking actions', 'take actions', 'remotely related', 'previous lectures', 'opinion holder', 'might want', 'might want', 'large body', 'large body', 'important role', 'important role', 'health condition', 'following lectures', 'following lectures', 'characterizing mostly', '2 perspectives', 'effective predictors', 'effective predictors', 'effective predictors', 'data generally', 'humans would', 'predictive values', 'frequent patterns', 'sentiment analysis', 'sentiment analysis', 'sentiment analysis', 'sentiment analysis', 'opinion analysis', 'opinion analysis', 'pattern occurs', 'interesting questions', 'actually understood', 'prediction errors', 'see patterns', 'collect data', 'machines would', 'different ways', 'multiple predictors', 'predicted values', 'predicted values', 'infer values', 'infer values', 'enriched features', 'big picture', 'big picture', 'useful data', 'useful data', 'subjective content', 'multiple sensors', 'first take', 'domain knowledge', 'start talking', 'seeing right', 'much interested', 'learning', 'important applications', 'collected next', 'also', 'content analysis', 'interesting variable', 'interesting variable', 'interesting knowledge', 'prediction problem', 'help', 'context', 'humans may', 'addressed yet', 'reference listed', 'different kind', 'general process', 'prediction accuracy', 'mine knowledge', 'actually predict', 'provide', 'provide', 'called', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'mining', 'mining', 'design', 'techniques', 'us', 'us', 'many', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'could', 'next', 'variables', 'variables', 'would', 'would', 'would', 'use', 'provides', 'perspective', 'much', 'loop', 'loop', 'leads', 'improve', 'important', 'important', 'get', 'course', 'course', 'course', 'course', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'patterns', 'patterns', 'patterns', 'patterns', 'analysis', 'analysis', 'humans', 'humans', 'humans', 'pattern', 'pattern', 'interesting', 'actually', 'actually', 'prediction', 'prediction', 'prediction', 'prediction', 'prediction', 'prediction', 'prediction', 'prediction', 'prediction', 'prediction', 'prediction', 'world', 'world', 'world', 'world', 'world', 'different', 'predictors', 'predictors', 'predictors', 'predictors', 'predictors', 'values', 'values', 'want', 'variable', 'put', 'picture', 'machines', 'general', 'general', 'general', 'general', 'general', 'general', 'features', 'features', 'features', 'features', 'difference', 'difference', 'content', 'content', 'content', 'content', 'sensors', 'sensors', 'sensors', 'problem', 'problem', 'problem', 'knowledge', 'knowledge', 'first', 'first', 'first', 'talking', 'right', 'process', 'mine', 'mine', 'mine', 'mine', 'mine', 'may', 'know', 'kind', 'interested', 'generally', 'applications', 'collect', 'collect', 'addressed', 'addressed', 'accuracy', 'accuracy', 'reference', 'reference', 'reference', 'predict', 'predict', 'predict', 'involved', 'involved', 'involved', 'collected', 'collected', 'collected', 'useful', 'useful', 'useful', 'useful', 'useful', 'useful', 'useful', 'work', 'work', 'well', 'well', 'well', 'well', 'weather', 'way', 'value', 'used', 'treat', 'topics', 'topics', 'testing', 'technique', 'talked', 'talk', 'subarea', 'still', 'space', 'someone', 'slide', 'slide', 'similarly', 'serves', 'sensor', 'seen', 'second', 'review', 'report', 'reflects', 'reflecting', 'recognize', 'question', 'question', 'question', 'problems', 'preparations', 'perturb', 'particular', 'order', 'order', 'opinions', 'opinions', 'occur', 'obtain', 'observer', 'observe', 'number', 'need', 'need', 'namely', 'mind', 'mentioned', 'meaning', 'maybe', 'matter', 'making', 'mainly', 'lot', 'lot', 'lot', 'lot', 'lot', 'looking', 'look', 'lecture', 'lecture', 'lecture', 'lecture', 'lead', 'later', 'label', 'kinds', 'keep', 'involvement', 'interpretable', 'instances', 'instances', 'information', 'improves', 'highlight', 'helpful', 'happens', 'going', 'going', 'going', 'going', 'going', 'goal', 'goal', 'goal', 'generation', 'generated', 'generated', 'generate', 'generate', 'generate', 'generate', 'generate', 'found', 'form', 'form', 'focus', 'finally', 'fed', 'extent', 'extent', 'example', 'example', 'etc', 'especially', 'emphasize', 'easy', 'discuss', 'direction', 'digest', 'detail', 'decide', 'created', 'controlling', 'control', 'contains', 'consuming', 'consumed', 'change', 'change', 'change', 'call', 'call', 'build', 'best', 'basically', 'basically', 'associated', 'associated', 'although', 'adjusting', 'adjusting', 'adjust', 'address', 'act', 'acquire', 'achieve']
lecture 49
['gaining increasing attention recently', 'almost allows partition text data', 'require contextual text mining', 'basically contextual text mining', 'combine non text data', 'indirect text context refers', 'course uses additional context', 'almost always available', 'contextual text mining', 'contextual text mining', 'contextual text mining', 'make opinion mining', 'derive sophisticated predictors', 'common research interests', 'interesting comparative analysis', '2012 presidential campaign', 'many interesting questions', 'data mining research', 'would allow us', 'would allow us', 'research topics published', 'broad analysis question', 'also reveal differences', 'additional information connected', 'research papers published', 'many interesting ways', 'many interesting ways', 'different conference names', 'text based prediction', 'often gives us', 'make topics associated', 'general provides meaning', 'comparing topics overtime', 'partition text data', 'making opinions connected', 'additional data related', 'also indirect context', 'rich context information', 'text context useful', 'compare different contexts', 'context would include', 'one social network', 'obtain additional context', 'compare sigir papers', 'include direct context', 'text mining', 'text often', 'allows us', 'allows us', 'interesting ways', 'interesting ways', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'partition data', 'partition data', 'data based', 'many applications', 'social network', 'social network', 'text yet', 'analyze text', 'topic mining', 'two contexts', 'presidential election', 'prediction problem', 'comparing topics', 'also partition', 'arbitrary ways', 'news data', 'different years', 'different years', 'different regions', 'sigir papers', 'direct context', 'useful knowledge', 'knowledge associated', 'conference venues', 'context data', 'two researchers', 'sudden changes', 'stock prices', 'stock prices', 'specific ones', 'separate unit', 'related data', 'one group', 'multiple kinds', 'issues mattered', 'gonna associate', 'goes beyond', 'generate even', 'enables discovery', 'discovery topics', 'complicated partitions', 'compare topics', 'also intersect', 'different context', 'different context', 'meta data', 'meta data', 'different venues', 'papers written', 'papers written', 'papers written', 'kdd papers', 'like sigir', 'remotely related', 'directly related', 'topic expressed', 'also treat', 'acl papers', 'time series', 'time series', 'includes time', 'see trends', 'paper id', 'another topic', 'another set', 'text', 'text', 'partition', 'course', 'information', 'connected', 'data', 'data', 'us', 'us', 'topics', 'topics', 'topics', 'question', 'opinions', 'one', 'include', 'include', 'compare', 'compare', 'also', 'also', 'papers', 'papers', 'papers', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'related', 'related', 'related', 'venues', 'obtain', 'obtain', 'meta', 'knowledge', 'another', 'topic', 'topic', 'treat', 'set', 'like', 'general', 'general', 'general', 'acl', 'time', 'time', 'time', 'time', 'time', 'see', 'see', 'paper', 'paper', 'well', 'well', 'well', 'well', 'way', 'variables', 'used', 'use', 'use', 'usa', 'subset', 'specifically', 'source', 'source', 'similarly', 'similarly', 'showing', 'show', 'rest', 'responses', 'regarded', 'regarded', 'possible', 'perhaps', 'people', 'partitioning', 'partitioning', 'partitioning', 'particular', 'outside', 'obviously', 'note', 'needed', 'need', 'need', 'need', 'mine', 'lot', 'look', 'location', 'location', 'location', 'location', 'listed', 'list', 'list', 'let', 'lecture', 'interested', 'independent', 'important', 'illustration', 'go', 'get', 'first', 'first', 'example', 'example', 'example', 'example', 'event', 'event', 'etc', 'difference', 'difference', 'difference', 'df', 'countries', 'could', 'could', 'correlated', 'contextualized', 'connect', 'compared', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'bottom', 'availability', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'authors', 'author', 'author', 'author', 'author', 'answer', 'age', 'affiliation', '1998']
lecture 50
['contextual text mining called contextual probabilistic latent semantic analysis', 'approach contextual probabilistic latent semantic analysis', 'introduce contextual probabilistic latent semantic analysis', 'word distributions actually tell us collection specific variations', 'estimate premise would naturally contain context variables', 'continue discussing contextual text mining', 'classical probabilistic model logic model', 'topic coverage might also vary according', 'another hurricane hurricane rita hit', 'major evaluation effort sponsored', 'explicitly add interesting context variables', 'enable cross collection comparison', 'perform comparative text mining', 'maybe particular topic like donation', 'makes 2 specific assumptions', 'another task introduced later', 'discover contextualized topics make', 'reveal spatial temporal patterns', 'different retrieval tasks though', 'contextual text mining', 'contextual text mining', 'contextual text mining', 'contextual text mining', 'gives us different views', 'might cover topics differently', 'technique would allow us', 'hurricane rita hit', 'top word top words', 'add context variables', 'topics given certain context', 'language modeling approach', 'would enable us', 'enterprise search tasks', 'high probability words shown', 'would allow us', 'yet another application', 'event impact analysis', 'topic coverage also depends', 'high level ideas', 'government response donation', 'assumption allows us', 'special temporal patterns', 'particularly sigir papers', 'two curves tracked', 'word like government', 'text retrieval conference', 'common theme indicating', 'general curves like', 'discover different variations', 'hit new orleans', 'tight little context', 'vector space model', 'comparing news articles', 'word distribution associated', 'probabilistic models', 'topics might represent', 'text given context', 'common topics shared', 'certain time period', 'research information retrieval', 'information retrieval research', 'different views associated', 'see clear dominance', 'next two cells', 'front block articles', 'first also choose', 'top one shows', 'time july 2005', 'common topic covered', 'common topics covered', 'common topics covered', 'different time periods', 'location like texas', 'bottom curve shows', 'still multiple topics', 'particular time period', 'different different contexts', 'see words like', 'language model paper', 'document specifically coverage', 'choose particular coverage', 'people might want', 'context variables', 'review common topics', 'boolean model etc', 'allow us', 'allow us', 'text would', 'one coverage distribution', 'word distributions', 'word distributions', 'word distributions', 'word distributions', 'word distributions', 'vary depending', 'topical collection', 'allows us', 'allows us', 'special patterns', 'maybe aid', 'explicitly specified', 'high probability', 'us government', 'specific views', 'common theme', 'themes like', 'would control', 'also shows', 'temporal chains', 'july 2005', 'would model', 'specific version', 'specific coverages', 'might get', 'might choose', 'hurricane katrina', 'hurricane katrina', 'hurricane katrina', 'hurricane katrina', 'hurricane katrina', 'world distributions', 'also considered', 'information retrieval', 'information retrieval', 'time given', 'different views', 'different views', 'specific context', 'wars involved killing', 'text data', 'conditional probability', 'cluster 2', 'cluster 2', 'dependency would', 'text articles', 'specific technique', 'government response', 'context associated', 'text collections', 'subtropical retrieval', 'reveal trends', 'retrieval models', 'retrieval models', 'retrieval models', 'research articles', 'particularly willing', 'new orleans', 'new orleans', 'new orleans', 'different weeks', 'different versions', 'different version', 'might pick', 'particular context', 'language model', 'language model', 'boolean model', 'slow response', 'contact information', 'high impact', 'time periods', 'different context', 'different context', 'particular topic', 'conditional distribution', 'different collections', 'particular coverage', 'another example', 'new audience', 'context information', 'also known', 'different topic', 'different locations', 'different locations', 'different locations', 'block articles', 'block articles', 'multiple collections', 'top left', 'different wars', 'two collections', 'theme coverage', 'time period', 'two topics', 'week four', 'weapon inspections', 'victim states', 'united nations', 'united nations', 'united nations', 'united nations', 'united nations', 'united nations', 'suggested readings', 'seminal paper', 'second cluster', 'sample words', 'peoples migrating', 'particular document', 'oil price', 'oil price', 'northern alliance', 'naturally', 'main paper', 'main difference', 'iraq war', 'iraq war', 'iraq war', 'get donate', 'following changes', 'first week', 'extra switches', 'em algorithm', 'conditional likelihood', 'completely general', 'clearly suggests', 'afghanistan war', 'afghanistan war', 'afghanistan war', 'additional result', 'topic variations', 'two wars', 'different variation', 'context influence', 'context influence', 'consider context', 'bring context', 'appropriate context', 'generative model', 'generated model', 'still use', '30 articles', '26 articles', 'third one', 'second one', 'choose one', 'next time', 'two events', 'yellow topic', 'main idea', 'collection', 'basic idea', 'topics overtime', 'three topics', 'red topics', 'visualization shows', 'examine variations', 'visualizations show', 'right side', 'covered mostly', 'view two', 'context depending', 'sample results', 'sample results', 'people talked', 'multiple sets', 'one idea', 'location texas', 'parameter estimation', 'necessary exactly', 'introduce', 'cplsa mainly', 'conditional probabilities', 'cluster around', 'apparently tricked', 'see variations', 'cplsa model', 'etc right', 'view three', 'simple extension', 'similar models', 'standard plsa', 'results show', 'means depending', 'example email', 'analyze topics', 'view one', 'use location', 'plsa model', 'generally assume', 'topic content', 'parameters etc', 'generation process', 'estimate', 'estimate', 'another', 'understand impact', 'word', 'specific', 'might', 'also', 'top', 'use event', 'text', 'text', 'text', 'text', 'text', 'particular', 'later', 'depends', 'cover', 'views', 'interesting', 'interesting', 'period', 'distribution', 'variations', 'variations', 'variations', 'variations', 'variations', 'shows', 'government', 'covered', 'covered', 'words', 'words', 'words', 'words', 'words', 'shown', 'shown', 'paper', 'next', 'general', 'first', 'choose', 'choose', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'context', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'model', 'articles', 'articles', 'articles', 'articles', 'one', 'one', 'one', 'one', 'time', 'time', 'time', 'time', 'time', 'time', 'time', 'location', 'location', 'collections', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'texas', 'texas', 'locations', 'idea', 'depending', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'coverage', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'wars', 'wars', 'wars', 'want', 'want', 'technique', 'technique', 'technique', 'technique', 'show', 'right', 'killing', 'killing', 'etc', 'etc', 'etc', 'contexts', 'contexts', 'results', 'results', 'people', 'people', 'people', 'impact', 'impact', 'impact', 'impact', 'visualization', 'variation', 'trends', 'review', 'review', 'review', 'process', 'probabilities', 'pick', 'mostly', 'means', 'generation', 'examine', 'exactly', 'events', 'estimation', 'document', 'document', 'document', 'document', 'dependency', 'cplsa', 'cplsa', 'around', 'apparently', 'analyze', 'event', 'event', 'event', 'event', 'event', 'involved', 'involved', 'involved', 'involved', 'bottom', 'bottom', 'bottom', 'bottom', 'view', 'view', 'view', 'view', 'view', 'view', 'use', 'use', 'use', 'use', 'use', 'use', 'understand', 'understand', 'similar', 'similar', 'parameters', 'parameters', 'left', 'left', 'known', 'known', 'extension', 'extension', 'content', 'content', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'plsa', 'plsa', 'plsa', 'plsa', 'plsa', 'sets', 'sets', 'sets', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'assume', 'assume', 'assume', 'assume', 'assume', 'assume', 'assume', 'withdraw', 'whereas', 'well', 'well', 'well', 'way', 'visualized', 'using', 'used', 'used', 'used', 'type', 'turns', 'trec', 'trec', 'trec', 'trec', 'tied', 'tied', 'tide', 'taken', 'surprising', 'surprising', 'suggest', 'study', 'study', 'state', 'start', 'start', 'spreading', 'south', 'solve', 'sociologist', 'slide', 'situation', 'simply', 'similarly', 'shift', 'seen', 'seem', 'seeing', 'secondly', 'say', 'say', 'say', 'say', 'relevant', 'related', 'region', 'region', 'references', 'recall', 'recall', 'publication', 'publication', 'problem', 'precisely', 'ponte', 'plot', 'picked', 'picked', 'pattern', 'occupation', 'obtained', 'obtain', 'obtain', 'middle', 'mentioned', 'meaningful', 'made', 'made', 'louisiana', 'lot', 'lot', 'lot', 'lot', 'lot', 'looking', 'look', 'look', 'look', 'look', 'look', 'let', 'let', 'let', 'let', 'let', 'let', 'lecture', 'lecture', 'launched', 'know', 'know', 'kind', 'kind', 'ir', 'involve', 'interpreted', 'interested', 'interested', 'initially', 'initially', 'influenced', 'indicates', 'indicated', 'indeed', 'imagine', 'hope', 'hope', 'hope', 'hope', 'hard', 'happened', 'gradually', 'got', 'going', 'going', 'going', 'going', 'going', 'going', 'goal', 'generate', 'generate', 'generate', 'generate', 'focus', 'flooding', 'flooding', 'flooding', 'fixed', 'firstly', 'familiar', 'extract', 'explain', 'explain', 'divide', 'discussion', 'discussion', 'discussion', 'differences', 'differences', 'differences', 'details', 'detail', 'detail', 'detail', 'depend', 'croft', 'criticism', 'course', 'course', 'course', 'course', 'could', 'corresponding', 'correspond', 'correlated', 'consideration', 'consequences', 'compare', 'column', 'column', 'column', 'city', 'city', 'city', 'city', 'chosen', 'choices', 'choices', 'choice', 'characterize', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'case', 'capture', 'bringing', 'basically', 'background', 'background', 'association', 'associate', 'apply', 'applications', 'addition', 'able', '1998', '1992']
lecture 51
['social media content might also form social networks', 'model called network supervised topic model', 'continue discussing contextual text mining', 'network supervised topic model modeling', 'often use maximum likelihood estimator', 'parameters would give us useful information', 'regular topic models like plsa', 'optimize another function f', 'say two social networks', 'one single objective function', 'regularizer function called r', 'netplsa would give much', 'research articles might form', 'rich information network environment', 'even sub networks', 'must cover similar topics', 'text data given parameters', 'text data alone based', 'single optimization framework', 'one suggested reading', 'current topic models', 'people might follow', 'people might claim', 'solving optimization problem', 'share common distribution', 'optimization objective function', 'modified object function', 'ir information retrieval', 'also use four topics', 'likelihood objective function', 'information retrieval second', 'capturing different heuristics', 'collaboration network would', 'help us uncover', 'text collection c', 'second equation shows', 'similar model could', 'plsa log likelihood', 'plsa log likelihood', 'co occurring words', 'similar topic distribution', 'network induced regularizers', 'form geographical network', 'topic model plsa', 'social network context', 'tells us basically', 'office general approach', 'text data given', 'text data given', 'jointly analyzing text', 'collaboration network information', 'mine text data', 'simple constraints imposed', 'cover similar topics', 'cover similar topics', 'parameters generated denoted', 'generate represent words', 'similar topic distributions', 'network constraint perspective', 'two subnetworks let', 'actually quite powerful', 'model parameters lambda', 'network graph g', 'two adjacent nodes', 'collaboration network tend', 'related data together', 'part fairly familiar', 'general idea clearly', 'topics correspond well', 'social network', 'social network', 'social network', 'objective function', 'function f', 'would allow', 'likelihood function', 'topic model', 'topic model', 'called netplsa', 'regular results', 'using network context', 'topic mining', 'research articles', 'information together', 'guide us', 'allows us', 'plsa alone', 'model parameters', 'fairly familiar', 'big environment', 'network must', 'probabilistic model', 'generative model', 'function combines', 'imposed based', 'data mining', 'data mining', 'text mining', 'topic distributions', 'topic distributions', 'topic coverage', 'topic coverage', 'content associated', 'content associated', 'analyzing topics', 'estimate models', 'second term', 'many words', 'collaboration network', 'collaboration network', 'text also', 'two people', 'represent taxes', 'even though', 'four topics', 'topic coverages', 'topic coverages', 'similar topics', 'similar topics', 'topics text', 'text data', 'text data', 'text data', 'text data', 'text data', 'mixed together', 'many different', 'different types', 'co occurrences', 'co occur', 'also vary', 'also try', 'adjacent nodes', 'adjacent nodes', 'quite general', 'two nodes', 'two nodes', 'similar ways', 'sample topics', 'meaningful topics', 'frontier topics', 'coherent topics', 'coherent topics', 'coherent topics', 'meta data', 'dblp data', 'data well', 'bibliographic data', 'treat text', 'text objects', 'text article', 'slide shows', 'standard plsa', 'four communities', 'four communities', 'four communities', 'four communities', 'four communities', 'slight variations', 'sample results', 'provide references', 'opinions expressed', 'new parameter', 'negative sign', 'negative sign', 'many problems', 'many cases', 'machine learning', 'machine learning', 'larger value', 'go back', 'enough time', 'coherent results', 'cannot generate', 'briefly introduce', 'also weight', 'active sign', 'use network', 'incorporating network', 'entire network', 'big network', 'using plsa', 'actually recognize', 'actually minimize', 'might', 'general view', 'general ideas', 'general ideas', 'basically plsa', 'parameters lambda', 'additional constraints', 'network constraints', 'network context', 'network context', 'strongly connected', 'next slide', 'lda also', 'joint analysis', 'high weight', 'also used', 'main idea', 'parameter lambda', 'whole sum', 'strong collaborators', 'node u', 'make sure', 'locations associated', 'help characterize', 'finally use', 'effect implemented', 'example topics', 'example text', 'general analysis', 'general idea', 'general idea', 'particular instantiation', 'paper referenced', 'others etc', 'network influence', 'means dm', 'content', 'context connects', 'see easily', 'impose constraints', 'often', 'give', 'give', 'form', 'form', 'form', 'model', 'model', 'authors connected', 'information', 'information', 'likelihood', 'topic', 'cover', 'useful', 'useful', 'based', 'articles', 'also', 'also', 'also', 'also', 'two', 'two', 'people', 'similar', 'similar', 'similar', 'similar', 'topics', 'topics', 'topics', 'data', 'data', 'data', 'data', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'plsa', 'plsa', 'plsa', 'plsa', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'parameters', 'well', 'use', 'use', 'use', 'use', 'use', 'tend', 'say', 'say', 'related', 'powerful', 'nodes', 'help', 'generated', 'generate', 'distributions', 'correspond', 'constraint', 'network', 'network', 'network', 'network', 'network', 'network', 'network', 'network', 'network', 'network', 'network', 'network', 'network', 'network', 'network', 'network', 'network', 'network', 'network', 'communities', 'general', 'general', 'general', 'general', 'general', 'using', 'using', 'regularizer', 'regularizer', 'regularizer', 'constraints', 'constraints', 'constraints', 'context', 'context', 'context', 'context', 'context', 'weight', 'slide', 'netplsa', 'netplsa', 'netplsa', 'netplsa', 'let', 'let', 'graph', 'graph', 'coverages', 'connected', 'analysis', 'idea', 'idea', 'idea', 'idea', 'basically', 'basically', 'basically', 'lambda', 'lambda', 'lambda', 'lambda', 'u', 'sum', 'strong', 'recognize', 'part', 'part', 'part', 'minimize', 'make', 'finally', 'effect', 'connects', 'characterize', 'associated', 'associated', 'associated', 'used', 'used', 'particular', 'particular', 'paper', 'paper', 'others', 'others', 'lda', 'lda', 'influence', 'influence', 'impose', 'impose', 'estimate', 'estimate', 'example', 'example', 'example', 'example', 'example', 'means', 'means', 'means', 'authors', 'authors', 'authors', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'zero', 'write', 'write', 'web', 'web', 'way', 'want', 'want', 'want', 'want', 'want', 'want', 'viewing', 'v', 'v', 'unable', 'twitter', 'third', 'think', 'technically', 'technically', 'subnetwork', 'subnetwork', 'structures', 'still', 'still', 'started', 'start', 'stand', 'square', 'smooth', 'simply', 'similarly', 'similarly', 'shown', 'showing', 'shared', 'set', 'set', 'separation', 'separate', 'seeing', 'says', 'says', 'says', 'right', 'revealed', 'respect', 'researchers', 'relations', 'reasonable', 'reason', 'read', 'publications', 'probability', 'probability', 'prior', 'prior', 'preferred', 'precisely', 'possible', 'picture', 'pick', 'perhaps', 'paths', 'optimizing', 'obtained', 'obtain', 'note', 'neighbors', 'necessarily', 'motivation', 'mostly', 'ml', 'mentioned', 'measure', 'may', 'maximize', 'maximize', 'maximize', 'maximize', 'makes', 'lot', 'look', 'look', 'look', 'look', 'living', 'listen', 'likelyhood', 'leverage', 'lecture', 'lecture', 'know', 'know', 'know', 'know', 'kinds', 'kind', 'kind', 'kind', 'kind', 'intuition', 'interesting', 'instead', 'indeed', 'incorporate', 'imposing', 'important', 'imagine', 'hoping', 'heuristic', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'get', 'front', 'friends', 'fourth', 'formula', 'following', 'flexible', 'fit', 'first', 'first', 'find', 'find', 'facebook', 'extension', 'experiments', 'essentially', 'ensure', 'english', 'edges', 'edge', 'easy', 'easy', 'discussed', 'differences', 'difference', 'difference', 'difference', 'details', 'details', 'details', 'detail', 'defined', 'defined', 'course', 'course', 'control', 'connections', 'connect', 'computing', 'combining', 'combined', 'combine', 'collaborations', 'case', 'case', 'case', 'case', 'capture', 'benefit', 'assumed', 'assumed', 'assume', 'applied', 'analyze', 'advantage', 'add', 'able', 'able', 'able']
lecture 52
['capture causal relation using somewhat past data', 'even using correlation measures like pearson correlation', 'indicate positive correlation w1and w3', 'effectively discover possibly causal topics based', 'continue discussing contextual text mining', 'help us infer new knowledge', 'dow jones industrial average', 'time series plus text data', 'top 3 words insignificant topics', 'non text data provide context', 'analyzing presidential election time series', 'called iterative causal topic model', 'topic models using time series', 'also many open challenges', 'combine non text data', 'iowa electronic market would', 'top ranked word list', 'new york times discovered', 'potentially discover causal topics', 'potentially discover causal topics', 'analyze word level correlation', 'non text time series', 'analyze text data together', 'texts semantically coherent topics', 'also get another subtopic', 'original topic topic one', '2000 presidential campaign election', 'time series context whether', 'iowa electronic market', 'apply regular topic modeling', 'new york times', 'new york times', 'new york times', 'contextual text mining', 'negatively correlated words w2', 'september 11 attack', 'september 11 attack', 'might provide insight', 'candi date causing', 'true causal relationship', 'true causal relations', 'text data together', 'mining text data', 'standard talking models', 'non text data', 'non text data', 'non text data', 'result help us', 'help us figure', 'social science study', 'also causal relation', 'auto regressive model', 'active research topic', 'crash actually happened', 'also support optimizig', 'also help interpret', 'iterative topic modeling', 'original topic one', 'original topic one', 'one simple solution', 'time series xt', 'time series feedback', 'external time series', 'external time series', 'external time series', 'external time series', 'external time series', 'stock market crash', 'still quite related', 'indeed quite related', 'pure topic models', 'companion text stream', 'pearson correlation', 'big data applications', 'statistically significant difference', 'necessarily mean anything', 'might discover w1', 'maximize light role', 'little bit discussion', 'indeed biased toward', 'actually quite simple', 'discover causal topics', 'causal topics ".', 'time series data', 'time series data', 'time series data', 'time series data', 'two suggested readings', 'regular topic model', 'presidential prediction market', 'text based prediction', 'text based prediction', 'granger causality test', 'granger causality test', 'companion news stream', 'companion news stream', 'use text mining', 'consider time lag', 'would mean x', 'frequently used measure', 'commonly used measure', 'commonly used measure', 'would contain topics', 'time series information', 'believe one candidate', 'might actually discover', 'generally would like', 'see computer technology', 'time series bias', 'many measures', 'whole text collection', 'apply topic models', 'might potentially explain', 'get another generation', 'topics whose coverage', 'topics might still', 'text data well', 'take topic one', 'represents external information', 'american airlines result', 'topic model discovery', 'stock price curves', 'sample results generated', 'topic model words', 'set time series', 'xt cause yt', 'four topics shown', 'know topic one', 'ranked based', 'highly correlated topics', 'causal relation', 'causal relation', 'maximizing topical coherence', 'analyzing text', 'use causality test', 'actually get topics', 'non text', 'effectively served', 'issues really mattered', 'might also want', 'strongly correlated words', 'strongest correlation', 'correlation measure', 'correlation analysis', 'presidential election', 'presidential election', 'presidential election', 'presidential election', 'word level', 'causality relation', 'new papers', 'iterative adjustment', 'also discover', 'causal topic', 'text data', 'text data', 'text data', 'text data', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'time series', 'topics based', 'topic models', 'topic models', 'topic models', 'text stream', 'text stream', 'one would', 'verify whether', 'might discover', 'many applications', 'contextual plsa', 'coherent semantically', 'regular topics', 'discover topics', 'put causal', 'causal scoring', 'causal perspective', 'causal influence', 'causal dimension', 'topic modeling', 'topic modeling', 'topic modeling', 'top words', 'causal topics', 'causal topics', 'prediction market', 'topic one', 'topic four', 'american airlines', 'topic model', 'topic model', 'topic model', 'news data', 'extension like', 'another scenario', 'another perspective', 'negatively correlated', 'simple approaches', 'semantically connected', 'news stream', 'text streams', 'text collection', 'involve text', 'second one', 'time stamp', 'would read', 'political science', 'october 2000', 'june 2000', 'biased towards', 'bias toward', 'model back', 'topic three', 'topic overtime', 'one candidate', 'widespread applications', 'patterns discovered', 'highly correlated', 'topics discovered', 'topic two', 'history information', 'history information', 'time period', 'time period', 'plsa would', 'stock prices', 'stock prices', 'stock prices', 'much related', 'causally related', 'optimizing causality', 'news event', 'topical coherence', 'certainly analyze', 'work site', 'whole process', 'tax cut', 'tax cut', 'sudden rise', 'strong correlations', 'strictly speaking', 'scored high', 'quotation marks', 'previous experiment', 'pattern annotation', 'past', 'often combined', 'next generation', 'models try', 'might figure', 'joint analysis', 'joined analysis', 'increasing since', 'gun control', 'government response', 'decision making', 'coverage overtime', 'cited reference', 'bias selection', 'already deviating', 'major topics', 'major topics', 'underlined words', 'selected words', 'red words', 'mixed words', 'best one', 'two companies', 'also explained', 'may 2000', 'clearly related', 'candidate names', 'news articles', 'news articles', 'sudden drop', 'sudden drop', 'sudden drop', 'nice way', 'might get', 'issue better', 'heuristic way', 'go beyond', 'go back', 'go back', 'alternate way', 'strongly correlated', 'strongly correlated', 'positively correlated', 'necessarily correlated', 'general topics', 'general topics', 'correlated topics', 'correlated topics', 'certainly rank', 'two subtopics', 'two subtopics', 'see airlines', 'price tends', 'important issues', 'important issues', 'important issues', 'correlation', 'ultimate goal', 'starting point', 'special cases', 'could read', 'correlated subtopics', 'better correlated', 'topics tend', 'meaningful topics', 'component words', 'results show', 'prediction purpose', 'best topics', 'clearly show', 'web etc', 'much clues', 'different correlations', 'using', 'using', 'using', 'using', 'say x', 'could get', 'basic idea', 'right side', 'people talk', 'never going', 'general problem', 'w3', 'top', 'knowledge', 'insignificant', 'help', 'even', 'even', 'discover', 'called', 'better approach', 'causal', 'like', 'like', 'data', 'data', 'data', 'data', 'data', 'data', 'data', 'text', 'text', 'text', 'text', 'text', 'text', 'election', 'approach works', 'one', 'time', 'time', 'time', 'would', 'w2', 'used', 'test', 'campaign', 'model', 'model', 'word', 'word', 'analyze', 'analyze', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'topic', 'coherent', 'also', 'also', 'also', 'also', 'actually', 'actually', 'discovered', 'information', 'stock', 'related', 'causality', 'causality', 'news', 'yt', 'x', 'take', 'strongly', 'shown', 'result', 'result', 'really', 'necessarily', 'might', 'might', 'might', 'might', 'might', 'might', 'measure', 'measure', 'measure', 'maximizing', 'maximize', 'mattered', 'lag', 'generated', 'generally', 'figure', 'discussion', 'discovery', 'difference', 'coverage', 'collection', 'bias', 'apply', 'apply', 'apply', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'topics', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'words', 'prediction', 'prediction', 'prediction', 'prediction', 'context', 'context', 'context', 'context', 'context', 'context', 'two', 'two', 'issues', 'issues', 'candidate', 'candidate', 'way', 'subtopics', 'results', 'results', 'important', 'go', 'get', 'get', 'get', 'get', 'get', 'get', 'get', 'general', 'drop', 'better', 'correlated', 'correlated', 'correlated', 'correlated', 'correlated', 'correlated', 'correlated', 'correlated', 'correlated', 'correlated', 'well', 'well', 'show', 'represents', 'represents', 'plsa', 'period', 'clearly', 'certainly', 'cause', 'cause', 'articles', 'price', 'price', 'price', 'coherence', 'coherence', 'coherence', 'tend', 'set', 'set', 'set', 'purpose', 'process', 'point', 'next', 'meaningful', 'goal', 'explained', 'could', 'could', 'component', 'cases', 'use', 'use', 'use', 'use', 'use', 'use', 'use', 'want', 'want', 'want', 'want', 'know', 'know', 'know', 'know', 'explain', 'explain', 'explain', 'explain', 'best', 'best', 'best', 'works', 'works', 'problem', 'problem', 'may', 'may', 'etc', 'etc', 'different', 'different', 'clues', 'clues', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'see', 'try', 'try', 'try', 'say', 'say', 'say', 'idea', 'idea', 'idea', 'approach', 'approach', 'approach', 'approach', 'approach', 'approach', 'approach', 'right', 'right', 'right', 'right', 'people', 'people', 'people', 'people', 'people', 'people', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'going', 'zoom', 'without', 'win', 'wikipedia', 'whenever', 'w4', 'w4', 'variations', 'usual', 'useful', 'useful', 'united', 'understand', 'understand', 'understand', 'uncover', 'trade', 'topicals', 'terrorism', 'techniques', 'target', 'talked', 'talked', 'talked', 'survey', 'suppose', 'supported', 'summarize', 'subset', 'stopped', 'stocks', 'start', 'solved', 'solve', 'solve', 'software', 'similar', 'sets', 'serve', 'serve', 'separate', 'separate', 'sense', 'sense', 'select', 'select', 'seen', 'seeing', 'said', 'restricted', 'rest', 'represent', 'regarded', 'reading', 'rather', 'question', 'question', 'question', 'pushing', 'published', 'psa', 'proposed', 'produced', 'prior', 'prior', 'prior', 'prior', 'predict', 'possible', 'points', 'picture', 'particular', 'particular', 'paper', 'output', 'output', 'otherwise', 'optimize', 'optimize', 'opinions', 'oil', 'number', 'number', 'necessary', 'must', 'motivation', 'mix', 'mentioned', 'mention', 'mention', 'mention', 'means', 'means', 'meaning', 'maybe', 'maybe', 'match', 'make', 'lot', 'look', 'look', 'look', 'look', 'look', 'look', 'look', 'look', 'look', 'literature', 'limited', 'likely', 'let', 'let', 'let', 'let', 'let', 'let', 'let', 'lecture', 'lecture', 'least', 'lda', 'known', 'knowing', 'knowing', 'iterate', 'iterate', 'introducing', 'internet', 'interesting', 'interested', 'interested', 'input', 'input', 'input', 'inject', 'induce', 'increase', 'improvement', 'improve', 'imagine', 'illustration', 'ideal', 'humans', 'however', 'hopefully', 'hope', 'hope', 'hand', 'hand', 'guide', 'guide', 'good', 'good', 'good', 'good', 'given', 'given', 'generate', 'generate', 'future', 'framework', 'framework', 'first', 'find', 'find', 'filtered', 'fed', 'far', 'extreme', 'expected', 'expect', 'example', 'example', 'example', 'example', 'example', 'example', 'example', 'examine', 'ensure', 'energy', 'end', 'earlier', 'drive', 'discussed', 'discussed', 'discussed', 'dimensions', 'details', 'detail', 'decompose', 'december', 'cplsa', 'course', 'course', 'correlate', 'com', 'choose', 'choice', 'caused', 'caused', 'caused', 'case', 'case', 'case', 'case', 'captures', 'candidates', 'call', 'call', 'buy', 'basically', 'basically', 'background', 'assess', 'ask', 'ask', 'apple', 'apple', 'airport', 'air', 'add', 'abortion', 'able', 'able', 'able', '2011']
lecture 53
['particularly logistical regression k nearest neighbor', 'particularly interesting area called deep learning', 'robust text mining technologies today tend', 'building practical text data application systems', 'two basic plan complementary relations', 'companion course called text retrieval', 'also help providing knowledge prominence', 'building efficient text mining systems', 'mostly used word based representations', 'big text data application system', 'particular help text mining systems', 'develop effective text analysis techniques', 'read frontier research papers', 'computing maximum likelihood estimator', 'text mining application systems', 'latent aspect rating analysis', 'help interpreting patterns later', 'help interpreting patterns discovered', 'practical text mining systems', 'harnessing big text data', 'category given text data', 'vector representations would allow', 'real world variables based', 'discover potentially paradigmatically relations', 'big text data applications', 'new techniques would lead', 'discover deeper knowledge buried', 'statistical learning techniques particularly', 'help non text data', 'text retrieval information retrieval', 'two important building blocks', 'many big data applications', 'information network mining techniques', 'analyze text information network', 'word similarity based evaluation', 'ordinal logistical regression', 'using non text data', 'maximum likelihood estimator', 'basic topic model plsa', 'method would allow us', 'course covered text mining', 'general data mining algorithms', 'using deep learning', 'text application system', 'given text reviews', 'interesting topical patterns', 'word association mining', 'introduced sentiment classification problem', 'big text data', 'supervised machine learning', 'reviewers latent weights', 'level takeaway messages', 'essential system component', 'discover paradigmatically relations', 'text based applications', 'text based applications', 'achieve better accuracy', 'potentially causal topics', 'text mining applications', 'invent new algorithms', 'text data mining', 'covers text retrieval', 'statistical machine learning', 'representing text data', 'non text data', 'non text data', 'share similar contexts', 'natural language processing', 'natural language processing', 'natural language processing', 'information theory concepts', 'contextual text mining', 'enriches text representation', 'enrich text representation', 'text based prediction', 'text based prediction', 'key takeaway messages', 'key takeaway messages', 'incorporate context variables', 'mine word associations', 'better text representation', 'relations might also', 'vector space model', 'word embedding technique', 'data mining techniques', 'probabilistic topic model', 'probabilistic topic model', 'analyze topic syntax', 'text analysis applications', 'many application areas', 'natural language used', 'another promising technique', 'also shown promise', 'discover new techniques', 'nowadays actually based', 'introduced naive bayes', 'courses would give', 'text data analysis', 'use social network', 'accurate knowledge discovery', 'modeling text data', 'work much better', 'predictive modeling component', 'pattern discovery would', 'enables quick response', 'similarity based approaches', 'relevant text data', 'analyzing text data', 'example bm 25', 'help analyzing topics', 'text categorisation method', 'really discuss anything', 'time series data', 'get evaluation right', 'another important concept', 'need two kinds', 'learn vector representation', 'search engine would', 'text categorization tasks', 'also allows us', 'interpreting patterns', 'also briefly reviewed', 'also briefly introduce', 'interesting applications', 'robust enough', 'current technologies', 'information retrieval', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'text mining', 'advanced nlp techniques', 'data mining', 'learned useful knowledge', 'word representation', 'big picture', 'word embedding', 'text retrieval', 'text retrieval', 'text retrieval', 'text retrieval', 'text applications', 'application opportunities', 'statistical learning', 'statistical learning', 'statistical learning', 'topic mining', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'text data', 'vector space', 'particularly talk', 'basic concepts', 'deeper knowledge', 'text representation', 'sentiment classification', 'allow us', 'allow us', 'better representation', 'two concepts', 'interesting features', 'basic principles', 'particular application', 'particularly talked', 'help convert', 'text similarity', 'syntagmatic relations', 'mutual information', 'many applications', 'text analysis', 'text analysis', 'text analysis', 'opinion mining', 'joint mining', 'general relations', 'retrieval techniques', 'two ways', 'two kinds', 'second area', 'research community', 'another technique', 'would give', 'allows us', 'using text', 'text clustering', 'text clustering', 'text clustering', 'text categorization', 'text categorization', 'text categorization', 'text categorization', 'original text', 'statistical analysis', 'predictive modeling', 'relevant data', 'relevant data', 'data size', 'contextual plsa', 'sentiment analysis', 'sentiment analysis', 'sentiment analysis', 'sentiment analysis', 'analyze topics', 'statistical techniques', 'general data', 'practical value', 'mixture model', 'generating model', 'generated model', 'generalized model', 'appealing model', 'many techniques', 'search engine', 'search engine', 'language models', 'explain patterns', 'discover topics', 'assigning weights', 'another reason', 'mine knowledge', 'mine knowledge', 'knowledge provenance', 'actionable knowledge', 'advanced algorithms', 'advanced algorithms', 'advanced algorithms', 'indexing technique', 'general method', 'commonly used', 'advanced techniques', 'advanced techniques', 'practical use', 'would suggest', 'would like', 'occurrence analysis', 'many cases', 'really reliable', 'really reflect', 'really go', 'prediction problem', 'categorization techniques', 'backbone techniques', 'serious evaluation', 'search engines', 'search engines', 'plsa seems', 'net plsa', 'key high', 'similar context', 'major topics', 'general concepts', 'useful applications', 'general techniques', 'also key', 'practical skills', 'review data', 'vice versa', 'users query', 'sophisticated features', 'small collection', 'small amount', 'slightly different', 'short period', 'provides directly', 'predicting values', 'open question', 'open challenges', 'mine opinions', 'may recall', 'make impact', 'large collection', 'idf weighting', 'gonna go', 'go back', 'generative classifiers', 'feature selection', 'em algorithm', 'effectively reduce', 'done yet', 'discriminative classifiers', 'decision making', 'current state', 'conditional probability', 'complex features', 'complete set', 'best position', 'additional predictors', 'measuring similarity', 'also suggested', 'also recommend', 'also join', 'also introduce', 'whole course', 'also introduced', 'useful technique', 'particular problem', 'overall ratings', 'introducing lda', 'decomposed ratings', 'nlp techniques', 'useful algorithms', 'also understanding', 'also discussed', 'also discussed', 'would provide', 'quite important', 'quite important', 'important points', 'important direction', 'specific measures', 'special case', 'special case', 'enough time', 'different aspects', 'different aspects', 'conditional entropy', 'base rule', 'discover words', 'use context', 'unsupervised way', 'also talk', 'understanding lda', 'regression', 'next point', 'misleading results', 'meaning elements', 'directly useful', 'attention recently', 'general way', 'mainly talked', 'help', 'help', 'based', 'also talked', 'also talked', 'also talked', 'well connected', 'system', 'non', 'network', 'mostly', 'good basis', 'efficient', 'effective', 'building', 'building', 'building', 'area', 'especially important', 'text', 'text', 'text', 'text', 'one example', 'data', 'data', 'data', 'tend', 'tend', 'practical', 'practical', 'method', 'language', 'covered', 'analyze', 'knowledge', 'knowledge', 'topic', 'topic', 'used', 'used', 'similarity', 'would', 'would', 'would', 'would', 'us', 'many', 'discovery', 'discover', 'discover', 'discover', 'briefly', 'analysis', 'analysis', 'algorithms', 'prediction', 'modeling', 'techniques', 'techniques', 'techniques', 'techniques', 'techniques', 'techniques', 'techniques', 'plsa', 'key', 'evaluation', 'evaluation', 'topics', 'world', 'world', 'work', 'using', 'using', 'using', 'tasks', 'reviewed', 'relevant', 'problem', 'problem', 'need', 'mine', 'learned', 'kinds', 'introduce', 'get', 'discuss', 'categorization', 'categorisation', 'areas', 'approaches', 'analyzing', 'analyzing', 'general', 'general', 'general', 'also', 'also', 'also', 'also', 'also', 'also', 'also', 'also', 'also', 'also', 'also', 'introduced', 'introduced', 'introduced', 'introduced', 'use', 'use', 'course', 'course', 'course', 'course', 'course', 'course', 'course', 'understanding', 'ratings', 'particular', 'particular', 'particular', 'particular', 'particular', 'particular', 'lda', 'enables', 'enables', 'discussed', 'context', 'context', 'context', 'actually', 'actually', 'important', 'important', 'important', 'important', 'important', 'important', 'important', 'important', 'important', 'important', 'important', 'important', 'time', 'time', 'time', 'time', 'talk', 'talk', 'skills', 'rule', 'measuring', 'measures', 'good', 'explain', 'example', 'example', 'example', 'example', 'entropy', 'case', 'case', 'basis', 'aspects', 'aspects', 'nlp', 'nlp', 'nlp', 'nlp', 'nlp', 'way', 'way', 'way', 'learn', 'learn', 'learn', 'learn', 'useful', 'useful', 'useful', 'useful', 'useful', 'useful', 'useful', 'useful', 'useful', 'useful', 'useful', 'useful', 'useful', 'review', 'review', 'results', 'results', 'recently', 'recently', 'point', 'point', 'especially', 'especially', 'elements', 'elements', 'one', 'one', 'one', 'talked', 'talked', 'talked', 'talked', 'talked', 'talked', 'talked', 'talked', 'talked', 'talked', 'talked', 'talked', 'talked', 'talked', 'talked', 'talked', 'talked', 'provide', 'provide', 'provide', 'provide', 'words', 'words', 'words', 'words', 'words', 'words', 'well', 'well', 'well', 'well', 'well', 'well', 'well', 'whether', 'whether', 'want', 'want', 'vision', 'verified', 'utility', 'understand', 'understand', 'understand', 'touch', 'touch', 'tools', 'theoretically', 'thank', 'thank', 'tf', 'texts', 'terms', 'tends', 'taking', 'taking', 'taken', 'take', 'take', 'svn', 'surely', 'summary', 'summary', 'subject', 'study', 'stress', 'still', 'spend', 'speech', 'solving', 'solve', 'society', 'simply', 'simpler', 'showed', 'sequences', 'sequences', 'sequence', 'semester', 'see', 'see', 'see', 'scenarios', 'revisit', 'reviewer', 'reviewer', 'result', 'remind', 'remember', 'rely', 'rely', 'relied', 'regarded', 'regarded', 'reference', 'realize', 'practice', 'popular', 'people', 'parts', 'part', 'part', 'paradigmatic', 'order', 'order', 'omitted', 'occurrences', 'occur', 'occur', 'observer', 'observed', 'oberved', 'nevertheless', 'needed', 'naturally', 'moreover', 'mention', 'meaningful', 'matter', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'lot', 'loop', 'look', 'look', 'location', 'let', 'lecture', 'know', 'know', 'know', 'know', 'know', 'know', 'know', 'know', 'kind', 'kind', 'interested', 'interested', 'infer', 'infer', 'infer', 'improve', 'impressive', 'implement', 'humans', 'humans', 'human', 'however', 'hope', 'hope', 'hope', 'got', 'generate', 'generalization', 'future', 'foundation', 'first', 'first', 'first', 'finally', 'finally', 'finally', 'finally', 'finally', 'figure', 'far', 'extend', 'express', 'explained', 'examining', 'examined', 'evaluate', 'etc', 'end', 'emphasize', 'emerged', 'discussions', 'discussion', 'discussion', 'discussing', 'detail', 'detail', 'detail', 'detail', 'detail', 'depth', 'depth', 'definitely', 'crucial', 'created', 'covering', 'coverage', 'cover', 'could', 'cost', 'contribute', 'consume', 'compute', 'compare', 'co', 'co', 'co', 'categories', 'capture', 'build', 'build', 'build', 'bm25', 'benefit', 'beginning', 'beginning', 'attracted', 'art', 'applied', 'applied', 'annotate', 'analytics', 'analytics', 'always', 'always', 'although', 'able']
