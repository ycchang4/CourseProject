In this lecture we give an overview of text mining and analytics. First, let's define the term text mining and the term text analytics. The title of this course is called Text Mining and Analytics, but the two terms text mining and text analytics are actually roughly the same. So we are not going to really distinguish them, and we're going to use them interchangeably. But the reason why we have chosen to use both terms in the title is because there is also some subtle difference if you look at the two phrases literally. Mining emphasizes more on the process, so it gives us an algorithmic view of the problem. Analytics on the other hand emphasizes more on the result or having a problem in mind. We're going to look at the text data to help us solve a problem. But again, as I said, we can treat these two terms roughly the same, and I think in the literature you probably will find the same. So we're not going to really distinguish them in the course. Both text mining and text analytics mean that we want to turn text data into high quality information or actionable knowledge. So in both cases we have the problem of dealing with a lot of text data and we hope to turn these text data into something more useful to us than the raw text data. And here we distinguish two different results one is high quality information, the other is actionable knowledge. Now, sometimes the boundary between the two is not so clear, but I also want to say a little bit about these two different angles of the result of text mining. In the case of high quality information we refer to more concise information about the topic, which might be much easier for humans to digest than the raw text data. For example, you might face a lot of reviews of a product. The more concise form of the information would be very concise summary of the major opinions about the features of the product. Positive about, let's say, battery life of a laptop. Now, this kind of results are very useful to help people digest text data, and so this is to minimize the human effort in consuming text data in some sense. The other kind of output is actionable knowledge. Here we emphasize the utility of the information or knowledge we discover from text data. It's actionable knowledge for some decision problem, or some actions to take. For example, we might be able to determine which product is more appealing to us all, a better choice for a shopping decision. Now, such an outcome could be called actionable knowledge because a consumer can take the knowledge and make a decision and act on it. So in this case, text mining supplies knowledge for optimal decision making. But again, the two are not so clearly distinguished, so we don't necessarily have to make a distinction. Text mining is also related to text retrieval, which is an essential component in any text mining systems. Now text retrieval refers to finding relevant information from a large amount of text data. So I've taught another separate MOOC on text retrieval and search engines, where we discussed various techniques for text retrieval. If you have taken that MOOC, you will find some overlap and it would be useful to know the background of text retrieval for understanding some of the topics in text mining. But if you have not taken that MOOC it's also fine, because in this more context mining and analytics we're going to repeat some of the key concepts that are relevant for text mining. But at the high level, let me also explain the relation between text retrieval and text mining. Text retrieval is very useful for text mining in two ways: First, text retrieval can be a pre-processor for text mining, meaning that it can help us turn big text data into a relatively small amount of most relevant text data, which is often what's needed for solving a particular problem. And in this sense, text retrieval also helps minimize human effort. Text retrieval is also needed for knowledge provenance and this roughly corresponds to the interpretation of text mining as turning text data into actionable knowledge. Once we find the patterns in text data or actionable knowledge, we generally would have to verify the knowledge by looking at the original text data so the users would have to have some text retrieval support to go back to the original text data to interpret the pattern, or to better understand the knowledge or to verify whether the pattern is really reliable. So this is a high level introduction to the concept of text mining and the relation between text mining and retrieval. Next, let's talk about text data as a special kind of data. Now it's interesting to view text data as data generated by humans as subjective sensors. So this slide shows an analogy between text data and non text data and between humans as subjective sensors and physical sensors such as network sensor or thermometer. So in general, a sensor will monitor the real world in some way it will sense some signal from the real world and then would report the signal as data in various forms, for example, a thermometer would watch the temperature of real world and then will report the temperature in particular format. Similarly a geo sensor would sense the location and then report the location specification, for example in the form of longitude value and lattitude value. Network sensor would monitor network traffic or activities in the network and report some digital format of data. Similarly, we can think of humans as subjective sensors that would observe the real world from some perspective, and then humans would express what they have observed in the form of text data. So in this sense human is actually a subjective sensor that would also sense what's happening in the world and then express what's observed in the form of data, in this case text data. Now looking at the text data in this way has the advantage of being able to integrate all kinds of data together, and that's indeed needed in most data mining problems. So here we are looking at the general problem of data mining, and in general we would be dealing with a lot of data about our world that are related to a problem. And in general would be dealing with both non text data and text data and of course the non text data are usually produced by physical sensors. And those non text data can be also of different formats - numerical data or categorical or relational data or multimedia data like a video or speech. So, these non text data are often very important in some problems. But text data is also very important, mostly because they contain a lot of semantic content and they often contain knowledge about the users, especially preferences and opinions of users. So, but by treating text data as the data observed from human sensors, we can treat all these data together in the same framework. So, data mining problem is basically to turn such data, turn all the data into actionable knowledge that we can take the advantage to change the real world, of course, for better. So this means that data mining problem is basically taking a lot of data as input and giving actionable knowledge as output. Inside the data mining module you can also see we have a number of different kinds of mining algorithms and this is because for different kinds of data we generally need different algorithms for mining the data. For example, video data might require computer vision to understand video content and that would facilitate the more effective mining and we also have a lot of general algorithms that are applicable to all kinds of data, and those algorithms of course are very useful, although for a particular kind of data we generally want to also develop special algorithms. So this course will cover specialized algorithms that are particularly useful for mining text data. 
So looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data. And we're going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in (the) real world. Especially for decision making or for completing whatever tasks that require text data to support, now because in general in many real world problems of data mining, we also tend to have other kinds of data that are non textual. So a more general picture would be to include non text data as well. And for this reason, we might be concerned with joint mining of text and non text data and so in this course we're going to focus more on text mining. But we can also touch how to join the analysis of both text data and non-text data. With this problem definition we can now look at the landscape of the topics in text mining analytics. Now this slide shows the process of generating text data in more detail. Most specifically, human sensor or human observer would look at the world from some perspective. Different people would be looking at the world from different angles and they will pay attention to different things. The same person at a different time might also pay attention to different aspects of the observed world. And so the human sensor would perceive the world from some perspective. And that human... The sensor would then form a view of the world and that can be called the observed world. Of course this would be different from the real world because of the perspective that the person has taken. This can often be biased also. Now the observable world can be represented as for example entity relation graphs or more in a more general way, using knowledge representation language. But in general, this is basically what a person has in mind about the world, and we don't really know what exactly it looks like, of course. But then the human would express what the person has observed using a natural language such as English, and the result is text data. Of course, the person could have used a different language to express what he or she has observed. In that case, we might have text data of mixed languages for different languages. So the main goal of text mining is actually to revert this process of generating test data. And we hope to be able to uncover some aspect in this process. And so specifically we can think about the mining, for example, knowledge about the language. And that means by looking at text data in English, we may be able to discover something about English... Some usage of English... Some patterns of English. So this is 1 type of mining problems where the result is some knowledge about language which may be useful in various ways. If you look at the picture, we can also then mine knowledge about the "Observed World". As so, this has much to do with mining the content of text data. We're going to look at the what the text data are about and then try to get the essence of it. Or extracting high quality information about a particular aspect of the world that we're interested in. For example, everything that has been said about a particular person or particular entity, and this can be regarded as mining content to describe the observed world in the user's mind, in the person's mind. If you look further then you can also imagine we can mine knowledge about this observer himself or herself. So this has also to do with using text data to infer some properties of this person. And these properties could include the mood of the person or sentiment of the person. And note that we distinguish the observed the world from the person because text data can describe what the person has observed in an objective way, but the description can be also subject with sentiment, and so in general you can imagine the text data would contain some factual descriptions of the world plus some subjective comments, so that's why it's also possible to do text mining to mine knowledge about the observer. Finally, if you look at the picture to the left side of this picture, then you can see we can certainly also say something about the real world, right? So indeed we can do text mining to infer other real world variables, and this is often called predictive analytics. And we want to predict the value of certain interesting variables. So this picture basically covered multiple types of knowledge that we can mine from text in general. When we infer other real world variables, we could also use some of the results from mining text data as intermediate results to help the prediction. For example, after we mine the content of text data, we might generate some summary of content, and that summary could be then used to help us predict the variables of the real world. Now of course, this is still generated from the original text data, but I want to emphasize here that often the processing of text data to generate some features that can help with the prediction, is very important. And that's why here we show that the results of some other mining tasks, including mining the content of text data and mining knowledge above the observer can all be very helpful for prediction. In fact, when we have a non-text data, we could also use the non-text data to help prediction. And of course, it depends on the problem. In general, non-text data can be very important for such prediction tasks. For example, if you want to predict the stocks. Stock prices or changes of stock prices based on discussion in the news articles or in social media, then this is an example of using text data to predict some other real world variables. Now in this case, obviously the historical stock price data would be very important for this prediction, and so that's example of non-text data that would be very useful for the prediction and we can combine both kinds of data to make the prediction. Now non-text data can be also useful for analyzing text by supplying context. When we look at the text data alone will be mostly looking at the content and opinions expressed in text. But text data generally have also context associated. For example, the time, the location, of that associated with the text data and these are useful context information. And the context can provide interesting angles for analyzing text data. For example, we might partition text data into different time periods because of the availability of time. Now we can analyze text data in each time period and then make a comparison. Similarly, we can partition text data based on locations or any metadata that's associated to form interesting comparison scenarios. So in this sense, non-text data can actually provide interesting angles or perspectives for text analysis, and can help us make context sensitive analysis of content or the language usage or the opinions about the observer or the authors of text data. We could analyze the sentiment in different context, so this is fairly general landscape of the topics in text mining and analytics. In this course we're going to selectively cover some of those topics. We actually hope to cover most of these general topics. First, we are going to cover natural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining. Second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge about a language. Third, we're going to talk about the topic mining and analysis, and this is only one way to analyze content of text, but it's a very useful way of analyzing content. It's also one of the most useful techniques in text mining. And then we're going to talk about opinion mining and sentiment analysis. So this can be regarded as one example of mining knowledge about the observer. And finally, we are going to cover a text based prediction problems where we try to predict some real world variable based on text data. So this slide also serves as a road map for this course. And will use this as outline for the topics that will cover in the rest of this course. 
This lecture is about the natural language content analysis. Natural language content analysis is the foundation of text mining. So we are going to first talk about this. And in particular, natural language processing. with a factor how we can represent text data? And this determines what algorithms can be used to analyze and mine text data. We're going to take a look at the basic concepts in natural language first. I'm going to explain these concepts using a simple example that you are seeing here. A dog is chasing a boy on the playground. Now this is a very simple sentence. When we read such a sentence, we don't have to think about it to get meaning of it. But when a computer has to understand the sentence, the computer has to go through several steps. First, the computer needs to know what are the words, how to segment the words. In English this is very easy as we can just look at the space and then the computer would need to know the categories of these words, syntactical categories. So for example Dog is a noun, chasing is the verb, boy is another noun, etc. And this is called a lexical analysis. In particular tagging these words with these syntactic categories is called a part of speech tagging. After that, the computer also needs to figure out the relation between these words. So A and the dog will form a noun phrase. On the playground would be a prepositional phrase, etc. And there are certain way for them to be connected together in order to generate the meaning. Some other combinations may not make sense. And this ..... This is called syntactic parsing. Or syntactical analysis or parsing of natural language sentence. The outcome is parse tree that you're seeing here. That tells us a structure of the sentence so that we know how we can interpret the sentence. But this is not semantics yet. So in order to get the meeting would have to map these phrases and these structures into some real world entities that we have in our mind. So dog is a concept that we know an A boy, the concept that we know. So connecting these phrases with what we know is understanding. For computer, would have to formally represent these entities by using symbols. So dog d1 means d1 is a dog, boy b1 means b1 refers to a boy, etc. And we also represented the chasing action as a predicate. So chasing is predic here with three arguments, d1, b1 and p1, which is a playground, right? So this is a formal representation of the semantics of this sentence. Once we reach that level of understanding, we might also make inferences. For example, if we assume there's a rule that says if someone is being chased than a person can get scared, then we can infer this boy might be scared. This is the inferred meaning based on our additional knowledge. And finally, we might even further to infer... might further infer what This sentence is requesting or why the person who said the sentence is saying this sentence. And so this has to do with understanding the purpose of saying this sentence, and this is called SPEECH Act analysis or pragmatic analysis. Which refers to the use of language. So in this case, person saying this might be reminding another person to bring back the dog. So this means when saying a sentence, the person actually takes the action. So the action here is to make a request. Now, this slide clearly shows that in order to really understand the sentence, there are a lot of things that the computer has to do now. In general, it's very hard for the computer to do everything, especially if we wanted to do everything correctly. This is the very difficult. Now the main reason why a natural language processing is very difficult because it's designed to make human communications efficient. As a result, for example, we omit a lot of common sense knowledge. Because we assume all the..... all of us have this knowledge, there's no need to encode this knowledge. And that makes communication efficient. We also keep a lot of ambiguities, like ambiguities of words. And this is again because we assume that we have the ability to disambiguate a word, so there's no problem with having the same word to mean, possibly different things in different context. Yet, for a computer this would be very difficult because the computer does not have the common sense knowledge that we do, so the computer would be confused indeed, and this makes it hard for natural language processing. Indeed, it makes it very hard for every step in the slide that I showed you earlier. Ambiguity is a main killer, meaning that in every step there are multiple choices and the computer would have to decide what's the right choice and that decision can be very difficult, as you will see also in a moment. And in general we need common sense reasoning. In order to fully understand the natural language and computers today don't yet have that, and that's why it's very hard for computers to precisely understanding natural language at this point. so here are some specific examples of challenges. Think about the word level ambiguity a word like design can be a noun or a verb, so we've got ambiguous part of speech tag. Root has also multiple meanings. It can be of mathematical sense, like in square root. Or it can be the root of a plant. Syntactic ambiguity refers to different interpretations of the sentence in terms of structures. So for example, natural language processing can actually be interpreted in two ways. So one is. The ordinary meaning that we will be getting. As well talking about this topic so it's processing of natural language. But there is also another possible interpretation, which is to say language processing is natural. Now we don't generally have this problem, but imagine for once a computer to determine the structure, the computer would actually have to make a choice between the two. Another classic example is a man saw a boy with a telescope. This ambiguity lies in the question who had the telescope. This  is called a prepositional phrase attachment ambiguity meaning... where to attach this prepositional phrase with a telescope? Should it modify the boy or should it be modifying saw, the verb? Another problem Anaphora resolution "John persuaded Bill to buy a TV for himself." Does himself referred to John or Bill? Pre supposition is another difficulty. He has quit Smoking implies that he smoked before and we need to have such knowledge in order to understand the languages. Because of these problems, the state of the art natural language processing techniques cannot do anything perfectly, even for the simplest part of speech tagging, we still cannot solve the whole problem. The accuracy that I listed here just about 97% was just taken from some studies earlier, and these studies obviously have to be using particular datasets, so the numbers here are not really meaningful if you take it out of the context of the data set that are used for evaluation, but I show these numbers may need to give you some sense about the accuracy or how well we can do things like this. It doesn't mean on any data set to the accuracy will be precisely 97%. But in general we can do part of speech tagging fairly well, although not perfectly. Parsing would be more difficult, but for partial parsing, meaning to get that some phrases correct, we can probably achieve 90% or better accuracy. But to get the complete parse tree correctly is still very very difficult. For semantic analysis, we can also do some aspects of semantic analysis, particularly extraction of entities and relations. For example, recognizing this is the person, that's the  location, this person and that person met in some place etc. it can also do a word sense disambiguation for some extent. We can figure out the occurrence of root in this sentence refers to the mathematical sense etc. Sentiment analysis is another aspect of semantic analysis that we can do. That means we can tag the sentences general positive when it's talking about product. Or talking about the person. Influence, however, is very hard and we generally cannot do that for any big domain, and it's only feasible for very limited domain. And that's a generally difficult problem in artificial intelligence. Speech Act analysis is also very difficult, and we can only do this properly for very specialized cases with a lot of help from human. To annotate enough data for the computer to learn from. So the slides also shows that computers are far from being able to understand natural language precisely, and that also explains why the text mining problem is difficult, because we cannot rely on mechanical approaces or computational methods to understand the language precisely. Therefore, we have to use whatever we have today particular statistical machine learning methods, or. Statistical analysis methods to try to get as much meaning out from the text as possible. And later you will see that there are actually many such algorithms that can indeed extract the interesting knowledge from text, even though we cannot really fully understand the meaning of all the natural language sentences precisely. 