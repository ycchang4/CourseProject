{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca7dc780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self):\n",
    "        p = []\n",
    "        with open(\"phrases.txt\") as f:\n",
    "            for line in f:\n",
    "                p.append(line.split())\n",
    "        self.p = dict(p)\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            s = utils.simple_preprocess(line)\n",
    "            i = 0\n",
    "            new_s = []\n",
    "            while i < len(s) - 1:\n",
    "                if (s[i+1] == self.p.get(s[i], False)):\n",
    "                    new_s.append(s[i] + \"_\" + s[i+1])\n",
    "                    i+=2\n",
    "                else:\n",
    "                    new_s.append(s[i])\n",
    "                    i+=1\n",
    "            yield new_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c3d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa044e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class moreCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in open(\"phrased_bags.dat\"):\n",
    "            yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c11f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 00:04:58,485 : INFO : collecting all words and their counts\n",
      "2021-11-28 00:04:58,487 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-11-28 00:04:58,562 : INFO : collected 6953 word types from a corpus of 57835 raw words and 300 sentences\n",
      "2021-11-28 00:04:58,563 : INFO : Creating a fresh vocabulary\n",
      "2021-11-28 00:04:58,569 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1739 unique words (25.01078671077233%% of original 6953, drops 5214)', 'datetime': '2021-11-28T00:04:58.569615', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-11-28 00:04:58,570 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 49031 word corpus (84.77738393706234%% of original 57835, drops 8804)', 'datetime': '2021-11-28T00:04:58.570115', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-11-28 00:04:58,577 : INFO : deleting the raw counts dictionary of 6953 items\n",
      "2021-11-28 00:04:58,578 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2021-11-28 00:04:58,579 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 35662.53080575807 word corpus (72.7%% of prior 49031)', 'datetime': '2021-11-28T00:04:58.579117', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-11-28 00:04:58,590 : INFO : estimated required memory for 1739 words and 100 dimensions: 2260700 bytes\n",
      "2021-11-28 00:04:58,591 : INFO : resetting layer weights\n",
      "2021-11-28 00:04:58,593 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-11-28T00:04:58.593118', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "2021-11-28 00:04:58,593 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1739 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2021-11-28T00:04:58.593616', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2021-11-28 00:04:58,675 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-28 00:04:58,676 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:04:58,680 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:04:58,680 : INFO : EPOCH - 1 : training on 57835 raw words (35656 effective words) took 0.1s, 420224 effective words/s\n",
      "2021-11-28 00:04:58,760 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-28 00:04:58,764 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:04:58,766 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:04:58,766 : INFO : EPOCH - 2 : training on 57835 raw words (35735 effective words) took 0.1s, 423288 effective words/s\n",
      "2021-11-28 00:04:58,846 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-28 00:04:58,848 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:04:58,849 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:04:58,850 : INFO : EPOCH - 3 : training on 57835 raw words (35632 effective words) took 0.1s, 433078 effective words/s\n",
      "2021-11-28 00:04:58,935 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-28 00:04:58,936 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:04:58,937 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:04:58,938 : INFO : EPOCH - 4 : training on 57835 raw words (35632 effective words) took 0.1s, 412274 effective words/s\n",
      "2021-11-28 00:04:59,022 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-28 00:04:59,023 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:04:59,024 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:04:59,024 : INFO : EPOCH - 5 : training on 57835 raw words (35595 effective words) took 0.1s, 418478 effective words/s\n",
      "2021-11-28 00:04:59,024 : INFO : Word2Vec lifecycle event {'msg': 'training on 289175 raw words (178250 effective words) took 0.4s, 414172 effective words/s', 'datetime': '2021-11-28T00:04:59.024616', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2021-11-28 00:04:59,025 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1739, vector_size=100, alpha=0.025>', 'datetime': '2021-11-28T00:04:59.025115', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccfe7f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 00:05:02,040 : INFO : collecting all words and their counts\n",
      "2021-11-28 00:05:02,042 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-11-28 00:05:05,845 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2021-11-28 00:05:05,846 : INFO : Updating model with new vocabulary\n",
      "2021-11-28 00:05:06,166 : INFO : Word2Vec lifecycle event {'msg': 'added 69587 new unique words (27.412213319467096%% of original 253854) and increased the count of 1703 pre-existing words (0.6708580522662633%% of original 253854)', 'datetime': '2021-11-28T00:05:06.166964', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-11-28 00:05:06,456 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2021-11-28 00:05:06,464 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2021-11-28 00:05:06,465 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12506280.016269669 word corpus (74.8%% of prior 16718844)', 'datetime': '2021-11-28T00:05:06.465465', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-11-28 00:05:06,916 : INFO : estimated required memory for 71290 words and 100 dimensions: 92677000 bytes\n",
      "2021-11-28 00:05:06,917 : INFO : updating layer weights\n",
      "2021-11-28 00:05:06,949 : INFO : Word2Vec lifecycle event {'update': True, 'trim_rule': 'None', 'datetime': '2021-11-28T00:05:06.949462', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "2021-11-28 00:05:06,949 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2021-11-28 00:05:06,950 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 71326 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2021-11-28T00:05:06.950462', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2021-11-28 00:05:07,954 : INFO : EPOCH 1 - PROGRESS: at 12.58% examples, 1558950 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:08,959 : INFO : EPOCH 1 - PROGRESS: at 25.46% examples, 1582420 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:09,960 : INFO : EPOCH 1 - PROGRESS: at 38.27% examples, 1594395 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:10,961 : INFO : EPOCH 1 - PROGRESS: at 51.09% examples, 1597920 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:11,968 : INFO : EPOCH 1 - PROGRESS: at 63.90% examples, 1597806 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:12,971 : INFO : EPOCH 1 - PROGRESS: at 76.54% examples, 1592856 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:13,975 : INFO : EPOCH 1 - PROGRESS: at 89.24% examples, 1590688 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:14,806 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-28 00:05:14,808 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:05:14,812 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:05:14,812 : INFO : EPOCH - 1 : training on 17005207 raw words (12507432 effective words) took 7.9s, 1591159 effective words/s\n",
      "2021-11-28 00:05:15,817 : INFO : EPOCH 2 - PROGRESS: at 12.82% examples, 1586085 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:16,821 : INFO : EPOCH 2 - PROGRESS: at 25.51% examples, 1586332 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:17,822 : INFO : EPOCH 2 - PROGRESS: at 38.04% examples, 1584930 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:18,824 : INFO : EPOCH 2 - PROGRESS: at 50.79% examples, 1588428 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:19,828 : INFO : EPOCH 2 - PROGRESS: at 63.67% examples, 1592267 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:20,832 : INFO : EPOCH 2 - PROGRESS: at 76.72% examples, 1596453 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:21,835 : INFO : EPOCH 2 - PROGRESS: at 89.59% examples, 1597074 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:22,636 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-28 00:05:22,638 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:05:22,640 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:05:22,640 : INFO : EPOCH - 2 : training on 17005207 raw words (12505677 effective words) took 7.8s, 1597774 effective words/s\n",
      "2021-11-28 00:05:23,645 : INFO : EPOCH 3 - PROGRESS: at 12.46% examples, 1541369 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:24,646 : INFO : EPOCH 3 - PROGRESS: at 24.46% examples, 1522343 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:25,651 : INFO : EPOCH 3 - PROGRESS: at 35.80% examples, 1491106 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:26,654 : INFO : EPOCH 3 - PROGRESS: at 47.56% examples, 1486033 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:27,662 : INFO : EPOCH 3 - PROGRESS: at 59.44% examples, 1485032 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:28,665 : INFO : EPOCH 3 - PROGRESS: at 71.90% examples, 1497013 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:29,673 : INFO : EPOCH 3 - PROGRESS: at 84.30% examples, 1500932 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:30,675 : INFO : EPOCH 3 - PROGRESS: at 96.77% examples, 1507055 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:30,923 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-28 00:05:30,924 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:05:30,927 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:05:30,928 : INFO : EPOCH - 3 : training on 17005207 raw words (12506404 effective words) took 8.3s, 1509367 effective words/s\n",
      "2021-11-28 00:05:31,931 : INFO : EPOCH 4 - PROGRESS: at 12.70% examples, 1574274 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:32,940 : INFO : EPOCH 4 - PROGRESS: at 25.46% examples, 1579427 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:33,947 : INFO : EPOCH 4 - PROGRESS: at 38.15% examples, 1584537 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:34,953 : INFO : EPOCH 4 - PROGRESS: at 50.85% examples, 1584578 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:35,954 : INFO : EPOCH 4 - PROGRESS: at 62.43% examples, 1558324 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:36,958 : INFO : EPOCH 4 - PROGRESS: at 74.43% examples, 1548720 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:37,964 : INFO : EPOCH 4 - PROGRESS: at 86.89% examples, 1545713 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:38,973 : INFO : EPOCH 4 - PROGRESS: at 99.35% examples, 1544958 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:39,017 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-28 00:05:39,018 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:05:39,021 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:05:39,021 : INFO : EPOCH - 4 : training on 17005207 raw words (12506592 effective words) took 8.1s, 1545593 effective words/s\n",
      "2021-11-28 00:05:40,023 : INFO : EPOCH 5 - PROGRESS: at 12.76% examples, 1584357 words/s, in_qsize 4, out_qsize 0\n",
      "2021-11-28 00:05:41,027 : INFO : EPOCH 5 - PROGRESS: at 25.63% examples, 1595867 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:42,028 : INFO : EPOCH 5 - PROGRESS: at 38.45% examples, 1602932 words/s, in_qsize 4, out_qsize 0\n",
      "2021-11-28 00:05:43,030 : INFO : EPOCH 5 - PROGRESS: at 51.21% examples, 1602112 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:44,033 : INFO : EPOCH 5 - PROGRESS: at 63.96% examples, 1601163 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:45,034 : INFO : EPOCH 5 - PROGRESS: at 76.90% examples, 1601794 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:46,035 : INFO : EPOCH 5 - PROGRESS: at 89.83% examples, 1603535 words/s, in_qsize 5, out_qsize 0\n",
      "2021-11-28 00:05:46,851 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 00:05:46,852 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:05:46,855 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:05:46,856 : INFO : EPOCH - 5 : training on 17005207 raw words (12506655 effective words) took 7.8s, 1596618 effective words/s\n",
      "2021-11-28 00:05:46,856 : INFO : Word2Vec lifecycle event {'msg': 'training on 85026035 raw words (62532760 effective words) took 39.9s, 1567013 effective words/s', 'datetime': '2021-11-28T00:05:46.856964', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2021-11-28 00:05:46,857 : INFO : collecting all words and their counts\n",
      "2021-11-28 00:05:46,858 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-11-28 00:05:46,908 : INFO : collected 12250 word types from a corpus of 216470 raw words and 160 sentences\n",
      "2021-11-28 00:05:46,909 : INFO : Updating model with new vocabulary\n",
      "2021-11-28 00:05:47,120 : INFO : Word2Vec lifecycle event {'msg': 'added 458 new unique words (3.7387755102040816%% of original 12250) and increased the count of 3677 pre-existing words (30.016326530612243%% of original 12250)', 'datetime': '2021-11-28T00:05:47.120462', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-11-28 00:05:47,139 : INFO : deleting the raw counts dictionary of 12250 items\n",
      "2021-11-28 00:05:47,140 : INFO : sample=0.001 downsamples 56 most-common words\n",
      "2021-11-28 00:05:47,141 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 180867.80429278902 word corpus (89.1%% of prior 202962)', 'datetime': '2021-11-28T00:05:47.141462', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-11-28 00:05:47,602 : INFO : estimated required memory for 4135 words and 100 dimensions: 5375500 bytes\n",
      "2021-11-28 00:05:47,602 : INFO : updating layer weights\n",
      "2021-11-28 00:05:47,639 : INFO : Word2Vec lifecycle event {'update': True, 'trim_rule': 'None', 'datetime': '2021-11-28T00:05:47.639962', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "2021-11-28 00:05:47,640 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2021-11-28 00:05:47,640 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 71784 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2021-11-28T00:05:47.640963', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2021-11-28 00:05:47,755 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-28 00:05:47,757 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:05:47,770 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:05:47,771 : INFO : EPOCH - 1 : training on 216470 raw words (176210 effective words) took 0.1s, 1366142 effective words/s\n",
      "2021-11-28 00:05:47,896 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-28 00:05:47,900 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:05:47,911 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:05:47,911 : INFO : EPOCH - 2 : training on 216470 raw words (175977 effective words) took 0.1s, 1268735 effective words/s\n",
      "2021-11-28 00:05:48,035 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-28 00:05:48,037 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:05:48,050 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:05:48,051 : INFO : EPOCH - 3 : training on 216470 raw words (176219 effective words) took 0.1s, 1279368 effective words/s\n",
      "2021-11-28 00:05:48,169 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-28 00:05:48,171 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:05:48,184 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:05:48,185 : INFO : EPOCH - 4 : training on 216470 raw words (175971 effective words) took 0.1s, 1329055 effective words/s\n",
      "2021-11-28 00:05:48,305 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-11-28 00:05:48,307 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-11-28 00:05:48,319 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-11-28 00:05:48,319 : INFO : EPOCH - 5 : training on 216470 raw words (176128 effective words) took 0.1s, 1325321 effective words/s\n",
      "2021-11-28 00:05:48,320 : INFO : Word2Vec lifecycle event {'msg': 'training on 1082350 raw words (880505 effective words) took 0.7s, 1297235 effective words/s', 'datetime': '2021-11-28T00:05:48.320465', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(880505, 1082350)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text8 = gensim.models.word2vec.Text8Corpus(\"text8\")\n",
    "model.build_vocab(text8, update=True)\n",
    "model.train(text8, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "more_sentences = moreCorpus()\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb7b8d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15114135"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('', 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c591b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 00:05:58,541 : INFO : Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2021-11-28 00:05:58,541 : INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b89d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitx = []\n",
    "hity = []\n",
    "pos = 0\n",
    "neg = 0\n",
    "with open(\"hits.dat\") as f:\n",
    "    for line in f:\n",
    "        for word in line:\n",
    "            try:\n",
    "                hitx.append(model.wv[word])\n",
    "                hity.append(1)\n",
    "                pos += 1\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "with open(\"miss.dat\") as f:\n",
    "    for line in f:\n",
    "        for word in line:\n",
    "            try:\n",
    "                hitx.append(model.wv[word])\n",
    "                hity.append(0)\n",
    "                neg += 1\n",
    "            except:\n",
    "                continue\n",
    "hitx = np.array(hitx)\n",
    "hity = np.array(hity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7550768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bias = np.log([pos/neg+pos])\n",
    "\n",
    "nnet = keras.Sequential()\n",
    "nnet.add(keras.layers.Dense(50, input_shape=(100,), activation='tanh'))\n",
    "nnet.add(keras.layers.Dropout(0.5))\n",
    "nnet.add(keras.layers.Dense(20, input_shape=(50,), activation='relu'))\n",
    "nnet.add(keras.layers.Dropout(0.5))\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "nnet.add(keras.layers.Dense(1, input_shape=(20,), activation='sigmoid', bias_initializer= output_bias))\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "nnet.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['binary_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8593c112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "30979/81288 [==========>...................] - ETA: 21s - loss: nan - binary_accuracy: 0.6881"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_92904/159723581.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m nnet.fit(\n\u001b[0m\u001b[0;32m      2\u001b[0m   \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhitx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramFiles\\conda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramFiles\\conda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramFiles\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramFiles\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramFiles\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramFiles\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mD:\\ProgramFiles\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\ProgramFiles\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramFiles\\conda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nnet.fit(\n",
    "  x=hitx,\n",
    "  y=hity,\n",
    "  shuffle=True,\n",
    "  epochs=3,\n",
    "  batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c0acb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31313264]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.predict(np.array([model.wv['a']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20e1940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This lecture is about the syntagmatic relation discovery and mutual information\n",
      " In this lecture, we're going to continue discussing syntagmatic relation discovery\n",
      " In particular, we're going to talk about another concept, the information theory, called mutual information\n",
      " And how it can be used to discover syntagmatic relations? Before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong syntagmatic relations globally from corpus\n",
      " So now we're going to introduce mutual information, which is another concept in information theory that allows us to, in some sense, normalize the conditional entropy to make\n",
      " a more comparable across different pairs\n",
      " In particular, mutual information, denoted by I(X;Y), measures the entropy reduction of X obtained from knowing Y\n",
      " More specifically the question we're interested in here, is how much reduction in the entropy of X can we obtain by knowing Y\n",
      " So mathematically, it can be defined as the difference between the original entropy of X and the conditional entropy of X given Y\n",
      " And you might see here you can see here\n",
      " It can also be defined as a reduction of entropy of Y, because of knowing X\n",
      " Normally the two conditional entropies H(X|Y) and H(Y|X) are not equal\n",
      " But interestingly, the reduction of entropy by knowing one of them is actually equal, so this quantity is called mutual information denoted by I here and this function has some interesting properties\n",
      " First, it's also non negative\n",
      " This is easy to understand becausw the original entropy is always not going to be lower than the possibly reduced conditional entropy\n",
      " In other words, the conditional entropy would never exceed the original entropy\n",
      " Knowing some information can always help us potentially, but won't hurt us in predicting X\n",
      " The second property is that it's symmetric while conditional entropy is not symmetrical\n",
      " Mutual information is\n",
      " The third property is that it reaches its minimum zero if and only if the two random variables are completely independent\n",
      " That means knowing one of them doesn't tell us anything about the other\n",
      " And this last property can be verified by simply looking at the equation above\n",
      " And it reaches 0 if and only if the conditional entropy of X given Y is exactly the same as original entropy of X\n",
      " So that means knowing why did not help at all, and that's when X&amp;Y are completely independent\n",
      " Now when we fix X to rank different Ys using conditional entropy would give the same order as ranking based on mutual information, because in the function here H of X is fixed because X is fixed\n",
      " So ranking based on mutual information is exactly the same as ranking based on the conditional entropy of X given Y\n",
      " But the mutual information allows us to compare different pairs of X&amp;Y, so that's why mutual information is more general and in general more useful\n",
      " So let's examine them intuition of using mutual information for syntagmatic relation mining\n",
      " Now the question we ask for syntactic relation mining is whenever eats occurs, what other words also tend to occur? So this question can be framed as a mutual information question, that is, which was have higher mutual information with eats\n",
      " So we're going to compute the mutual information between eats and other words\n",
      " And if we do that, and it's basically a based on the same intuition as in conditional entropy, we will see that words that are strongly associated with each will tend to have high mutual information, whereas words that are not related\n",
      " We have lower mutual information, so this I give some example here\n",
      " The mutual information between eats and meats, which is the same as between meats and eats cause major information is symmetric is expected to be higher than The mutual information between eats and the\n",
      " Because knowing the doesn't really help us predict eats\n",
      " Similarly knowing eats doesn't help us predicting the as well\n",
      " And you also can easily see that the mutual information between聽 a word and itself is the largest which is equal to the mutual info\n",
      " The entropy of this word\n",
      " So because in this case the reduction is maximum because knowing one would allow us to predict the other completely so the conditional entropy is zero\n",
      " Therefore the mutual information reaches its maximum\n",
      " It's going to be larger than or equal to the mutual information between eats and another word\n",
      " In other words, picking any other word, and computing mutual information between eats and that word, you won't get any mutual information larger than the mutual information between eats and itself\n",
      " So now let's think about how to compute the mutual information\n",
      " Now, in order to do that, we often\n",
      " use a different form of mutual information, and we can mathematically write the mutual information into the form shown on this slide, where we essentially see a formula that computes what's called KL-divergences or callback labeler divergance\n",
      " This is another term in information theory that measures the divergance between two distributions\n",
      " Now if you look at the formula, it's also sum over many combinations of different values of the two random variables, but inside the sum mainly we're doing a comparison between 2 joint distributions\n",
      " The numerator has the joint actual observed\n",
      " Join the distribution of the two random variables\n",
      " The bottom part of the denominator can be interpreted as the expected joint distribution of the two random variables\n",
      " If there were independent\n",
      " Because when two random variables are independent, they joined distribution is equal to the product of the two probabilities\n",
      " So this comparison would tell us whether the two variables are indeed independent if there indeed independent, then we would expect that the two are the same\n",
      " But if the numerator is different from the denominator, that would mean the two variables are not independent, and that helps measure the association\n",
      " The sum is simply to take into consideration of all the combinations of the values of these two random variables\n",
      " In our case, each random variable can choose one of the two values 0 or 1, so we have four combinations here\n",
      " So if we look at this form of mutual information it shows that the mutual information measures the diversions of the actual joint distribution from the expected distribution under the independence assumption\n",
      " The larger this divergence is, the higher the mutual information would be\n",
      " So now let's further look at the what are exactly the probabilities involved in this formula of mutual information\n",
      " And here I listed all the probabilities involved and it's easy for you to verify that basically we have first 2 probabilities corresponding to the presence or absence of each word\n",
      " So for W1, we have two probabilities shown here\n",
      " They should sum to 1 because a word can either be present or absent in the segment\n",
      " And similarly for the second word, we also have two probabilities representing presence or absence of this word, and this sums to one as well\n",
      " And then finally we have a lot of joint probabilities that represented the scenarios of Co-occurrences of the two words\n",
      " And they are shown here\n",
      " Right, so this sums to 1 because the two words can only have these four possible scenarios\n",
      " Either they both occur\n",
      " So in that case both variables will have a value of one or one of them occurs\n",
      " There are two scenarios\n",
      " In these two cases, one of the random variables will be equal to 1 and the other would be 0\n",
      " And finally we have the scenario when none of them occurs\n",
      " So this is when the two variables taking a value of 0\n",
      " And they're summing up to 1, so these are the probabilities involved in the calculation of mutual information\n",
      " here\n",
      " Once we know how to calculate these probabilities, we can easily calculate the mutual information\n",
      " It's also interesting to note that there are some relations or constraints among these probabilities, and we already saw two of them, so the in the previous slide that you have seen that the marginal probabilities of these words sum to one, and we also have seen this constraint that says the two words can only have these four different scenarios of Co occurrences, but we also have some additional constraints listed in the bottom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " And so, for example, this one means if we add up the probabilities that we observe the two words occur together and the probabilities when the word the first word occurs and the second word doesn't occur, we get exactly the probability that the first word is observed\n",
      " In other words, and when the word is observed when the first word is observed and there are only two scenarios depending on weather second word is also observed\n",
      " So this probability captures the first scenario when the signal word actually is also observed\n",
      " And this captures the second scenario when the seond word is not observed, so we only see the first word\n",
      " And it's easy to see the other equations also follow the same reasoning\n",
      " Now these equations allow us to compute some probabilities based on other probabilities\n",
      " And this can simplify the computation\n",
      " So more specifically, and if we know the probability that a word is present, and in this case right? So if we know this\n",
      " And if we know the presence of the probability of presence of the second word, then we can easily compute their absence probability, right? It's very easy to use this equation to do that\n",
      " An so we this will take care of the computation of these probabilities of presence or absence of each word\n",
      " Now let's look at their joint distribution, right? Let's assume that we also have available probability that they occur together\n",
      " Now it's easy to see that we can actually compute the all the rest of these probabilities based on these\n",
      " Specifically, for example, using this equation, we can compute the probability that the first word occurred and the second word did not, because we know these probabilities in the boxes\n",
      " And similarly, using this equation we can compute the probability that we observe only the second word\n",
      " And then finally we\n",
      " This probability can be calculated by using this equation, because now this is known and this is also known and this is already known right? So this can be easier to calculate\n",
      " Right, so now this can be calculated\n",
      " So this slide shows that we only need to know how to compute these three probabilities that are shown in the boxes, namely the presence of each word and the Co occurrence of both words in a segment\n"
     ]
    }
   ],
   "source": [
    "tol = 0.1\n",
    "\n",
    "with open(\"textanalytics.txt\") as f:\n",
    "    docs = f.readlines()\n",
    "    line = docs[11]\n",
    "    for sentence in line.split(\".\"):\n",
    "        accu = 0.0\n",
    "        cnt = 0\n",
    "        for word in utils.simple_preprocess(sentence):\n",
    "            try:\n",
    "                accu += nnet.predict(np.array([model.wv[word.strip()]]))\n",
    "                cnt+=1\n",
    "            except:\n",
    "                continue\n",
    "        try:\n",
    "            accu /= cnt\n",
    "            if (accu > tol):\n",
    "                print(sentence)\n",
    "        except:\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
