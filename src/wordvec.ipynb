{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9621a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self):\n",
    "        p = []\n",
    "        with open(\"phrases.txt\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                phrase = line.split()\n",
    "                prefix = phrase[0]\n",
    "                for x in phrase[1:]:\n",
    "                    p.append((prefix, x))\n",
    "                    prefix += \"_\" + x\n",
    "        self.p = dict(p)\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "\n",
    "        for line in open(corpus_path, encoding=\"utf-8\"):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            s = utils.simple_preprocess(line)\n",
    "            i = 0\n",
    "            new_s = []\n",
    "            while i < len(s) - 1:\n",
    "                if (s[i+1] == self.p.get(s[i], False)):\n",
    "                    new_s.append(s[i] + \"_\" + s[i+1])\n",
    "                    i+=2\n",
    "                else:\n",
    "                    new_s.append(s[i])\n",
    "                    i+=1\n",
    "            nns =[]\n",
    "            i = 0\n",
    "            while i < len(new_s) - 1:\n",
    "                if (new_s[i+1] == self.p.get(new_s[i], False)):\n",
    "                    nns.append(new_s[i] + \"_\" + new_s[i+1])\n",
    "                    i+=2\n",
    "                else:\n",
    "                    nns.append(new_s[i])\n",
    "                    i+=1\n",
    "            yield nns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf77d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f75cca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class moreCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in open(\"phrased_bags.dat\"):\n",
    "            yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b95b3f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 15:58:41,137 : INFO : collecting all words and their counts\n",
      "2021-12-06 15:58:41,138 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-12-06 15:58:41,233 : INFO : collected 6936 word types from a corpus of 57535 raw words and 300 sentences\n",
      "2021-12-06 15:58:41,233 : INFO : Creating a fresh vocabulary\n",
      "2021-12-06 15:58:41,240 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 1730 unique words (24.942329873125722%% of original 6936, drops 5206)', 'datetime': '2021-12-06T15:58:41.240736', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-12-06 15:58:41,241 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 48750 word corpus (84.73103328408794%% of original 57535, drops 8785)', 'datetime': '2021-12-06T15:58:41.241736', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-12-06 15:58:41,248 : INFO : deleting the raw counts dictionary of 6936 items\n",
      "2021-12-06 15:58:41,248 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2021-12-06 15:58:41,249 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 35443.77175028181 word corpus (72.7%% of prior 48750)', 'datetime': '2021-12-06T15:58:41.249736', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-12-06 15:58:41,261 : INFO : estimated required memory for 1730 words and 100 dimensions: 2249000 bytes\n",
      "2021-12-06 15:58:41,261 : INFO : resetting layer weights\n",
      "2021-12-06 15:58:41,263 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-12-06T15:58:41.263734', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "2021-12-06 15:58:41,263 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 1730 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2021-12-06T15:58:41.263734', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2021-12-06 15:58:41,363 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-06 15:58:41,367 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:58:41,368 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:58:41,369 : INFO : EPOCH - 1 : training on 57535 raw words (35251 effective words) took 0.1s, 356088 effective words/s\n",
      "2021-12-06 15:58:41,461 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-06 15:58:41,462 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:58:41,466 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:58:41,467 : INFO : EPOCH - 2 : training on 57535 raw words (35374 effective words) took 0.1s, 366201 effective words/s\n",
      "2021-12-06 15:58:41,561 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-06 15:58:41,562 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:58:41,566 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:58:41,566 : INFO : EPOCH - 3 : training on 57535 raw words (35496 effective words) took 0.1s, 361539 effective words/s\n",
      "2021-12-06 15:58:41,661 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-06 15:58:41,662 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:58:41,662 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:58:41,663 : INFO : EPOCH - 4 : training on 57535 raw words (35413 effective words) took 0.1s, 372575 effective words/s\n",
      "2021-12-06 15:58:41,754 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-06 15:58:41,755 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:58:41,759 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:58:41,760 : INFO : EPOCH - 5 : training on 57535 raw words (35433 effective words) took 0.1s, 370530 effective words/s\n",
      "2021-12-06 15:58:41,760 : INFO : Word2Vec lifecycle event {'msg': 'training on 287675 raw words (176967 effective words) took 0.5s, 356387 effective words/s', 'datetime': '2021-12-06T15:58:41.760873', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2021-12-06 15:58:41,761 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=1730, vector_size=100, alpha=0.025>', 'datetime': '2021-12-06T15:58:41.761875', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9638d2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 15:58:44,082 : INFO : collecting all words and their counts\n",
      "2021-12-06 15:58:44,084 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-12-06 15:58:47,873 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2021-12-06 15:58:47,874 : INFO : Updating model with new vocabulary\n",
      "2021-12-06 15:58:48,193 : INFO : Word2Vec lifecycle event {'msg': 'added 69596 new unique words (27.41575866442916%% of original 253854) and increased the count of 1694 pre-existing words (0.6673127073041984%% of original 253854)', 'datetime': '2021-12-06T15:58:48.193027', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-12-06 15:58:48,475 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2021-12-06 15:58:48,482 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2021-12-06 15:58:48,483 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12506280.016269669 word corpus (74.8%% of prior 16718844)', 'datetime': '2021-12-06T15:58:48.483027', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-12-06 15:58:48,917 : INFO : estimated required memory for 71290 words and 100 dimensions: 92677000 bytes\n",
      "2021-12-06 15:58:48,917 : INFO : updating layer weights\n",
      "2021-12-06 15:58:48,956 : INFO : Word2Vec lifecycle event {'update': True, 'trim_rule': 'None', 'datetime': '2021-12-06T15:58:48.956026', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "2021-12-06 15:58:48,956 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2021-12-06 15:58:48,957 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 71326 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2021-12-06T15:58:48.957029', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2021-12-06 15:58:49,962 : INFO : EPOCH 1 - PROGRESS: at 12.05% examples, 1489953 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:58:50,964 : INFO : EPOCH 1 - PROGRESS: at 24.81% examples, 1542258 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:58:51,964 : INFO : EPOCH 1 - PROGRESS: at 37.39% examples, 1558466 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:58:52,968 : INFO : EPOCH 1 - PROGRESS: at 50.26% examples, 1571712 words/s, in_qsize 4, out_qsize 0\n",
      "2021-12-06 15:58:53,971 : INFO : EPOCH 1 - PROGRESS: at 62.55% examples, 1564914 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:58:54,972 : INFO : EPOCH 1 - PROGRESS: at 75.25% examples, 1568365 words/s, in_qsize 6, out_qsize 0\n",
      "2021-12-06 15:58:55,975 : INFO : EPOCH 1 - PROGRESS: at 87.95% examples, 1568484 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:58:56,907 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-06 15:58:56,907 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:58:56,910 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:58:56,910 : INFO : EPOCH - 1 : training on 17005207 raw words (12506095 effective words) took 8.0s, 1572586 effective words/s\n",
      "2021-12-06 15:58:57,915 : INFO : EPOCH 2 - PROGRESS: at 12.64% examples, 1566756 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:58:58,915 : INFO : EPOCH 2 - PROGRESS: at 25.51% examples, 1590089 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:58:59,921 : INFO : EPOCH 2 - PROGRESS: at 38.45% examples, 1601680 words/s, in_qsize 4, out_qsize 0\n",
      "2021-12-06 15:59:00,923 : INFO : EPOCH 2 - PROGRESS: at 51.03% examples, 1595224 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:01,925 : INFO : EPOCH 2 - PROGRESS: at 63.90% examples, 1599014 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:02,927 : INFO : EPOCH 2 - PROGRESS: at 76.95% examples, 1602134 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:03,927 : INFO : EPOCH 2 - PROGRESS: at 89.83% examples, 1602800 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:04,707 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-06 15:59:04,708 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:59:04,712 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:59:04,712 : INFO : EPOCH - 2 : training on 17005207 raw words (12505451 effective words) took 7.8s, 1603211 effective words/s\n",
      "2021-12-06 15:59:05,718 : INFO : EPOCH 3 - PROGRESS: at 12.82% examples, 1587384 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:06,722 : INFO : EPOCH 3 - PROGRESS: at 25.69% examples, 1597822 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:07,723 : INFO : EPOCH 3 - PROGRESS: at 38.33% examples, 1596189 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:08,725 : INFO : EPOCH 3 - PROGRESS: at 51.03% examples, 1595072 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:09,729 : INFO : EPOCH 3 - PROGRESS: at 63.90% examples, 1598498 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:10,731 : INFO : EPOCH 3 - PROGRESS: at 76.84% examples, 1599321 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:11,734 : INFO : EPOCH 3 - PROGRESS: at 89.77% examples, 1600783 words/s, in_qsize 4, out_qsize 0\n",
      "2021-12-06 15:59:12,525 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-06 15:59:12,526 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:59:12,530 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:59:12,530 : INFO : EPOCH - 3 : training on 17005207 raw words (12506163 effective words) took 7.8s, 1600020 effective words/s\n",
      "2021-12-06 15:59:13,534 : INFO : EPOCH 4 - PROGRESS: at 12.87% examples, 1597724 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:14,536 : INFO : EPOCH 4 - PROGRESS: at 25.81% examples, 1608014 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:15,538 : INFO : EPOCH 4 - PROGRESS: at 38.68% examples, 1613494 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:16,539 : INFO : EPOCH 4 - PROGRESS: at 51.50% examples, 1611848 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:17,541 : INFO : EPOCH 4 - PROGRESS: at 64.32% examples, 1610600 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:18,550 : INFO : EPOCH 4 - PROGRESS: at 76.60% examples, 1594602 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:19,554 : INFO : EPOCH 4 - PROGRESS: at 88.30% examples, 1574240 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:20,555 : INFO : EPOCH 4 - PROGRESS: at 99.88% examples, 1557621 words/s, in_qsize 2, out_qsize 1\n",
      "2021-12-06 15:59:20,556 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-06 15:59:20,556 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:59:20,559 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:59:20,560 : INFO : EPOCH - 4 : training on 17005207 raw words (12507924 effective words) took 8.0s, 1558068 effective words/s\n",
      "2021-12-06 15:59:21,564 : INFO : EPOCH 5 - PROGRESS: at 11.29% examples, 1395857 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:22,564 : INFO : EPOCH 5 - PROGRESS: at 23.16% examples, 1441623 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:23,565 : INFO : EPOCH 5 - PROGRESS: at 34.86% examples, 1454085 words/s, in_qsize 4, out_qsize 0\n",
      "2021-12-06 15:59:24,566 : INFO : EPOCH 5 - PROGRESS: at 46.68% examples, 1461498 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:25,568 : INFO : EPOCH 5 - PROGRESS: at 58.32% examples, 1460857 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:26,568 : INFO : EPOCH 5 - PROGRESS: at 69.55% examples, 1452007 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:27,570 : INFO : EPOCH 5 - PROGRESS: at 80.78% examples, 1442638 words/s, in_qsize 4, out_qsize 0\n",
      "2021-12-06 15:59:28,574 : INFO : EPOCH 5 - PROGRESS: at 91.95% examples, 1435832 words/s, in_qsize 5, out_qsize 0\n",
      "2021-12-06 15:59:29,256 : INFO : worker thread finished; awaiting finish of 2 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 15:59:29,257 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:59:29,260 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:59:29,260 : INFO : EPOCH - 5 : training on 17005207 raw words (12504961 effective words) took 8.7s, 1437619 effective words/s\n",
      "2021-12-06 15:59:29,261 : INFO : Word2Vec lifecycle event {'msg': 'training on 85026035 raw words (62530594 effective words) took 40.3s, 1551487 effective words/s', 'datetime': '2021-12-06T15:59:29.261131', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2021-12-06 15:59:29,261 : INFO : collecting all words and their counts\n",
      "2021-12-06 15:59:29,264 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-12-06 15:59:29,310 : INFO : collected 12250 word types from a corpus of 216470 raw words and 160 sentences\n",
      "2021-12-06 15:59:29,311 : INFO : Updating model with new vocabulary\n",
      "2021-12-06 15:59:29,516 : INFO : Word2Vec lifecycle event {'msg': 'added 458 new unique words (3.7387755102040816%% of original 12250) and increased the count of 3677 pre-existing words (30.016326530612243%% of original 12250)', 'datetime': '2021-12-06T15:59:29.516174', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-12-06 15:59:29,538 : INFO : deleting the raw counts dictionary of 12250 items\n",
      "2021-12-06 15:59:29,539 : INFO : sample=0.001 downsamples 56 most-common words\n",
      "2021-12-06 15:59:29,539 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 180867.80429278902 word corpus (89.1%% of prior 202962)', 'datetime': '2021-12-06T15:59:29.539723', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-12-06 15:59:29,990 : INFO : estimated required memory for 4135 words and 100 dimensions: 5375500 bytes\n",
      "2021-12-06 15:59:29,991 : INFO : updating layer weights\n",
      "2021-12-06 15:59:30,028 : INFO : Word2Vec lifecycle event {'update': True, 'trim_rule': 'None', 'datetime': '2021-12-06T15:59:30.028747', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "2021-12-06 15:59:30,029 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2021-12-06 15:59:30,030 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 71784 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2021-12-06T15:59:30.030749', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2021-12-06 15:59:30,154 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-06 15:59:30,157 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:59:30,171 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:59:30,171 : INFO : EPOCH - 1 : training on 216470 raw words (176059 effective words) took 0.1s, 1254829 effective words/s\n",
      "2021-12-06 15:59:30,292 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-06 15:59:30,293 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:59:30,306 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:59:30,307 : INFO : EPOCH - 2 : training on 216470 raw words (176175 effective words) took 0.1s, 1319183 effective words/s\n",
      "2021-12-06 15:59:30,421 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-06 15:59:30,423 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:59:30,434 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:59:30,434 : INFO : EPOCH - 3 : training on 216470 raw words (176305 effective words) took 0.1s, 1398424 effective words/s\n",
      "2021-12-06 15:59:30,553 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-06 15:59:30,555 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:59:30,566 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:59:30,566 : INFO : EPOCH - 4 : training on 216470 raw words (176090 effective words) took 0.1s, 1346572 effective words/s\n",
      "2021-12-06 15:59:30,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-12-06 15:59:30,684 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-12-06 15:59:30,695 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-12-06 15:59:30,695 : INFO : EPOCH - 5 : training on 216470 raw words (176104 effective words) took 0.1s, 1384320 effective words/s\n",
      "2021-12-06 15:59:30,696 : INFO : Word2Vec lifecycle event {'msg': 'training on 1082350 raw words (880733 effective words) took 0.7s, 1323107 effective words/s', 'datetime': '2021-12-06T15:59:30.696749', 'gensim': '4.1.3.dev0', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(880733, 1082350)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text8 = gensim.models.word2vec.Text8Corpus(\"text8\")\n",
    "model.build_vocab(text8, update=True)\n",
    "model.train(text8, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "more_sentences = moreCorpus()\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aae9d97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clustering', 0.7773785591125488),\n",
       " ('machine_learning', 0.7473621368408203),\n",
       " ('retrieval', 0.7466416358947754),\n",
       " ('designing', 0.7424742579460144),\n",
       " ('utility', 0.7303199172019958),\n",
       " ('relevance_judgments', 0.7278493046760559),\n",
       " ('crm', 0.7263140678405762),\n",
       " ('inverted_index', 0.714715301990509),\n",
       " ('routing', 0.7104891538619995),\n",
       " ('debugging', 0.7065703272819519)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('search_engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e40e7529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34124392"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"test\", \"analyze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "600b178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 15:59:35,631 : INFO : Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2021-12-06 15:59:35,632 : INFO : NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10a9bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitx = []\n",
    "hity = []\n",
    "pos = 0\n",
    "neg = 0\n",
    "                \n",
    "with open(\"miss.dat\") as f:\n",
    "    for line in f:\n",
    "        for word in utils.simple_preprocess(line):\n",
    "            try:\n",
    "                hitx.append(model.wv[word])\n",
    "                hity.append(0.0)\n",
    "                neg += 1\n",
    "            except:\n",
    "                continue\n",
    "with open(\"hits.dat\") as f:\n",
    "    for line in f:\n",
    "        for word in utils.simple_preprocess(line):\n",
    "            try:\n",
    "                vec = model.wv[word]\n",
    "                hitx.append(vec)\n",
    "                hity.append(1.0)\n",
    "                pos += 1\n",
    "            except:\n",
    "                continue\n",
    "hitx = np.array(hitx)\n",
    "hity = np.array(hity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8139ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_bias = np.log([pos/neg])\n",
    "output_bias = tf.keras.initializers.Constant(initial_bias)\n",
    "nnet = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='tanh'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid',bias_initializer= output_bias)])\n",
    "nnet.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c6a3b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "13009/13009 [==============================] - 6s 481us/step - loss: 0.4327 - binary_accuracy: 0.8038\n",
      "Epoch 2/3\n",
      "13009/13009 [==============================] - 6s 477us/step - loss: 0.4101 - binary_accuracy: 0.8175\n",
      "Epoch 3/3\n",
      "13009/13009 [==============================] - 6s 476us/step - loss: 0.4031 - binary_accuracy: 0.8213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d279674a00>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.fit(\n",
    "  x=hitx,\n",
    "  y=hity,\n",
    "  shuffle=True,\n",
    "  epochs=3,\n",
    "  batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58c116cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79793197]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.predict(np.array([model.wv['dcg']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ccbb144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastGetter:\n",
    "    def __init__(self, m, wv):\n",
    "        self.model = m\n",
    "        self.wv = wv\n",
    "        self.d = {}\n",
    "    def get(self, x):\n",
    "        r = self.d.get(x, 100)\n",
    "        if (r != 100):\n",
    "            return r\n",
    "        else:\n",
    "            r = self.model.predict(np.array([model.wv[x.strip()]]))\n",
    "            self.d[x] = r\n",
    "            return r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bf3b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = FastGetter(nnet, model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13ffc9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' reliably', ' Consider the sentence', ' Semantic matching of terms', ' Consider the word level ambiguity', ' And these words can help matching documents where the original query words have not occurred', ' In terms of semantic analysis', ' Pragmatic analysis', ' The same word can have different syntactic categories', ' This technical code would allow us to add additional words to the query and those additional words could be related to the query words', ' Such relations can be extracted by using the current natural language processing techniques', ' For example, we have techniques that can allow us to extract the entities and relations mentioned in text articles', 'This lecture is about natural language content analysis', \" Finally, we're going to cover the relation between natural language processing and text retrieval\", ' We also keep a lot of ambiguities because we assume the receiver or the hearer could know how to disambiguate ambiguous word based on the knowledge or the context', ' The two users of off may have different syntactic categories', ' For example, design can be a noun or a verb', ' First, what is natural language processing? Which is the main technique for processing natural language to obtain understanding? The second is the state of the art in NLP, which stands for natural language processing', \" 'cause knowledge graph would contain entities and their relations, and this goes beyond the simple bag of words representation and such technique should help us improve the search engine utility significantly\", ' So that means the NLP techniques must be general, robust, and efficient, and that just implies today we can only use fairly shallow and NLP techniques for text retrieval', ' So as a result, we have robust and general natural language processing techniques that can process a lot of text data', ' The main topic of this lecture, natural language processing can actually be interpreted in two ways in terms of the structure', ' If you see matching of some of the query words in a text document, chances are that that document is about the topic, although there are exceptions', \" And, if you don't restrict the text domain or the use of words, then these techniques tend not to work well\", ' Because we have a lot of background and knowledge to help us disambiguate the ambiguity', \" Such a representation is often sufficient, and that's also the representation that the major search engines today, like a Google or Bing or using\", ' There are also syntactical ambiguities, for example', \" There was another reason why we have not used the sophisticated NLP techniques in modern search engines, and that's because some retrieval techniques actually naturally solve the problem of NLP\", \" So that's the topic of this lecture\", ' So these are examples of semantic analysis and they help us to obtain partial understanding of the sentences', ' Another common example of an ambiguous sentence is the following', ' An in some evaluation results we have seen above 90% accuracy in terms of partial parsing of sentences', \" There's no need to invent the different words for different meanings\", ' So this is some extra knowledge that you would infer based on understanding of the text', ' Although this is still an open topic for research and exploration', ' So this is called a bag of words representation', ' This is a general challenging in artificial intelligence', ' In fact, most search engines today use something called a bag of words representation', ' That means we can get noun phrase structures or verbal phrases structures, or some segment of the sentence understood correctly in terms of the structure', \" topic itself, but because of its relevance to the topic we talk about, it's useful for you to know the background\", ' And we need to figure out the syntactic categories of those words', ' Think about the social media data, the accuracy likely is lower', \" However, in the long run we still need deeper natural language processing techniques in order to improve the accuracy of the current search engines, and it's particularly needed for complex search tasks\", ' If you look at the world alone, it would be ambiguous, but when the user uses the word in the query, usually there are other words', \" And we'll keep duplicated occurrences of words\", ' Ambiguity, or PP attachment ambiguity', \" So what does that mean for text retrieval? In text retrieval, we're dealing with all kinds of text\", ' In terms of inference, we are not there yet, partly because of the general difficulty of inference and uncertainties', ' So here we show we have noun phrases as intermediate components and then verbal phrases', ' As you see from this picture, this is really the first step to process any text data, text data in natural languages', ' On the other hand, the deeper understanding techniques tend not to scale up well, meaning that they would fail on some unrestricted text', ' This is especially useful for review analysis, for example', \" That's because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence\", ' Computers unfortunately, are hard to obtain such understanding', ' We can also do sentiment analysis, meaning to figure out the weather sentence is positive or negative', ' The data that are very different from the training data, so this pretty much summarizes the state of the art of natural language processing', ' Then you would never match applet or with very small probability, right? So this is the case when some retrieval techniques naturally achieve the goal of word sense disambiguation', ' But we have some techniques that would allow us to do partial understanding of the sentence', ' It also would have to maintain a large knowledge knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world', \" In summary in this lecture we talked about what is an NLP and We've talked about the state of the art techniques, what we can do, what we cannot do, and finally, we also explain the why bag of words representation remains the dominant representation used in modern search engines, even though deeper NLP would be needed for future search engines\", \" It's very hard to restrict the text to a certain domain\", ' That is to turn text data into simply a bag of words, meaning we will keep individual words, but will ignore all the orders of words', ' Finally we have a sentence', \" It's also hard to do precise deep semantic analysis, so here's example in the sentence\", \" And that context can help us naturally prefer documents where Java is referring to program language 'cause those documents would probably match applet as well if Java occurs in the document in a way that it means coffee\", ' This ambiguity can be very hard to disambiguate, and you can imagine example where you have to use a lot of knowledge in the context of the sentence or from the background in order to figure out who actually had the telescope', ' Of course there are many queries that are not answered well by the current search engines and they do require a representation that would go beyond the bag of words representation that would require more natural language processing to be done', \" For example, parts of speech tagging or partial parsing or recognizing sentiment, and those are not deep understanding 'cause we're not really understanding the exact meaning of a sentence\", ' Alright, so this is an example of syntactic ambiguity where we have different structures that can be applied to the same sequence of words', ' Or for question answering', ' We could overload the same word with different meanings without the problem', ' So this is called entity extraction', ' And to get this structure we need to do something called a syntactic analysis or parsing, and we may have a parser', ' In a shallow way, meaning we only do superficial analysis', ' But this structure shows what we might get if we look at the sentence and try to interpret the sentence', ' But yet this representation tends to actually work pretty well for most search tasks, and this is partly because the search task is not all that difficult', ' So this achieves to some extent', ' We can also do word sense disambiguation to some extent', ' So that makes natural language processing difficult for computers', ' So those techniques also helped us bypass some of the difficulties in natural language processing', ' And common sense reasoning is often required', \" That's partly also because we don't have complete semantic representation for natural language text, so this is hard yet in some domains, perhaps in limited domains, when you have a lot of restrictions on the word uses, you maybe do may be able to perform inference\", ' For example, recognizing the mentions of people, locations, organisations, etc in text', ' We are far from being able to do a complete understanding of a sentence', \" They may work well based on machine learning techniques on the data that are similar to the training data that the program has been trained on, but generally wouldn't work well\", \" When you represent the text in this way, you ignore a lot of other information and that just makes it harder to understand the exact meaning of a sentence, because we've lost the order\", ' Because of these reasons, this makes every step in natural language processing difficult', ' But the fundamental reason why a natural language processing is difficult for computers is simply because natural language has not been designed for computers', ' So computers have to understand natural language to some extent in order to make use of the data', \" Of course, within such a short amount of time, we can't really give you a complete view of NLP, which is big field an either expect to see multiple courses on natural language processing\", ' We can figure out whether this word in this sentence would have certain meaning in another context', ' And we also often dealing with a lot of text data', ' We usually think of this as processing of natural language', ' So imagine a computer wants to understand all these subtle differences and meanings', ' Now from this representation we could also further infer some other things, and we might indeed naturally think of something else when we read the text, and this is called inference', ' So in comparison, some other tasks, for example machine translation, would require you to understand the language accurately, otherwise the translation would be wrong', ' To some extent, but in general we cannot really do that', ' Finally, presupposition is another problem', ' Now, this is probably the simplest representation you can possibly think of', ' Most of the existing work has been evaluated using news data set and so a lot of these numbers are more or less biased toward news data', \" It's also hard to do general, complete parsing, and again this same sentence that you saw before is example\", ' For computers, ambiguity is the main difficulty', ' First what is NLP? Well the best way to explain it is to think about if you see a text in a foreign language that you can understand', ' The numbers might be lower', ' So this is called lexical analysis or part of speech tagging', ' But for a computer would have to use symbols to denote that, right? So we would use a symbol D1 that denote a dog and B1 to denote a boy and then P1 to denote the playground, playground', \" We don't have any problem with understanding this sentence\", ' In terms of parsing, we can do partial parsing pretty well', ' So this is how computer would obtain some understanding of this sentence', \" It's not giving us a complete understanding as I showed it before for this sentence, but it would still help us gain understanding of the content, and these can be useful\", ' In order to understand the speech actor of a sentence', ' Speech Act analysis is also far from being done, and we can only do that analysis for various special cases', ' But imagine what the computer would have to do in order to understand it, or in general it would have to do the following', \" Now we generally don't have a problem with these ambiguities\", ' The word root may have multiple meanings, so square root in math sense, or the root of a plant', ' So this slide sort of gives simplified view of state of the art technologies', ' So in comparison, search task is all relatively easy', ' The question here is does himself refer to John or Bill? So again, this is something that you have to use some background or the context to figure out', ' As a result, we omit a lot of common sense knowledge because we assume everyone knows about that', \" Now at this point you would know the structure of this sentence, but still you don't know the meaning of the sentence, so we have to go further to semantic analysis\", ' Another example is', \" That's also hard\", ' Some technical code feedback which we will talk about later in some of the lectures', ' Imagine it has four or five prepositional phrases, and there are even more possibilities to figure out', ' When I have applet there that implies', ' So for example, here it shows that A and a dog would go together to form a noun phrase', ' So although the sentence looks very simple, it actually is pretty hard, and in cases when the sentence is very long', \" How do we define owns exactly the word own is something that we understand, but it's very hard to precisely describe the meaning of own for computers\", ' It would have to use a lot of knowledge to figure that out', ' Some entities are harder than others', \" If you try to make a fine grained distinguishing, it's not that easy to figure out such differences\", ' In our mind, we usually can map such a sentence to what we already know in our knowledge base', ' They natural languages are designed for us to communicate', ' Like we say something to basically achieve some goal', ' We may be able to recognize the relations, for example this person visited that place or this person met that person, or this company acquired another company', ' So as a result, we are still not perfect, in fact, far from perfect in understanding natural language using computers', ' You can even go further to understand why the person said this sentence, so this has reduced the use of language', ' Again, I have to say these numbers are relative to the data set in some other data sets', ' First we have to know dogs are a noun chasing is a verb etc', ' For example program languages', ' The computer could figure out it has a different meaning', ' Now this looks like a simple task, but think about the example here', \" They don't have such a knowledge base, they are still incapable of doing reasoning under uncertainties\", \" For example, I'm looking for usage of Java applet\", ' You might be able to think of other meanings', ' But you could also think of this as you say, language processes is natural', \" After that, we're going to figure out the structure of the sentence\", \" There's some purpose there, and this has to do with the use of language\", ' In this case, the person who said this sentence might be reminding another person to bring back the dog', ' So one example is word sense disambiguation', ' That could be one possible intent to reach this level of understanding would require all these steps', \" I only cited one here and that's a good starting point\", ' Now what do you have to do in order to understand that text? This is basically what computers are facing, right? So looking at the simple sentence like a dog is chasing a boy on the playground', ' So this roughly gives you some idea about the state of the art', \" Now this number is obviously based on a certain data set, so don't take this literally\", ' Now in this case, the question is who had the telescope? Right, this is called a prepositional phrase attachment', ' If you want to know more, you can take a look at some additional readings', ' And there is a reason for that', ' This is called', ' And a Computer would have to go through all these steps in order to completely understand this sentence', \" And for example, you might imagine a dog that looks like that there's a boy and there's some activity here\", ' Some words would go together 1st and then they will go together with other words', \" In case you haven't been exposed to that\", ' Google has recently launched Knowledge Graph and this is one step toward that goal', ' It could mean coffee, or could mean program language', ' Yet we humans have no trouble with understanding that, we instantly will get everything', ' Java Means program language', ' Those are harder for us, so natural languages is designed to make our communication efficient', ' So for example, if you believe that if someone is being chased and this person might be scared with this rule, you can see computers could also infer that this boy may be scared', ' Think for a moment to see if you can figure that out', ' This obviously implies that he smoked before', \" So that's the first step\", \" Now there is also chasing activity that's happening here, so we have a relation chasing here that connects all these symbols\", \" And we won't have dog and is to go first, and there are some structures that are not just right\", ' We can do part of speech tagging pretty well, so I showed 97% accuracy here', ' Another example of difficulties is anaphora resolution, so think about the sentence like John persuaded Bill to buy a TV for himself', ' A computer program that would automatically create this structure', ' Of course I put in parentheses is here, but not all', \" They're not perfect, but they can do well for some entities\", ' There are other languages designed for computers', \" So This is why it's very difficult\", \" We're going to cover three things\", \" It just shows that we can do it pretty well, but it's still not perfect\", \" And then we also talk a little bit about what we can't do\", ' Think about the world like Java', \" Again, it's not perfect, but you can do something in that direction\", ' A man saw a boy with a telescope', ' So let me give you some examples of challenges here', ' Thanks', \" And so we can't even do one hundred percent part of speech tagging\", ' So I could mention some of them', ' John owns a restaurant', ' He has quit smoking']\n"
     ]
    }
   ],
   "source": [
    "tol = 0.7\n",
    "\n",
    "\n",
    "with open(\"textretrieval.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        li = []\n",
    "        for sentence in line.split(\".\"):\n",
    "            accu = 0.0\n",
    "            cnt = 0\n",
    "            for word in utils.simple_preprocess(sentence):\n",
    "                try:\n",
    "                    accu += fg.get(word)\n",
    "                    cnt+=1\n",
    "                except:\n",
    "                    continue\n",
    "            try:\n",
    "                accu /= cnt\n",
    "                li.append((accu[0], sentence))\n",
    "#                     if (accu > tol):\n",
    "#                         print(sentence)\n",
    "#                     else:\n",
    "#                         print(\"passed\")\n",
    "            except:\n",
    "                continue\n",
    "        li.sort(reverse=True)\n",
    "        li = [i for _, i in li]\n",
    "        print(li)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56c45116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = [\"1\", \"2\",\"3\", \"4\"]\n",
    "\".\".join(li[:round(len(li)/2)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
