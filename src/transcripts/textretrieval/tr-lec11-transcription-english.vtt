WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-27T00:07:10.2685102Z by ClassTranscribe

00:00:00.280 --> 00:00:03.010
This lecture is about the inverted

00:00:03.010 --> 00:00:04.220
index construction.

00:00:13.730 --> 00:00:15.830
In this lecture we will continue the

00:00:15.830 --> 00:00:17.770
discussion of system implementation.

00:00:17.770 --> 00:00:20.050
In particular, we're going to discuss

00:00:20.050 --> 00:00:22.090
how to construct the inverted index.

00:00:25.070 --> 00:00:27.105
The construction of the inverted index

00:00:27.105 --> 00:00:29.256
is actually very easy if the data set

00:00:29.256 --> 00:00:31.900
is very small, it's very easy to

00:00:31.900 --> 00:00:34.330
construct dictionary and then store the

00:00:34.330 --> 00:00:35.730
postings in a file.

00:00:36.410 --> 00:00:39.770
The problem is that when our data is

00:00:39.770 --> 00:00:43.040
not able to fit to the memory then we

00:00:43.040 --> 00:00:44.690
have to use some special methods to

00:00:44.690 --> 00:00:45.450
deal with it.

00:00:46.080 --> 00:00:49.210
Unfortunately, in most retrieval

00:00:49.210 --> 00:00:51.050
applications, that data set would be

00:00:51.050 --> 00:00:53.360
large and they generally cannot be

00:00:53.360 --> 00:00:55.430
loaded into memory at once.

00:00:56.660 --> 00:00:58.570
There are many approaches to solve

00:00:58.570 --> 00:01:03.110
the problem and sorting based method is

00:01:03.110 --> 00:01:03.840
quite common.

00:01:03.840 --> 00:01:06.720
It works in four steps as shown here.

00:01:06.720 --> 00:01:09.220
First, you collect the local term ID,

00:01:09.220 --> 00:01:11.380
document ID, and frequency tuples.

00:01:11.380 --> 00:01:13.190
Basically you will not count the terms

00:01:13.190 --> 00:01:15.750
in a small set of documents.

00:01:16.390 --> 00:01:18.850
And then once you collect those

00:01:18.850 --> 00:01:23.660
counts, you can sort those counts based on

00:01:23.660 --> 00:01:26.750
terms so that you build a local

00:01:26.750 --> 00:01:29.400
partial inverted index, and these are

00:01:29.400 --> 00:01:32.850
called rounds. And then you write them

00:01:32.850 --> 00:01:36.725
into a temporary file on the disk, and

00:01:36.725 --> 00:01:39.160
then you merge in step three would do

00:01:39.160 --> 00:01:41.880
pair-wise merging of these runs until

00:01:41.880 --> 00:01:44.070
you eventually merge all the runs

00:01:44.070 --> 00:01:46.530
to generate a single inverted index.

00:01:47.430 --> 00:01:49.910
So this is an illustration of this

00:01:49.910 --> 00:01:50.470
method.

00:01:50.470 --> 00:01:52.630
On the left, you see some documents.

00:01:53.540 --> 00:01:55.970
And on the right, we have shown

00:01:55.970 --> 00:01:57.020
Term Lexicon

00:01:57.890 --> 00:01:59.840
and DocumentID Lexicon.

00:01:59.840 --> 00:02:01.660
These lexicons are to

00:02:02.360 --> 00:02:05.150
map stream-based representations of

00:02:05.150 --> 00:02:08.110
document IDs or terms into integer

00:02:08.110 --> 00:02:10.900
representations or to

00:02:12.550 --> 00:02:16.110
map back from integers to the stream

00:02:16.110 --> 00:02:16.740
representation.

00:02:17.650 --> 00:02:19.320
The reason why we are

00:02:19.320 --> 00:02:21.510
interested in using integers to represent

00:02:21.510 --> 00:02:26.080
these IDs is because integers are often

00:02:26.080 --> 00:02:26.910
easier to handle.

00:02:26.910 --> 00:02:28.980
For example, integers can be used as

00:02:28.980 --> 00:02:31.670
index for array. They are also

00:02:31.670 --> 00:02:35.380
easy to compress. So this is

00:02:36.940 --> 00:02:39.200
one reason why we tend to map these

00:02:39.200 --> 00:02:40.660
strings into integers.

00:02:42.060 --> 00:02:44.160
So that well, so that we don't have to

00:02:44.160 --> 00:02:46.070
carry these strings around.

00:02:46.620 --> 00:02:48.030
So how does this approach work?

00:02:48.030 --> 00:02:49.380
Well, it's very simple.

00:02:49.380 --> 00:02:51.490
We're going to scan these documents

00:02:51.490 --> 00:02:54.620
sequentially, and then parse the

00:02:54.620 --> 00:02:55.230
document,

00:02:55.230 --> 00:02:57.410
and count the frequencies of terms.

00:02:58.170 --> 00:03:02.235
In this stage, we generally sort

00:03:02.235 --> 00:03:04.149
the frequencies by document IDs

00:03:04.150 --> 00:03:06.330
because we process each document

00:03:06.330 --> 00:03:07.070
sequentially.

00:03:07.070 --> 00:03:09.475
So we first encounter all the terms in

00:03:09.475 --> 00:03:11.200
the first document.

00:03:11.200 --> 00:03:15.473
Therefore, the document IDs are all 1s in

00:03:15.473 --> 00:03:16.440
this case.

00:03:17.070 --> 00:03:21.740
And this will be followed by

00:03:21.740 --> 00:03:23.970
document IDs 2s.

00:03:24.560 --> 00:03:27.640
They are naturally sorted in

00:03:27.640 --> 00:03:28.070
this order

00:03:28.070 --> 00:03:29.980
just because we process the data in the

00:03:29.980 --> 00:03:32.690
sequential order at some point we will

00:03:32.690 --> 00:03:34.930
run out of memory and then we would

00:03:34.930 --> 00:03:38.590
have to write them into the disk.

00:03:38.590 --> 00:03:41.690
But before we do that, we can sort

00:03:41.690 --> 00:03:45.540
them just use whatever memory we have.

00:03:45.540 --> 00:03:48.619
We can sort them and then

00:03:49.230 --> 00:03:51.000
this time we're going to sort based on

00:03:51.000 --> 00:03:51.980
term IDs.

00:03:51.980 --> 00:03:56.350
Notice that here we're using the

00:03:56.350 --> 00:03:57.240
term IDs 

00:03:57.960 --> 00:04:01.260
as a key to sort. So all the entries

00:04:01.260 --> 00:04:02.840
that share the same term would be

00:04:02.840 --> 00:04:03.870
grouped together.

00:04:03.870 --> 00:04:05.580
In this case, we can see 

00:04:07.590 --> 00:04:11.660
IDs of documents that match the term

00:04:11.660 --> 00:04:13.380
one will be grouped together.

00:04:14.000 --> 00:04:16.567
And we're going to write this into the

00:04:16.567 --> 00:04:19.320
disk as a temporary file, and that

00:04:19.320 --> 00:04:21.990
would allow us to use the memory to

00:04:21.990 --> 00:04:22.385
process

00:04:22.385 --> 00:04:23.788
the next batch of documents.

00:04:23.788 --> 00:04:26.393
And we're going to do that for all the

00:04:26.393 --> 00:04:26.719
documents.

00:04:26.720 --> 00:04:28.435
So we are going to write a lot of

00:04:28.435 --> 00:04:31.249
temporary files into the disk.

00:04:32.620 --> 00:04:34.480
And then the next stage is merge

00:04:34.480 --> 00:04:36.535
sort. We are going to merge

00:04:36.535 --> 00:04:38.240
them and sort them.

00:04:38.240 --> 00:04:40.330
Eventually we will get a single

00:04:40.330 --> 00:04:43.550
inverted index with their entries

00:04:43.550 --> 00:04:45.400
are sorted based on term IDs.

00:04:46.850 --> 00:04:48.790
And on the top we can see these are the

00:04:48.790 --> 00:04:51.730
old entries for the documents that

00:04:51.730 --> 00:04:53.560
match term ID1.

00:04:53.560 --> 00:04:55.900
So this is basically how we can do

00:04:57.400 --> 00:05:00.490
the construction of inverted index even

00:05:00.490 --> 00:05:03.070
though the data cannot be

00:05:03.780 --> 00:05:05.480
 loaded into the memory.

00:05:06.550 --> 00:05:11.550
Now we mentioned earlier that because

00:05:11.550 --> 00:05:14.190
of postings are very large, it's

00:05:14.190 --> 00:05:15.640
desirable to compress them.

00:05:15.640 --> 00:05:17.920
So let's now talk a little bit about

00:05:17.920 --> 00:05:20.120
how we compress inverted index.

00:05:21.010 --> 00:05:24.370
The idea of compression in general is

00:05:24.370 --> 00:05:27.650
to leverage skewed distributions of values,

00:05:27.650 --> 00:05:29.860
and we generally have to use variable

00:05:29.860 --> 00:05:32.200
length encoding instead of the fixed

00:05:32.200 --> 00:05:36.165
length encoding as we use by default

00:05:36.165 --> 00:05:38.870
in program languages like C++.

00:05:40.000 --> 00:05:42.630
So how can we leverage the skewed

00:05:42.630 --> 00:05:45.720
distributions of values to compress

00:05:45.720 --> 00:05:46.590
these values?

00:05:48.080 --> 00:05:52.575
In general, we would use fewer bits to

00:05:52.575 --> 00:05:54.640
encode those frequently awards at the

00:05:54.640 --> 00:05:57.980
cost of using longer bits to encode

00:05:57.980 --> 00:06:00.230
those rare values.

00:06:00.230 --> 00:06:02.495
So in our case, let's think about how

00:06:02.495 --> 00:06:04.760
we can compress the TF (term frequency).

00:06:05.540 --> 00:06:07.580
Now if you can picture what the

00:06:07.580 --> 00:06:09.900
inverted index would look like, and you

00:06:09.900 --> 00:06:12.620
see in postings, there are a lot of

00:06:12.620 --> 00:06:13.515
term frequencies.

00:06:13.515 --> 00:06:17.530
Those are the frequencies of terms in

00:06:17.530 --> 00:06:18.600
all those documents.

00:06:19.420 --> 00:06:20.180
Now,

00:06:21.230 --> 00:06:23.590
if you think about what kind of

00:06:23.590 --> 00:06:25.750
values are most frequent there, you

00:06:25.750 --> 00:06:28.100
probably will be able to guess that

00:06:28.100 --> 00:06:30.380
small numbers tend to occur far more

00:06:30.380 --> 00:06:32.460
frequently than large numbers.

00:06:32.460 --> 00:06:33.110
Why?

00:06:33.900 --> 00:06:36.943
Think about the distribution of words

00:06:36.943 --> 00:06:40.270
and this is due to the Zipf's law, and

00:06:40.270 --> 00:06:42.680
many words occur rarely.

00:06:42.680 --> 00:06:44.770
So we see a lot of small numbers.

00:06:44.770 --> 00:06:47.100
Therefore we can use fewer bits for

00:06:47.100 --> 00:06:51.100
the small but highly frequent

00:06:51.100 --> 00:06:51.940
integers

00:06:53.470 --> 00:06:56.140
 at the cost of using more bits for

00:06:56.140 --> 00:06:57.160
large integers.

00:06:58.270 --> 00:06:59.790
This is a trade off, of course.

00:06:59.790 --> 00:07:01.880
If the values are distributed uniformly,

00:07:01.880 --> 00:07:04.960
this one will save us a lot of space.

00:07:05.530 --> 00:07:08.210
But because we tend to see many small

00:07:08.210 --> 00:07:10.450
values, they are very frequent.

00:07:10.450 --> 00:07:14.070
We can save on average, even though

00:07:14.070 --> 00:07:16.550
sometimes when we see a large number we

00:07:16.550 --> 00:07:17.830
have to use a lot of bits.

00:07:19.670 --> 00:07:21.730
What about the document IDs that we

00:07:21.730 --> 00:07:23.160
also saw in postings?

00:07:23.160 --> 00:07:25.970
They are not distributed in the

00:07:25.970 --> 00:07:27.860
skewed way right.

00:07:28.840 --> 00:07:30.050
So how can we deal with that?

00:07:30.050 --> 00:07:31.370
Well, it turns out that you can use a

00:07:31.370 --> 00:07:33.660
trick called d-gap and that is to

00:07:33.660 --> 00:07:35.590
restore the difference of this term

00:07:35.590 --> 00:07:36.190
IDs.

00:07:38.700 --> 00:07:42.040
And we can imagine if a term has

00:07:42.040 --> 00:07:44.773
matched many documents, then there

00:07:44.773 --> 00:07:46.464
will be long list of document IDs.

00:07:46.464 --> 00:07:48.473
So when we take the gap, I'm going to

00:07:48.473 --> 00:07:50.560
take the difference between adjacent

00:07:50.560 --> 00:07:51.949
the document IDs.

00:07:51.950 --> 00:07:55.200
Those gaps will be small, so we will again

00:07:55.200 --> 00:07:57.030
see a lot of small numbers.

00:07:57.640 --> 00:07:59.810
Whereas if a term according only a few

00:07:59.810 --> 00:08:01.836
documents, then the gap would be large.

00:08:01.836 --> 00:08:03.950
The large numbers will not be frequent.

00:08:03.950 --> 00:08:05.880
So this creates some skewed

00:08:05.880 --> 00:08:07.790
distribution that would allow us to 

00:08:09.110 --> 00:08:10.570
compress these values.

00:08:11.590 --> 00:08:14.100
And this is also possible because in

00:08:14.100 --> 00:08:14.900
order to

00:08:15.510 --> 00:08:18.125
uncover or uncompress these document

00:08:18.125 --> 00:08:18.530
IDs.

00:08:18.530 --> 00:08:20.550
We have to sequential process the data.

00:08:21.340 --> 00:08:23.720
Because we store the difference and in

00:08:23.720 --> 00:08:26.245
order to recover the exact the document

00:08:26.245 --> 00:08:28.945
ID, we have to 1st recover the previous

00:08:28.945 --> 00:08:31.010
document IDs and then we can add the

00:08:31.010 --> 00:08:33.170
difference to the previous document ID

00:08:33.170 --> 00:08:36.420
to restore the current document ID.

00:08:36.420 --> 00:08:38.840
Now this was possible because we

00:08:38.840 --> 00:08:41.140
only need to have sequential access to

00:08:41.140 --> 00:08:42.790
those documents IDs.

00:08:42.790 --> 00:08:45.184
Once we look up a term we fetch all the

00:08:45.184 --> 00:08:46.695
document IDs that match the term.

00:08:46.695 --> 00:08:48.620
Then we sequentially process them.

00:08:48.620 --> 00:08:50.060
So it's very natural.

00:08:50.060 --> 00:08:52.130
That's why this trick works.

00:08:53.510 --> 00:08:54.940
And there are many different methods

00:08:54.940 --> 00:08:56.810
for encoding. For example, the binary code

00:08:56.810 --> 00:09:01.140
is a commonly used code in

00:09:01.140 --> 00:09:02.830
programming languages where we use

00:09:02.830 --> 00:09:05.320
basically fixed length encoding.

00:09:05.910 --> 00:09:08.510
Unary code, Gamma code, and Delta code are

00:09:08.510 --> 00:09:10.200
all possibilities and there are many

00:09:10.200 --> 00:09:11.860
other possibilities. So let's look at

00:09:11.860 --> 00:09:13.280
some of them in more detail.

00:09:13.280 --> 00:09:15.800
Binary coding is really equal length

00:09:15.800 --> 00:09:18.530
encoding, and that's our property for

00:09:18.530 --> 00:09:20.260
randomly distributed values.

00:09:20.870 --> 00:09:23.960
The unary coding is a variable length

00:09:23.960 --> 00:09:24.680
encoding method.

00:09:24.680 --> 00:09:28.440
In this case, an integer that's at least

00:09:28.440 --> 00:09:32.153
one would be encoded as x-1, one

00:09:32.153 --> 00:09:33.531
bits followed by zero.

00:09:33.531 --> 00:09:36.966
So for example, three would be encoded

00:09:36.966 --> 00:09:39.680
as two ones, followed by zero, whereas

00:09:39.680 --> 00:09:42.601
five will be encoded as 4 ones,

00:09:42.601 --> 00:09:44.780
followed by zero etc.

00:09:44.780 --> 00:09:48.480
So now you can imagine how many bits do

00:09:48.480 --> 00:09:51.035
we have to use for a large number like

00:09:51.035 --> 00:09:51.530
100.

00:09:52.340 --> 00:09:54.920
So how many bits do we have to use exactly

00:09:54.920 --> 00:09:56.870
the full number like 100?

00:09:57.970 --> 00:09:58.690


00:09:59.320 --> 00:10:02.580
Exactly, we have to use 100 bits, right?

00:10:02.580 --> 00:10:05.100
So it's the same number of bits as the

00:10:05.100 --> 00:10:08.180
value of this number, so this is very

00:10:08.180 --> 00:10:08.960
inefficient.

00:10:08.960 --> 00:10:11.915
If you will likely see some large

00:10:11.915 --> 00:10:13.670
numbers, imagine if you occasionally

00:10:13.670 --> 00:10:14.786
see a number like 1000.

00:10:14.786 --> 00:10:18.190
You have to use 1000 bits, so this only

00:10:18.190 --> 00:10:20.260
works well if you are absolutely sure

00:10:20.260 --> 00:10:22.989
that there will be no large numbers,

00:10:22.990 --> 00:10:26.820
mostly very, very often using very

00:10:26.820 --> 00:10:27.710
small numbers.

00:10:28.440 --> 00:10:30.380
How do you decode this code?

00:10:30.380 --> 00:10:32.190
Since these are variable length

00:10:32.190 --> 00:10:34.820
encoding methods and you can't just

00:10:34.820 --> 00:10:36.570
count how many bits, and then they just

00:10:36.570 --> 00:10:40.965
stop. You can say 8 bits or 32 bits,

00:10:40.965 --> 00:10:44.640
then you will start another code.

00:10:44.640 --> 00:10:48.090
There are variable lengths, so you'll

00:10:48.090 --> 00:10:51.060
have to rely on some mechanism. In this

00:10:51.060 --> 00:10:51.330
case,

00:10:51.330 --> 00:10:53.686
for unary you can see it's very easy to

00:10:53.686 --> 00:10:54.780
see the boundary.

00:10:54.780 --> 00:10:56.940
Now you can easily see zero with

00:10:56.940 --> 00:10:59.000
signal, the end of encoding.

00:10:59.290 --> 00:11:01.165
So you just count how many ones you

00:11:01.165 --> 00:11:03.566
have seen and until you hit zero you

00:11:03.566 --> 00:11:05.300
have finished one number.

00:11:05.300 --> 00:11:06.970
You will start another number.

00:11:07.860 --> 00:11:10.107
Now we just saw that the unary coding

00:11:10.107 --> 00:11:14.190
is too aggressive in rewarding small

00:11:14.190 --> 00:11:14.900
numbers.

00:11:14.900 --> 00:11:17.750
If you occasionally, you can see a very

00:11:17.750 --> 00:11:18.670
big number.

00:11:18.670 --> 00:11:19.910
It would be a disaster.

00:11:19.910 --> 00:11:22.630
So what about some other, less

00:11:22.630 --> 00:11:25.440
aggressive method?

00:11:25.440 --> 00:11:26.970
Well, Gamma coding is one of them.

00:11:26.970 --> 00:11:31.866
And in this method we are going to use unary

00:11:31.866 --> 00:11:35.680
coding for a transformed form of the

00:11:35.680 --> 00:11:39.230
value, so it's one plus the floor of the

00:11:39.280 --> 00:11:40.380
log of X.

00:11:41.090 --> 00:11:44.650
So the magnitude of this value is much

00:11:44.650 --> 00:11:47.090
lower than the original X.

00:11:47.830 --> 00:11:50.990
So that's why we can afford

00:11:50.990 --> 00:11:52.430
using unary code for that.

00:11:52.430 --> 00:11:55.950
So we will first have the Unary

00:11:55.950 --> 00:11:58.746
code for this log of X.

00:11:58.746 --> 00:12:01.169
This will be followed by a uniform code

00:12:01.170 --> 00:12:03.350
or binary code, and this is basically

00:12:03.350 --> 00:12:07.130
the same uniform code and binary code

00:12:07.130 --> 00:12:10.500
are the same, and we're going to use

00:12:10.500 --> 00:12:13.836
this code to code the remaining part of

00:12:13.836 --> 00:12:15.230
the value of X.

00:12:16.020 --> 00:12:19.420
And this is basically precise X - 1

00:12:19.420 --> 00:12:22.030
to the floor of the log of X.

00:12:25.060 --> 00:12:27.240
So the unary code basically coded the

00:12:27.240 --> 00:12:30.940
floor of log of X, add one there.

00:12:31.620 --> 00:12:37.660
But the remaining part will be

00:12:37.660 --> 00:12:40.669
using uniform code to actually code the

00:12:40.670 --> 00:12:43.850
difference between the X and this 

00:12:45.270 --> 00:12:48.090
two to the log of X.

00:12:49.440 --> 00:12:54.480
And it's easy to show that for this

00:12:54.480 --> 00:12:58.180
value this difference, we only need to

00:12:58.180 --> 00:13:03.680
use up to this many bits and floor of

00:13:03.680 --> 00:13:05.420
log of X bits.

00:13:06.430 --> 00:13:08.650
This is easy to understand, if the

00:13:08.650 --> 00:13:10.300
difference is too large then we would

00:13:10.300 --> 00:13:13.100
have a higher floor of log of X.

00:13:14.230 --> 00:13:15.450
So here are some examples.

00:13:15.450 --> 00:13:17.966
For example, 3 is encoded as

00:13:17.966 --> 00:13:18.359
101.

00:13:18.920 --> 00:13:22.350
The first 2 digits are the unary code,

00:13:22.350 --> 00:13:22.690
right.

00:13:22.690 --> 00:13:25.320
So this is for the value 2.

00:13:26.740 --> 00:13:31.100
10 encodes 2 in unary coding.

00:13:32.380 --> 00:13:37.360
And so that means log of X, the floor of

00:13:37.360 --> 00:13:40.760
log of X is 1 because we want actually

00:13:40.760 --> 00:13:43.993
use unary code to encode one plus the

00:13:43.993 --> 00:13:45.509
floor of log of X.

00:13:45.510 --> 00:13:47.912
Since this is 2, then we know that the

00:13:47.912 --> 00:13:50.400
floor of log of X is actually one.

00:13:51.910 --> 00:13:54.670
But three is still larger than two

00:13:54.670 --> 00:13:57.120
to the one, so the difference is 1, and

00:13:57.120 --> 00:14:00.310
then one is encoded here at the end.

00:14:00.310 --> 00:14:03.600
So that's why we have 101 for 3.

00:14:05.850 --> 00:14:09.610
Similarly, 5 is encoded as 110

00:14:09.610 --> 00:14:11.159
followed by 01

00:14:12.850 --> 00:14:15.810
and in this case, the unary code

00:14:15.810 --> 00:14:17.510
encodes 3.

00:14:18.440 --> 00:14:21.180
And so this is the unary code 110.

00:14:22.370 --> 00:14:25.190
And so the floor of log of X is 2.

00:14:26.510 --> 00:14:29.360
And that means we are going to compute the

00:14:29.360 --> 00:14:31.615
difference between 5 and 2 to the

00:14:31.615 --> 00:14:33.990
two, and that's one, and so we now have

00:14:33.990 --> 00:14:35.800
again one at the end.

00:14:35.800 --> 00:14:37.800
But this time we're going to use 2

00:14:37.800 --> 00:14:42.340
bits, cause with this level floor of log

00:14:42.340 --> 00:14:47.200
of X we could have more numbers 5, 6, 7

00:14:47.200 --> 00:14:49.810
they would all share the same prefix

00:14:49.810 --> 00:14:50.550
here,110.

00:14:50.550 --> 00:14:53.100
So in order to differentiate them we

00:14:53.100 --> 00:14:56.274
have to use 2 bits in the end to

00:14:56.274 --> 00:14:56.979
differentiate them.

00:14:57.600 --> 00:15:00.019
So you can imagine six would be 10

00:15:00.020 --> 00:15:03.050
here in the end instead of 01 after

00:15:03.050 --> 00:15:03.810
110.

00:15:04.590 --> 00:15:07.380
It's also true that the form of gamma

00:15:07.380 --> 00:15:08.630
code is always:

00:15:10.080 --> 00:15:12.839
first odd number of bits and in the

00:15:12.840 --> 00:15:15.090
center in there is a zero.

00:15:15.090 --> 00:15:17.460
That's the end of the unary code.

00:15:18.230 --> 00:15:21.422
And before that, on the left side

00:15:21.422 --> 00:15:24.290
of this zero, there will be all ones an

00:15:24.290 --> 00:15:28.010
on the right side of this zero, it's

00:15:28.010 --> 00:15:30.400
binary coding or uniform coding.

00:15:32.450 --> 00:15:36.510
So how can you decode such a code?

00:15:36.510 --> 00:15:39.880
You again first do unary coding right

00:15:39.880 --> 00:15:42.360
Once you hit zero, you have got the

00:15:42.360 --> 00:15:43.220
unary code.

00:15:43.770 --> 00:15:45.450
And this also would tell you how many

00:15:45.450 --> 00:15:47.980
bits you have to read further to decode

00:15:47.980 --> 00:15:49.130
the uniform code.

00:15:50.490 --> 00:15:53.460
So this is how you can decode Gamma code.

00:15:53.460 --> 00:15:55.550
There was also the error code that's

00:15:55.550 --> 00:15:57.540
basically the same as gamma code, except

00:15:57.540 --> 00:16:00.223
that you replace the unary prefix with

00:16:00.223 --> 00:16:02.490
the gamma code, so that's even less

00:16:02.490 --> 00:16:05.410
conservative than gamma code in terms

00:16:05.410 --> 00:16:08.303
of rewarding the small integers.

00:16:08.303 --> 00:16:10.610
So that means it's OK if you

00:16:10.610 --> 00:16:12.290
occasionally see a large number.

00:16:12.290 --> 00:16:15.804
It's OK with delta code.

00:16:15.804 --> 00:16:19.359
It's also fine with gamma code, it's

00:16:19.360 --> 00:16:20.120
really

00:16:20.910 --> 00:16:24.220
a big loss for Unary code and they are all

00:16:24.220 --> 00:16:27.340
operating of course at different

00:16:27.340 --> 00:16:31.110
degrees of favoring small

00:16:31.110 --> 00:16:31.850
integers.

00:16:31.850 --> 00:16:33.480
And that also means.

00:16:35.260 --> 00:16:37.400
They would be appropriate for a certain

00:16:37.400 --> 00:16:39.340
distribution, but none of them is

00:16:39.340 --> 00:16:41.000
perfect for all distributions.

00:16:41.620 --> 00:16:43.980
Which method works the best with

00:16:43.980 --> 00:16:45.360
have to depend on the actual

00:16:45.360 --> 00:16:47.420
distribution in your data set.

00:16:47.420 --> 00:16:50.210
For inverted index compression, people

00:16:50.210 --> 00:16:52.380
have found that gamma coding seems to

00:16:52.380 --> 00:16:53.080
work well.

00:16:55.210 --> 00:16:58.200
So how do I uncompressed invert index

00:16:58.200 --> 00:17:00.250
when we just talked about this first,

00:17:00.250 --> 00:17:03.410
you decode those encode integers and we

00:17:03.410 --> 00:17:05.880
just I think discussed how we decode

00:17:05.880 --> 00:17:08.180
unary coding and gamma coding.

00:17:09.090 --> 00:17:10.360
So I won't repeat.

00:17:10.920 --> 00:17:13.250
And what about the document IDs that

00:17:13.250 --> 00:17:16.910
might be compressed using d-gap. We're

00:17:16.910 --> 00:17:18.310
going to do sequential decoding.

00:17:18.310 --> 00:17:21.590
So suppose the encoded ID list is x1,

00:17:21.590 --> 00:17:23.600
x2, x3, etc.

00:17:23.600 --> 00:17:26.658
We first decode x1 to obtain the

00:17:26.658 --> 00:17:28.399
first document ID ID1.

00:17:28.400 --> 00:17:31.010
Then we would decode x2, which is

00:17:31.010 --> 00:17:32.690
actually the difference between the

00:17:32.690 --> 00:17:34.422
second ID and the first one.

00:17:34.422 --> 00:17:36.820
So we have to add the decoder value of

00:17:36.820 --> 00:17:41.030
x2 to ID1 to recover the value

00:17:41.290 --> 00:17:42.550
of the

00:17:43.140 --> 00:17:46.460
ID at this second position, right.

00:17:46.460 --> 00:17:48.560
So this is where you can see the

00:17:48.560 --> 00:17:51.920
advantage of converting document IDs

00:17:51.920 --> 00:17:53.990
into integers, and that allows us to do

00:17:53.990 --> 00:17:56.070
this kind of compression and we just

00:17:56.070 --> 00:17:58.190
repeat until we decode all the

00:17:58.190 --> 00:18:00.872
documents every time we use the

00:18:00.872 --> 00:18:03.200
document ID in the previous position to

00:18:03.200 --> 00:18:05.490
help you recover the document ID in the

00:18:05.490 --> 00:18:06.250
next position.


