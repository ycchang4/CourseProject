WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-27T00:07:15.3611970Z by ClassTranscribe

00:00:00.280 --> 00:00:02.410
This lecture is about the evaluation of

00:00:02.410 --> 00:00:03.850
text retrieval systems.

00:00:13.960 --> 00:00:16.750
In the previous lectures we have talked

00:00:16.750 --> 00:00:19.350
about a number of text retrieval

00:00:19.350 --> 00:00:21.570
methods, different kinds of ranking

00:00:21.570 --> 00:00:22.190
functions.

00:00:22.970 --> 00:00:25.170
But how do we know which one works the

00:00:25.170 --> 00:00:25.690
best?

00:00:26.920 --> 00:00:28.513
In order to answer this question, we

00:00:28.513 --> 00:00:30.850
have to compare them and that means we

00:00:30.850 --> 00:00:32.770
have to evaluate these retrieval

00:00:32.770 --> 00:00:33.350
methods.

00:00:34.650 --> 00:00:36.650
So this is the main topic of this

00:00:36.650 --> 00:00:37.120
lecture.

00:00:39.960 --> 00:00:41.706
First, let's think about the why do we

00:00:41.706 --> 00:00:42.419
have to do evaluation?

00:00:42.420 --> 00:00:44.507
I already give one reason and that is

00:00:44.507 --> 00:00:46.980
we have to use evaluation to figure out

00:00:46.980 --> 00:00:49.360
which retrieval method works better.

00:00:50.200 --> 00:00:51.940
Now this is very important for

00:00:51.940 --> 00:00:55.020
advancing our knowledge, otherwise we

00:00:55.020 --> 00:00:56.770
wouldn't know whether a new idea works

00:00:56.770 --> 00:00:58.500
better than old idea.

00:01:00.340 --> 00:01:01.920
In the beginning of this course, we

00:01:01.920 --> 00:01:04.110
talked about the problem of text

00:01:04.110 --> 00:01:04.475
retrieval.

00:01:04.475 --> 00:01:07.350
We compared it with database retrieval.

00:01:08.370 --> 00:01:11.720
There we mentioned that text retrieval

00:01:11.720 --> 00:01:13.630
is empirically defined problem.

00:01:14.300 --> 00:01:17.470
So evaluation must rely on users.

00:01:18.110 --> 00:01:20.350
Which system works better would have to

00:01:20.350 --> 00:01:22.290
be judged by our users

00:01:24.880 --> 00:01:27.820
So this becomes a very challenging

00:01:27.820 --> 00:01:28.380
problem.

00:01:28.380 --> 00:01:28.760
Because.

00:01:28.760 --> 00:01:31.880
How can we get users involved in the

00:01:31.880 --> 00:01:32.167
evaluation?

00:01:32.167 --> 00:01:34.270
How can we do a fair comparison of

00:01:34.270 --> 00:01:35.140
different methods?

00:01:37.340 --> 00:01:39.340
So just go back to the reasons for

00:01:39.340 --> 00:01:40.140
evaluation.

00:01:40.910 --> 00:01:42.780
I listed two reasons here.

00:01:42.780 --> 00:01:44.330
The second reason is basically what I

00:01:44.330 --> 00:01:46.960
just said, but there is also another

00:01:46.960 --> 00:01:49.660
reason which is to assess the actual

00:01:49.660 --> 00:01:51.800
utility of text retrieval system.

00:01:51.800 --> 00:01:53.640
Now imagine you're building your own

00:01:53.640 --> 00:01:54.880
search engine applications.

00:01:54.880 --> 00:01:57.810
It would be interested in knowing how well

00:01:57.810 --> 00:02:00.090
your search engine works for your

00:02:00.090 --> 00:02:00.780
users.

00:02:01.350 --> 00:02:04.075
So in this case, matches must reflect

00:02:04.075 --> 00:02:06.180
the utility to the actual users

00:02:06.180 --> 00:02:08.340
in a real application. And typically

00:02:08.340 --> 00:02:10.080
this has to be done by using user

00:02:10.080 --> 00:02:13.660
studies and using the real search

00:02:13.660 --> 00:02:14.030
engine.

00:02:16.180 --> 00:02:18.390
In the second case, or for the second

00:02:18.390 --> 00:02:18.820
reason.

00:02:19.810 --> 00:02:22.160
The measures actually only to be

00:02:22.160 --> 00:02:24.660
correlated with the utility to actual

00:02:24.660 --> 00:02:25.250
users.

00:02:26.030 --> 00:02:27.700
Thus they don't have to accurately

00:02:27.700 --> 00:02:30.210
reflect the exact utility to users.

00:02:31.870 --> 00:02:34.050
So the measure only needs to be good

00:02:34.050 --> 00:02:37.450
enough to tell which method works

00:02:37.450 --> 00:02:37.880
better.

00:02:38.730 --> 00:02:40.580
And this is usually done through a test

00:02:40.580 --> 00:02:44.060
collection, and this is the main idea

00:02:44.060 --> 00:02:46.950
that we'll be talking about in this

00:02:46.950 --> 00:02:47.720
course.

00:02:47.720 --> 00:02:50.930
This has been very important for

00:02:50.930 --> 00:02:54.360
comparing different algorithms and for

00:02:54.360 --> 00:02:56.510
improving search engine system in

00:02:56.510 --> 00:02:57.060
general.

00:02:58.790 --> 00:03:01.820
So next we talk about what to measure

00:03:01.820 --> 00:03:02.120
right?

00:03:02.120 --> 00:03:03.570
There are many aspects of a search

00:03:03.570 --> 00:03:05.310
engine that we can measure we can

00:03:05.310 --> 00:03:06.070
evaluate.

00:03:06.640 --> 00:03:08.440
And here I listed the three major

00:03:08.440 --> 00:03:08.890
aspects.

00:03:08.890 --> 00:03:11.070
One is effectiveness or accuracy.

00:03:11.070 --> 00:03:13.020
How accurate the other search results.

00:03:13.610 --> 00:03:15.340
In this case, we're measuring systems

00:03:15.340 --> 00:03:17.310
capability of ranking relevant

00:03:17.310 --> 00:03:19.710
documents on top of non random ones.

00:03:20.610 --> 00:03:21.860
The second is efficiency.

00:03:21.860 --> 00:03:23.170
How quickly can a user get some

00:03:23.170 --> 00:03:23.730
results?

00:03:24.370 --> 00:03:26.240
How much computing resources are needed

00:03:26.240 --> 00:03:27.590
to answer query?

00:03:27.590 --> 00:03:29.010
So in this case we need to measure the

00:03:29.010 --> 00:03:31.390
space and time overhead of the system.

00:03:32.410 --> 00:03:34.470
The third aspect is usability.

00:03:34.470 --> 00:03:36.485
Basically, the question is how useful

00:03:36.485 --> 00:03:38.570
is a system for real user tasks.

00:03:38.570 --> 00:03:41.040
Here, obviously interfaces and many

00:03:41.040 --> 00:03:44.230
other things are also important, and we

00:03:44.230 --> 00:03:45.420
typically would have to do user

00:03:45.420 --> 00:03:46.000
studies.

00:03:47.300 --> 00:03:50.440
Now in this course we are going to talk mostly

00:03:50.440 --> 00:03:52.200
about effectiveness and accuracy

00:03:52.200 --> 00:03:55.340
measures because the efficiency and

00:03:55.340 --> 00:03:58.670
usability dimensions are not really

00:03:58.670 --> 00:04:01.070
unique to search engines and so.

00:04:01.670 --> 00:04:05.840
They are needed for evaluating any

00:04:05.840 --> 00:04:08.670
other software systems, and there is

00:04:08.670 --> 00:04:10.825
also good coverage of such materials in

00:04:10.825 --> 00:04:11.800
other courses.

00:04:13.240 --> 00:04:15.930
But how to evaluate a search engines

00:04:15.930 --> 00:04:19.595
quality or accuracy is something unique

00:04:19.595 --> 00:04:21.360
to text retrieval, and we're going to

00:04:21.360 --> 00:04:22.770
talk a lot about this.

00:04:22.770 --> 00:04:25.545
The main idea that people have proposed

00:04:25.545 --> 00:04:29.237
for using a test set to evaluate text

00:04:29.237 --> 00:04:32.150
retrieval algorithm is called the Cranfield

00:04:32.150 --> 00:04:33.560
evaluation methodology.

00:04:33.560 --> 00:04:36.950
This one actually was developed a long

00:04:36.950 --> 00:04:40.050
time ago, developed in 1960s.

00:04:40.050 --> 00:04:42.860
It's a methodology for laboratory test.

00:04:43.250 --> 00:04:46.590
Of system components, it's actually

00:04:46.590 --> 00:04:48.510
methodology that has been very useful

00:04:48.510 --> 00:04:50.810
not just for search engine evaluation,

00:04:50.810 --> 00:04:53.260
but also for evaluating virtually all

00:04:53.260 --> 00:04:54.830
kinds of empirical tasks.

00:04:55.530 --> 00:04:58.080
And for example, in natural language

00:04:58.080 --> 00:05:00.850
processing or in other fields where the

00:05:00.850 --> 00:05:02.910
problem is empirically defined, we

00:05:02.910 --> 00:05:05.106
typically would need to use such a

00:05:05.106 --> 00:05:05.473
methodology.

00:05:05.473 --> 00:05:09.090
And today with the Big Data Challenge

00:05:09.090 --> 00:05:12.190
with use of machine learning everywhere,

00:05:12.190 --> 00:05:14.990
this methodology has been very

00:05:14.990 --> 00:05:17.480
popular, but it was first developed for

00:05:17.480 --> 00:05:19.560
search engine application in 1960s.

00:05:19.560 --> 00:05:22.195
So the basic idea of this approach is

00:05:22.195 --> 00:05:25.030
to build a reusable test collections

00:05:25.030 --> 00:05:26.430
and define measures.

00:05:27.090 --> 00:05:29.220
One such a test collection is build.

00:05:29.220 --> 00:05:31.025
It can be used again and again to test

00:05:31.025 --> 00:05:33.190
the different algorithms, and we're

00:05:33.190 --> 00:05:34.630
going to define measures that would

00:05:34.630 --> 00:05:36.800
allow you to quantify the performance

00:05:36.800 --> 00:05:39.850
of a system or an algorithm.

00:05:40.920 --> 00:05:42.650
So how exactly would this work?

00:05:42.650 --> 00:05:44.940
We're going to have a sample collection

00:05:44.940 --> 00:05:46.560
of documents and this is just to

00:05:46.560 --> 00:05:48.340
simulate the real document collection

00:05:48.340 --> 00:05:49.340
in search application.

00:05:49.890 --> 00:05:51.540
We can also have a sample set of

00:05:51.540 --> 00:05:53.140
queries or topics.

00:05:53.140 --> 00:05:55.400
This is to simulate users queries.

00:05:56.160 --> 00:05:58.010
Then we'll have to have relevance

00:05:58.010 --> 00:05:58.860
judgments.

00:05:58.860 --> 00:06:01.270
These are judgments of which documents

00:06:01.270 --> 00:06:03.170
should be returned for which queries.

00:06:03.780 --> 00:06:06.020
Ideally they have to be made by users

00:06:06.020 --> 00:06:08.440
who formulated the queries, 'cause

00:06:08.440 --> 00:06:11.020
those are the people that know exactly what

00:06:11.020 --> 00:06:13.150
documents would be useful, and then

00:06:13.150 --> 00:06:14.520
finally we have to have measures to

00:06:14.520 --> 00:06:17.110
quantify how well systems result 

00:06:17.110 --> 00:06:19.626
matches the ideal ranked list that

00:06:19.626 --> 00:06:22.500
would be constructed based on users

00:06:22.500 --> 00:06:23.840
relevance judgments.

00:06:24.420 --> 00:06:29.820
So this methodology is very useful for

00:06:29.820 --> 00:06:32.035
starting retrieval algorithms because

00:06:32.035 --> 00:06:34.730
the tested connection can be reused

00:06:34.730 --> 00:06:38.370
many times and it would also provide a

00:06:38.370 --> 00:06:40.700
fair comparison for all the methods.

00:06:41.270 --> 00:06:44.000
We have the same criteria, same data

00:06:44.000 --> 00:06:46.340
set to be used to compare different

00:06:46.340 --> 00:06:47.085
algorithms.

00:06:47.085 --> 00:06:49.460
This allows us to compare a new

00:06:49.460 --> 00:06:52.130
algorithm with an older algorithm that

00:06:52.130 --> 00:06:53.960
was developed many years ago by

00:06:53.960 --> 00:06:55.550
using the same standard.

00:06:55.550 --> 00:06:57.770
So this is an illustration of how this

00:06:57.770 --> 00:06:58.250
works.

00:06:58.250 --> 00:07:01.180
So as I said, we need the queries that

00:07:01.180 --> 00:07:02.220
are shown here. We have

00:07:02.830 --> 00:07:04.670
Q1Q2, etc.

00:07:04.670 --> 00:07:06.470
We also need the documents that's

00:07:06.470 --> 00:07:08.330
called a document collection and on the

00:07:08.330 --> 00:07:10.060
right side you see we need relevance

00:07:10.060 --> 00:07:11.120
judgments.

00:07:11.120 --> 00:07:12.720
These are basically.

00:07:14.090 --> 00:07:17.540
The binary judgments of documents with

00:07:17.540 --> 00:07:19.080
respect to a query.

00:07:19.080 --> 00:07:22.044
So, for example d1 is judged as

00:07:22.044 --> 00:07:24.740
being relevant to Q1, D2 is judged as

00:07:24.740 --> 00:07:25.980
being relevant as well.

00:07:25.980 --> 00:07:27.760
And d3 is judged as non

00:07:27.760 --> 00:07:28.082
relevant.

00:07:28.082 --> 00:07:29.750
The two, Q1, etc.

00:07:30.340 --> 00:07:32.620
These would be created by users.

00:07:33.930 --> 00:07:36.370
But once we have these and then we

00:07:36.370 --> 00:07:38.326
basically have a text collection and

00:07:38.326 --> 00:07:41.750
then if you have two systems you want

00:07:41.750 --> 00:07:44.640
to compare them then you can just run

00:07:44.640 --> 00:07:47.180
each system on these queries and

00:07:47.180 --> 00:07:49.770
documents and each system would then

00:07:49.770 --> 00:07:50.550
return results.

00:07:50.550 --> 00:07:53.995
Let's say if the query is Q1 and then

00:07:53.995 --> 00:07:56.119
we would have results.

00:07:56.119 --> 00:08:01.100
Here I show R sub A as results from

00:08:01.100 --> 00:08:02.310
system A.

00:08:02.310 --> 00:08:05.040
So this is remember we talked about.

00:08:05.180 --> 00:08:07.640
Task of computing approximation of the

00:08:07.640 --> 00:08:10.980
relevant document set R sub A  is

00:08:10.980 --> 00:08:14.110
system A's approximation here.

00:08:14.870 --> 00:08:17.910
And R sub B is system B's

00:08:17.910 --> 00:08:20.020
approximation of relevant documents.

00:08:20.970 --> 00:08:22.830
Now let's take a look at these results.

00:08:22.830 --> 00:08:24.390
So which is better now?

00:08:24.390 --> 00:08:26.560
Imagine for a user, which one would you

00:08:26.560 --> 00:08:27.090
like?

00:08:28.170 --> 00:08:30.480
Now let's take a look at the both 

00:08:30.480 --> 00:08:31.200
results.

00:08:32.150 --> 00:08:34.576
And there are some differences, and

00:08:34.576 --> 00:08:36.773
there are some documents that are

00:08:36.773 --> 00:08:38.860
returned by both systems.

00:08:38.860 --> 00:08:41.120
But if you look at the results, you

00:08:41.120 --> 00:08:44.370
would feel that well, maybe A is better

00:08:44.370 --> 00:08:46.300
in the sense that we don't have many

00:08:46.300 --> 00:08:48.520
non relevant documents and among the

00:08:48.520 --> 00:08:49.920
three documents returned,

00:08:49.920 --> 00:08:51.920
two of them are relevant, so that's

00:08:51.920 --> 00:08:53.840
good, it's precise.

00:08:55.310 --> 00:08:57.160
On the other hand, one can also say,

00:08:57.160 --> 00:08:59.950
maybe B is better because we've got

00:08:59.950 --> 00:09:01.240
more relevant documents.

00:09:01.240 --> 00:09:03.100
We've got 3 instead of two.

00:09:04.050 --> 00:09:06.100
So which one is better and how do we

00:09:06.100 --> 00:09:07.020
quantify this?

00:09:08.710 --> 00:09:10.865
Obviously this question highly depends

00:09:10.865 --> 00:09:13.313
on the users task and it depends on

00:09:13.313 --> 00:09:14.720
users as well.

00:09:14.720 --> 00:09:16.520
You might be able to imagine for some

00:09:16.520 --> 00:09:19.170
users may be system A is better.

00:09:19.840 --> 00:09:21.450
If the user is not interested in

00:09:21.450 --> 00:09:22.940
getting all the relevant document.

00:09:23.970 --> 00:09:25.970
But in this case, the user

00:09:25.970 --> 00:09:27.795
doesn't have to read many and the

00:09:27.795 --> 00:09:29.780
user would see most of the relevant

00:09:29.780 --> 00:09:30.370
documents.

00:09:30.940 --> 00:09:32.190
On the one hand,  one can also

00:09:32.190 --> 00:09:35.450
imagine the user might need to have as

00:09:35.450 --> 00:09:37.110
many relevant documents as possible.

00:09:37.110 --> 00:09:38.150
For example, if you are doing a

00:09:38.150 --> 00:09:40.000
literature survey, you might be in the

00:09:40.000 --> 00:09:42.420
segment category and you might find that

00:09:42.420 --> 00:09:43.950
system B is better.

00:09:43.950 --> 00:09:46.219
So in that case we will have to also

00:09:46.220 --> 00:09:48.230
define measures to quantify them.

00:09:49.110 --> 00:09:51.030
And we might need to define multiple

00:09:51.030 --> 00:09:51.400
measures.

00:09:51.400 --> 00:09:53.680
Because users have different

00:09:53.680 --> 00:09:55.800
perspectives of looking at the results.


