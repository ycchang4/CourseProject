WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-27T00:07:07.8952793Z by ClassTranscribe

00:00:00.280 --> 00:00:01.980
This lecture is about the query

00:00:01.980 --> 00:00:04.120
likelihood probabilistic retrieval

00:00:04.120 --> 00:00:04.580
model.

00:00:13.940 --> 00:00:16.090
In this lecture we continue the

00:00:16.090 --> 00:00:18.080
discussion of probabilistic retrieval

00:00:18.080 --> 00:00:18.520
model.

00:00:19.070 --> 00:00:20.420
In particular, we're going to talk

00:00:20.420 --> 00:00:22.460
about the query likelihood retrieval

00:00:22.460 --> 00:00:22.960
function.

00:00:25.760 --> 00:00:28.100
In the query likelihood retrieval

00:00:28.100 --> 00:00:28.750
model.

00:00:29.340 --> 00:00:32.680
Our idea is to model how likely a user

00:00:32.680 --> 00:00:34.640
who likes a document would pose a

00:00:34.640 --> 00:00:35.630
particular query.

00:00:36.630 --> 00:00:40.080
So in this case you can imagine if a

00:00:40.080 --> 00:00:42.700
user likes this particular document

00:00:42.700 --> 00:00:45.410
about the presidential campaign news.

00:00:46.720 --> 00:00:50.270
Then we can assume the user would use

00:00:50.270 --> 00:00:52.600
this document as a basis to post a

00:00:52.600 --> 00:00:54.970
query to try to retrieve this document.

00:00:57.170 --> 00:01:00.270
So we can imagine the user could use a

00:01:00.270 --> 00:01:01.030
process.

00:01:01.950 --> 00:01:04.436
That works as follows, where we assume

00:01:04.436 --> 00:01:07.400
that the query is generated by sampling

00:01:07.400 --> 00:01:08.810
words from the document.

00:01:10.420 --> 00:01:14.280
So for example, a user might pick a word

00:01:14.280 --> 00:01:16.910
like presidential from this document.

00:01:17.580 --> 00:01:19.530
And then use this as a query word.

00:01:20.500 --> 00:01:22.060
And then the user would pick another

00:01:22.060 --> 00:01:24.910
word like "campaign" and that will be

00:01:24.910 --> 00:01:26.080
the second query word.

00:01:27.250 --> 00:01:29.630
Now this of course is assumption that

00:01:29.630 --> 00:01:32.700
we have made about how a user would

00:01:32.700 --> 00:01:33.700
pose a query.

00:01:35.050 --> 00:01:36.930
Whether user actually followed this

00:01:36.930 --> 00:01:37.780
process.

00:01:38.350 --> 00:01:40.250
Maybe a different question, but this

00:01:40.250 --> 00:01:43.140
assumption has allowed us to formulate

00:01:43.140 --> 00:01:44.560
characterize this conditional

00:01:44.560 --> 00:01:45.500
probability.

00:01:46.280 --> 00:01:49.180
And this allows us to also not rely on

00:01:49.180 --> 00:01:51.650
the big table that I showed you earlier

00:01:51.650 --> 00:01:55.040
to use empirical data to estimate this

00:01:55.040 --> 00:01:55.860
probability.

00:01:56.700 --> 00:01:59.170
And this is why we can use this idea to

00:01:59.170 --> 00:01:59.390
them.

00:01:59.390 --> 00:02:01.746
Further derive retrieval function that

00:02:01.746 --> 00:02:03.240
we can implement with the program

00:02:03.240 --> 00:02:03.770
language.

00:02:04.800 --> 00:02:06.520
So as you see, the assumption that

00:02:06.520 --> 00:02:09.540
we've made here is each query word is

00:02:09.540 --> 00:02:13.020
independently sampled and also each

00:02:13.020 --> 00:02:17.300
word is basically obtained from the

00:02:17.300 --> 00:02:18.040
document.

00:02:20.820 --> 00:02:23.710
So now let's see how this works

00:02:23.710 --> 00:02:24.500
exactly.

00:02:26.360 --> 00:02:28.240
Well, since we are computing the query

00:02:28.240 --> 00:02:28.680
likelihood.

00:02:29.390 --> 00:02:32.770
Then the probability here is just the

00:02:32.770 --> 00:02:34.780
probability of this particular query,

00:02:34.780 --> 00:02:36.490
which is a sequence of words.

00:02:37.080 --> 00:02:39.500
And we make the assumption that each

00:02:39.500 --> 00:02:43.080
word is generated independently, so as

00:02:43.080 --> 00:02:45.476
a result, the probability of the query

00:02:45.476 --> 00:02:47.896
is just a product of the probability of

00:02:47.896 --> 00:02:49.080
each query word.

00:02:49.990 --> 00:02:51.640
Now, how do we compute the probability

00:02:51.640 --> 00:02:52.620
of each query word

00:02:52.620 --> 00:02:56.100
Well based on the assumption that a word

00:02:56.100 --> 00:02:58.400
is picked from the document.

00:02:59.030 --> 00:03:01.650
That the user has in mind. Now we know the

00:03:01.650 --> 00:03:03.940
probability of each word is just to

00:03:03.940 --> 00:03:06.833
the relative frequency of the word in

00:03:06.833 --> 00:03:08.039
the document.

00:03:08.040 --> 00:03:11.399
So for example, the probability of

00:03:11.400 --> 00:03:13.140
presidential given the document.

00:03:13.700 --> 00:03:16.320
Would be just the count of presidential

00:03:16.320 --> 00:03:18.450
in the document divided by the total

00:03:18.450 --> 00:03:20.330
number of words in the document or

00:03:20.330 --> 00:03:21.240
document length.

00:03:22.970 --> 00:03:26.020
So with this these assumptions, we now

00:03:26.020 --> 00:03:27.820
have actually simple formula for

00:03:27.820 --> 00:03:28.705
retrieval, right?

00:03:28.705 --> 00:03:31.010
We can use this to rank our documents.

00:03:32.460 --> 00:03:34.030
So does this model work?

00:03:34.030 --> 00:03:35.035
Let's take a look.

00:03:35.035 --> 00:03:37.230
Here are some example documents that

00:03:37.230 --> 00:03:38.260
you have seen before.

00:03:38.260 --> 00:03:40.460
Suppose now the query is presidential

00:03:40.460 --> 00:03:44.450
campaign and we see the formula here on

00:03:44.450 --> 00:03:45.050
the top.

00:03:45.800 --> 00:03:47.650
So how do we score these documents?

00:03:47.650 --> 00:03:48.655
It's very simple, right?

00:03:48.655 --> 00:03:50.510
We just count how many times we have

00:03:50.510 --> 00:03:52.621
seen "presidential" or how many times we

00:03:52.621 --> 00:03:54.690
have seen campaign et cetera and within

00:03:54.690 --> 00:03:57.150
here for d4 and we have seen presidential

00:03:57.150 --> 00:04:00.190
twice that's two over the length

00:04:00.190 --> 00:04:04.273
of Document 4 multiplied by 1 over length of

00:04:04.273 --> 00:04:07.256
document 4 for probability of campaign.

00:04:07.256 --> 00:04:09.470
And similarly we can get probabilities

00:04:09.470 --> 00:04:11.220
for the other two documents.

00:04:13.270 --> 00:04:16.220
Now if you look at this, these numbers

00:04:16.220 --> 00:04:19.350
or these formulas for scoring all these

00:04:19.350 --> 00:04:20.090
documents.

00:04:20.890 --> 00:04:23.230
It seems to make sense be cause if we

00:04:23.230 --> 00:04:26.610
assume D3 and D4 have about the

00:04:26.610 --> 00:04:29.110
same length than looks like we're going to

00:04:29.110 --> 00:04:33.330
rank D4 above D3, and

00:04:33.330 --> 00:04:37.615
which is above D2 as we would expect,

00:04:37.615 --> 00:04:40.390
looks like it did capture the TF

00:04:40.390 --> 00:04:41.250
heuristic.

00:04:41.870 --> 00:04:44.960
And so this seems to work well.

00:04:45.670 --> 00:04:46.500
However.

00:04:47.810 --> 00:04:49.740
If we try a different query like this

00:04:49.740 --> 00:04:51.680
one presidential campaign update.

00:04:53.020 --> 00:04:54.460
Then we might see a problem.

00:04:55.520 --> 00:04:56.550
What problem?

00:04:56.550 --> 00:04:59.720
Well think about the update now.

00:04:59.720 --> 00:05:01.650
None of these documents has mentioned

00:05:01.650 --> 00:05:02.420
update.

00:05:02.420 --> 00:05:05.030
So according to our assumption that a

00:05:05.030 --> 00:05:07.310
user would pick a word from a document

00:05:07.310 --> 00:05:09.770
to generate a query, then the

00:05:09.770 --> 00:05:12.160
probability of obtaining a word like

00:05:12.160 --> 00:05:12.870
update.

00:05:13.430 --> 00:05:16.280
Would be what? Would be 0, right?

00:05:17.190 --> 00:05:19.760
So that caused a problem because

00:05:19.760 --> 00:05:22.030
we cause all these documents to have zero

00:05:22.030 --> 00:05:23.920
probability of generating this query.

00:05:25.220 --> 00:05:28.120
Now, while it's fine to have zero

00:05:28.120 --> 00:05:30.330
probability for D2 which is non

00:05:30.330 --> 00:05:33.550
relevant, it's not OK to have zero for

00:05:33.550 --> 00:05:36.220
D3 and D4, because now we

00:05:36.220 --> 00:05:38.480
no longer can distinguish them.

00:05:38.480 --> 00:05:40.483
What's worse, we can't even distinguish

00:05:40.483 --> 00:05:42.130
them from D2, right?

00:05:42.130 --> 00:05:44.930
So that's obviously not desirable.

00:05:44.930 --> 00:05:47.950
Now, whenever we've had such result.

00:05:48.550 --> 00:05:50.470
We should think about what has caused

00:05:50.470 --> 00:05:51.140
this problem.

00:05:52.440 --> 00:05:54.500
So we have to examine what assumptions

00:05:54.500 --> 00:05:55.830
have been made.

00:05:56.420 --> 00:05:58.790
As we derive this ranking function.

00:05:59.710 --> 00:06:01.430
Now, if you examine those assumptions

00:06:01.430 --> 00:06:03.410
carefully, you would realize what has

00:06:03.410 --> 00:06:04.730
caused this problem.

00:06:05.430 --> 00:06:05.920
Right?

00:06:06.570 --> 00:06:09.250
So take a moment to think about what do

00:06:09.250 --> 00:06:12.030
you think is the reason why update has

00:06:12.030 --> 00:06:13.200
zero probability.

00:06:14.030 --> 00:06:15.080
And how do we fix it?

00:06:17.530 --> 00:06:20.220
Right, so if you think about this for a

00:06:20.220 --> 00:06:22.150
moment, you realize that that's because

00:06:22.150 --> 00:06:24.640
we have made assumption that every

00:06:24.640 --> 00:06:26.820
query word must be drawn from the

00:06:26.820 --> 00:06:28.560
document in the user's mind.

00:06:29.110 --> 00:06:31.490
So in order to fix this, we have to

00:06:31.490 --> 00:06:32.230
assume that

00:06:32.940 --> 00:06:35.290
the user could have drawn a word not

00:06:35.290 --> 00:06:37.360
necessarily from the document, so let's

00:06:37.360 --> 00:06:39.610
improve the model and the improvement

00:06:39.610 --> 00:06:42.150
here is to say that instead of drawing

00:06:42.150 --> 00:06:44.790
a word from the document, let's imagine

00:06:44.790 --> 00:06:46.240
that the user would actually draw a

00:06:46.240 --> 00:06:48.278
word from a document model.

00:06:48.278 --> 00:06:50.420
So I showed model here.

00:06:50.420 --> 00:06:52.020
We assume that this document is

00:06:52.020 --> 00:06:55.100
generated using this unigram language

00:06:55.100 --> 00:06:55.810
model.

00:06:55.810 --> 00:06:57.670
Now this model.

00:06:58.670 --> 00:07:00.258
Doesn't necessarily assign zero

00:07:00.258 --> 00:07:01.500
probability for update.

00:07:01.500 --> 00:07:03.420
In fact that we consume this model does

00:07:03.420 --> 00:07:05.380
not assign zero probability for any

00:07:05.380 --> 00:07:05.810
word.

00:07:06.600 --> 00:07:07.910
Now if we think in this way,

00:07:07.910 --> 00:07:09.750
then the generation process is a little

00:07:09.750 --> 00:07:10.480
bit different.

00:07:10.480 --> 00:07:12.510
Now the user has this model in mind.

00:07:12.510 --> 00:07:14.180
Instead of this particular document.

00:07:14.830 --> 00:07:16.620
Although the model has to be estimated

00:07:16.620 --> 00:07:17.810
based on the document.

00:07:18.740 --> 00:07:21.120
So the user can again generate the

00:07:21.120 --> 00:07:23.480
query using a similar process, namely

00:07:23.480 --> 00:07:25.500
pick a word. For example, presidential.

00:07:26.180 --> 00:07:27.900
And another word, campaign.

00:07:28.890 --> 00:07:30.890
Now the difference is that this time we

00:07:30.890 --> 00:07:32.570
can also pick a word like update even

00:07:32.570 --> 00:07:33.950
though update does not occur in the

00:07:33.950 --> 00:07:36.140
document to potentially generate the

00:07:36.140 --> 00:07:38.820
query word like update so that

00:07:40.100 --> 00:07:42.280
a query with update want to have zero

00:07:42.280 --> 00:07:43.130
probabilities.

00:07:43.730 --> 00:07:45.950
So this will fix our problem, and it's

00:07:45.950 --> 00:07:47.660
also reasonable because we're now

00:07:47.660 --> 00:07:49.400
thinking of what the user is looking

00:07:49.400 --> 00:07:51.230
for in a more general way.

00:07:51.230 --> 00:07:53.520
That is unigram language model instead

00:07:53.520 --> 00:07:54.780
of a fixed document.

00:07:54.780 --> 00:07:57.010
So how do we compute this query 

00:07:57.010 --> 00:07:57.420
likelihood?

00:07:57.420 --> 00:07:58.890
If we make this assumption?

00:07:58.890 --> 00:08:00.855
Well, it involves 2 steps, right?

00:08:00.855 --> 00:08:02.969
The first is to compute this model.

00:08:04.020 --> 00:08:05.780
And we call it the document language

00:08:05.780 --> 00:08:06.710
model here.

00:08:07.290 --> 00:08:09.800
For example, I've shown two

00:08:10.520 --> 00:08:12.930
possible language models here is made

00:08:12.930 --> 00:08:14.290
based on two documents.

00:08:14.950 --> 00:08:16.240
And then given a query and I get

00:08:16.240 --> 00:08:17.510
data mining algorithms.

00:08:17.510 --> 00:08:20.380
The second step would just compute

00:08:20.380 --> 00:08:23.100
the likelihood of this query and by

00:08:23.100 --> 00:08:25.500
making independent assumptions we

00:08:25.500 --> 00:08:27.270
could then have this probability as a

00:08:27.270 --> 00:08:29.160
product of the probability of each

00:08:29.160 --> 00:08:29.920
query word.

00:08:30.800 --> 00:08:33.016
But we do this for both documents and

00:08:33.016 --> 00:08:34.657
then we're going to score these two

00:08:34.657 --> 00:08:35.950
documents and then rank them.

00:08:36.640 --> 00:08:38.756
So that's the basic idea of this query,

00:08:38.756 --> 00:08:40.936
likelihood retrieval function.

00:08:40.936 --> 00:08:44.900
So more generally then, this ranking

00:08:44.900 --> 00:08:46.970
function would look like the following

00:08:46.970 --> 00:08:49.490
right here we assume that the query has

00:08:49.490 --> 00:08:50.370
N words.

00:08:51.620 --> 00:08:55.340
W one through WN, and then the scoring

00:08:55.340 --> 00:08:55.710
function.

00:08:55.710 --> 00:08:57.490
The ranking function is.

00:08:59.220 --> 00:09:02.350
Probability that we observe this query

00:09:02.350 --> 00:09:04.190
given that the user is thinking of this

00:09:04.190 --> 00:09:04.880
document.

00:09:05.960 --> 00:09:08.410
And this is assumed to be product of

00:09:08.410 --> 00:09:11.280
probabilities of all individual words.

00:09:11.280 --> 00:09:13.350
This is based on the independence

00:09:13.350 --> 00:09:13.920
assumption.

00:09:15.450 --> 00:09:18.560
Now we actually often score the

00:09:18.560 --> 00:09:21.630
document for this query by using log of

00:09:21.630 --> 00:09:24.630
the query likelihood as shown on the

00:09:24.630 --> 00:09:25.460
second line.

00:09:26.560 --> 00:09:27.780
Now we do this.

00:09:28.350 --> 00:09:31.590
To avoid having a lot of small

00:09:31.590 --> 00:09:32.670
probabilities.

00:09:33.860 --> 00:09:36.360
We multiply together and this could

00:09:36.360 --> 00:09:38.800
cause underflow and we might lose

00:09:38.800 --> 00:09:42.040
precision by transforming the value with

00:09:42.040 --> 00:09:43.780
a logarithm function.

00:09:43.780 --> 00:09:46.030
We maintain the order of these

00:09:46.030 --> 00:09:49.340
documents, yet we can avoid the

00:09:49.340 --> 00:09:50.750
underflow problem.

00:09:52.070 --> 00:09:53.920
So if we take logarithm transformation,

00:09:53.920 --> 00:09:55.380
of course the product that would become

00:09:55.380 --> 00:09:58.805
a sum as shown on the second line here.

00:09:58.805 --> 00:10:01.340
So it's a sum over all the query words

00:10:01.340 --> 00:10:02.590
inside the sum.

00:10:02.590 --> 00:10:04.810
The value is log of the probability of

00:10:04.810 --> 00:10:07.140
this word given by the document.

00:10:08.290 --> 00:10:10.820
And then we can further rewrite the sum

00:10:10.820 --> 00:10:12.420
into a different form.

00:10:14.220 --> 00:10:16.830
So in the first sum here.

00:10:19.010 --> 00:10:20.180
In this sum,

00:10:21.760 --> 00:10:24.370
We have it all over the query words N

00:10:24.370 --> 00:10:25.230
query words.

00:10:28.690 --> 00:10:31.920
And in this sum we have a sum over all

00:10:31.920 --> 00:10:34.550
the possible words, but we put a count

00:10:34.550 --> 00:10:37.000
here of each word in the query.

00:10:37.000 --> 00:10:38.687
Essentially we are only considering the

00:10:38.687 --> 00:10:40.780
words in the query because if a word is

00:10:40.780 --> 00:10:42.520
not in the query, the count would be

00:10:42.520 --> 00:10:43.500
0.

00:10:43.500 --> 00:10:45.835
So we're still considering only these N

00:10:45.835 --> 00:10:46.160
words.

00:10:46.710 --> 00:10:48.470
But we are using a different form, as

00:10:48.470 --> 00:10:50.120
if we're going to take sum over all

00:10:50.120 --> 00:10:51.760
the words in the vocabulary.

00:10:52.870 --> 00:10:54.800
And of course, a word might occur

00:10:54.800 --> 00:10:56.380
multiple times in the query.

00:10:56.380 --> 00:10:58.650
That's why we have a count here.

00:11:01.430 --> 00:11:04.050
And then this part is log of the

00:11:04.050 --> 00:11:05.870
probability of the word given by the

00:11:05.870 --> 00:11:07.060
document language model.

00:11:08.020 --> 00:11:10.410
So you can see in this retrieval

00:11:10.410 --> 00:11:12.675
function we actually know the count of the word in

00:11:12.675 --> 00:11:13.150
the query.

00:11:13.150 --> 00:11:14.910
So the only thing that we don't know is

00:11:14.910 --> 00:11:16.320
this document language model.

00:11:17.730 --> 00:11:19.870
Therefore, we have converted the

00:11:19.870 --> 00:11:22.410
retrieval problem, include the problem of

00:11:22.410 --> 00:11:24.140
estimating this document language

00:11:24.140 --> 00:11:24.670
model.

00:11:25.610 --> 00:11:28.140
So that we can compute the probability

00:11:28.140 --> 00:11:29.910
of each query word given by this

00:11:29.910 --> 00:11:30.430
document.

00:11:32.120 --> 00:11:33.842
And different estimation methods

00:11:33.842 --> 00:11:35.537
here would lead to different

00:11:35.537 --> 00:11:36.400
ranking functions.

00:11:36.400 --> 00:11:38.494
Now this is just like a different ways

00:11:38.494 --> 00:11:41.518
to place a document vector in the vector

00:11:41.518 --> 00:11:43.215
space would lead to a different

00:11:43.215 --> 00:11:44.972
 ranking function in the vector

00:11:44.972 --> 00:11:45.558
space model.

00:11:45.558 --> 00:11:47.710
Here different ways to estimate these

00:11:47.710 --> 00:11:49.850
document language model would lead to

00:11:49.850 --> 00:11:52.600
a different ranking function for query

00:11:52.600 --> 00:11:53.290
likelihood.


