WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-27T00:01:06.9077206Z by ClassTranscribe

00:00:00.300 --> 00:00:02.070
This lecture is about the

00:00:02.070 --> 00:00:04.660
discriminative classifiers for text

00:00:04.660 --> 00:00:05.380
categorization.

00:00:12.850 --> 00:00:14.490
In this lecture, we're going to

00:00:14.490 --> 00:00:16.320
continue talking about how to do text

00:00:16.320 --> 00:00:18.860
categorization and cover discriminative

00:00:18.860 --> 00:00:19.700
approaches.

00:00:19.700 --> 00:00:22.385
This is a slide that you have seen from

00:00:22.385 --> 00:00:24.277
the discussion of Naive Bayes

00:00:24.277 --> 00:00:26.310
classifier, where we have shown that

00:00:26.310 --> 00:00:28.470
although naive Bayes classifier tries

00:00:28.470 --> 00:00:30.920
to model the generation of text data

00:00:30.920 --> 00:00:33.450
from each categories, we can actually

00:00:33.450 --> 00:00:37.135
use bayes rule and to eventually rewrite

00:00:37.135 --> 00:00:40.467
the scoring function as you see on this

00:00:40.467 --> 00:00:42.500
slide and this scoring function is

00:00:42.500 --> 00:00:43.050
basically a

00:00:43.110 --> 00:00:45.733
weighted combination of a lot of

00:00:45.733 --> 00:00:48.100
word features where the feature values

00:00:48.100 --> 00:00:50.340
are word count and the feature weights

00:00:50.340 --> 00:00:52.880
are the log of probability ratios of

00:00:52.880 --> 00:00:55.470
the word given by two distributions

00:00:55.470 --> 00:00:55.850
here.

00:00:57.140 --> 00:01:00.716
Now this kind of scoring function can

00:01:00.716 --> 00:01:03.170
be actually a general scoring function

00:01:03.170 --> 00:01:06.120
where we can in general represent text

00:01:06.120 --> 00:01:08.005
data as a feature vector.

00:01:08.005 --> 00:01:10.303
Of course the features don't have to

00:01:10.303 --> 00:01:13.103
be all the words and their features can

00:01:13.103 --> 00:01:15.430
be other signals that we want to use.

00:01:16.130 --> 00:01:18.550
And we mentioned that this is

00:01:18.550 --> 00:01:19.800
precisely

00:01:20.800 --> 00:01:22.486
similar to logistic regression.

00:01:22.486 --> 00:01:23.960
So in this lecture we're going to

00:01:23.960 --> 00:01:26.290
introduce some discriminative

00:01:26.290 --> 00:01:27.420
classifiers.

00:01:27.420 --> 00:01:30.250
They try to model the conditional

00:01:30.250 --> 00:01:33.040
distribution of labels given the data

00:01:33.040 --> 00:01:36.560
directly rather than using Bayes rule

00:01:36.560 --> 00:01:38.810
to compute that indirectly.

00:01:38.810 --> 00:01:41.026
As we have seen in naive bayes.

00:01:41.026 --> 00:01:43.600
So the general idea of logistical

00:01:43.600 --> 00:01:48.910
regression is to model the dependency

00:01:48.910 --> 00:01:51.920
of the binary response variable Y here,

00:01:52.270 --> 00:01:53.680
On some predictors.

00:01:54.240 --> 00:01:56.010
That are denoted as X.

00:01:56.010 --> 00:01:59.970
So here we have also changed the

00:01:59.970 --> 00:02:02.800
notation to X

00:02:04.710 --> 00:02:08.090
For feature values you may recall in the

00:02:08.090 --> 00:02:10.860
previous slide we have used Fi to

00:02:10.860 --> 00:02:12.870
represent the feature values.

00:02:13.750 --> 00:02:18.700
An here we use the notation of X

00:02:18.700 --> 00:02:22.360
vector, which is more common when we

00:02:22.940 --> 00:02:25.930
Introduce such machine learning

00:02:25.930 --> 00:02:29.830
algorithms, so X is our input, it's a

00:02:29.830 --> 00:02:30.440
vector.

00:02:31.960 --> 00:02:34.500
And with M features.

00:02:35.050 --> 00:02:37.583
And each feature has a value X sub I here

00:02:37.583 --> 00:02:39.820
and our goal is model the dependency

00:02:39.820 --> 00:02:42.950
of this binary response variable on all

00:02:42.950 --> 00:02:44.130
these features.

00:02:44.130 --> 00:02:45.950
So in our categorization problem we

00:02:45.950 --> 00:02:47.900
have two categories, lets say theta 1 and

00:02:47.900 --> 00:02:52.360
theta 2, and we can use the Y value to

00:02:52.360 --> 00:02:53.744
denote the two categories.

00:02:53.744 --> 00:02:56.575
And when Y is 1 it means the category of

00:02:56.575 --> 00:02:59.240
the documents first class theta 1

00:03:00.850 --> 00:03:03.149
Now the goal here is to model the

00:03:03.150 --> 00:03:05.770
conditional probability of Y given X

00:03:05.770 --> 00:03:09.149
directly as opposed to model the

00:03:09.150 --> 00:03:11.650
generation of X&amp;Y as in the case of

00:03:11.650 --> 00:03:12.550
Naive Bayes.

00:03:13.330 --> 00:03:15.150
And another advantage of this kind of

00:03:15.150 --> 00:03:16.970
approach is that it would allow many

00:03:16.970 --> 00:03:18.950
other features than words to be used

00:03:18.950 --> 00:03:19.750
in this vector.

00:03:19.750 --> 00:03:21.920
Since we're not modeling the generation

00:03:21.920 --> 00:03:24.560
of this vector and we can plug in any

00:03:24.560 --> 00:03:26.930
signals that we want, so this is

00:03:26.930 --> 00:03:29.120
potentially advantages for doing text

00:03:29.120 --> 00:03:29.830
categorization.

00:03:31.270 --> 00:03:33.290
So most specifically, in logistic

00:03:33.290 --> 00:03:36.000
regression the assumed functional

00:03:36.000 --> 00:03:39.650
form of y depending on X is the

00:03:39.650 --> 00:03:43.050
following, and this is very closed,

00:03:43.050 --> 00:03:46.980
closely related to the log or log odds

00:03:46.980 --> 00:03:49.830
that I introduced in the naive bayes or

00:03:49.830 --> 00:03:52.090
log of probability ratio of the two

00:03:52.090 --> 00:03:55.230
categories that you have seen on the

00:03:55.230 --> 00:03:56.340
previous slide.

00:03:57.110 --> 00:03:59.630
So that this is what I meant, right?

00:03:59.630 --> 00:04:02.280
So in the case of Naive Bayes, we

00:04:02.280 --> 00:04:04.680
compute this by using bayes rule and

00:04:04.680 --> 00:04:07.765
eventually we have reached a formula

00:04:07.765 --> 00:04:09.603
that look like this.

00:04:09.603 --> 00:04:10.780
That looks like this.

00:04:12.870 --> 00:04:17.080
But here we actually would assume

00:04:17.080 --> 00:04:22.350
explicitly that we would model our

00:04:24.040 --> 00:04:27.400
Probability of Y given X.

00:04:28.420 --> 00:04:35.460
As directly as a function of these

00:04:35.460 --> 00:04:36.480
features.

00:04:37.370 --> 00:04:40.820
So most specifically, we assume that

00:04:40.820 --> 00:04:45.710
log of the ratio of probability of y =

00:04:45.710 --> 00:04:48.300
1 and the probability of y = 0.

00:04:48.960 --> 00:04:51.510
Is a function of X.

00:04:54.350 --> 00:04:56.850
And so it's a function of X, and it's a

00:04:56.850 --> 00:04:58.850
linear combination of these feature

00:04:58.850 --> 00:05:00.940
values, controlled by beta values.

00:05:02.240 --> 00:05:05.346
And since we know that probability of

00:05:05.346 --> 00:05:10.740
y = 0 is 1 minus probability of y = 1,

00:05:10.740 --> 00:05:14.040
and this can be also written in this

00:05:14.040 --> 00:05:14.400
way.

00:05:15.890 --> 00:05:19.020
So this is a log odds ratio.

00:05:19.700 --> 00:05:20.290
Here.

00:05:21.890 --> 00:05:23.860
And so in logistic regression, we

00:05:23.860 --> 00:05:25.505
basically assume that the probability

00:05:25.505 --> 00:05:31.770
of y = 1  given X is dependent on this

00:05:31.770 --> 00:05:33.370
linear combination of all these

00:05:33.370 --> 00:05:33.890
features.

00:05:34.450 --> 00:05:36.380
So it's just one of the many possible

00:05:36.380 --> 00:05:39.530
ways of assuming that the dependency,

00:05:39.530 --> 00:05:41.890
But this particular form has been quite

00:05:41.890 --> 00:05:45.050
useful, and it has also has some nice

00:05:45.050 --> 00:05:45.840
properties.

00:05:46.600 --> 00:05:48.990
So if we rewrite this question to

00:05:48.990 --> 00:05:51.570
actually express the probability of Y

00:05:51.570 --> 00:05:54.970
given X in terms of X by taking by

00:05:54.970 --> 00:05:56.860
getting rid of the logarithm and we get

00:05:56.860 --> 00:05:58.810
this functional form and this is called

00:05:58.810 --> 00:06:01.130
a logistical function, it's a

00:06:01.130 --> 00:06:04.590
transformation of X into Y.

00:06:06.330 --> 00:06:07.220
As you see.

00:06:08.710 --> 00:06:09.780
on the right side here.

00:06:10.420 --> 00:06:14.320
So that the Xs will be mapped into

00:06:14.320 --> 00:06:17.510
a range of values from zero to 1.0.

00:06:18.420 --> 00:06:20.860
You can see, and that's precisely what

00:06:20.860 --> 00:06:21.410
we want.

00:06:21.410 --> 00:06:23.360
Since we have a probability here.

00:06:24.210 --> 00:06:26.730
And the function form looks like this.

00:06:27.820 --> 00:06:30.780
So this is the basic idea of logistic

00:06:30.780 --> 00:06:33.330
regression, and it's a very useful

00:06:33.330 --> 00:06:35.440
classifier that can be used to do a lot

00:06:35.440 --> 00:06:38.210
of classification tasks, including text

00:06:38.210 --> 00:06:39.080
categorization.

00:06:41.620 --> 00:06:44.300
So as in all cases of model, we would

00:06:44.300 --> 00:06:46.160
be interested in estimating the

00:06:46.160 --> 00:06:48.210
parameters and in fact in all the machine

00:06:48.210 --> 00:06:49.310
learning programs.

00:06:49.310 --> 00:06:51.330
Once you set up the model set of

00:06:51.330 --> 00:06:52.570
objective function.

00:06:53.140 --> 00:06:57.050
To model the classifier, then the next

00:06:57.050 --> 00:06:59.500
step is to compute the parameter

00:06:59.500 --> 00:06:59.855
values.

00:06:59.855 --> 00:07:01.856
In general, we're going to adjust these

00:07:01.856 --> 00:07:03.830
parameter values, optimize the

00:07:03.830 --> 00:07:05.680
performance of classifier on the

00:07:05.680 --> 00:07:06.280
training data.

00:07:06.280 --> 00:07:08.576
So in our case, let's assume we have

00:07:08.576 --> 00:07:09.689
training data.

00:07:09.689 --> 00:07:13.310
The training data here, X i and Y i and

00:07:13.310 --> 00:07:16.540
each pair is basically feature vector

00:07:16.540 --> 00:07:21.500
of X and a known label for that X Y,

00:07:21.500 --> 00:07:22.840
either one or zero.

00:07:23.790 --> 00:07:26.830
So in our case we are interested in

00:07:26.830 --> 00:07:29.970
maximizing this conditional likelihood.

00:07:30.690 --> 00:07:33.960
The condition likelihood here is

00:07:33.960 --> 00:07:38.420
basically to model y given the

00:07:38.420 --> 00:07:39.670
observed X.

00:07:41.140 --> 00:07:43.350
So it's not like a.

00:07:44.210 --> 00:07:47.110
It's not like a modeling X, but rather

00:07:47.110 --> 00:07:48.530
we're going to model this.

00:07:50.940 --> 00:07:52.650
Note that this is a conditional

00:07:52.650 --> 00:07:55.190
probability of Y given X.

00:07:56.400 --> 00:07:59.030
And this is also precisely what we want

00:07:59.030 --> 00:08:00.030
for classification.

00:08:00.650 --> 00:08:02.560
Now, so the likelihood function would

00:08:02.560 --> 00:08:04.380
be just a product over all the training

00:08:04.380 --> 00:08:05.010
cases.

00:08:06.620 --> 00:08:09.740
And in each case, this is the modeled

00:08:09.740 --> 00:08:11.535
probability of observing this

00:08:11.535 --> 00:08:12.870
particular training case.

00:08:12.870 --> 00:08:16.970
So given a particular XI, how likely

00:08:16.970 --> 00:08:19.346
we are going to observe the corresponding Y i

00:08:19.346 --> 00:08:21.956
of course, Y I could be one or zero

00:08:21.956 --> 00:08:23.890
and in fact the function form here

00:08:23.890 --> 00:08:27.330
would vary depending on whether Y sub I

00:08:27.330 --> 00:08:28.890
is one or zero.

00:08:28.890 --> 00:08:32.140
If it's one will be taking this form.

00:08:33.560 --> 00:08:35.230
And that's basically the logistical

00:08:35.230 --> 00:08:36.260
regression function.

00:08:36.260 --> 00:08:38.360
But what about this if it's 0?

00:08:38.920 --> 00:08:43.200
Well, if it's zero then we have to use a

00:08:43.200 --> 00:08:45.390
different form and that's this one.

00:08:48.480 --> 00:08:50.480
Now how do we get this one?

00:08:50.480 --> 00:08:53.070
That's just one minus the probability

00:08:53.070 --> 00:08:54.970
of y = 1, right?

00:08:55.850 --> 00:08:58.240
And you can easily see this now.

00:08:58.240 --> 00:09:00.990
The key point here is that the function

00:09:00.990 --> 00:09:03.590
form here depends on the observed.

00:09:03.590 --> 00:09:06.810
Y I if it's one, it has a different

00:09:06.810 --> 00:09:08.359
form than when it's 0.

00:09:09.220 --> 00:09:11.620
And if you think about when we want to

00:09:11.620 --> 00:09:13.720
maximize this probability we will

00:09:13.720 --> 00:09:16.020
basically going to want this

00:09:16.020 --> 00:09:19.050
probability to be as high as possible

00:09:19.050 --> 00:09:21.500
when the label is one.

00:09:21.500 --> 00:09:24.795
That means the document is in topic

00:09:24.795 --> 00:09:25.350
one.

00:09:26.680 --> 00:09:29.580
But if the document is not we are going to

00:09:29.580 --> 00:09:32.320
maximize this value, and what's going

00:09:32.320 --> 00:09:35.696
to happen is actually to make this

00:09:35.696 --> 00:09:38.390
value as small as possible.

00:09:38.990 --> 00:09:40.180
Because they sum to one.

00:09:40.890 --> 00:09:42.490
When I maximize this one.

00:09:43.130 --> 00:09:45.750
It's equivalent to minimize this one.

00:09:47.930 --> 00:09:50.390
So you can see basically the if we

00:09:50.390 --> 00:09:51.360
maximize the conditional likelihood

00:09:51.360 --> 00:09:53.470
we're going to basically

00:09:53.470 --> 00:09:56.430
try to make the prediction on the

00:09:56.430 --> 00:09:58.450
training data as accurate as possible.

00:10:01.110 --> 00:10:03.230
So, as in other cases, when compute

00:10:03.230 --> 00:10:04.810
the maximum likelihood estimator

00:10:04.810 --> 00:10:07.040
Basically lets go find a beta

00:10:07.040 --> 00:10:09.070
value, a set of beta values that will

00:10:09.070 --> 00:10:11.200
maximize this conditional likelihood.

00:10:12.040 --> 00:10:14.490
And this again then gives us a standard

00:10:14.490 --> 00:10:15.335
optimization problem.

00:10:15.335 --> 00:10:18.600
In this case, it can be also solved in

00:10:18.600 --> 00:10:19.700
many ways.

00:10:19.700 --> 00:10:21.830
Newtons method is a popular way to

00:10:21.830 --> 00:10:23.340
solve this problem.

00:10:23.340 --> 00:10:25.255
There are other methods as well, but in

00:10:25.255 --> 00:10:27.590
the end will we're going to get the set

00:10:27.590 --> 00:10:30.250
of beta values once we have the beta

00:10:30.250 --> 00:10:33.900
values, then we have a well defined

00:10:33.900 --> 00:10:37.940
scoring function to help us classify a

00:10:37.940 --> 00:10:39.630
document right?

00:10:39.630 --> 00:10:40.503
So what's the function?

00:10:40.503 --> 00:10:41.510
Well, it's this one.

00:10:42.650 --> 00:10:45.690
If we have all the betavalues

00:10:45.690 --> 00:10:48.050
already known, all we need is to

00:10:48.050 --> 00:10:48.360
compute

00:10:48.360 --> 00:10:50.510
The Xi's for that document.

00:10:51.100 --> 00:10:52.940
And then plugging those values that

00:10:52.940 --> 00:10:54.700
will give us a estimate.

00:10:54.700 --> 00:10:57.070
The probability that the document is in

00:10:57.070 --> 00:10:58.160
category one.

00:10:59.050 --> 00:11:02.630
OK, so much for logistical regression.

00:11:02.630 --> 00:11:04.820
Let's also introduce another

00:11:04.820 --> 00:11:07.110
discriminative classifier called K

00:11:07.110 --> 00:11:08.100
nearest neighbors.

00:11:08.100 --> 00:11:10.030
Now in general, I should say there are

00:11:10.030 --> 00:11:11.980
many such approaches.

00:11:12.530 --> 00:11:15.070
And thorough introduction to all of them

00:11:15.070 --> 00:11:17.306
is clearly beyond the scope of this

00:11:17.306 --> 00:11:20.086
course and you should take a machine

00:11:20.086 --> 00:11:21.362
learning course or read more about

00:11:21.362 --> 00:11:23.360
machine learning to know about them.

00:11:23.360 --> 00:11:25.420
Here, just want to include the basic

00:11:25.420 --> 00:11:27.400
introduction to some of the most

00:11:27.400 --> 00:11:29.310
commonly used classifiers, since you

00:11:29.310 --> 00:11:31.820
might use them often for text

00:11:31.820 --> 00:11:33.050
categorization.

00:11:33.050 --> 00:11:35.670
So the second classifier, is called k

00:11:35.670 --> 00:11:36.530
nearest neighbors.

00:11:36.530 --> 00:11:39.630
In this approach, we're going to also

00:11:39.630 --> 00:11:42.670
estimate the conditional probability of

00:11:42.670 --> 00:11:43.060
label.

00:11:43.110 --> 00:11:44.730
Given data, but in a very different

00:11:44.730 --> 00:11:45.020
way.

00:11:45.020 --> 00:11:48.674
So the idea is to keep all the training

00:11:48.674 --> 00:11:50.664
examples and then once we see a text

00:11:50.664 --> 00:11:53.030
object that we want to classify, we're

00:11:53.030 --> 00:11:55.315
going to find the K examples in the

00:11:55.315 --> 00:11:57.580
training set and that are most similar

00:11:57.580 --> 00:11:59.190
to this text object.

00:11:59.190 --> 00:12:01.470
Basically this is to find the neighbors

00:12:01.470 --> 00:12:04.487
of this text object in the training

00:12:04.487 --> 00:12:05.349
data set.

00:12:05.920 --> 00:12:08.110
So once we found we found the

00:12:08.110 --> 00:12:10.480
neighborhood and found the objects that

00:12:10.480 --> 00:12:11.730
are close to the.

00:12:12.450 --> 00:12:14.300
The object we're interested in

00:12:14.300 --> 00:12:17.490
classifying and say we have found the K

00:12:17.490 --> 00:12:18.370
nearest neighbors.

00:12:18.370 --> 00:12:20.272
That's why this method is called K

00:12:20.272 --> 00:12:21.057
nearest neighbors.

00:12:21.057 --> 00:12:22.850
And then we're going to assign the

00:12:22.850 --> 00:12:24.800
category that's most common in these

00:12:24.800 --> 00:12:25.450
neighbors.

00:12:26.130 --> 00:12:27.650
Basically, we're going to allow these

00:12:27.650 --> 00:12:30.010
neighbors to vote for the category of

00:12:30.010 --> 00:12:31.530
the object that we're interested in

00:12:31.530 --> 00:12:32.160
classifying.

00:12:33.360 --> 00:12:36.670
Now that means if most of them have a

00:12:36.670 --> 00:12:38.190
particular category, lets say category 1

00:12:38.190 --> 00:12:40.330
then we're going to say this current

00:12:40.330 --> 00:12:41.970
object will have category one.

00:12:42.970 --> 00:12:44.300
This approach, can also be improved

00:12:44.300 --> 00:12:46.894
by considering the distance of

00:12:46.894 --> 00:12:48.900
a neighbor and the current object.

00:12:48.900 --> 00:12:51.650
Basically, we can assume a close

00:12:51.650 --> 00:12:53.725
neighbor will have more saying about

00:12:53.725 --> 00:12:56.052
the category of this object, so we can

00:12:56.052 --> 00:12:58.110
have we can give such a neighbor more

00:12:58.110 --> 00:13:01.260
influence on the vote and we can take

00:13:01.260 --> 00:13:03.765
weighted sum of their votes based on

00:13:03.765 --> 00:13:04.700
the distances.

00:13:06.010 --> 00:13:07.600
But the general idea is to look at the

00:13:07.600 --> 00:13:09.850
neighborhood and then try to assess the

00:13:09.850 --> 00:13:11.920
category based on the categories of the

00:13:11.920 --> 00:13:12.520
neighbors.

00:13:13.110 --> 00:13:15.100
Intuitively, this makes a lot of sense.

00:13:15.830 --> 00:13:18.680
But mathematically, this can also be

00:13:18.680 --> 00:13:21.670
regarded as a way to directly estimate

00:13:21.670 --> 00:13:23.400
the conditional probability of label

00:13:23.400 --> 00:13:26.950
given data that is P of Y given X.

00:13:28.040 --> 00:13:29.630
Now I'm going to explain this intuition

00:13:29.630 --> 00:13:32.190
in the moment, but before we proceed,

00:13:32.190 --> 00:13:36.510
let me emphasize that we do need a

00:13:36.510 --> 00:13:38.930
similarity function here in order for

00:13:38.930 --> 00:13:39.650
this work.

00:13:40.200 --> 00:13:42.260
I note that in naive base classifier we

00:13:42.260 --> 00:13:43.990
did not need a similarity function.

00:13:44.650 --> 00:13:46.630
An in logistical regression, we did not

00:13:46.630 --> 00:13:48.110
talk about the similarity function

00:13:48.110 --> 00:13:48.570
either.

00:13:48.570 --> 00:13:50.660
But here we explicitly requires a

00:13:50.660 --> 00:13:51.700
similarity function.

00:13:52.380 --> 00:13:54.309
Now this similarity function.

00:13:54.310 --> 00:13:57.690
Actually is a good opportunity for us

00:13:57.690 --> 00:14:01.210
to inject any of our insights about

00:14:01.210 --> 00:14:02.170
features.

00:14:02.170 --> 00:14:06.110
Basically, effective features are those

00:14:06.110 --> 00:14:09.131
that would make the objects that are in

00:14:09.131 --> 00:14:12.420
the same category look more similar,

00:14:12.420 --> 00:14:15.170
but distinguishing objects in different

00:14:15.170 --> 00:14:16.480
categories.

00:14:16.480 --> 00:14:18.330
So the design of this similarity

00:14:18.330 --> 00:14:20.705
function is closely tied to the design

00:14:20.705 --> 00:14:23.010
of the features in logistic regression.

00:14:23.550 --> 00:14:25.620
and other classifiers, so let's

00:14:25.620 --> 00:14:28.220
illustrate how K-NN works.

00:14:28.220 --> 00:14:30.690
Suppose we have a lot of training

00:14:30.690 --> 00:14:31.670
instances here.

00:14:32.230 --> 00:14:35.346
And I've colored them differently and

00:14:35.346 --> 00:14:38.490
to show just different categories.

00:14:38.490 --> 00:14:40.670
Now suppose we have a new object in the

00:14:40.670 --> 00:14:42.990
center that we want to classify.

00:14:43.550 --> 00:14:45.150
So according to this approach we're going to 

00:14:45.150 --> 00:14:46.380
find the neighbors.

00:14:46.380 --> 00:14:48.610
And let's first think of a special case

00:14:48.610 --> 00:14:51.290
of finding just one neighbor, the closest

00:14:51.290 --> 00:14:51.660
neighbor.

00:14:52.950 --> 00:14:55.000
Now in this case, if the, let's assume

00:14:55.000 --> 00:14:57.830
the closest neighbor is the box filled

00:14:57.830 --> 00:15:01.855
with diamonds and so then we're going to

00:15:01.855 --> 00:15:02.252
say.

00:15:02.252 --> 00:15:06.372
Well, since this is in, this object is

00:15:06.372 --> 00:15:08.030
in category of diamonds.

00:15:08.030 --> 00:15:10.206
Let's say then we're going to say,

00:15:10.206 --> 00:15:12.560
well, we're going to assign the same

00:15:12.560 --> 00:15:15.140
Category to our text object.

00:15:17.130 --> 00:15:18.860
But let's also look at the another

00:15:18.860 --> 00:15:20.490
possibility of finding a larger

00:15:20.490 --> 00:15:21.320
neighborhood.

00:15:21.320 --> 00:15:24.410
So let's think about the four

00:15:24.410 --> 00:15:25.230
neighbors.

00:15:25.920 --> 00:15:27.730
In this case, we're going to include a

00:15:27.730 --> 00:15:30.780
lot of other solid field boxes in red

00:15:30.780 --> 00:15:31.350
or pink.

00:15:32.100 --> 00:15:36.320
So in this case now we  are going to notice that

00:15:36.320 --> 00:15:38.240
among the four neighbors there are actually

00:15:38.240 --> 00:15:40.496
three neighbors in a different

00:15:40.496 --> 00:15:40.993
category.

00:15:40.993 --> 00:15:43.530
So if we take a vote, then we'll

00:15:43.530 --> 00:15:46.610
conclude the object is actually of a

00:15:46.610 --> 00:15:47.570
different category.

00:15:47.570 --> 00:15:50.230
So this both illustrates how K

00:15:50.230 --> 00:15:52.700
nearest neighbor works and also

00:15:52.700 --> 00:15:55.925
illustrates some potential problems of

00:15:55.925 --> 00:15:56.650
this classifier.

00:15:56.650 --> 00:15:58.405
Basically the results might depend on

00:15:58.405 --> 00:16:00.950
the K and indeed K is an important

00:16:00.950 --> 00:16:02.290
parameter to optimize.

00:16:03.880 --> 00:16:07.533
Now you can intuitively imagine if we

00:16:07.533 --> 00:16:10.000
have a lot of neighbors around this

00:16:10.000 --> 00:16:13.221
object and then we'll be OK because we

00:16:13.221 --> 00:16:14.790
have a lot of neighbors for help us

00:16:14.790 --> 00:16:16.100
decide categories.

00:16:16.100 --> 00:16:19.110
But if we have only a few, then the

00:16:19.110 --> 00:16:20.950
decision may not be reliable.

00:16:20.950 --> 00:16:24.110
So on the one hand we want to find more

00:16:24.110 --> 00:16:24.940
neighbors right?

00:16:24.940 --> 00:16:27.220
And then we have more votes, but on the

00:16:27.220 --> 00:16:29.483
other hand as we try to find the more

00:16:29.483 --> 00:16:32.070
neighbors, we actually could risk on

00:16:32.070 --> 00:16:33.930
getting neighbors that are not

00:16:33.980 --> 00:16:37.080
really similar to this instance, they

00:16:37.080 --> 00:16:38.970
might be actually far away as you try

00:16:38.970 --> 00:16:40.756
to get more neighbors, so although you

00:16:40.756 --> 00:16:42.290
get more neighbors to vote, but those

00:16:42.290 --> 00:16:45.010
neighbors aren't necessary so helpful

00:16:45.010 --> 00:16:46.520
because they are not very similar to

00:16:46.520 --> 00:16:47.270
the object.

00:16:47.270 --> 00:16:50.430
So the parameter as there has to be set

00:16:50.430 --> 00:16:53.110
empirically and typically you can

00:16:53.110 --> 00:16:54.760
optimize such a parameter by using

00:16:54.760 --> 00:16:56.090
cross validation.

00:16:56.090 --> 00:16:57.550
Basically, you're going to separate

00:16:57.550 --> 00:17:01.160
your training data into two parts and

00:17:01.160 --> 00:17:03.580
then you're going to use one part to

00:17:03.580 --> 00:17:05.060
actually help you choose.

00:17:05.270 --> 00:17:05.950
The

00:17:06.740 --> 00:17:09.000
The parameter K here or some other

00:17:09.000 --> 00:17:11.080
parameters in other classifiers, and

00:17:11.080 --> 00:17:13.870
then you're going to assume this number

00:17:13.870 --> 00:17:15.550
that works well on your training set

00:17:15.550 --> 00:17:20.050
would be actually the best for your

00:17:20.050 --> 00:17:20.860
future data.

00:17:23.160 --> 00:17:25.500
So as I mentioned that KNN can be

00:17:25.500 --> 00:17:28.160
actually regarded as estimate of

00:17:28.160 --> 00:17:30.090
conditional probability of Y given X,

00:17:30.090 --> 00:17:31.610
and that's why we put this in the

00:17:31.610 --> 00:17:33.750
category of discriminative approaches.

00:17:34.460 --> 00:17:36.790
So the key assumption that we made

00:17:36.790 --> 00:17:38.560
in this approach is that the

00:17:38.560 --> 00:17:41.083
distribution of the label given the

00:17:41.083 --> 00:17:43.313
document or probability of a category

00:17:43.313 --> 00:17:44.199
given document.

00:17:44.850 --> 00:17:47.630
For example, probability of theta I

00:17:47.630 --> 00:17:51.940
given document D is locally smoothed and

00:17:51.940 --> 00:17:53.966
that just means we're going to assume

00:17:53.966 --> 00:17:56.850
that this probability is the same for

00:17:56.850 --> 00:17:59.120
all the documents in this region.

00:17:59.120 --> 00:18:00.760
R  here.

00:18:01.440 --> 00:18:03.290
And suppose we draw a neighborhood and

00:18:03.290 --> 00:18:04.925
we're going to assume in this

00:18:04.925 --> 00:18:06.702
neighborhood, since the data instances

00:18:06.702 --> 00:18:08.835
 are very similar, we're going

00:18:08.835 --> 00:18:11.130
to assume that the conditional

00:18:11.130 --> 00:18:13.066
distribution of the label, given the

00:18:13.066 --> 00:18:15.000
data, would be roughly the same.

00:18:15.000 --> 00:18:17.450
If D is not different, very different

00:18:17.450 --> 00:18:19.859
than we're going to assume that the

00:18:19.860 --> 00:18:22.240
probability of theta given D would be

00:18:22.240 --> 00:18:24.430
also similar, and so that's a very key

00:18:24.430 --> 00:18:26.590
assumption, and that that's.

00:18:27.880 --> 00:18:29.870
That's actually important assumption

00:18:29.870 --> 00:18:32.290
that would allow us to do a lot of

00:18:32.290 --> 00:18:34.390
machine learning, but in reality,

00:18:34.390 --> 00:18:36.140
whether this is true of course would

00:18:36.140 --> 00:18:38.180
depend on how we define similarity,

00:18:38.180 --> 00:18:39.990
because the neighborhood is largely

00:18:39.990 --> 00:18:42.089
determined by our similarity function.

00:18:42.090 --> 00:18:44.180
If our similarity function captures

00:18:44.180 --> 00:18:46.820
objects that do follow similar

00:18:46.820 --> 00:18:49.190
distributions, then this assumption is

00:18:49.190 --> 00:18:49.720
OK.

00:18:49.720 --> 00:18:51.820
But if our similarity function could

00:18:51.820 --> 00:18:52.800
not capture that.

00:18:52.800 --> 00:18:54.570
Obviously the assumption would be a

00:18:54.570 --> 00:18:56.113
problem, and then the classifier would

00:18:56.113 --> 00:18:56.940
not be accurate.

00:18:59.270 --> 00:19:01.600
Let's proceed with this assumption.

00:19:01.600 --> 00:19:03.420
Then what we are saying is that in

00:19:03.420 --> 00:19:05.992
order to estimate the probability of a

00:19:05.992 --> 00:19:08.410
category given a document, we can try

00:19:08.410 --> 00:19:11.350
to estimate the probability of the

00:19:11.350 --> 00:19:14.070
category given that entire region.

00:19:14.070 --> 00:19:17.010
Now this has the benefit of course, of

00:19:17.010 --> 00:19:18.820
bringing additional data points to help

00:19:18.820 --> 00:19:20.410
us estimate this probability.

00:19:21.130 --> 00:19:23.650
And so this is precise idea of KNN.

00:19:23.650 --> 00:19:27.150
Basically now we can use the known

00:19:27.150 --> 00:19:29.800
categories of all the documents in this

00:19:29.800 --> 00:19:31.700
region to estimate this probability.

00:19:33.720 --> 00:19:36.390
And I have even given a formula here

00:19:36.390 --> 00:19:38.920
where you can see we just count the

00:19:38.920 --> 00:19:41.299
topics in this region and then

00:19:41.300 --> 00:19:43.256
normalize that by the total number of

00:19:43.256 --> 00:19:44.789
documents in the region.

00:19:44.790 --> 00:19:47.584
So the numerator that you see here c

00:19:47.584 --> 00:19:50.798
of Theta and R is a count of the

00:19:50.798 --> 00:19:53.950
documents in region R with category

00:19:53.950 --> 00:19:54.870
Theta I.

00:19:54.870 --> 00:19:56.520
Since these are training documents, we

00:19:56.520 --> 00:19:57.660
know they're categories.

00:19:57.660 --> 00:19:59.471
We can simply count how many times we

00:19:59.471 --> 00:20:01.298
have seen sports here, how many times

00:20:01.298 --> 00:20:03.220
we have seen science etc.

00:20:03.220 --> 00:20:04.340
And then

00:20:04.390 --> 00:20:06.556
denominator is just a total number of

00:20:06.556 --> 00:20:07.039
documents

00:20:07.040 --> 00:20:09.380
training documents in this region, so

00:20:09.380 --> 00:20:11.400
this gives us a rough estimate of which

00:20:11.400 --> 00:20:12.949
category is most popular in this

00:20:12.950 --> 00:20:14.620
neighborhood, and we're going to assign

00:20:14.620 --> 00:20:16.590
the popular category to our data

00:20:16.590 --> 00:20:18.995
objective since it falls into this

00:20:18.995 --> 00:20:19.330
region.


