WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-03-29T11:25:44.1582307Z by ClassTranscribe

00:00:00.290 --> 00:00:03.180
In general, we can use the empirical

00:00:03.180 --> 00:00:06.710
counts of events in the observed data

00:00:06.710 --> 00:00:08.250
to estimate probabilities.

00:00:15.210 --> 00:00:17.140
and a commonly used technique is called

00:00:17.140 --> 00:00:19.400
a maximum likelihood estimate, where we

00:00:19.400 --> 00:00:21.860
simply normalize the observed accounts.

00:00:22.450 --> 00:00:25.909
So if we do that, we can see we can

00:00:25.910 --> 00:00:29.890
compute these probabilities as follows

00:00:29.890 --> 00:00:33.135
for estimating the probability that we

00:00:33.135 --> 00:00:37.830
see a word occurring in segment, we

00:00:37.830 --> 00:00:41.280
simply normalize the counts of segments

00:00:41.280 --> 00:00:42.530
that contain this word.

00:00:42.530 --> 00:00:45.980
So let's first take a look at the data

00:00:45.980 --> 00:00:46.360
here.

00:00:47.250 --> 00:00:50.540
On the right side you see I listed some

00:00:50.540 --> 00:00:53.570
hypothesizes that data these are

00:00:53.570 --> 00:00:54.300
segments.

00:00:54.870 --> 00:00:57.770
And in some segments you see both words

00:00:57.770 --> 00:00:58.200
occur.

00:00:58.200 --> 00:01:00.230
Their indicator as once for both

00:01:00.230 --> 00:01:00.900
columns.

00:01:01.490 --> 00:01:03.320
In some other cases, only one word

00:01:03.320 --> 00:01:05.485
occurs, so only that column has one and

00:01:05.485 --> 00:01:07.250
the other column has zero.

00:01:07.250 --> 00:01:10.160
And of course in some other cases, none

00:01:10.160 --> 00:01:12.220
of the words occur, so they are both

00:01:12.220 --> 00:01:12.940
zeros.

00:01:13.780 --> 00:01:14.480
And

00:01:15.130 --> 00:01:17.400
For estimating these probabilities, we

00:01:17.400 --> 00:01:18.830
simply need to collect the three

00:01:18.830 --> 00:01:19.370
counts.

00:01:20.230 --> 00:01:22.733
So the three counts of 1st, the count

00:01:22.733 --> 00:01:23.451
of W.

00:01:23.451 --> 00:01:24.970
1 and that's the total number of

00:01:24.970 --> 00:01:27.409
segments that contain world W one.

00:01:27.410 --> 00:01:30.676
It's just the ones in the column of W

00:01:30.676 --> 00:01:33.129
one we can just count how many ones we

00:01:33.130 --> 00:01:33.890
have seen there.

00:01:33.890 --> 00:01:37.424
The second counter is for word 2 and we

00:01:37.424 --> 00:01:39.415
just count the ones in the second

00:01:39.415 --> 00:01:39.699
column.

00:01:40.330 --> 00:01:42.240
And these this would give us a total

00:01:42.240 --> 00:01:44.730
number of segments that contain W2.

00:01:45.390 --> 00:01:48.670
The third account is when both words

00:01:48.670 --> 00:01:51.380
occurred, so this is time we're going

00:01:51.380 --> 00:01:53.510
to count the segments where both

00:01:53.510 --> 00:01:55.430
columns have ones.

00:01:56.050 --> 00:01:58.444
And then so this would give us the

00:01:58.444 --> 00:02:00.670
total number of segments where we have

00:02:00.670 --> 00:02:03.130
seen both W and W2.

00:02:03.130 --> 00:02:06.010
Once we have these counts, we can just

00:02:06.010 --> 00:02:06.580
normalize.

00:02:06.580 --> 00:02:09.251
These counts by n, which is the total

00:02:09.251 --> 00:02:11.156
number of segments and this will give

00:02:11.156 --> 00:02:13.330
us the probabilities that we need to

00:02:13.330 --> 00:02:14.860
compute mutual information.

00:02:15.810 --> 00:02:18.690
Now there is a small problem.

00:02:18.690 --> 00:02:21.182
When we have zero counts sometimes and

00:02:21.182 --> 00:02:23.943
in this case we don't want a zero

00:02:23.943 --> 00:02:26.310
probability because our data maybe a

00:02:26.310 --> 00:02:29.280
small sample and in general we would

00:02:29.280 --> 00:02:32.210
believe that it's potentially possible

00:02:32.210 --> 00:02:34.460
for award to occur in any context.

00:02:34.460 --> 00:02:36.970
So to address this problem we can use a

00:02:36.970 --> 00:02:39.170
technique called smoothing and that's

00:02:39.170 --> 00:02:41.630
basically to add some small constant to

00:02:41.630 --> 00:02:45.050
discounts and then so that we don't get

00:02:45.050 --> 00:02:47.069
a zero probability in any case.

00:02:48.380 --> 00:02:49.740
Now, the best way to understand the

00:02:49.740 --> 00:02:52.390
smoothing is imagine that we actually.

00:02:52.940 --> 00:02:55.400
Observed more data than we actually

00:02:55.400 --> 00:02:55.733
have.

00:02:55.733 --> 00:02:58.980
We will pretend we observe some pseudo

00:02:58.980 --> 00:03:01.320
segments that are illustrated on the

00:03:01.320 --> 00:03:04.540
top on the right side of the slide and

00:03:04.540 --> 00:03:07.950
these pseudo segments would contribute

00:03:07.950 --> 00:03:10.740
additional counts of these words so

00:03:10.740 --> 00:03:12.630
that no event will have zero

00:03:12.630 --> 00:03:14.090
probability probability.

00:03:15.060 --> 00:03:16.630
Now, in particular, we introduce the

00:03:16.630 --> 00:03:18.320
four pseudo segments.

00:03:18.320 --> 00:03:20.340
Each is weighted 1/4.

00:03:21.080 --> 00:03:23.200
And these represent the four different

00:03:23.200 --> 00:03:25.120
combinations of occurrences of these

00:03:25.120 --> 00:03:25.670
words.

00:03:25.670 --> 00:03:29.200
So now each event, each combination

00:03:29.200 --> 00:03:32.210
will have at least one count or at

00:03:32.210 --> 00:03:33.350
least non zero counter.

00:03:33.350 --> 00:03:34.975
From these pseudo segment.

00:03:34.975 --> 00:03:38.205
So in the actual segments that we

00:03:38.205 --> 00:03:40.490
observed, it's OK if we haven't

00:03:40.490 --> 00:03:42.150
observed all the combinations.

00:03:44.150 --> 00:03:46.635
So more specifically, you can see the

00:03:46.635 --> 00:03:47.750
point of five.

00:03:47.750 --> 00:03:50.830
Here actually comes from the two ones

00:03:50.830 --> 00:03:53.420
in the two pseudo segments, because

00:03:53.420 --> 00:03:56.400
each is weighted 1/4, we added them up.

00:03:56.400 --> 00:03:57.570
We get .5.

00:03:59.410 --> 00:04:03.060
And similarly this .05 comes from one

00:04:03.060 --> 00:04:06.510
single pseudo segment that indicates

00:04:06.510 --> 00:04:08.400
the two words occur together.

00:04:09.320 --> 00:04:11.280
And of course, in the denominator we

00:04:11.280 --> 00:04:13.760
add the total number of pseudo segments

00:04:13.760 --> 00:04:14.363
that we added.

00:04:14.363 --> 00:04:17.115
In this case we added a 4th through the

00:04:17.115 --> 00:04:17.460
segments.

00:04:17.460 --> 00:04:19.820
Each is weighted 1/4, so the total the

00:04:19.820 --> 00:04:21.570
sum is actually one.

00:04:21.570 --> 00:04:23.370
So that's why in the denominator you

00:04:23.370 --> 00:04:24.340
still want there.

00:04:25.870 --> 00:04:27.730
So this basically concludes the

00:04:27.730 --> 00:04:29.650
discussion of how to compute the mutual

00:04:29.650 --> 00:04:32.010
information, how to use this for

00:04:32.010 --> 00:04:33.990
syntagmatic relation discovery.

00:04:35.480 --> 00:04:35.800
No.

00:04:35.800 --> 00:04:40.160
So, to summarize, select the cinematic

00:04:40.160 --> 00:04:42.550
relation can generally be discovered by

00:04:42.550 --> 00:04:44.040
measuring correlations between

00:04:44.040 --> 00:04:45.465
occurrences of two words.

00:04:45.465 --> 00:04:47.810
We introduce the three concepts from

00:04:47.810 --> 00:04:49.760
information theory, entropy, which

00:04:49.760 --> 00:04:51.810
meshes uncertainly over random variable

00:04:51.810 --> 00:04:55.310
X conditional entropy, which measures

00:04:55.310 --> 00:04:56.320
the entropy of X.

00:04:56.320 --> 00:04:58.115
Given we know why.

00:04:58.115 --> 00:05:01.200
And mutual information of X&amp;Y which

00:05:01.200 --> 00:05:03.440
matches the entropy reduction of X.

00:05:04.440 --> 00:05:07.930
Due to knowing why or entropy reduction

00:05:07.930 --> 00:05:10.540
of why do too knowing eggs?

00:05:11.140 --> 00:05:13.410
They are the same, so these three

00:05:13.410 --> 00:05:15.349
concepts are actually very useful for

00:05:15.350 --> 00:05:17.250
other applications as well.

00:05:17.250 --> 00:05:18.760
That's why we spend some time to

00:05:18.760 --> 00:05:20.550
explain this in detail, but in

00:05:20.550 --> 00:05:23.130
particular there also very useful for

00:05:23.130 --> 00:05:25.080
discovering syntagmatic relations.

00:05:25.650 --> 00:05:29.510
In particular, mutual information is a

00:05:29.510 --> 00:05:32.015
principled way for discovering such a

00:05:32.015 --> 00:05:32.430
relation.

00:05:32.430 --> 00:05:35.770
It allows us to have values computer on

00:05:35.770 --> 00:05:38.360
different pairs of words that are

00:05:38.360 --> 00:05:40.980
comfortable, and so we can rank these

00:05:40.980 --> 00:05:43.150
pairs and discover the strongest

00:05:43.150 --> 00:05:45.510
cinematical relationship from

00:05:45.510 --> 00:05:47.410
collection of documents.

00:05:47.410 --> 00:05:50.210
Now note that there is some relation

00:05:50.210 --> 00:05:52.230
between syntactic medical relation

00:05:52.230 --> 00:05:55.020
discovery and paradigmatically relation

00:05:55.020 --> 00:05:55.520
discovery.

00:05:55.850 --> 00:05:57.880
So we already discussed the possibility

00:05:57.880 --> 00:06:02.870
of using BM 25 to achieve waiting for

00:06:02.870 --> 00:06:05.940
terms in the context to potentially

00:06:05.940 --> 00:06:08.220
also suggest the candidates that have

00:06:08.220 --> 00:06:10.130
seen like medical relations with the

00:06:10.130 --> 00:06:11.170
candidate word.

00:06:11.170 --> 00:06:13.810
But here, once we use mutual

00:06:13.810 --> 00:06:15.483
information to discover Syntagmatic

00:06:15.483 --> 00:06:19.410
relations, we can also represent the

00:06:19.410 --> 00:06:23.260
context with this mutual information as

00:06:23.260 --> 00:06:23.860
weights.

00:06:24.420 --> 00:06:28.010
So this would give us another way to

00:06:28.010 --> 00:06:29.650
represent the context.

00:06:30.200 --> 00:06:33.100
Of a word like a cat, and if we do the

00:06:33.100 --> 00:06:34.740
same for all the words, then we can

00:06:34.740 --> 00:06:36.740
cluster these words or computer

00:06:36.740 --> 00:06:39.290
similarity between these words based on

00:06:39.290 --> 00:06:41.000
their context similarity.

00:06:41.000 --> 00:06:43.750
So this provides yet another way to do

00:06:43.750 --> 00:06:45.460
term waiting for paradigmatic.

00:06:45.460 --> 00:06:48.060
A relation discovery an.

00:06:52.120 --> 00:06:54.270
So to summarize, this whole part about

00:06:54.270 --> 00:06:56.090
word Association mining, we introduce

00:06:56.090 --> 00:06:58.080
the two basic associations, called

00:06:58.080 --> 00:07:00.780
Paradigmatic and Syntagmatic relations.

00:07:00.780 --> 00:07:01.857
These are fairly general.

00:07:01.857 --> 00:07:04.370
They can be applied to any items in any

00:07:04.370 --> 00:07:06.960
language, so the units don't have to be

00:07:06.960 --> 00:07:09.460
worse than they can be phrases or

00:07:09.460 --> 00:07:10.090
entities.

00:07:10.940 --> 00:07:13.560
Are we introduced multiple statistical

00:07:13.560 --> 00:07:15.170
approaches for discovering them?

00:07:15.170 --> 00:07:17.895
Then it showing that pure statistical

00:07:17.895 --> 00:07:19.800
approaches are visible?

00:07:21.300 --> 00:07:24.070
Available for discovering both kinds of

00:07:24.070 --> 00:07:26.040
relations, and they can be combined to

00:07:26.040 --> 00:07:26.470
perform.

00:07:26.470 --> 00:07:28.030
Join the analysis as well.

00:07:28.670 --> 00:07:31.610
These approaches can be applied to any

00:07:31.610 --> 00:07:33.960
text with no helmet human effort.

00:07:34.680 --> 00:07:36.180
And mostly becausw.

00:07:36.180 --> 00:07:38.920
They are based on counting of words.

00:07:39.480 --> 00:07:41.140
Yet they can actually discover

00:07:41.140 --> 00:07:42.750
interesting relations of words.

00:07:43.400 --> 00:07:45.200
We can also use different ways to

00:07:45.200 --> 00:07:47.470
define context and segment and this

00:07:47.470 --> 00:07:48.860
would lead to some interesting

00:07:48.860 --> 00:07:50.440
variations of applications.

00:07:50.440 --> 00:07:52.520
For example, the context can be very

00:07:52.520 --> 00:07:54.860
narrow, like a few words around a word

00:07:54.860 --> 00:07:58.740
or sentence or maybe paragraphs and

00:07:58.740 --> 00:08:00.450
using different contexts, which allows

00:08:00.450 --> 00:08:02.220
you to discover different flavors of

00:08:02.220 --> 00:08:03.900
paradigmatic relations.

00:08:05.170 --> 00:08:08.040
And similarly, counting Co occurrences

00:08:08.040 --> 00:08:11.430
using, let's say mutual information to

00:08:11.430 --> 00:08:14.260
discover syntagmatic relations, we also

00:08:14.260 --> 00:08:16.144
have to define the segment and the

00:08:16.144 --> 00:08:18.605
segment can be defined as an arrow,

00:08:18.605 --> 00:08:23.190
text window or longer text article and

00:08:23.190 --> 00:08:24.900
this would give us different kinds of

00:08:24.900 --> 00:08:25.730
associations.

00:08:26.570 --> 00:08:29.170
These discovery associations can

00:08:29.170 --> 00:08:29.980
support them.

00:08:29.980 --> 00:08:32.530
Any other applications in both

00:08:32.530 --> 00:08:34.640
information retrieval and text data

00:08:34.640 --> 00:08:35.130
mining.

00:08:37.780 --> 00:08:40.240
So here are some recommended readings.

00:08:41.250 --> 00:08:42.820
If you want to know more about the

00:08:42.820 --> 00:08:45.280
topic, the 1st is a book with a chapter

00:08:45.280 --> 00:08:47.485
on locations which is quite relevant to

00:08:47.485 --> 00:08:49.430
the topic of these lectures.

00:08:50.710 --> 00:08:52.890
The second is the article about the

00:08:52.890 --> 00:08:56.200
using various statistical measures to

00:08:56.200 --> 00:08:58.060
discover lexical atoms.

00:08:58.060 --> 00:09:01.520
Those are phrases that are non

00:09:01.520 --> 00:09:04.280
composition compositional or for

00:09:04.280 --> 00:09:07.070
example hot dog is not really a dog

00:09:07.070 --> 00:09:07.810
that's hot.

00:09:08.420 --> 00:09:11.440
Blue chip is not a chip that's blue,

00:09:11.440 --> 00:09:13.470
and the paper has a discussion about to

00:09:13.470 --> 00:09:15.500
some techniques for discovering such

00:09:15.500 --> 00:09:16.210
phrases.

00:09:17.260 --> 00:09:21.630
The third one is new paper on unified

00:09:21.630 --> 00:09:24.140
way to discover both paradigmatic a

00:09:24.140 --> 00:09:26.930
relation and select medical relations

00:09:26.930 --> 00:09:29.410
using random walks on world graphs.


