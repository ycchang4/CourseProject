WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-27T00:00:20.5112033Z by ClassTranscribe

00:00:00.300 --> 00:00:02.350
This lecture is about the latent

00:00:02.350 --> 00:00:04.290
Dirichlet allocation or LDA.

00:00:13.700 --> 00:00:14.990
In this lecture, we're going to

00:00:14.990 --> 00:00:17.160
continue talking about topic

00:00:17.160 --> 00:00:17.840
models.

00:00:17.840 --> 00:00:18.720
In particular,

00:00:18.720 --> 00:00:20.970
we are going to talk about some extensions of

00:00:20.970 --> 00:00:26.830
PLSA, and one of them is LDA or latent

00:00:26.830 --> 00:00:27.910
Dirichlet allocation.

00:00:30.670 --> 00:00:32.950
So the plan for this lecture is to

00:00:32.950 --> 00:00:33.720
cover two things.

00:00:33.720 --> 00:00:37.150
One is to extend the PLSA with prior

00:00:37.150 --> 00:00:39.190
knowledge that would allow us to have

00:00:39.190 --> 00:00:42.030
in some sense a user controlled PLSA, so

00:00:42.030 --> 00:00:44.090
it doesn't blindly just listen to data

00:00:44.090 --> 00:00:46.940
but also would listen to our needs.

00:00:46.940 --> 00:00:50.370
The second is to extend the PLSA as a

00:00:50.370 --> 00:00:54.346
generative model fully generated model.

00:00:54.346 --> 00:00:57.990
This has led to the development of

00:00:57.990 --> 00:01:00.400
Latent Dirichlet Allocation or LDA.

00:01:01.120 --> 00:01:03.200
So first let's talk about the PLSA

00:01:03.200 --> 00:01:04.130
with prior knowledge.

00:01:04.970 --> 00:01:07.760
In practice, when we apply PLSA to

00:01:07.760 --> 00:01:10.330
analyze text data, we might have

00:01:10.330 --> 00:01:11.710
additional knowledge that we want to

00:01:11.710 --> 00:01:13.890
inject to guide the analysis.

00:01:14.620 --> 00:01:17.440
The standard PLSA is going to blindly

00:01:17.440 --> 00:01:19.750
listen to the data by using maximum

00:01:19.750 --> 00:01:20.690
likelihood estimator.

00:01:20.690 --> 00:01:23.470
We are going to just fit data as much as we can

00:01:23.470 --> 00:01:25.710
and get some insight about data.

00:01:25.710 --> 00:01:28.260
This is also very useful, but sometimes

00:01:28.260 --> 00:01:30.470
a user might have some expectations

00:01:30.470 --> 00:01:32.200
about which topics to analyze.

00:01:32.200 --> 00:01:34.415
For example, we might expect to see

00:01:34.415 --> 00:01:36.050
retrieval models as a topic in

00:01:36.050 --> 00:01:37.010
information retrieval.

00:01:37.880 --> 00:01:40.410
We also may be interested in certain

00:01:40.410 --> 00:01:43.800
aspects such as battery and memory when

00:01:43.800 --> 00:01:45.700
looking at the opinions about the

00:01:45.700 --> 00:01:47.420
laptop, because the user is

00:01:47.420 --> 00:01:48.720
particularly interested in these

00:01:48.720 --> 00:01:49.360
aspects.

00:01:50.840 --> 00:01:52.380
Now, a user may also have knowledge

00:01:52.380 --> 00:01:53.780
about the topic coverage.

00:01:54.510 --> 00:01:56.610
And we may know which topic is

00:01:56.610 --> 00:01:58.158
definitely not covered in which

00:01:58.158 --> 00:02:00.140
document or is covered in the document.

00:02:00.140 --> 00:02:03.490
For example, we might have seen those

00:02:03.490 --> 00:02:05.520
tags topic tags assigned to

00:02:05.520 --> 00:02:06.230
documents.

00:02:06.810 --> 00:02:09.245
And those tag could be treated as

00:02:09.245 --> 00:02:11.780
topics if we do that, then a document

00:02:11.780 --> 00:02:13.895
that can only be generated using topics

00:02:13.895 --> 00:02:15.720
corresponding to the tags already

00:02:15.720 --> 00:02:16.920
assigned to the document.

00:02:16.920 --> 00:02:19.260
If the document is not assigned to a tag,

00:02:19.260 --> 00:02:21.370
we're going to say there's no way for

00:02:21.370 --> 00:02:25.585
using that topic to generate the

00:02:25.585 --> 00:02:26.010
document.

00:02:26.010 --> 00:02:28.271
The document must be generated by using

00:02:28.271 --> 00:02:30.627
the topics corresponding to the

00:02:30.627 --> 00:02:32.080
assigned tags.

00:02:32.080 --> 00:02:33.580
So the question is, how can we

00:02:33.580 --> 00:02:35.140
incorporate such knowledge into

00:02:35.140 --> 00:02:35.810
PLSA?

00:02:35.810 --> 00:02:37.399
It turns out that there's a.

00:02:37.450 --> 00:02:39.840
A very elegant way of doing that, and

00:02:39.840 --> 00:02:41.470
that's all incorporated such knowledge

00:02:41.470 --> 00:02:43.770
as priors on the models.

00:02:44.390 --> 00:02:46.360
And you may recall in Bayesian

00:02:46.360 --> 00:02:49.460
inference we use prior together with

00:02:49.460 --> 00:02:51.950
data to estimate parameters, and this

00:02:51.950 --> 00:02:53.300
is precisely what will happen.

00:02:54.160 --> 00:02:57.280
So in this case we can use maximum a

00:02:57.280 --> 00:03:00.487
posteriori estimate, also called map

00:03:00.487 --> 00:03:02.590
estimate, and the formula is given

00:03:02.590 --> 00:03:02.820
here.

00:03:02.820 --> 00:03:04.630
Basically is to maximize the posterior

00:03:04.630 --> 00:03:06.940
distribution probability and this is a

00:03:06.940 --> 00:03:09.313
combination of the likelihood of data

00:03:09.313 --> 00:03:10.458
and the prior.

00:03:10.458 --> 00:03:12.240
So what would happen is that we're

00:03:12.240 --> 00:03:14.766
going to have an estimate that listens to

00:03:14.766 --> 00:03:18.010
the data and also listens to our prior

00:03:18.010 --> 00:03:19.010
preferences.

00:03:19.010 --> 00:03:21.570
We can use this prior, which is denoted

00:03:21.570 --> 00:03:24.330
as P of Lambda to encode.

00:03:24.380 --> 00:03:26.940
All kinds of preferences and

00:03:26.940 --> 00:03:27.700
constraints.

00:03:27.700 --> 00:03:30.830
So for example, we can use this to

00:03:30.830 --> 00:03:34.720
encode the need of having precisely 1

00:03:34.720 --> 00:03:35.730
background the topic.

00:03:36.340 --> 00:03:39.559
Now this can be encoded as a prior

00:03:39.560 --> 00:03:42.820
because we can say the prior for the

00:03:42.820 --> 00:03:47.469
parameters is only a non zero if the

00:03:47.470 --> 00:03:50.260
plan does contain one topic that's

00:03:50.260 --> 00:03:52.252
equivalent to the background language

00:03:52.252 --> 00:03:52.595
model.

00:03:52.595 --> 00:03:55.480
In other words, in other cases if it's

00:03:55.480 --> 00:03:56.909
not like that, we're going to say

00:03:56.910 --> 00:03:59.210
supplier says it's impossible.

00:03:59.210 --> 00:04:02.065
So the probability of that kind of

00:04:02.065 --> 00:04:05.040
model setting would be 0 according to

00:04:05.040 --> 00:04:05.790
our prior.

00:04:07.920 --> 00:04:11.590
So now we can also, for example use the

00:04:11.590 --> 00:04:15.340
prior to force particular choice of

00:04:15.340 --> 00:04:18.660
topic to have a probability of a

00:04:18.660 --> 00:04:19.240
certain number.

00:04:19.240 --> 00:04:21.550
For example, we can force the document

00:04:21.550 --> 00:04:24.510
D to choose topic one with

00:04:24.510 --> 00:04:26.949
probability of 1/2.

00:04:27.500 --> 00:04:31.420
Or we can prevent a topic from being

00:04:31.420 --> 00:04:33.109
used in generated document.

00:04:33.110 --> 00:04:34.950
So we can say the third topic should

00:04:34.950 --> 00:04:36.256
not be user generated.

00:04:36.256 --> 00:04:39.110
Document D will set to the Pi value to

00:04:39.110 --> 00:04:40.670
0 for that topic.

00:04:41.680 --> 00:04:44.080
We can also use the prior to favor set

00:04:44.080 --> 00:04:46.210
of parameters with topics that assign

00:04:46.210 --> 00:04:48.060
high probabilities to some particular

00:04:48.060 --> 00:04:48.510
words.

00:04:48.510 --> 00:04:50.040
In this case, we're not going to say

00:04:50.040 --> 00:04:51.550
it's impossible, but we're going to

00:04:51.550 --> 00:04:53.830
just strongly favor certain kind of

00:04:53.830 --> 00:04:54.670
distributions.

00:04:55.540 --> 00:04:57.180
And you will see example later.

00:04:57.180 --> 00:05:00.130
The map can be computed using a similar

00:05:00.130 --> 00:05:02.570
EM algorithm as we have used for that

00:05:02.570 --> 00:05:05.050
maximum likelihood estimator with just

00:05:05.050 --> 00:05:06.620
some modification to smallest

00:05:06.620 --> 00:05:08.540
parameters reflect the prior

00:05:08.540 --> 00:05:09.560
preferences.

00:05:10.260 --> 00:05:12.820
And in such a estimate, if we use a

00:05:12.820 --> 00:05:14.810
special form of the prior called

00:05:14.810 --> 00:05:17.506
conjugate prior, then the functional

00:05:17.506 --> 00:05:19.628
form of the prior will be similar to

00:05:19.628 --> 00:05:20.104
the data.

00:05:20.104 --> 00:05:22.812
As a result, we can combine the two and

00:05:22.812 --> 00:05:26.570
the consequences that you can basically

00:05:26.570 --> 00:05:29.510
convert the influence of the prior into

00:05:29.510 --> 00:05:32.800
the influence of having additional

00:05:32.800 --> 00:05:35.379
pseudo data because the two functional

00:05:35.380 --> 00:05:37.180
forms are the same and they can be

00:05:37.180 --> 00:05:37.770
combined.

00:05:37.770 --> 00:05:40.290
So the effect is as if we had more

00:05:40.290 --> 00:05:40.710
data.

00:05:41.160 --> 00:05:44.390
And this is convenient for computation.

00:05:44.390 --> 00:05:46.870
It doesn't mean conjugate prior is the

00:05:46.870 --> 00:05:48.480
best way to define the prior.

00:05:50.030 --> 00:05:52.380
So now let's look at the specific

00:05:52.380 --> 00:05:52.850
example.

00:05:52.850 --> 00:05:55.500
Suppose the user is particularly

00:05:55.500 --> 00:05:57.460
interested in battery life of a laptop,

00:05:57.460 --> 00:05:59.100
and we're analyzing reviews.

00:05:59.100 --> 00:06:02.170
So the prior says that the distribution

00:06:02.170 --> 00:06:04.790
should contain one distribution that

00:06:04.790 --> 00:06:06.930
would assign high probabilities to

00:06:06.930 --> 00:06:08.659
battery, and life.

00:06:09.470 --> 00:06:12.140
So we could do say there's a

00:06:12.140 --> 00:06:14.150
distribution that's entirely

00:06:14.150 --> 00:06:16.280
concentrated on battery life and we all

00:06:16.280 --> 00:06:18.530
priors is that one of your distributions

00:06:18.530 --> 00:06:20.549
should be very similar to this.

00:06:20.550 --> 00:06:23.790
Now if we use map estimator with the

00:06:23.790 --> 00:06:27.208
conjugated prior, which is Dirichlet

00:06:27.208 --> 00:06:30.770
prior Dirichlet distribution based on

00:06:30.770 --> 00:06:33.840
this preference, then the only

00:06:33.840 --> 00:06:35.642
difference in the EM algorithm is in

00:06:35.642 --> 00:06:36.520
the M step.

00:06:36.520 --> 00:06:38.535
When we re estimate word distributions,

00:06:38.535 --> 00:06:40.160
we are going to add.

00:06:40.220 --> 00:06:44.650
Additional counts to reflect our prior

00:06:44.650 --> 00:06:44.970
right?

00:06:44.970 --> 00:06:49.119
So here you can see the pseudocounts

00:06:49.120 --> 00:06:52.083
are defined the based on the

00:06:52.083 --> 00:06:54.150
probability of words in our prior.

00:06:54.150 --> 00:06:56.827
So battery obviously will have a high

00:06:56.827 --> 00:06:58.907
pseudocounts similar life would have

00:06:58.907 --> 00:07:00.950
also high pseudocounts or the other

00:07:00.950 --> 00:07:01.300
words.

00:07:01.300 --> 00:07:03.597
We have 0 pseudocounts because their

00:07:03.597 --> 00:07:06.050
probability is zero in the prior and

00:07:06.050 --> 00:07:07.895
when you see this is also controlled by

00:07:07.895 --> 00:07:10.140
a parameter mu and

00:07:10.230 --> 00:07:13.290
We're going to add mu multiplied by the

00:07:13.290 --> 00:07:16.292
probability of W given our prior

00:07:16.292 --> 00:07:19.280
distribution to the connected counts.

00:07:19.280 --> 00:07:23.600
When we re estimate the when we re

00:07:23.600 --> 00:07:26.780
estimate the this world distribution

00:07:26.780 --> 00:07:27.040
right?

00:07:27.040 --> 00:07:28.970
So this is the only step that changed

00:07:28.970 --> 00:07:31.510
and the changes happened here and

00:07:31.510 --> 00:07:33.570
before we just collect the counts of

00:07:33.570 --> 00:07:35.920
words that we believe have been

00:07:35.920 --> 00:07:37.390
generated from this topic.

00:07:37.390 --> 00:07:40.820
But now we force this distribution.

00:07:40.870 --> 00:07:43.520
To give more probabilities to these

00:07:43.520 --> 00:07:47.143
words by adding them to the

00:07:47.143 --> 00:07:50.240
pseudocounts so to artificially in

00:07:50.240 --> 00:07:52.500
effect, we artificially inflated their

00:07:52.500 --> 00:07:54.982
probabilities and to make this

00:07:54.982 --> 00:07:57.590
distribution we also need to add this

00:07:57.590 --> 00:08:00.140
many pseudocounts to the denominator.

00:08:00.140 --> 00:08:02.065
This is the total sum of all the

00:08:02.065 --> 00:08:03.965
pseudocounts we have added for all the

00:08:03.965 --> 00:08:04.317
words.

00:08:04.317 --> 00:08:06.075
This would make this again a

00:08:06.075 --> 00:08:06.490
distribution.

00:08:07.250 --> 00:08:09.650
Now, this is a intuitively very

00:08:09.650 --> 00:08:11.480
reasonable way of modifying the EM

00:08:11.480 --> 00:08:14.110
algorithm and theoretically speaking,

00:08:14.110 --> 00:08:15.890
this deal works, and it computes the

00:08:15.890 --> 00:08:16.940
map estimator.

00:08:17.600 --> 00:08:21.170
It's useful to think about two specific

00:08:21.170 --> 00:08:23.240
extreme cases of Mu.

00:08:23.240 --> 00:08:24.806
Now can you picture.

00:08:24.806 --> 00:08:26.820
Think about what would happen if we set

00:08:26.820 --> 00:08:27.590
Mu to Zero.

00:08:27.590 --> 00:08:29.975
Well, that's essentially to remove this

00:08:29.975 --> 00:08:32.900
prior, so mu in some sense indicates

00:08:32.900 --> 00:08:34.510
our strength on prior.

00:08:35.180 --> 00:08:37.350
Now what would happen if we set Mu to

00:08:37.350 --> 00:08:38.860
positive Infinity?

00:08:38.860 --> 00:08:41.220
Well, that's to say this price is so

00:08:41.220 --> 00:08:42.690
strong that we're not going to listen

00:08:42.690 --> 00:08:43.850
to the data at all.

00:08:43.850 --> 00:08:46.760
So in the end you can see in this case

00:08:46.760 --> 00:08:49.660
we can do make one distribution fixed

00:08:49.660 --> 00:08:50.730
to the prior.

00:08:50.730 --> 00:08:52.090
You see why?

00:08:52.090 --> 00:08:55.853
When mu is Infinity, we basically let

00:08:55.853 --> 00:08:57.340
this one dominate.

00:08:57.340 --> 00:09:00.059
In fact, we are going to set this one.

00:09:00.650 --> 00:09:04.302
to precise this distribution, so in

00:09:04.302 --> 00:09:07.200
this case it is this distribution, and

00:09:07.200 --> 00:09:09.330
that's why we said the background

00:09:09.330 --> 00:09:11.060
language model is in fact a way to

00:09:11.060 --> 00:09:13.460
enforce a prior, because we force

00:09:13.460 --> 00:09:16.300
one distribution to be exactly the same

00:09:16.300 --> 00:09:19.076
as what we give, that's the background

00:09:19.076 --> 00:09:19.412
distribution.

00:09:19.412 --> 00:09:22.906
So in this case we can even force the

00:09:22.906 --> 00:09:25.220
distribution to entirely focused on

00:09:25.220 --> 00:09:25.990
battery life.

00:09:25.990 --> 00:09:27.760
But of course this won't work well

00:09:27.760 --> 00:09:29.815
'cause it cannot attract other words,

00:09:29.815 --> 00:09:31.530
it would affect the accuracy of

00:09:31.530 --> 00:09:32.270
counting.

00:09:32.320 --> 00:09:34.550
Topics about the battery life so in

00:09:34.550 --> 00:09:36.610
practice mu is set somewhere in

00:09:36.610 --> 00:09:37.560
between, of course.

00:09:38.730 --> 00:09:41.043
So this is one way to impose our prior.

00:09:41.043 --> 00:09:44.050
We can also impose some other

00:09:44.050 --> 00:09:44.560
constraints.

00:09:44.560 --> 00:09:46.070
For example, we can set any parameters

00:09:46.070 --> 00:09:48.507
for constraints, including zero as needed.

00:09:48.507 --> 00:09:51.520
For example, we may want to set one of

00:09:51.520 --> 00:09:53.060
the pis to 0.

00:09:55.180 --> 00:09:57.780
And this would mean we don't allow that

00:09:57.780 --> 00:10:00.240
topic to participate in generating that

00:10:00.240 --> 00:10:00.840
document.

00:10:00.840 --> 00:10:02.810
And this is only reasonable, of course,

00:10:02.810 --> 00:10:05.475
when we have prior knowledge

00:10:05.475 --> 00:10:07.240
that strongly suggests this.


