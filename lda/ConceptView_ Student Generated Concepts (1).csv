,Lecture,Concepts,Reviewer#1 Concepts,Reviewer#2 Concepts,,Consolidated Student Concepts,TA Bhavya Concepts,TA Assma Concepts
Text Retrieval and Search Engines,Text Retrieval and Search Engines,(enter concept terms covered in lecture separated by a semicolon  ; in this column. Must include at least 3 concepts per lecture) SAMPLE: concept A; concept B; concept C,reviewer enters list of concepts covered in lecture separated by a semicolon ; in this column IF there are suggested edits the reviewer sees. This column can remain empty if the reviewer agrees with the initial editor's concept list.,,,#TODO automatically consolidate student and reviewer suggested concepts,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb2,Lecture 1,Lexical Analysis(POS tagging);Semantic Analysis(Parsing);Syntactical ambiguities;Prepositional Phrase attachment ambiguity,Pragmatic Analysis; State of the art of NLP; Shallow vs Deep NLP; Text Retrieval; Bag of words; Suggestion: Add concept surrounded by parentheses as their own concepts,"Introduction to NLP; Lexical, Semantic & Pragmatic Analysis 
of text; Challenges in Parsing; Bag of words representation",,,,Overview of NLP; Lexical analysis; Part of speech tagging; Syntactic analysis; Parsing; Semantic analysis; Inference; Pragmatic analysis; Ambiguity; NLP techniques for Text retrieval; Bag of words
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb3,Lecture 2,Relevant text data; pull mode; push mode; search engines; recommender systems;querying; browsing;information seeking; sightseeing;text access;,,Text Access; Push vs Pull Modes of Text Access; Querying vs Browsing; Reccomender Systems; Information Seeking as Sightseeing,,,,Text access; Pull mode; Push mode; Querying; Browsing; 
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb4,Lecture 3,Definition of text retrieval; Text Retrieval vs. Database Retrieval; Formal Formulation of Text Retrieval,Looks good :),Looks good!,,,,Text retrieval problem definition; Vocabulary; Query; Document; Collection; Document selection; Document Ranking; Absolute Relevance; Relative Relevance; Probability ranking principle
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb5,Lecture 4,Designing Ranking Functions; Types of Models; Common Ideas in Models; Choosing the Best Model; ,Looks good!,Looks good!,,,,Ranking function; Retrieval model; Similarity based models; Probabilistic models; Probabilistic inference models; Axiomatic models; Term frequency; Document length; Document frequency
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb6,Lecture 5,Vector space model framework; Similarity based models; Document ranking functions,Vector Space Model; VSM Illustration; What VSM Lacks; ,Looks good!,,,,Vector space model; 
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb7,Lecture 6,Vector Space Model; Bit Vector; how to define Vector Space Model; Similarity function example; BOW with VSM; Simplest VSM (BOW + bit vector + dot product),Looks good!,"Vector Space Model,Simplest instantiation,Bag of Words,Bit Vector,Similarity Instantiation,Dot Product",,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb8,Lecture 7,Problems with simplest VSM; Improved VSM Instantiation; Term Frequency Weighting; IDF Weighting,Vector Space Model: improved instantiation;Term Frequency Vector;Term Frequency Weighting;Dot Product;Stop Word;Inverse Document Frequency,Looks good!,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb9,Lecture 8,TF transformation; Ranking function with TF-IDF weighting; BM25 Transformation; Reasons for using Sublinear TF transformation,Looks good!,Vector Space Model with TF-IDF; Term frequency weight;,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb10,Lecture 9,Document Length Normalization; Pivoted Length Normalization; BM25/Okapi; BM25+; Improved VSM,Looks good!,Looks good!,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb11,Lecture 10,Typical TR System Architecture; Tokenization;Indexing;Inverted Index;Inverted Index for Fast Search;Empirical Distribution of Words;Zipf’s Law;Data Structures for Inverted Index,"Typical TR System Architecture; 
Three parts of search engine: Indexer, Scorer and Feedback; mechanism
Tokenization;Indexing;
Inverted Index Implementation;
Inverted Index query variation support for fast search;
Empirical Distribution of Words;
Zipf’s Law;
Stop and rare word distribution
Data Structures for Inverted Index Dictionary and Postings;","Looks Good
Consice and Sufficient",,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb12,Lecture 11,Sort-based Inversion; Inverted Index Compression;Integer Compression;Uncompress Inverted Index,"Inverted Index Construction;
Inverted Index Construction for Large collections;
Sort-based Inverted Index Inversion;
Inverted Index Compression;
Term-Frequency and Doc ID compression;
Binary, Uniary, Gamma and Delta coding;
Uncompress and Decoding Inverted Index;",,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb13,Lecture 12,Fast Search; Inverted Index; Score Accumulators; Text Retrieval; Ranking Documents; Inverse Document Frequency,,Looks good! Suggestion: add: 'Techniques to scale up Fast Search',,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb14,Lecture 13,"Text Retrival Systems (Why, What, How); Cranfield Evaluation Method; Test Collection Evaluation",interactive IR evaluation; test set IR evaluation;,Evaluation of text retrieval systems; ,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb15,Lecture 14,Quantitatively compared retrieval systems;Test collection evaluation;Measure precision P;Measure recall R;Precision @10 documents;F-measure;Tradeoff between P and R,,Looks good!,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb16,Lecture 15,Evaluate Ranked List;Precision-Recall Curve;Comparing P-R Curve;Area Under Curve(AUC);Average Precision,,,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb17,Lecture 16,Arithmetic mean of average precision over a set of queries (MAP); geometric mean of average precision of a set of queries(gMAP); Mean Peciprocal Rank (special case),Looks good,"Looks Good
Concise and Sufficient",,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb18,Lecture 17,Multi-level relevance judgement;Cumulative Gain;Normalized DCG;,Looks good,Discounted Cumulative Gain,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb19,Lecture 18,"Pratical issues of Text Retrieval 
Statistical Signifance Tests 
Pooling","Challenges in Creating Test Collection; Representative Queries; Relevance Judgement, Measures, Statistical Significance Tests; Sign Test; Wilcoxon Test; Pooling; Summary TR Evaluation",Looks Good!,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb20,Lecture 19,Probabilistic retrieval model; Language model; Query likelihood retrieval model; Divergence from randomness model; ,Probabilistic Retrieval Model; Query Likelihood Retrieval Model; Divergence from Randomness; Probability of Relevance; Conditional Probability; Imaginary Relevant Document,Probabilistic Retrieval Model; Probabalistic Models; Classic Probabalistic Model; Language Model; Query Likelihood Retrieval Model; Query Likelihood,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb21,Lecture 20,"Statistical Language Model;
Unigram Language Model;
Text Generation with Unigram Language Model;
Uses is Language Model:Topic Representation, Discovering Word Associations",,,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb22,Lecture 21,"Query Likelihood
Query likelihood retrieval function
TF heuristic
Unigram language model
Document language model
Log of query likelihood to avoid underflow problem
",Query Likelihood Ranking;,Use the estimation of document language model as part of the ranking function,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb23,Lecture 22,Ranking Function based on Query Likelihood; Maximum Likelihood Estimate; Smoothed Likelihood Model; Probability of Unseen Words; Ranking Function with Smoothing,,Probabilistic retrieval model; query likelihood retrieval model; Max likelihood estimate; ranking funxtion with smoothing,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb24,Lecture 23,Smoothing Ranking Function; Probabalistic Modeling; Smoothing query likelihood; deriving general ranking function,Query likelihood retrieval function; smoothing ranking function; TF-IDF weighting; doc length normalization,Smoothing of a Language Model: Benefit of rewriting; Probability of a unseen word; General ranking formula; Scoring ,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb25,Lecture 24,Smoothing Query Likelihood; Linear Interpolation (Jelinek-Mercer) Smoothing; Dirichlet Prior (Bayesian) Smoothing,smoothing methods for langugage model; probabilistic retrieval model; query likelihood; linear interpolation (Jelinek-Mercer) smoothing; Dirichlet Prior (Bayesian) Smoothing,Query likelihood+smmothing; Linear interpolation smoothing; Dirichlet Prior smoothing,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb26,Lecture 25,JM Smoothing; Dirichlet Prior Smoothing; TF-IDF Weighting; Doc Length Normalization,,"Suggestion:
1. Combine TF-IDF Weighting and Doc Length Normalization (it's kind of one topic)
2. Add new topic ""Single Smoothing Parameter""",,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb27,Lecture 26,Relevance Feedback; Pseudo feedback; Implicit feedback ,"Suggestion: 
1. Add ""explict relevance""","Suggestion:
1. May consider adding 'Text Retrieval' as an overall scope",,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb28,Lecture 27,Vector space model feedback; query modification; Rocchio Feedback; query expansion; relevance feedback; pseudo feedback; ,VSM feedback; Rocchio feedback; Pseudo Feedback; Relevance Feedback,"VSM feedback; Rocchio feedback, retrieval accuracy, positive centroid vector, negative centroid vector",,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb29,Lecture 28,Kullback-Liebler (KL) Divergence; Cross entropy; Generative Mixture Model; Pseudo-Feedback Query Model; Feedback; Language Models,"Language Model feedback, Kullback-Leibler (KL) divergence retrieval model, cross entropy, query likelihood, generative mixture model",feedback in text retrieval; KL divergence retrieval model; generative mixture model;maximum likelihood ,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb30,Lecture 29,Web Search Engines; Web search challenges & opportunities; Link Analysis & crawling strategies,,"Suggestions:
1. Remove ‘MapReduce’ and ‘Link analysis’ as these concepts are mentioned, but not really covered in this lecture.
2. Add ""Web search challenges & opportunities; Basic search engine technologies; Crawler problems; Major crawling strategies; Crawling scenarios""",,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb31,Lecture 30,Web Indexing; Google File System (GFS) Architecture; MapReduce; Hadoop; Inverted Indexing; Map Function; Reduce Function,Web indexing; GFS architecture; MapReduce: A frameworking for parallel programming; MapReduce Computation Pipeline; Word counting; Word counting: Map function and Reduce function; Inverted Indexing with MapReduce; ,Looks good to me! :),,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb32,Lecture 31,"Ranking algorithm for web search; Exploiting inter-documentlink; Page rank: Capturing page ""popularity""",ranking algorithms based on machine laerning algorithms; Archor text for providing external details;  PageRank invented by Google; ,Link Analysis for web search; Ranking Algorithms for web search; Inter-Document Links; PageRank ,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb33,Lecture 32,Random surfing model; Page Rank algorithm with examples; Equilibrium Equation ,Random Surfing Model and Equlibrium Equation for Page Rank Algorithm;  Example for multiplication in PageRank,,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb34,Lecture 33,Link Analysis; HITS Algorithm; Hypertext-Induced Topic Search,,The concepts covered are listed by the editor,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb35,Lecture 34,"Combining features to improve robustness & accuracy of ranking, weighting features for optimization, empirical evaluation of ranking functions","Suggest instead:
Combining Ranking Features; Creating Relevance Function; Fitting Relevance Function on Training Data ",Combining Ranking Features; Creating Relevance Function; Relationship between Relevance Function and Training Data,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb36,Lecture 35,Rank;Regression-based approach; Logistic regression; maximizing likelihood,,,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb37,Lecture 36,Optimizing ranking measures; ranking applications; combining features; robust ranking,Add: Learning to rank - Summary,Remove: robust ranking.  Add: Advanced Learning to Rank Algorithms; Trends in Learning to Rank;  Edit: ranking applications -> Applications of Learning to Rank; optimizing ranking measures -> Machine Learning to optimize MAP or nDCG ranking measures,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb38,Lecture 37,next generation search engines; data-user-service triangle; future intelligent information systems,All major concepts covered are listed.,,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb39,Lecture 38,Recommender systems; pull mode; search engines; filtering system; push mode; content based filtering system; problems in content-based filtering; binary classifier; initialization; learning from judgements; information filtering,covered all important topics,"nothing from editor... Here are some important concepts from me - Recommendation system, Filtering System, Content-based filtering, Information based retrieval system",,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb40,Lecture 39,content-based filtering; threshold learning; adaptive learning algorithm,Add: Difficulties in Threshold Learning; Exploration vs Exploitation; Empirical Utility Optimization; Beta-Gamma Threshold Learning; Content-based Filtering Summary,,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb41,Lecture 40,Collaberative filtering; Recommender system; Text access,"Add: Basic Filtering Question; What is Collaborative Filtering; Collaborative Filtering Assumptions; Collaborative Filtering Problem
Remove: User Similarity Measurement (it's in the next lecture) ",User Similarity measurement and Text access are not aprt of the Lecture 40. Collaborative Flltering concepts and associated assumptions and the actual problem statement for CF have been discussed.,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb42a,Lecture 41,Recommender system; Collaborative Filtering; Collaborative Filtering Problem; Memory Based Approaches; User Similarity Measures; Pearson Correlation Coefficient; Cosine Measure,lgtm,Add: How to improve User Similarity Measures,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb43,Lecture 42,Summary of Recommender Systems; Why Filtering/Recommendation is “easy; Why Filtering/Recommendation is “hard”; Content-based vs Collaborative filtering vs Hybrid; Combining recommendation with Push+Pull; Advanced algorithms,"
Summary of Recommender Systems; Filtering/Recommendation; Filtering; Content-based vs. Collaborative Filtering vs. Hybird; Recommendation combined w/search; Advanced algorithms",Summary of Recommender Systems; Content-based vs. Collaborative filtering vs. Hybrid; ,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=8c6b7d5c-a376-489a-8232-324ca5992eb44,Lecture 43,Natural Language Content Analysis; Text Retrieval Methods; Vector Space Model; Probabilistic Model; Retrieval Systems; Model Evaluation; Feedback Techniques; Web Search; Recommendation Systems; Text Mining Methods,text ranking problem; MapReduce,"Ranking systems, Query vs browsing, Text analysis vs Text mining comparison, Rocchio feedback",,,,
,,,,,,,,
Text Mining and Analytics,,,,,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a8,Lecture 1,Text Mining and Text Analytics;Text vs Non-Text Data;General Problem of Data Mining;Interaction with Real world through Humans or sensors,,Covered all topics,,Text Mining and Text Analytics;Text vs Non-Text Data;General Problem of Data Mining;Interaction with Real world through Humans or sensors,Text Mining; Text Analytics; Relationship between text mining and retrieval; Analogy between text data and non-text data; Data Mining problem,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a9,Lecture 2,observed world; text data; joint mining of text + non-text data; mining knowledge about language; mining content of text data; mining knowledge about the observer; predictive analytics; natural language processing & text representation; word association mining & analysis; topic mining & analysis; option mining & sentiment analysis; text-based prediction,covered everything in the lecture,,,observed world; text data; joint mining of text and non-text data; mining knowledge about language; mining content of text data; mining knowledge about the observer; predictive analytics; natural language processing and text representation; word association mining and analysis; topic mining and analysis; opinion mining and sentiment analysis; text-based prediction,Text Mining; topics in text mining and analytics; process of generating text data,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a10,Lecture 3,NLP; Lexical analysis; Syntactic parsing; semantic analysis; speech act; ambiguity;statistical analysis methods;text mining,,Covered all topics,,NLP; Lexical analysis; Syntactic parsing; semantic analysis; speech act; ambiguity; statistical analysis methods; text mining,natural language processing; natural language content analysis; NLP; syntactic parsing; syntactic analysis; Lexical analysis; semantic analysis; pragmatic analysis; speech act analysis; ambiguity; part of speech tagging ,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a11,Lecture 4,shalow NLP (statistic analysis) and deep NLP; general statistical analysis; supervised machine learning,,,,shallow NLP; deep NLP; general statistical analysis; supervised machine learning; statistical analysis,shallow NLP; deep NLP,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a12,Lecture 5,"Text representation; Ways of Text representation; NLP concepts; Text Analysis; Word segmentation; 
",,"string of characters, sequence of words, part of speech tags, syntactic structures, entity-relation recognition, logic predicates, speech acts, trade off between deeper NLP and shadow NLP",,Text representation; Ways of Text representation; NLP concepts; Text Analysis; Word segmentation; string of characters; sequence of words; part of speech tags; syntactic structures; entity-relation recognition; logic predicates; speech acts; trade off between deep NLP and shallow NLP,Text representation; string of characters; sequence of words; part of speech tags; syntactic structures; entity-relation recognition; logic predicates; speech acts; trade off between deep NLP and shallow NLP,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a13,Lecture 6,text representation; text analysis; natural language processing; representation in relation to the mining algorithms it enables; efficiency of word based representation;,,string text representation; words text representation; text representation by adding syntactice structures; text representation by adding entities and relations; text representation by adding logic predicates; advantages of the word-based text representation,,text representation; text analysis; natural language processing; representation in relation to the mining algorithms it enables; efficiency of word based representation; string text representation; words text representation; text representation by adding syntactic structures; text representation by adding entities and relations; text representation by adding logic predicates; advantages of the word-based text representation,text representation; string representation; word based representation; syntactical structure representation; logical representation,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a14,Lecture 7,Word association; Paradigmatic relation; Syntagmatic relation; Context similarity; Joint discovery,Word association; Paradigmatic Relation; Syntagmatic Relation; Joint Discovery,Word Association;Paradignatic Relation;Syntagmatic Relation;Context Similarity;,,Word association; Paradigmatic relation; Syntagmatic relation; Context similarity; Joint discovery,word association mining and analysis;  Paradigmatic relation; Syntagmatic relation; Context similarity; word co-occurrences,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a15,Lecture 8,Defining Paradigmatic Relations; Word Context; Measuring Context Similarity; Representing Documents with Vectors; Expected Overlap of Words in Context (EOWC); EOWC Analysis,Paradigmatic Relation Discovery; Methods for Context Similarity; Vector Space Model for Paradigmatic Relation; Expected Overlap of Words in Context,Paradigmatic relations;Word association;Similarity function;Expected Overlap of Words in Context;,,Paradigmatic Relation Discovery; Methods for Context Similarity; Vector Space Model for Paradigmatic Relation; Expected Overlap of Words in Context; Paradigmatic relations; Word association; Similarity function; Defining Paradigmatic Relations; Word Context; Measuring Context Similarity; Representing Documents with Vectors ,Paradigmatic Relation Discovery; word context; context similarity; Vector Space Model for Paradigmatic Relation discovery; Expected Overlap of Words in Context; EOWC,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a16,Lecture 9,Paradigmatic Relations;EOWC;Term Frequency;Inverse Document Frequency;Paradigmatic Relation Mining;BM25;Syntagmatic Relations,EOWC Review; Term Frequency Transformations; Inverse Document Frequency Weighting; Paradigmatic Relation Mining with BM25; Syntagmatic Relation Mining with BM25,,,Paradigmatic Relations; EOWC; Term Frequency; Inverse Document Frequency; Paradigmatic Relation Mining; BM25; Syntagmatic Relations; EOWC Review; Term Frequency Transformations; Inverse Document Frequency Weighting; Paradigmatic Relation Mining with BM25; Syntagmatic Relation Mining with BM25,Term frequency transformations; BM25 transformation; IDF weighting; Paradigmatic Relation Mining with BM25; Syntagmatic Relation Mining with BM25,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a17,Lecture 10,Syntagmatic Relation; Entropy; Correlated Occurences; Word Prediction; Randomness of a Random Variable,looks good,Defining Syntagmatic Relations; Intuitive Word Prediction; Entropy; Entropy Calculation Example; Word Prediction with Entropy,,Syntagmatic Relation; Entropy; Correlated Occurences; Word Prediction; Randomness of a Random Variable; Defining Syntagmatic Relations; Intuitive Word Prediction; Entropy; Entropy Calculation Example; Word Prediction with Entropy,Syntagmatic Relation discovery; entropy; word prediction,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a18,Lecture 11,Entropy;Conditional Entropy;Syntagmatic Relation;Word Association,syntagmatic relation discovery; conditional entropy; minimum possible entropy; reduction of entropy; mining syntagmatic relations; upper bounds of entropy; comparing entropy,Suggestion: Separate by ';' instead of new lines; Mining Syntagmatic Relations with Conditional Entropy,,Entropy; Conditional Entropy; Syntagmatic Relation; Word Association; syntagmatic relation discovery; minimum possible entropy; reduction of entropy; mining syntagmatic relations; upper bounds of entropy; comparing entropy; Mining Syntagmatic Relations with Conditional Entropy,Syntagmatic Relation discovery; conditional entropy; conditional entropy for mining syntagmatic relations,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a19,Lecture 12,mutual information definition and use in syntagmatic relation discovery;asymmetry of entropy change between word pairs;symmetry of word pair mutual information;mutual information computation;,KL divergence; comparison between conditional entropy vs mutual information; ,,,mutual information definition and use in syntagmatic relation discovery; asymmetry of entropy change between word pairs; symmetry of word pair mutual information;mutual information computation; KL-divergence; comparison between conditional entropy and mutual information; ,syntagmatic relation discovery; mutual information; mutual information for syntagmatic relation mining; KL-divergence,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a20,Lecture 13,Estimation of probabilities to derive mutual information ; Smoothing Technique ; Summary of syntagmatic relation discovery ; Relation between Syntagmatic and Paradigmatic relations ; Summary of word association mining,"EDIT to fix typo: Estimation of probabilities to derive mutual information;
CHANGE syntagmatic relation discovery - a quick recap TO summary of syntagmatic relation discovery
CHANGE word associace mining - a quick recap TO summary of word association mining
REMOVE pure statistical approaches towards word association discovery",all good after edits,,Estimation of probabilities to derive mutual information; Smoothing Technique ; Summary of syntagmatic relation discovery ; Relation between Syntagmatic and Paradigmatic relations ; Summary of word association mining,maximum likelihood estimate; smoothing; mutual information; syntagmatic relation discovery; Relation between Syntagmatic and Paradigmatic relation discovery; word association mining,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a21,Lecture 14,Content mining; topic mining and analysis; motivation and applications; topics as knowledge about the world; task definiton of topic mining and anlysis and its subtasks,Text Data; Context and Non-Text Data; Discover Topics from Text Data; Rank each Document with each Topic; Input is Docs; Output is Topics and Doc-Topic Coverage,all good with suggestions,,Content mining; topic mining and analysis; motivation and applications; topics as knowledge about the world; task definiton of topic mining and analysis and its subtasks; Text Data; Context and Non-Text Data; Discover Topics from Text Data; Rank each Document with each Topic; Input is Docs; Output is Topics and Doc-Topic Coverage,topic mining and analysis; tasks of topic mining and analysis; topic as knowledge about the world,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a22,Lecture 15,Define a Topic Using a Single Term; Mine k Topic Terms from Collection C; Topic Coverage Function; Problems with Term as Topic;,,,,Define a Topic Using a Single Term; Mine k Topic Terms from Collection C; Topic Coverage Function; Problems with Term as Topic;,term as a topic; mine k topical terms; topic coverage,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a23,Lecture 16,Probabilistic Topic Models; Term as Topic; Word Distribution; Modeling of Data Generation; Parameter Estimation,"Term as a Topic; Topic as Word Distribution; Word Weights; Input is C, k , V; Output is Coverage; Text Mining Generative Model",in suggestion #1: 'Review Problems with Term as a Topic',,"Probabilistic Topic Models; Term as a Topic; Word Distribution; Modeling of Data Generation; Parameter Estimation; Review Problems with Term as a Topic; Topic as Word Distribution; Word Weights; Input is C, k, V; Output is Coverage; Text Mining Generative Model",Probabilistic Topic Models; generative model of text mining; word distribution; coverage of topics,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a24,Lecture 17,"probabilistic topic model; generative model; 
language model; unigram language model; statistical language model; 
word sequence; word distribution; probability distribution; 
maximum likelihood estimate; 
text data; text mining; topic analysis; vocabulary",Looks good,,,probabilistic topic model; generative model; language model; unigram language model; statistical language model; word sequence; word distribution; probability distribution; maximum likelihood estimate; text data; text mining; topic analysis; vocabulary,statistical language models;generative model; unigram language model; sampling; estimation; maximum likelihood estimate,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a25,Lecture 18,"Maximum liklihood estimation;
Bayesian estimation;
Maximum a posteriori (MAP) estimation;
Illustration of Baysian estimation/inference;","Additions: Prior, Posterior, Likelihood, Posterior Mode/Mean, Prior Mode/Mean",looks good after suggested edits,,Maximum likelihood estimation; Bayesian estimation; Maximum a posteriori estimation; Illustration of Baysian estimation/inference; Prior; Posterior; Likelihood; Posterior Mode/Mean; Prior Mode/Mean; MAP estimation ,Maximum likelihood estimation; Bayesian estimation; Bayes rule; Maximum a posteriori; MAP estimate; mode of prior; posterior mode; Bayesian inference; posterior mean; likelihood function; word distribution ,
https://classtranscribe. illinois .edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a26,Lecture 19,Topic Model: Single Topic Mining; Unigram Language Model;  Likelihood function for Single topic mining;  Computation of Maximum likelihood Estimate; Lagrange multiplier approach,,"CHANGE topic model: mining one topic TO Single Topic Mining
CHANGE likelihood function: mining one topic TO Likelihood function for Single topic mining


Author: Thank you for suggestion, incorporated the comments",,Single Topic Mining; Unigram Language Model;  Likelihood function for Single topic mining;  Computation of Maximum likelihood Estimate; Lagrange multiplier approach,probabilistic topic models; mining one topic; unigram language model; likelihood function; maximum likelihood estimate; Lagrange multiplier approach; Lagrange function ,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a27,Lecture 20,Probalistic Topic Models: Mixture of Unigram Language Models; probabilistic topic models; mixture model; unigram; language model; mixture of two unigram language models; component model; filtering out background; words; generative model,Looks good with suggestions provided by Reviewer#2,Maximum likelihood,,Probabilistic Topic Models; Mixture of Unigram Language Models; mixture model; unigram; language model; mixture of two unigram language models; component model; filtering out background; words; generative model; Maximum likelihood,Probabilistic Topic Models; Mixture of Unigram Language Models; background topic; background word distribution; mixture model; likelihood function,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a28,Lecture 21,mixture model estimation; probabilistic topic models; mixture model; background model; maximum likelihood estimator; word distributions; parameter estimation,mixture model estimation; probabilistic topic models; factoring out background words; mixture model; maximum likelihood; background model; likelihood function,looks good,,mixture model estimation; probabilistic topic models; mixture model; background model; maximum likelihood estimator; word distributions; parameter estimation; factoring out background words; maximum likelihood; likelihood function,mixture model estimation; probabilistic topic models; behavior of mixture model; maximum likelihood estimator,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a29,Lecture 22,Behavior of Mixture Model; Collaboration and Competiton; Response to Data Frequency ,"Add: likelihood functions, background language model",Looks good. Topics covered,,Behavior of Mixture Model; Collaboration and Competiton; Response to Data Frequency; likelihood functions; background language model ,behavior of mixture model; response to data frequency; maximum likelihood estimator; estimation problem of mixture model,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a30,Lecture 23,probabilistic topic model; mixture model; expectation maximization algorithm; EM algorithm; Bayesian inference; Bayes rule; background model; latent variable,"expectation maximization algorithm; EM algorithm; latent variable z; 
maximum likelihood estimate; probabilistic topic model; 
mixture model; word probability; word distribution; 
background words; background model; background word distribution; 
topic model; topic word distribution; 
Bayesian inference; Bayes rule; Bayes formula; prior",All topics covered (with Reviewer #1's suggestions),,"probabilistic topic model; mixture model; expectation maximization algorithm; EM algorithm; Bayesian inference; Bayes rule; background model; latent variable; latent variable z; maximum likelihood estimate; word probability; word distribution; 
background words; background model; background word distribution; topic model; topic word distribution; Bayesian inference; Bayes rule; Bayes formula; prior",expectation maximization algorithm; em algorithm; probabilistic topic models; two component mixture model; maximum likelihood estimator; Bayesian inference; prior; evidence; Bayes rule; latent variable,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a31,Lecture 24,Expectation-Maximization Algorithm; Hidden Variable (z); Predicting z value with Bayes Rule; E-Step; M-Step; Inferring topic vs background words,"EM algorithm;
iterative algorithm; hill climbing algorithm; local maximum; 
likelihood function; log likelihood; 
latent variable; hidden variable; binary variable; z value; 
word distribution; 
background; background model; background word distribution; 
topic; topic distribution; topic word distribution; 
Bayes rule; probability; parameters; inferred word distribution","Reviewer 1 as base:
Remove: binary variable, background, probabilitity, parameters (all too general concepts)",,"Expectation-Maximization Algorithm; Hidden Variable (z); Predicting z value with Bayes Rule; E-Step; M-Step; Inferring topic vs background words; EM algorithm;
iterative algorithm; hill climbing algorithm; local maximum; 
likelihood function; log likelihood; 
latent variable; hidden variable;  z value; 
word distribution; 
background model; background word distribution; 
topic; topic distribution; topic word distribution; 
Bayes rule; inferred word distribution",expectation maximization algorithm; em algorithm; hidden variable; E-step; M-step; hill climbing algorithm; likelihood function; local maximum ,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a32,Lecture 25,"Ranking functions, Jelinek-Mercer Smoothing, Dirichlet Prior Smoothing, TD-IDF weighting, Document length normalization, Smoothing parameter, Query likelihood Probablistic Model","Incorrect - no talk about smoothing in this lecture. Was there a mistake on the video for the original reviewer?
New proposal: EM algorithm; likelihood function; mixture model; local maximum; global maximum; hill-climbing algorithm","Agree with the first reviewer, the initial content topics are not correct, assume used wrong set of lectures.
Support first reviewers proposal: EM Algorithm; likelihood function; mixture model; local maximum; global maximum; hill climbing algorithm",,EM Algorithm; likelihood function; mixture model; local maximum; global maximum; hill climbing algorithm,EM algorithm; local maximum; hill climbing algorithm; likelihood function; lower bound ,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a33,Lecture 26,"Documents as a sample of mixed topics -
Determine which words are from which distribution; Mining multiple topics from text; Generating text from multuple topics;
Probability of observing a word from multiple topic distributions; Probabilistic Latent Semantic Analysis (PLSA) - likelihood function; Maximum Likelihood Parameter Estimation","Add: topic models; probabilistic modeling, background language model, topic coverages in documents, word probabilities in topics",looks good !!,,"Documents as a sample of mixed topics- Determine which words are from which distribution; Mining multiple topics from text; Generating text from multuple topics;
Probability of observing a word from multiple topic distributions; Probabilistic Latent Semantic Analysis (PLSA)- likelihood function; Maximum Likelihood Parameter Estimation; topic models; probabilistic modeling, background language model, topic coverages in documents, word probabilities in topics ",Probabilistic Latent Semantic Analysis; PLSA; Mining multiple topics from text; likelihood function for PLSA: maximum likelihood estimation,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a34,Lecture 27,EM algorithm for PLSA; Hidden variable; Bayes rule; Coverage; PLSA; Pre-determined background LM; Discriminative topics; K word distributions; Proportion of each topic,,unknown parameters in PLSA; output and its application,,EM algorithm for PLSA; Hidden variable; Bayes rule; Coverage; PLSA; Pre-determined background LM; Discriminative topics; K word distributions; Proportion of each topic; unknown parameters in PLSA; output and its application,EM algorithm; PLSA; E-step; M-step;  implementation of EM algorithm,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a35,Lecture 28,Latent Dirichlet Allocation; Extending PLSA with prior knowledge; Extending PLSA as a generative model; Maximum a Posteriori Estimate (MAP); Influence of conjugate prior similar to that of pseudo data; EM algorithm with conjugate prior; Pseudo count contribution tunable by mu,,"Consider adding: Encoding prior knowledge, preferences and constraints with a prior",,"Latent Dirichlet Allocation; Extending PLSA with prior knowledge; Extending PLSA as a generative model; Maximum a Posteriori Estimate (MAP); Influence of conjugate prior similar to that of pseudo data; EM algorithm with conjugate prior; Pseudo count contribution tunable by mu; Encoding prior knowledge, preferences and constraints with a prior",Latent Dirichlet Allocation; LDA; maximum a posteriori estimate; MAP estimate; conjugate prior; prior knowledge,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a36,Lecture 29,"Latent Dirichlet Allocation (LDA); Deficiency of Probabilistic Latent Semantic Analysis (PLSA); LDA as Generative Model; Dirichlet Prior; LDA as a Bayesian version of PLSA; Parameter Regularization in LDA; Collapsed Gibbs Sampling algorithm, Topic modeling; Mixture models; LDA likelihood function; Maximum likelihood estimator",,Add: topic modeling; mixture models; likelihood function for LDA; maximum likelihood estimator,,"Latent Dirichlet Allocation; LDA; Deficiency of Probabilistic Latent Semantic Analysis; Deficiency of PLSA; LDA as Generative Model; Dirichlet Prior; LDA as a Bayesian version of PLSA; Parameter Regularization in LDA; Collapsed Gibbs Sampling algorithm, Topic modeling; Mixture models; LDA likelihood function; Maximum likelihood estimator; mixture models; likelihood function for LDA",PLSA; LDA; generative model; dirichlet prior; likelihood function,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a37,Lecture 30,Text Clustering; Clustering Bias; Exploratory Text Analysis; Application of Text Clustering; Text Mining; Topic Mining and Analysis,"Remove ""Exploratory Text Analysis"", ""Text Mining"" and ""Topic Mining and Analysis"" as these terms might have been referred in the lecture but those concepts are not explained in-depth in this lecture.",,,Text Clustering; Clustering Bias; Application of Text Clustering,text clustering; clustering bias; exploratory text analysis,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a38,Lecture 31,Text Clustering; Generative probabilistic models; clustering results; Topic mining revisited; Mining one topic revisited; generative model; generative topic model revisited; Mixture model for document clustering; ,Consider add: likelihood function,Likelihood Function; difference between topic model and mixture model,,Text Clustering; Generative probabilistic models; clustering results; Topic mining revisited; Mining one topic revisited; generative model; generative topic model revisited; Mixture model for document clustering; Likelihood Function; difference between topic model and mixture model,Text Clustering; Generative probabilistic models; Mixture model for document clustering; topic model; likelihood function,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a39,Lecture 32,"Generative probabilistic models; text clustering; likelihood function, mixture model for document clustering; unigram language models; parameter estimation, maximum likelihood estimate; Bayes",Consider add: prior; cluster allocation,cluster allocation,,"Generative probabilistic models; text clustering; likelihood function, mixture model for document clustering; unigram language models; parameter estimation, maximum likelihood estimate; Bayes; prior; cluster allocation",generative probabilistic models; text clustering; mixture model for document clustering; assigning document to clusters; likelihood function,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a40,Lecture 33,Text clustering; Generative probabilistic models; EM algorithm for document clustering; Normalization to avoid underflow; Use of average distribution as normalizer,"Could add Generative Model for Clustering. Otherwise, looks good.",Could also include: Maximum Likelihood Estimator,,Text clustering; Generative probabilistic models; EM algorithm for document clustering; Normalization to avoid underflow; Use of average distribution as normalizer; Maximum Likelihood Estimator; Generative Model for Clustering,generative probabilistic models; text clustering; underflow; normalization; Generative Model for Clustering,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a41,Lecture 34,Text Clustering; Similarity-based Approaches; Hierarchical Agglomerative Clustering (HAC); Single-link; Complete-link; Average-link; K-Means Clustering,"Could also include:
Strategies for Optimal Clustering; Similarity of K-Means Algorithm with EM Algorithm","Suggestions:
1. Consider adding 'Group Similarity'
2. Consider 'Similarity-based Clustering' instead of 'Similarity-based Approaches'
3. Consider changing 'Hierarchical Agglomerative Clustering' to 'Agglomerative Hierarchical Clustering'
4. Consider adding 'algorithm' to the similarity methods (e.g.'Single-link algorithm', etc.).",,Text Clustering; Similarity-based Clustering; Agglomerative Hierarchical Clustering; HAC; Single-link algorithm; Complete-link algorithm; Average-link algorithm; K-Means Clustering; Strategies for Optimal Clustering; Similarity of K-Means Algorithm with EM Algorithm; Group Similarity,Similarity-based Clustering; Hierarchical Clustering; hierarchical agglomerative clustering; divisive; k-means; group similarity; single link; complete link; average link,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a42,Lecture 35,Evaluation of Text Clustering; Direct Evaluation of Text Clusters; Indirect Evaluation of Text Clusters; Clustering Bias; Gold Standard; Normalized Mutual Information; Baseline System; F-measure,"Consider adding cluster evaluation concepts such as ""Gold Standard"" and ""Baseline System"" and briefly mentioned evaluation metrics - ""Purity"", ""Normalized Mutual Information"" and ""F-measure""  ","Consider:
1. Adding 'Clustering Bias'",,Evaluation of Text Clustering; Direct Evaluation of Text Clusters; Indirect Evaluation of Text Clusters; Clustering Bias; Gold Standard; Normalized Mutual Information; Baseline System; F-measure; Closeness; Usefulness,evaluation of text clustering; clustering bias; direct evaluation; indirect evaluation; normalized mutual information; optimal number of clusters,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a43,Lecture 36,"data mining and analytics
opinion mining and sentiment analysis
Example of Text Categorization
Variant of Text Categorization
Why Text Categorization",,,,data mining and analytics; opinion mining and sentiment analysis; Example of Text Categorization; Variants of Text Categorization; Why Text Categorization,text categorization; internal categories; external categories; binary categorization; K-category categorization; hierarchical categorization; joint categorization; ,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a44,Lecture 37,Manual Text Categorization; Automatic Text Categorization; Machine Learning Text Categorization; Classifier; Generative Classifiers; Discriminative Classifiers;  ,,Looks good!,,Manual Text Categorization; Automatic Text Categorization; Machine Learning; Text Categorization; Classifier; Generative Classifiers; Discriminative Classifiers;  ,manual methods for text categorization; training data; machine learning; supervised learning; generative classifiers; discriminative classifiers,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a45,Lecture 38,Baye's Rule; Naive Bayes Classifier; Independence Assumption; Prior estimation; Posterior probability; word distriibutions; underflow; smoothing; mixture model; training data; comparing with logistic regression; feature values; word features; category bias; background language model; parameter tuning; discriminative weighting.,Add: Text Clustering; topic likelihood; conditional probability; text categorization; ,Looks good,,Baye's Rule; Naive Bayes Classifier; Independence Assumption; Prior estimation; Posterior probability; word distributions; underflow; smoothing; mixture model; training data; comparing with logistic regression; feature values; word features; category bias; background language model; parameter tuning; discriminative weighting; Text Clustering; topic likelihood; conditional probability; text categorization,generative probabilistic models; text categorization; Naive Bayes classifier; prior; word distributions; smoothing;category bias; features; scoring function;,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a46,Lecture 39,"Discriminative Classifier: Logistic Regression - Overview; Relationship between Logistic Regression and Naive Bayes Classifier; Logistic Regression - How to estimate parameters; K-Nearest Neighbors - Overview;
K-Nearest Neighbors - Illustration; K-Nearest Neighbors - Estimation Approach.",,Looks good!,,Discriminative Classifier: Logistic Regression - Overview; Relationship between Logistic Regression and Naive Bayes Classifier; Logistic Regression - How to estimate parameters; K-Nearest Neighbors - Overview; K-Nearest Neighbors - Illustration; K-Nearest Neighbors - Estimation Approach,discriminative classifiers; text categorization; logistic regression; log odd ratio; K nearest neighbors; KNN,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a47,Lecture 40,"Discriminative Classifier: Support Vector Machine; 
How to find the best Linear Separator in SVM; 
Simple Case: Linear SVM; 
Advanced Case: Linear SVM with Soft Margin; 
Summary of Text Categorization Methods.",,"Suggestions:
1) May be add how to design effective features.
2) May be add How to obtain enough training examples",,Discriminative Classifier: Support Vector Machine; How to find the best Linear Separator in SVM; Simple Case: Linear SVM; Advanced Case: Linear SVM with Soft Margin; Summary of Text Categorization Methods; How to design effective features; How to obtain enough training examples,discriminative classifier; text categorization; support vector machine; svm; linear separator; margin; linear svm; soft margin; feature design; error analysis; feature selection; dimension reduction; deep learning; semi-supervised learning; domain adaptation,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a48,Lecture 41,General Evaluation Methodology;Classification Accuracy;Problems with Classification Accuracy;Per-Document Evaluation;Per-Category Evaluation;Combine Precision and Recall: F-Measure,It looks all good to me ,"Suggestions:
1. May consider adding 'Text Categorization';
2. May consider removing 'Classification Accuracy' since it has covered in Problems with Classification Accuracy.",,General Evaluation Methodology; Classification Accuracy; Problems with Classification Accuracy;Per-Document Evaluation;Per-Category Evaluation;Combine Precision and Recall: F-Measure; Text Categorization,evaluation of text categorization; cranfield evaluation methodology; ground truth; classification accuracy; imbalanced test set; false positive; true positive; false negative; true negative; precision; recall; F measure; F1,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a49,Lecture 42,"Macro Average over all Documents, Macro Average over all Categories, Micro-Average of Precision and Recall, Text Categorization: Evaluation, Text Categorization: Ranking","Suggestion:
- Topics are mostly covered well, just one point can be added as 
Intended use of Measures",,,"Macro Average over all Documents, Macro Average over all Categories, Micro-Average of Precision and Recall, Text Categorization: Evaluation, Text Categorization: Ranking; Intended use of Measures",evaluation of text categorization; macro average; micro average; ranking accuracy,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a50,Lecture 43,Subjective vs. Objective Sensor; Definition of Opinion; Elements of Opinion Representation; Basic Opinion Representation; Enriched Opinion Representation; Opinion Mining with Explicit vs. Implicit Opinion Holder and Target; Variations of Opinion by elements; Author’s opinion vs. Another person’s opinion; Indirect Opinion Inferred from factual statements; Task Definition of Opinion Mining; Uses of Opinion Mining,looks good!,Looks good!,,Subjective vs. Objective Sensor; Definition of Opinion; Elements of Opinion Representation; Basic Opinion Representation; Enriched Opinion Representation; Opinion Mining with Explicit vs. Implicit Opinion Holder and Target; Variations of Opinion by elements; Author’s opinion vs. Another person’s opinion; Indirect Opinion Inferred from factual statements; Task Definition of Opinion Mining; Uses of Opinion Mining,opinion mining; sentiment analysis; subjective sensors; opinion holder; opinion representation; opinion target; opinion content; opinion context; opinion sentiment; indirect opinion; inferred opinion; author's opinion; reported opinion ,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a51,Lecture 44,"Sentiment Classification; Text features for Sentiment Analysis; List of text features used;Feature design & construction for Text Categorization;
Note: The Text features are discussed breifly like Character N-gram, Frequent patterns, POS tag etc., IMHO Word n-gram , POS and other text features discussed should not be part of key topics discussed.",Topics look good.,12/12/2020 since no editor input by now I will just put the concept list I have here: Sentiment Classification; Opinion Holder; Opinion Target; Opinion; Polarity Analysis; Emotion Analysis; Character n-grams; Word n-gram; POS tag n-grams; Word Classes; Frequent Patterns in Text; Parse Tree-Based Features; Pattern Discovery Algorithms; Exhaustivity; Specificity.,,Sentiment Classification; Text features for Sentiment Analysis; List of text features used;Feature design & construction for Text Categorization;,sentiment classification; polarity analysis; emotion analysis; feature design; ,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a52,Lecture 45,Logistical Regression; Ordinal Logistic Regression; Sentiment Analysis; Sentiment Classification; Rating Prediction; Text Categorization; Binary Sentiment Categorization; Multi-Level Categorization/Ratings; Logistic Regression Classifiers; Independent/Dependent Classifiers,Looks fine,"Suggestions: some terms might be too broad while others are too in depth, for example text categorization as a concept is good but it might be too broad",,Logistical Regression; Ordinal Logistic Regression; Sentiment Analysis; Sentiment Classification; Rating Prediction; Binary Sentiment Categorization; Multi-Level Categorization; Multi-Level Ratings; Logistic Regression Classifiers; Independent Classifiers; Dependent Classifiers,logistic regression; sentiment analysis; rating prediction; ordinal logistic regression; multi level rating; k-1 independent logistic regression classifiers; ,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a53,Lecture 46,Opinion mining; Sentiment Analysis; Latent Aspect Rating Analysis; Opinion-based entity ranking; Aspect-level opinion summarization; Reviewer preference analysis; Solving LARA; Aspect segmentation; Latent rating regression; Finding aspect rating; User preference rating;, ,Opinion mining; Sentiment Analysis; Latent Aspect Rating Analysis;  Opinion-based entity ranking; Aspect-level opinion summarization; Reviewer preference analysis; Solving LARA; Aspect segmentation; Latent rating regression; Finding aspect rating; User preference rating; User feedback and review analysis; Aspect ratings; Aspect weights; Aspect segments; Correlated words; Topic models; Sentiment weights; Observed information; Latent; Generating Model; Probability Maximization; Normal Distribution; Weighted average; Gaussian distribution; Multivariate Gaussian prior; Positive sentiment; Negative sentiment; Maximum likelihood; ,,Opinion mining; Sentiment Analysis; Latent Aspect Rating Analysis;  Opinion-based entity ranking; Aspect-level opinion summarization; Reviewer preference analysis; Solving LARA; Aspect segmentation; Latent rating regression; Finding aspect rating; User preference rating; User feedback and review analysis; Aspect ratings; Aspect weights; Aspect segments; Correlated words; Topic models; Sentiment weights; Observed information; Latent; Generating Model; Probability Maximization; Normal Distribution; Weighted average; Gaussian distribution; Multivariate Gaussian prior; Positive sentiment; Negative sentiment; Maximum likelihood,Latent Aspect Rating Analysis;Opinion mining; Sentiment Analysis; aspect ratings; aspect weights; aspect segments; latent rating regression;,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a54,Lecture 47,Latent Aspect Rating Analysis; Opinion Mining;Unified Generative Model;Rated Aspect Summarization;Discovering Consumer Preferences;User Rating Behavior Analysis; Personalized Ranking of Entities,"Latent Aspect Rating Analysis; Opinion Mining; Unified Generative Model; Rated Aspect Summarization; Discovering Consumer Preferences; User Rating Behaviour Analysis; Personalized Ranking of Entities; LARA;  Aspect ratings; Aspect weights; Aspect segments; Regression model; Aspect Segmentation; Topic model, Word distribution; PLSA; Rating decomposition; Sentiment weights; Feature representation; Ordinal regression; ",,,"Latent Aspect Rating Analysis; Opinion Mining; Unified Generative Model; Rated Aspect Summarization; Discovering Consumer Preferences; User Rating Behaviour Analysis; Personalized Ranking of Entities; LARA;  Aspect ratings; Aspect weights; Aspect segments; Regression model; Aspect Segmentation; Topic model, Word distribution; PLSA; Rating decomposition; Sentiment weights; Feature representation; Ordinal regression; ",Latent Aspect Rating Analysis; unified generative problem; ,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a55,Lecture 48,"Text-Based Prediction;
Data Mining Loop;
Joint Mining of Non-Text and Text data;
Predictive Model;
Human role in Data mining Loop;
Active Learning;
Effective predictors;
Unified view of Text Mining and Analysis;
Contextual Text Mining;
Pattern Annotation;","Could also include:
Sub-tasks of text-based prediction task",Could also include: Pattern Annotation,,Text-Based Prediction; Data Mining Loop; Joint Mining of Non-Text and Text data; Predictive Model; Human role in Data mining Loop; Active Learning; Effective predictors; Unified view of Text Mining and Analysis; Contextual Text Mining; Pattern Annotation; Sub-tasks of text-based prediction task; Pattern Annotation,Text-Based Prediction;Joint Mining of Non-Text and Text data; data mining loop; contextual text mining; pattern annotation,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a56,Lecture 49,Contextual Text Mining; Text Partition; Document Context; Document Relevance; Comparative Analysis; Topic Discovery; Topic Meaning,,"Could change ""text partition"" to ""partioning of text""",,Contextual Text Mining; partitioning of text; Document Context; Document Relevance; Comparative Analysis; Topic Discovery; Topic Mining,contextual text mining; context; ,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a57,Lecture 50,"contextual text mining; contextual probabilistic latent semantic analysis;   CPLSA; 
generative models; probabilistic latent semantic analysis; PLSA; context variables; generative model; topic content; 
topic coverage; topic analysis; text analysis; text mining; contextualized topic discovery;
conditional likelihood; probabilistic models; word distributions; EM algorithm; 
parameter estimation; data visualization; trends; time series; conditional distribution; plot; 
curves; conditional probability; spatial temporal patterns; spatial distribution; temporal change; vector space model;
Boolean Model; information retrieval; 
",Could also include: Event Impact Analysis,"Could also include: Retrieval Models, Cross Collection ",,"contextual text mining; contextual probabilistic latent semantic analysis;  CPLSA; generative models; probabilistic latent semantic analysis; PLSA; context variables; generative model; topic content; topic coverage; topic analysis; text analysis; text mining; contextualized topic discovery; conditional likelihood; probabilistic models; word distributions; EM algorithm; parameter estimation; data visualization; trends; time series; conditional distribution; plot; curves; conditional probability; spatial temporal patterns; spatial distribution; temporal change; vector space model; Boolean Model; information retrieval; Event Impact Analysis; Retrieval Models, Cross Collection ",contextual text mining; contextual probabilistic latent semantic analysis; CPLSA; generation process,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a58,Lecture 51,CPLSA; PLSA; EM algorithm; context variables; contextual text mining; topic coverage; word distribution; semantic analysis; retrieval model,Looks good.,contextual text mining; social network context of text data; collaboration networks; geographic networks; joint analysis of text & network context; text metadata; network supervised topic modeling; probabilistic topic modeling; optimization; maximize likelihood estimation; heuristics; PLSA [probabilistic latent semantic analysis]; LDA [latent Dirichlet allocation]; applied mathematics; statistical topic modeling; network analysis; regularized topic modeling; Topic Modeling with Network Structure (TMN); adjacent vertices;,,CPLSA; PLSA; EM algorithm; context variables; contextual text mining; topic coverage; word distribution; semantic analysis; retrieval model; social network context of text data; collaboration networks; geographic networks; joint analysis of text and network context; text metadata; network supervised topic modeling; probabilistic topic modeling; optimization; maximize likelihood estimation; heuristics; PLSA; probabilistic latent semantic analysis; LDA; latent Dirichlet allocation; applied mathematics; statistical topic modeling; network analysis; regularized topic modeling; Topic Modeling with Network Structure; TMN; adjacent vertices;,contextual text mining; network context; Network supervised topic model;  NetPLSA; ,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a59,Lecture 52,Text Mining; Causal links; time series supervision; Granger Causality Test;  Correlation; Iterative Topic Modeling  ,contextual text mining with time series; joint analysis of text and time series data to discover causal topics; causal topic discovery; contextual PLSA (CPLSA); Iterative Causal Topic Modeling; Correlation Analysis; Topic Model Discovery; Autoregressive model; Pearson correlation; causality measures; text based prediction; big data; infer new knowledge; optimizing decision making; pattern annotation; ,Text Mining; Causal Topics; Time Series; Iterative Causal Topic Modeling; Topic Coherence; Topic - Time Series Correlation/Causality; Granger Causality Test.,,Text Mining; Causal links; time series supervision; Granger Causality Test; Correlation; Iterative Topic Modeling; contextual text mining with time series; joint analysis of text and time series data to discover causal topics; causal topic discovery; contextual PLSA; CPLSA; Iterative Causal Topic Modeling; Correlation Analysis; Topic Model Discovery; Autoregressive model; Pearson correlation; causality measures; text based prediction; big data; infer new knowledge; optimizing decision making; pattern annotation; Causal Topics; Time Series; Topic Coherence; Topic - Time Series Correlation; Topic - Time Series Causality; ,contextual text mining; time series context; Iterative Causal Topic Model; Granger Causality Test;,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a60,Lecture 53,"Natural Language Processing (NLP); text representation; word association mining & analysis; topic syntax analysis; topic mining & analysis; topic discovery; opinion mining; sentiment analysis; text-based prediction; knowledge discovery; state of the art text mining; word representation; statistical analysis; word-based representations; statistical techniques; statistical learning techniques in text mining; statistical learning; machine learning; applied mathematics; word association mining & analysis; paradigmatic & syntagmatic relations; sequences of data; text similarity; vector space model (for text representation); retrieval techniques; BM25; TF-IDF weighting; measuring text similarity; text retrieval; information theory; entropy; conditional entropy; mutual information; co-occurrence analysis of text data; text categorization; feature selection; probabilistic topic model; PLSA probabilistic latent semantic analysis; LDA latent dirichlet allocation; comparison / contrast of PLSA & LDA in practice, efficiency, etc.; generative model; maximum likelihood estimator (MLE); expectation maximization (EM) algorithm & applications to computing MLE; text clustering; text categorization; text mining application systems; mixture models; probabilistic topic models; similarity based approaches to text clustering; generative classifiers; Bayes rule; Naive Bayes; discriminative classifiers; logistic regression; k nearest neighbors; Support Vector Machines (SVMs); evaluation of results in text clustering & text categorization; sentiment classification problem; ordinal logistic regression; latent aspect rating analysis; text-based prediction; contextual text mining; contextual PLSA; NetPLSA; causal topic mining; time series data in text mining; areas for further research; NLP; statistical machine learning; supervised machine learning; deep learning; big data; word embedding technique; predictive modeling; data mining; information network mining; text retrieval; information retrieval; search engines; knowledge prominence; indexing techniques; ",,Looks good.,,text mining,,
https://classtranscribe.illinois.edu/video?id=cc2a1228-1f1e-4299-bbdd-ec01df0d4258&begin=14&from=sharedlink,Lecture 29,Web Search ; Web search challenges ; Web crawling,Lecture 29 is about text categorization methods instead of web search. ,Looks good.,,,,
https://classtranscribe.illinois.edu/offering/248bb7af-03d7-4ee6-abed-13642dae9c09#plid=860a1749-998e-406f-a8be-ee2d693f05a61,Lecture 35,objectice & subjective sensors; Opinion; Opinion Holder;Opinion target; opinion content; Opinion context; Opinion sentiment; Why opinion mining,Lecture 35 is about Text Clustering Evaluation instead of Opinion Mining,"Lecture 35 is not about Opinion Mining, some concepts I can add instead are: Clustering Bias, Direct Evaluation of Text Clusters, Indirect Evaluation of Text Clusters, Closeness, Usefulness",,,,
