WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-27T00:07:09.5641184Z by ClassTranscribe

00:00:00.280 --> 00:00:02.700
This lecture is about the web indexing.

00:00:11.860 --> 00:00:14.290
In this lecture we will continue

00:00:14.290 --> 00:00:16.740
talking about the web search and we're

00:00:16.740 --> 00:00:19.220
going to talk about how to create a web

00:00:19.220 --> 00:00:20.750
scale index.

00:00:24.550 --> 00:00:27.380
So once we crawled the web, we've got a

00:00:27.380 --> 00:00:29.020
lot of web pages.

00:00:29.570 --> 00:00:31.640
The next step is to use the indexer

00:00:31.640 --> 00:00:31.770
to

00:00:31.770 --> 00:00:33.810
Create the inverted index.

00:00:36.420 --> 00:00:38.600
In general, we can use the standard

00:00:38.600 --> 00:00:41.120
information retrieval techniques for

00:00:41.120 --> 00:00:43.360
creating the index, and that is what we

00:00:43.360 --> 00:00:45.790
talked about in the previous lecture.

00:00:45.790 --> 00:00:47.393
But there are new challenges that we

00:00:47.393 --> 00:00:50.380
have to solve for web scale indexing

00:00:50.380 --> 00:00:52.080
and the two main challenges.

00:00:52.080 --> 00:00:54.770
Our scalability and efficiency.

00:00:54.770 --> 00:00:58.210
The index would be so large that it

00:00:58.210 --> 00:01:01.220
cannot actually fit in into any single

00:01:01.220 --> 00:01:03.780
machine or a single disk, so we have to

00:01:03.780 --> 00:01:06.000
store the data on multiple machines.

00:01:06.790 --> 00:01:09.430
Also, because the data is so large,

00:01:09.430 --> 00:01:11.900
it's beneficial to process the data in

00:01:11.900 --> 00:01:14.030
parallel so that we can produce the

00:01:14.030 --> 00:01:15.450
index quickly.

00:01:15.450 --> 00:01:17.390
Now, to address these challenges,

00:01:17.390 --> 00:01:19.200
Google has made a number of

00:01:19.200 --> 00:01:20.110
innovations.

00:01:20.110 --> 00:01:22.390
One is the Google file system that's a

00:01:22.390 --> 00:01:24.552
general distributed file system that

00:01:24.552 --> 00:01:27.560
can help programmers manage files

00:01:27.560 --> 00:01:28.420
stored on

00:01:29.960 --> 00:01:30.940
a cluster of machines.

00:01:31.920 --> 00:01:33.270
The second is MapRecuce.

00:01:33.270 --> 00:01:35.290
This is a general software framework

00:01:35.290 --> 00:01:37.320
for supporting parallel computation.

00:01:38.840 --> 00:01:42.550
Hadoop is the most known open source

00:01:42.550 --> 00:01:46.340
implementation of map reduce, now used

00:01:46.340 --> 00:01:47.890
in many applications.

00:01:49.900 --> 00:01:51.500
So this is the architecture of the

00:01:51.500 --> 00:01:52.780
Google File System.

00:01:53.670 --> 00:01:56.760
It uses very simple centralized

00:01:56.760 --> 00:01:59.520
management mechanism to manage it all

00:01:59.520 --> 00:02:02.700
the specific locations of files, so

00:02:02.700 --> 00:02:05.650
that maintains the file name, space and

00:02:05.650 --> 00:02:08.730
look up a table to know where exactly

00:02:08.730 --> 00:02:09.920
each file is stored.

00:02:10.770 --> 00:02:13.390
The application client would then talk

00:02:13.390 --> 00:02:17.190
to this GFS master and then obtain

00:02:17.190 --> 00:02:20.350
specific locations of the files that

00:02:20.350 --> 00:02:21.490
they want to process.

00:02:22.790 --> 00:02:27.155
And once the GFS client obtained

00:02:27.155 --> 00:02:27.820
the.

00:02:28.860 --> 00:02:30.560
The specific information about the

00:02:30.560 --> 00:02:33.580
files, then the application client can

00:02:33.580 --> 00:02:37.710
talk to the specific servers where the

00:02:37.710 --> 00:02:41.750
data actually sit directly so that you can avoid

00:02:41.750 --> 00:02:44.100
involving other nodes in the network.

00:02:45.950 --> 00:02:48.550
So when this file system stores the

00:02:48.550 --> 00:02:52.980
files on machines the system also

00:02:52.980 --> 00:02:57.167
would create a fixed size of chunks

00:02:57.167 --> 00:02:59.450
so that data files are separated into

00:02:59.450 --> 00:03:01.430
many chunks.

00:03:01.430 --> 00:03:04.480
Each chunk is 64 MB, so it's

00:03:04.480 --> 00:03:06.370
pretty big, and that's a property for

00:03:06.370 --> 00:03:08.510
large data processing.

00:03:08.510 --> 00:03:11.080
These chunks are replicated to ensure

00:03:11.080 --> 00:03:14.410
reliability, so this is something that

00:03:14.410 --> 00:03:16.050
the programmer

00:03:16.100 --> 00:03:17.960
doesn't have to worry about.

00:03:19.350 --> 00:03:21.110
It's all taken care of by this fire

00:03:21.110 --> 00:03:23.420
system, so from the application

00:03:23.420 --> 00:03:25.380
perspective, the programmer would see

00:03:25.380 --> 00:03:28.400
this as if it's a normal file.

00:03:28.400 --> 00:03:30.650
The program doesn't have to know where

00:03:30.650 --> 00:03:34.120
exactly it's stored and can just

00:03:34.120 --> 00:03:37.860
invoke high level operators to process

00:03:37.860 --> 00:03:38.550
the file.

00:03:39.870 --> 00:03:40.480
(And)

00:03:41.450 --> 00:03:42.790
Another feature is that the data

00:03:42.790 --> 00:03:44.280
transfers directly between

00:03:44.280 --> 00:03:47.160
application and chunk servers, so it's

00:03:47.160 --> 00:03:48.820
efficient in this sense.

00:03:51.070 --> 00:03:54.300
On top of the Google File System and

00:03:54.300 --> 00:03:56.530
Google also proposed map reduce as a

00:03:56.530 --> 00:03:57.910
general framework for parallel

00:03:57.910 --> 00:03:58.490
programming.

00:03:59.090 --> 00:04:03.230
Now this is very useful to support a

00:04:03.230 --> 00:04:05.800
task like a building inverted index.

00:04:06.560 --> 00:04:10.390
So this framework is

00:04:12.030 --> 00:04:15.045
hiding a lot of low level features from

00:04:15.045 --> 00:04:15.880
the program.

00:04:15.880 --> 00:04:18.960
As a result, the programmer can make a

00:04:18.960 --> 00:04:21.880
minimum effort to create a application

00:04:21.880 --> 00:04:26.190
that can be run on large cluster in

00:04:26.190 --> 00:04:26.870
parallel.

00:04:27.770 --> 00:04:31.000
And so some of the low level details

00:04:31.000 --> 00:04:34.140
hidden in the framework, including the

00:04:34.140 --> 00:04:38.010
specific network communications or load

00:04:38.010 --> 00:04:42.920
balancing or where the tasks are

00:04:42.920 --> 00:04:43.860
executed.

00:04:43.860 --> 00:04:46.100
All these details are hidden from the

00:04:46.100 --> 00:04:46.810
programmer.

00:04:47.760 --> 00:04:50.055
There is also a nice feature which is

00:04:50.055 --> 00:04:52.110
the built-in fault tolerance.

00:04:52.110 --> 00:04:56.930
If one server is broken, let's say

00:04:56.930 --> 00:04:59.590
service down and then some tasks may

00:04:59.590 --> 00:05:02.370
not be finished, then the map reduce

00:05:02.370 --> 00:05:04.675
mechanism would know that the task has

00:05:04.675 --> 00:05:07.370
not been done, so automatically

00:05:07.370 --> 00:05:09.785
dispatches the task on other servers

00:05:09.785 --> 00:05:12.245
that can do the job and therefore again

00:05:12.245 --> 00:05:12.850
the program

00:05:12.850 --> 00:05:14.745
doesn't have to worry about that.

00:05:14.745 --> 00:05:17.760
So here's how MapReduce works.

00:05:17.760 --> 00:05:18.460
The input

00:05:18.570 --> 00:05:21.570
data will be separated into a number of

00:05:21.570 --> 00:05:22.770
key value pairs.

00:05:22.770 --> 00:05:25.260
Now, what exactly is in the value will

00:05:25.260 --> 00:05:27.290
depend on the data, and it's actually a

00:05:27.290 --> 00:05:29.585
fairly general framework to allow you

00:05:29.585 --> 00:05:31.930
to just partition the data into

00:05:31.930 --> 00:05:33.910
different parts, and each part can be

00:05:33.910 --> 00:05:35.700
then processed in parallel.

00:05:36.980 --> 00:05:39.870
Each key value pair would be send

00:05:39.870 --> 00:05:41.100
to a map function.

00:05:42.060 --> 00:05:44.310
The programmer would write map function

00:05:44.310 --> 00:05:44.940
of course.

00:05:45.500 --> 00:05:48.350
And then the map function would then

00:05:48.350 --> 00:05:50.620
process this key value pair and would

00:05:50.620 --> 00:05:52.940
generator a number of other key value

00:05:52.940 --> 00:05:53.640
pairs.

00:05:53.640 --> 00:05:55.710
Of course the new key is usually

00:05:55.710 --> 00:06:00.000
different from the old key that's given

00:06:00.000 --> 00:06:01.250
to the map as input.

00:06:01.980 --> 00:06:03.730
And these key value pairs are the

00:06:03.730 --> 00:06:07.150
output of the map function and all the

00:06:07.150 --> 00:06:09.620
outputs of all the map functions would

00:06:09.620 --> 00:06:10.620
be then collected.

00:06:11.170 --> 00:06:13.480
And then there will be for the sorted

00:06:13.480 --> 00:06:16.695
based on the key, and the result is

00:06:16.695 --> 00:06:19.270
that all the values that are associated

00:06:19.270 --> 00:06:21.960
with the same key would be then grouped

00:06:21.960 --> 00:06:22.740
together.

00:06:22.740 --> 00:06:26.810
So now we've got a pair of a key and a

00:06:26.810 --> 00:06:29.060
set of values that are attached to this

00:06:29.060 --> 00:06:29.410
key.

00:06:31.560 --> 00:06:34.560
So this will then be sent to a Reduce

00:06:34.560 --> 00:06:35.050
function.

00:06:36.170 --> 00:06:37.780
Now of course, each Reduce function

00:06:37.780 --> 00:06:40.597
will handle a differentÂ 

00:06:40.597 --> 00:06:45.550
key, so we will send these output values

00:06:45.550 --> 00:06:49.040
to multiple, reduce functions, each

00:06:49.040 --> 00:06:50.720
handling unique key.

00:06:52.130 --> 00:06:55.370
A reduce function would then process the

00:06:55.370 --> 00:06:55.970
input.

00:06:56.550 --> 00:07:00.460
Which is a key and a set of values to

00:07:00.460 --> 00:07:03.360
produce another set of key values as

00:07:03.360 --> 00:07:04.120
the output.

00:07:04.810 --> 00:07:07.030
So these output values will be then

00:07:07.030 --> 00:07:10.715
collected together to form the final

00:07:10.715 --> 00:07:11.360
output.

00:07:12.310 --> 00:07:15.300
Right, so this is the general framework

00:07:15.300 --> 00:07:17.020
of MapReduce.

00:07:17.020 --> 00:07:19.780
Now the programmer only needs to write the

00:07:19.780 --> 00:07:23.140
Map function and the Reduce function.

00:07:23.140 --> 00:07:25.340
Everything else is actually taken care

00:07:25.340 --> 00:07:27.290
of by the MapReduce framework.

00:07:27.960 --> 00:07:30.590
So you can see the program really only

00:07:30.590 --> 00:07:32.160
needs to do minimum work.

00:07:32.800 --> 00:07:36.690
And with such a framework the input data can be

00:07:36.690 --> 00:07:38.760
partitioned into multiple parts, each

00:07:38.760 --> 00:07:42.365
is processed in parallel, first by map,

00:07:42.365 --> 00:07:48.230
and then in the process after we reach

00:07:48.230 --> 00:07:51.299
the reduce stage, then multiple reduce

00:07:51.300 --> 00:07:54.390
functions can also further process

00:07:55.750 --> 00:07:58.720
the different keys and their associated

00:07:58.720 --> 00:08:02.030
values in parallel, so it achieves

00:08:02.030 --> 00:08:03.360
(some)

00:08:05.250 --> 00:08:08.080
It achieves the purpose of parallel

00:08:08.080 --> 00:08:10.460
processing of large data set.

00:08:10.460 --> 00:08:12.950
So let's take a look at a simple

00:08:12.950 --> 00:08:15.180
example and that's what accounting.

00:08:16.060 --> 00:08:20.110
How the input is files containing

00:08:20.110 --> 00:08:20.730
words.

00:08:21.460 --> 00:08:23.260
And the output that we want to generate

00:08:23.260 --> 00:08:25.460
is the number of occurrences of

00:08:25.460 --> 00:08:27.240
each word, so it's the word account.

00:08:27.940 --> 00:08:30.570
We know this kind of counting would be

00:08:30.570 --> 00:08:33.580
useful to, for example, assess the

00:08:33.580 --> 00:08:35.360
popularity of a word in a large

00:08:35.360 --> 00:08:38.150
collection, and this is useful for

00:08:38.150 --> 00:08:40.410
achieving effect of IDF weiging.

00:08:41.100 --> 00:08:42.060
Or search.

00:08:42.780 --> 00:08:44.690
So how can we solve this problem?

00:08:44.690 --> 00:08:46.670
One natural thought is that.

00:08:47.400 --> 00:08:50.320
This task can be done in parallel by

00:08:50.320 --> 00:08:52.389
simply counting different parts of the

00:08:52.390 --> 00:08:55.000
file in parallel, and then in the end we

00:08:55.000 --> 00:08:57.270
just combine all the counts, and that's

00:08:57.270 --> 00:09:00.730
precisely the idea of what we can do

00:09:00.730 --> 00:09:01.990
with MapReduce.

00:09:02.740 --> 00:09:05.740
We can parallelize on lines in this

00:09:05.740 --> 00:09:06.600
input file.

00:09:07.590 --> 00:09:10.290
So more specifically, we can assume the

00:09:10.290 --> 00:09:13.280
input to each map function is key

00:09:14.870 --> 00:09:16.830
value pair that represents the line

00:09:16.830 --> 00:09:20.369
number and the stream on that line.

00:09:20.370 --> 00:09:23.650
So the first line, for example, has the

00:09:23.650 --> 00:09:27.790
key of 1 and the value is "Hello World"

00:09:27.790 --> 00:09:28.640
Â "Bye World"

00:09:28.640 --> 00:09:29.890
and just 4 words.

00:09:31.240 --> 00:09:33.760
On that line so this key value pair will

00:09:33.760 --> 00:09:35.904
be sent to a map function.

00:09:35.904 --> 00:09:38.422
The map function would then just count

00:09:38.422 --> 00:09:42.039
the words in this line, and in this

00:09:42.040 --> 00:09:43.525
case of course there are only four

00:09:43.525 --> 00:09:43.880
words.

00:09:43.880 --> 00:09:46.320
Each word gets a count of one, and

00:09:46.320 --> 00:09:49.620
these are the output that you see here

00:09:49.620 --> 00:09:50.680
on this slide.

00:09:50.680 --> 00:09:51.990
From this map function.

00:09:52.690 --> 00:09:55.080
So the map function is really very

00:09:55.080 --> 00:09:56.770
simple if you look at the what the

00:09:56.770 --> 00:09:59.040
pseudocode looks like on the right

00:09:59.040 --> 00:10:02.100
side you see it simply needs to iterate

00:10:02.100 --> 00:10:05.200
over all the words in this line and

00:10:05.200 --> 00:10:09.580
then just call the collect function, which

00:10:09.580 --> 00:10:12.580
means it would then send the world and

00:10:12.580 --> 00:10:14.094
the counter to the collector.

00:10:14.094 --> 00:10:16.030
The collector would then try to sort all

00:10:16.030 --> 00:10:16.320
these.

00:10:16.880 --> 00:10:20.020
key value pairs from different

00:10:20.020 --> 00:10:22.350
Map functions, so the function is very

00:10:22.350 --> 00:10:25.690
simple and the programmer specifies

00:10:25.690 --> 00:10:28.980
this function as a way to process each

00:10:28.980 --> 00:10:30.500
part of the data.

00:10:31.500 --> 00:10:32.959
Of course, the second line will be

00:10:32.960 --> 00:10:34.620
handled by a different map function,

00:10:34.620 --> 00:10:36.890
which will produce a similar output.

00:10:36.890 --> 00:10:39.360
OK, now the output from the map

00:10:39.360 --> 00:10:41.430
functions will be then send to a

00:10:41.430 --> 00:10:42.980
collector and the collector will do the

00:10:42.980 --> 00:10:45.185
internal grouping or sorting.

00:10:45.185 --> 00:10:48.170
So at this stage you can see we have collected

00:10:48.170 --> 00:10:51.520
multiple pairs, each pair is a word and

00:10:51.520 --> 00:10:53.806
its count in the line.

00:10:53.806 --> 00:10:59.570
So once we see all these pairs then we

00:10:59.570 --> 00:11:02.476
can sort them based on the key which is

00:11:02.476 --> 00:11:03.280
the word.

00:11:03.550 --> 00:11:06.040
So we will collect all the counts of

00:11:06.040 --> 00:11:08.780
a word like a "Bye" here together.

00:11:09.370 --> 00:11:10.620
An similarly we do that

00:11:10.620 --> 00:11:13.900
for other words like "Hadoop", "Hello" etc.

00:11:13.900 --> 00:11:16.160
So each world now is attached to a

00:11:16.160 --> 00:11:18.430
number of values, a number of

00:11:18.430 --> 00:11:19.180
counts.

00:11:20.590 --> 00:11:24.230
And these counts represent the

00:11:24.230 --> 00:11:26.470
occurrences of this word in different

00:11:26.470 --> 00:11:27.040
lines.

00:11:27.770 --> 00:11:31.340
So now we have got a new pair of a key

00:11:31.340 --> 00:11:34.060
and a set of values and this pair will

00:11:34.060 --> 00:11:36.600
then be feeding to reduce function.

00:11:36.600 --> 00:11:39.300
So the reduce function now would have

00:11:39.300 --> 00:11:42.680
to finish the job of counting the total

00:11:42.680 --> 00:11:44.030
occurrences of this word.

00:11:44.030 --> 00:11:46.040
Now it has already got all these

00:11:46.040 --> 00:11:48.270
partial accounts, so all it needs to do

00:11:48.270 --> 00:11:51.067
is similarly to add them up so the reduce

00:11:51.067 --> 00:11:53.647
function shown here is very simple as well.

00:11:53.647 --> 00:11:55.800
You have a counter and then iterate

00:11:55.800 --> 00:11:58.180
over all the words that you see

00:11:58.300 --> 00:12:00.590
in this array, and then you just

00:12:00.590 --> 00:12:01.860
accumulated the count.

00:12:02.420 --> 00:12:05.640
And then finally output the key and the

00:12:05.640 --> 00:12:07.880
total count, and that's precisely what

00:12:07.880 --> 00:12:10.400
we want as the output of this whole

00:12:10.400 --> 00:12:11.050
program.

00:12:12.130 --> 00:12:14.320
So you can see this is already very

00:12:14.320 --> 00:12:16.930
similar to building an inverted index,

00:12:16.930 --> 00:12:19.460
and if you think about it, the output

00:12:19.460 --> 00:12:22.699
here is indexed by world and we have

00:12:22.699 --> 00:12:23.980
already got the dictionary.

00:12:23.980 --> 00:12:26.960
Basically we have got the counts, but

00:12:26.960 --> 00:12:31.310
what's missing is the document IDs and

00:12:31.310 --> 00:12:32.400
the specific

00:12:33.320 --> 00:12:36.940
frequency counts of words in those

00:12:36.940 --> 00:12:38.980
documents, so we can modify this

00:12:38.980 --> 00:12:41.120
slightly to actually build inverted

00:12:41.120 --> 00:12:42.370
index in parallel.

00:12:42.370 --> 00:12:45.039
So here's one way to do that.

00:12:45.039 --> 00:12:48.280
So in this case we can assume the input

00:12:48.280 --> 00:12:51.460
to map function is a pair of a key,

00:12:51.460 --> 00:12:55.670
which denotes the document ID and the value

00:12:55.670 --> 00:12:59.027
denoting the stream for that document.

00:12:59.027 --> 00:13:01.616
So it's all the words in that document,

00:13:01.616 --> 00:13:03.730
and so the map function will do

00:13:03.730 --> 00:13:05.300
something very similar to what we have

00:13:05.300 --> 00:13:07.130
seen in the word count example.

00:13:07.870 --> 00:13:11.470
It simply groups all the counts of this

00:13:11.470 --> 00:13:14.980
word in this document together and it

00:13:14.980 --> 00:13:17.210
would then generate the set of key

00:13:17.210 --> 00:13:17.970
value pairs.

00:13:17.970 --> 00:13:19.530
Each key is a word.

00:13:20.240 --> 00:13:23.700
And the value is the count of this

00:13:23.700 --> 00:13:26.480
orld in this document, plus the

00:13:26.480 --> 00:13:27.500
document ID.

00:13:27.500 --> 00:13:30.000
Now you can easily see why we need to

00:13:30.000 --> 00:13:31.460
add document ID here.

00:13:31.460 --> 00:13:33.685
Of course later in the inverted index

00:13:33.685 --> 00:13:35.640
we would like to keep this information

00:13:35.640 --> 00:13:38.040
so the map function should keep track

00:13:38.040 --> 00:13:40.300
of it and this can be sent to the

00:13:40.300 --> 00:13:41.490
reduce function later.

00:13:42.250 --> 00:13:44.840
Similarly, another document D2 can be

00:13:44.840 --> 00:13:47.150
processed in the same way, so in the

00:13:47.150 --> 00:13:48.650
end that again there was a sorting

00:13:48.650 --> 00:13:49.920
mechanism that would group them

00:13:49.920 --> 00:13:52.680
together and then we will have just

00:13:53.630 --> 00:13:57.395
a key like "Java" associated with all the

00:13:57.395 --> 00:14:01.410
documents that match this key or the

00:14:01.410 --> 00:14:03.050
documents where "Java" occurred.

00:14:04.000 --> 00:14:05.440
And the counts.

00:14:05.440 --> 00:14:08.191
So the counts of Java in those

00:14:08.191 --> 00:14:08.577
documents.

00:14:08.577 --> 00:14:11.394
And this will be collected together and

00:14:11.394 --> 00:14:15.125
this will be also fed into the reduce

00:14:15.125 --> 00:14:15.562
function.

00:14:15.562 --> 00:14:17.870
So now you can see the reduce function

00:14:17.870 --> 00:14:19.120
has already got input

00:14:19.120 --> 00:14:21.350
that looks like a inverted index entry,

00:14:21.350 --> 00:14:21.735
right?

00:14:21.735 --> 00:14:24.972
So it's just the word and all the

00:14:24.972 --> 00:14:27.477
documents that contain the word and

00:14:27.477 --> 00:14:30.266
the frequencies of the word in those

00:14:30.266 --> 00:14:30.617
documents.

00:14:30.617 --> 00:14:33.060
So all it needs to do is simply to

00:14:33.060 --> 00:14:34.380
concatenate them

00:14:34.620 --> 00:14:40.850
into a continuous chunk of data, and

00:14:40.850 --> 00:14:42.450
this can be then written into a file

00:14:42.450 --> 00:14:43.590
system.

00:14:43.590 --> 00:14:46.260
So basically the reduce function is

00:14:46.260 --> 00:14:48.080
going to do very minimum work.

00:14:49.370 --> 00:14:50.650
So this is

00:14:52.900 --> 00:14:55.910
pseudocode for inverted index

00:14:55.910 --> 00:14:57.190
construction.

00:14:57.190 --> 00:15:00.340
Here we see two functions.

00:15:01.880 --> 00:15:04.090
Procedure Map and procedure Reduce.

00:15:05.140 --> 00:15:08.150
And a program with the specify these two

00:15:08.150 --> 00:15:12.159
functions to program on top of map

00:15:12.160 --> 00:15:16.830
reduce and you can see basically they

00:15:16.830 --> 00:15:18.460
are doing what I just described.

00:15:18.460 --> 00:15:22.140
In the case of map, it's going to count

00:15:22.140 --> 00:15:25.260
the occurrences of word using

00:15:25.260 --> 00:15:28.050
associative array and will output the

00:15:28.050 --> 00:15:30.200
old accounts together with the document

00:15:30.200 --> 00:15:32.220
ID here.

00:15:34.280 --> 00:15:39.070
So this is the reduce function

00:15:39.070 --> 00:15:42.310
On the other hand, simply concatenates

00:15:42.310 --> 00:15:47.040
all the input that it has been given

00:15:47.040 --> 00:15:50.300
and then put them together as one

00:15:50.300 --> 00:15:53.180
single entry for this key.

00:15:53.180 --> 00:15:57.470
So this is a very simple MapReduce

00:15:57.470 --> 00:15:59.080
function, yet it would allow us to

00:15:59.080 --> 00:16:02.100
construct the inverted index at very

00:16:02.100 --> 00:16:04.050
large scale and the data can be

00:16:04.050 --> 00:16:04.980
processed

00:16:05.030 --> 00:16:06.310
by different machines.

00:16:06.870 --> 00:16:09.510
The program doesn't have to take care

00:16:09.510 --> 00:16:11.080
of the details.

00:16:12.010 --> 00:16:16.180
So this is how we can do parallel index

00:16:16.180 --> 00:16:19.060
construction for web search.

00:16:19.990 --> 00:16:23.070
So to summarize, web scale indexing

00:16:23.070 --> 00:16:24.930
requires some new techniques that go

00:16:24.930 --> 00:16:27.390
beyond the standard traditional

00:16:27.390 --> 00:16:30.360
indexing techniques, mainly will have

00:16:30.360 --> 00:16:32.280
to store the index on multiple

00:16:32.280 --> 00:16:35.110
machines, and this is usually done by

00:16:35.110 --> 00:16:37.350
using file system like a Google File

00:16:37.350 --> 00:16:39.520
System, a distributed file system.

00:16:40.100 --> 00:16:43.534
And secondly, it requires creating the

00:16:43.534 --> 00:16:45.810
index in parallel because it's so large

00:16:45.810 --> 00:16:48.560
it takes a long time to create the index

00:16:48.560 --> 00:16:49.900
for all the documents.

00:16:49.900 --> 00:16:52.770
So if we can do it in parallel it will

00:16:52.770 --> 00:16:54.950
be much faster and this is done by

00:16:54.950 --> 00:16:56.840
using the MapReduce framework.

00:16:57.700 --> 00:17:00.090
Note that the post the GFS and

00:17:00.090 --> 00:17:02.350
MapReduce frameworks are very general

00:17:02.350 --> 00:17:04.200
so they can also support many other

00:17:04.200 --> 00:17:05.050
applications.


