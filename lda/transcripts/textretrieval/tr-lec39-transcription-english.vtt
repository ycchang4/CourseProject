WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-27T00:07:09.9901975Z by ClassTranscribe

00:00:00.280 --> 00:00:01.940
There are some interesting

00:00:01.940 --> 00:00:03.920
challenges in threshold learning in the

00:00:03.920 --> 00:00:04.760
filtering problem.

00:00:04.760 --> 00:00:07.550
So here I show the historical data that

00:00:07.550 --> 00:00:09.980
you can collect in a filtering system

00:00:09.980 --> 00:00:13.560
so you can see the scores and the

00:00:13.560 --> 00:00:15.870
status of relevance.

00:00:15.870 --> 00:00:19.120
So the first one it has a score of 36.5

00:00:19.120 --> 00:00:20.770
and it's relevant.

00:00:30.690 --> 00:00:33.950
The second one is non-relevant and etc.

00:00:33.950 --> 00:00:36.400
Of course we have a lot of documents

00:00:36.400 --> 00:00:37.920
for which we don't know the status

00:00:37.920 --> 00:00:39.540
because we have never delivered them to

00:00:39.540 --> 00:00:40.070
the user.

00:00:40.800 --> 00:00:43.340
So as you can see here, we only see the

00:00:43.340 --> 00:00:46.950
judgments of documents delivered to the

00:00:46.950 --> 00:00:49.520
user, so this is not a random sample,

00:00:49.520 --> 00:00:51.650
so it's censored data.

00:00:51.650 --> 00:00:53.380
It's kind of biased.

00:00:54.360 --> 00:00:56.925
So that creates some difficulty for

00:00:56.925 --> 00:00:59.560
learning, and secondly there are in

00:00:59.560 --> 00:01:01.093
general very little labeled data, 

00:01:01.093 --> 00:01:04.967
and very few relevant data, so

00:01:04.967 --> 00:01:06.650
it's also challenging for machine

00:01:06.650 --> 00:01:07.870
learning approaches.

00:01:07.870 --> 00:01:11.960
Typically they require more

00:01:11.960 --> 00:01:15.112
training data, and in the extreme case

00:01:15.112 --> 00:01:17.250
at the beginning we don't even have any

00:01:17.250 --> 00:01:18.695
label data as well.

00:01:18.695 --> 00:01:20.700
The system still has to make a

00:01:20.700 --> 00:01:22.750
decision, so that's a very difficult

00:01:22.750 --> 00:01:23.990
problem at the beginning.

00:01:24.370 --> 00:01:27.130
Finally, there is also this issue of

00:01:27.130 --> 00:01:28.820
exploration versus exploitation

00:01:28.820 --> 00:01:29.540
tradeoff.

00:01:30.250 --> 00:01:36.157
Now this means we also want to explore

00:01:36.157 --> 00:01:39.150
the document space a little bit

00:01:39.150 --> 00:01:42.506
and to see if the user might be interested

00:01:42.506 --> 00:01:45.470
in documents that we haven't delivered.

00:01:45.470 --> 00:01:48.257
So in other words, we're going to

00:01:48.257 --> 00:01:51.570
explore the space of user interests by

00:01:51.570 --> 00:01:53.330
testing whether the user might be

00:01:53.330 --> 00:01:56.419
interested in some other documents that

00:01:56.420 --> 00:01:59.760
currently are not matching the users'

00:01:59.760 --> 00:02:00.730
interests so well.

00:02:01.550 --> 00:02:02.680
So how do we do that?

00:02:02.680 --> 00:02:04.310
Well, we could lower the threshold a

00:02:04.310 --> 00:02:07.230
little bit and do just deliver some

00:02:07.230 --> 00:02:10.200
near misses to the user to see what the

00:02:10.200 --> 00:02:11.610
user would respond, 

00:02:13.120 --> 00:02:16.540
to see how the user would respond

00:02:16.540 --> 00:02:18.990
to this extra document.

00:02:19.800 --> 00:02:22.400
And this is the trade-off because on the

00:02:22.400 --> 00:02:24.426
one hand you want to explore, but on

00:02:24.426 --> 00:02:26.790
the other hand you don't want to really

00:02:26.790 --> 00:02:28.460
explore too much 'cause then you would

00:02:28.460 --> 00:02:31.025
overdeliver non-relevant information.

00:02:31.025 --> 00:02:34.370
So exploitation means you would exploit

00:02:34.370 --> 00:02:35.760
what you learned about user.

00:02:35.760 --> 00:02:37.730
Let's say you know the user is interested in

00:02:37.730 --> 00:02:39.742
this particular topic so you don't want

00:02:39.742 --> 00:02:41.495
to deviate that much.

00:02:41.495 --> 00:02:44.402
But if you don't deviate at all then

00:02:44.402 --> 00:02:45.480
you don't explore it all.

00:02:45.480 --> 00:02:46.445
That's also not good.

00:02:46.445 --> 00:02:49.160
You might miss opportunity to learn another

00:02:49.160 --> 00:02:50.320
interest of the user.

00:02:51.820 --> 00:02:53.970
So this is a dilemma.

00:02:54.690 --> 00:02:57.430
And that's also a difficult problem to

00:02:57.430 --> 00:02:57.860
solve.

00:02:58.780 --> 00:03:00.470
Now how do we solve these problems?

00:03:00.470 --> 00:03:00.805
In general, 

00:03:00.805 --> 00:03:02.730
I think one can use the empirical

00:03:02.730 --> 00:03:05.705
utility optimization strategy, and this

00:03:05.705 --> 00:03:07.783
strategy is basically to optimize the

00:03:07.783 --> 00:03:10.085
threshold based on historical data,

00:03:10.085 --> 00:03:11.920
just as you have seen on the previous

00:03:11.920 --> 00:03:12.760
slide.

00:03:12.760 --> 00:03:15.322
So you can just compute the utility on

00:03:15.322 --> 00:03:17.520
the training data for each candidate 

00:03:17.520 --> 00:03:18.750
score threshold.

00:03:18.750 --> 00:03:21.625
Pretend what if I cut at this point.

00:03:21.625 --> 00:03:23.810
What if I can cut at a different

00:03:23.810 --> 00:03:26.510
scoring threshold point what would

00:03:26.510 --> 00:03:28.560
happen, what's utility?

00:03:28.830 --> 00:03:30.640
Since these are training data, we can

00:03:30.640 --> 00:03:33.530
kind of compute the utility, right?

00:03:33.530 --> 00:03:35.910
We know their relevance status or we

00:03:35.910 --> 00:03:39.270
assume that we know relevant status

00:03:39.270 --> 00:03:41.370
that's based on approximation of

00:03:41.370 --> 00:03:42.100
clickthroughs.

00:03:42.100 --> 00:03:44.630
So then we can just choose the

00:03:44.630 --> 00:03:46.460
threshold that gives the maximum

00:03:46.460 --> 00:03:48.170
utility on the training data.

00:03:49.570 --> 00:03:52.470
But this of course doesn't account for

00:03:52.470 --> 00:03:55.280
exploration that we just talked about.

00:03:56.250 --> 00:03:58.300
And there is also the difficulty of

00:03:58.300 --> 00:04:00.360
bias training sample as we mentioned.

00:04:01.410 --> 00:04:03.720
So in general we can only get upper

00:04:03.720 --> 00:04:06.325
bound for the true optimal

00:04:06.325 --> 00:04:10.330
threshold, because the threshold might

00:04:10.330 --> 00:04:12.360
be actually lower than this.

00:04:13.080 --> 00:04:15.350
So it's possible that the discarded

00:04:15.350 --> 00:04:18.110
item might be actually interesting to

00:04:18.110 --> 00:04:18.740
the user.

00:04:19.680 --> 00:04:21.595
So how do we solve this problem where

00:04:21.595 --> 00:04:23.010
we generate and

00:04:23.010 --> 00:04:25.300
as I said, we can lower the threshold

00:04:25.300 --> 00:04:27.760
to explore a little bit, so here's one

00:04:27.760 --> 00:04:29.260
particular approach called better

00:04:29.260 --> 00:04:30.670
gamma threshold learning.

00:04:30.670 --> 00:04:32.390
So the idea is following.

00:04:32.390 --> 00:04:35.640
So here I show a ranked list of all the

00:04:35.640 --> 00:04:37.460
training documents that we have seen so

00:04:37.460 --> 00:04:39.380
far, and they are ranked by their

00:04:39.380 --> 00:04:41.981
positions. And on the y-axis, 

00:04:41.981 --> 00:04:43.068
We show the utility.

00:04:43.068 --> 00:04:44.900
Of course this function depends on how

00:04:44.900 --> 00:04:47.260
you specify the coefficients in the

00:04:47.260 --> 00:04:49.150
utility function, but we can then

00:04:49.150 --> 00:04:49.750
imagine.

00:04:49.800 --> 00:04:52.150
that depending on the cut off position, we

00:04:52.150 --> 00:04:54.650
will have a utility that means.

00:04:54.650 --> 00:04:58.360
Suppose I cut at this position and that

00:04:58.360 --> 00:04:59.959
would be the utility.

00:05:01.080 --> 00:05:04.530
So we can, for example identify some

00:05:04.530 --> 00:05:06.442
cutting cut off point.

00:05:06.442 --> 00:05:10.310
The optimal point theta optimal is

00:05:10.310 --> 00:05:12.940
the point when we would achieve the

00:05:12.940 --> 00:05:15.750
maximum utility if we had chosen this

00:05:15.750 --> 00:05:16.570
threshold.

00:05:17.390 --> 00:05:20.907
And there is also zero threshold zero

00:05:20.907 --> 00:05:23.645
utility threshold, and you can see at

00:05:23.645 --> 00:05:27.470
this cut off the utility is 0. Now what

00:05:27.470 --> 00:05:28.260
does that mean?

00:05:28.260 --> 00:05:30.340
That means if I lower the threshold a

00:05:30.340 --> 00:05:33.452
little bit and now I reach this

00:05:33.452 --> 00:05:35.890
threshold, the utility would be lower,

00:05:35.890 --> 00:05:38.700
but it's still positive it's still non

00:05:38.700 --> 00:05:39.870
negative at least.

00:05:39.870 --> 00:05:43.670
So it's not as high as the optimal

00:05:43.670 --> 00:05:44.650
utility.

00:05:45.680 --> 00:05:50.010
But it gives us a safe point to explore

00:05:50.010 --> 00:05:50.820
the threshold.

00:05:51.380 --> 00:05:53.629
As I just explained, it's desirable to

00:05:53.630 --> 00:05:56.493
explore the interest space, so it's

00:05:56.493 --> 00:05:58.840
desirable to lower the threshold based

00:05:58.840 --> 00:05:59.840
on your training data.

00:06:00.520 --> 00:06:02.350
So that means in general we want to set

00:06:02.350 --> 00:06:04.690
the threshold somewhere in this range.

00:06:04.690 --> 00:06:06.650
Let's say we can use alpha to control

00:06:06.650 --> 00:06:07.040
the deviation from

00:06:08.450 --> 00:06:11.920
the optimal utility

00:06:11.920 --> 00:06:14.776
point so you can see the formula of the

00:06:14.776 --> 00:06:15.600
threshold would be just the

00:06:15.600 --> 00:06:18.545
interpolation of the zero utility

00:06:18.545 --> 00:06:20.590
threshold and the optimal utility

00:06:20.590 --> 00:06:21.030
threshold.

00:06:22.370 --> 00:06:25.330
Now the question is how should we set

00:06:25.330 --> 00:06:25.870
alpha?

00:06:27.440 --> 00:06:30.170
And when should we deviate more from the

00:06:30.170 --> 00:06:32.070
optimal utility point?

00:06:33.630 --> 00:06:36.370
Well this can depend on multiple factors and

00:06:36.370 --> 00:06:38.450
one way to solve the problem is to

00:06:38.450 --> 00:06:40.320
encourage this

00:06:41.170 --> 00:06:44.520
threshold mechanism to explore up to

00:06:44.520 --> 00:06:47.510
the zero point, and that's a safe

00:06:47.510 --> 00:06:49.290
point, but we're not going to

00:06:49.290 --> 00:06:51.686
necessarily reach all the way to the

00:06:51.686 --> 00:06:54.099
zero point, but rather we're going to

00:06:54.100 --> 00:06:57.180
use other parameters to further define

00:06:57.180 --> 00:06:59.490
alpha, and this specifically is as

00:06:59.490 --> 00:07:00.060
follows.

00:07:00.950 --> 00:07:02.849
So there will be a beta parameter to

00:07:02.850 --> 00:07:06.600
control the deviation from the optimal

00:07:06.600 --> 00:07:08.650
threshold, and this can be based on

00:07:08.650 --> 00:07:11.430
for example can be accounting for the

00:07:11.430 --> 00:07:12.970
overfitting to the training data let's say.

00:07:12.970 --> 00:07:16.390
And so this can be just

00:07:16.390 --> 00:07:17.610
adjustment factor.

00:07:17.610 --> 00:07:19.450
But what's more interesting is this

00:07:19.450 --> 00:07:22.655
gamma parameter here and you can see in

00:07:22.655 --> 00:07:22.970
this formula, 

00:07:24.380 --> 00:07:29.230
gamma is controlling the

00:07:29.230 --> 00:07:34.380
influence of the number of examples in

00:07:34.380 --> 00:07:35.540
training dataset.

00:07:36.100 --> 00:07:40.180
So you can see it in this formula as

00:07:40.180 --> 00:07:41.770
N which denotes the number of

00:07:41.770 --> 00:07:45.110
training examples becomes bigger, then

00:07:45.110 --> 00:07:48.890
it would actually encourage less

00:07:48.890 --> 00:07:49.895
exploration.

00:07:49.895 --> 00:07:53.090
In other words, when N is very small,

00:07:53.090 --> 00:07:55.670
it would try to explore more, and that

00:07:55.670 --> 00:08:00.460
just means if we have seen few examples

00:08:00.460 --> 00:08:02.890
we're not sure whether we have exhausted

00:08:02.890 --> 00:08:04.140
the space of interests.

00:08:04.140 --> 00:08:05.310
So we would explore.

00:08:05.860 --> 00:08:08.590
But as we have seen many examples from

00:08:08.590 --> 00:08:11.160
the user, many data points, then we

00:08:11.160 --> 00:08:12.719
feel that we probably don't have to

00:08:12.720 --> 00:08:13.483
explore more.

00:08:13.483 --> 00:08:16.280
So this gives us a dynamic strategy for

00:08:16.280 --> 00:08:17.507
exploration, right?

00:08:17.507 --> 00:08:19.940
The more examples we have seen, the

00:08:19.940 --> 00:08:21.830
less explosion we're going to do, so

00:08:21.830 --> 00:08:24.020
the threshold would be closer to the

00:08:24.020 --> 00:08:25.325
optimal threshold.

00:08:25.325 --> 00:08:27.180
So that's the basic idea of this

00:08:27.180 --> 00:08:27.620
approach.

00:08:28.260 --> 00:08:31.520
Now this approach, it actually has been

00:08:31.520 --> 00:08:33.940
working well in some evaluation studies

00:08:33.940 --> 00:08:35.340
empirically effective.

00:08:35.910 --> 00:08:40.070
And also can work on arbitrary utility

00:08:40.070 --> 00:08:42.450
with a proper lower bound.

00:08:42.450 --> 00:08:46.289
And it explicitly addresses the exploration-

00:08:46.290 --> 00:08:49.100
exploitation tradeoff and it kind of uses

00:08:49.100 --> 00:08:52.480
the zero utility threshold point as a

00:08:52.480 --> 00:08:55.649
safeguard for exploration and

00:08:55.650 --> 00:08:57.510
exploitation tradeoff, we are never

00:08:57.510 --> 00:09:01.800
going to explore further than the zero

00:09:01.800 --> 00:09:02.540
utility point.

00:09:02.540 --> 00:09:05.030
So if you take the analogy of gambling

00:09:05.030 --> 00:09:06.480
and you don't want to risk on

00:09:06.740 --> 00:09:10.220
losing money, so it's a safe strategy.

00:09:10.220 --> 00:09:11.510
The conservative strategy for

00:09:11.510 --> 00:09:12.230
exploration.

00:09:13.150 --> 00:09:15.720
And the problem is, of course this

00:09:15.720 --> 00:09:17.300
approach is purely heuristic.

00:09:17.920 --> 00:09:20.090
And the zero utility lower bound is

00:09:20.090 --> 00:09:22.090
also often too conservative.

00:09:22.700 --> 00:09:25.080
And there are of course more advanced

00:09:25.080 --> 00:09:27.200
machine learning approaches that have

00:09:27.200 --> 00:09:29.600
been proposed for solving these

00:09:29.600 --> 00:09:32.990
problems, and this is the active

00:09:32.990 --> 00:09:34.020
research area.

00:09:35.120 --> 00:09:38.160
So to summarize, there are two

00:09:38.160 --> 00:09:41.580
strategies for recommender systems or

00:09:41.580 --> 00:09:42.900
filtering systems.

00:09:42.900 --> 00:09:44.870
One is content based which is looking

00:09:44.870 --> 00:09:46.595
at the item similarity.

00:09:46.595 --> 00:09:48.566
The other is collaborative filtering,

00:09:48.566 --> 00:09:50.550
which is looking at the user

00:09:50.550 --> 00:09:51.515
similarity.

00:09:51.515 --> 00:09:54.436
In this lecture, we've covered the

00:09:54.436 --> 00:09:56.900
content based filtering approach in the

00:09:56.900 --> 00:09:58.733
next lecture we're going to talk about

00:09:58.733 --> 00:10:00.952
collaborative filtering. 

00:10:00.952 --> 00:10:02.560
In content-based filtering system,

00:10:03.350 --> 00:10:07.270
We generally have to solve several

00:10:07.270 --> 00:10:09.476
problems related to filtering decision

00:10:09.476 --> 00:10:11.300
and learning etc.

00:10:11.300 --> 00:10:13.240
And such a system can actually be

00:10:13.240 --> 00:10:16.670
built based on a search engine system

00:10:16.670 --> 00:10:19.940
by adding a threshold mechanism, and

00:10:19.940 --> 00:10:22.370
adding adaptive learning algorithm to

00:10:22.370 --> 00:10:24.080
allow the system to learn from

00:10:24.080 --> 00:10:27.500
long-term feedback from the user.


