WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-27T00:01:10.6361404Z by ClassTranscribe

00:00:00.300 --> 00:00:03.440
So I just showed you that empirically

00:00:03.440 --> 00:00:05.660
the likelihood will converge, but

00:00:05.660 --> 00:00:08.140
theoretically it can also be proved

00:00:08.140 --> 00:00:11.010
that EM algorithm with converge to a

00:00:11.010 --> 00:00:11.820
local maximum.

00:00:11.820 --> 00:00:14.330
So here is just the illustration of

00:00:14.330 --> 00:00:17.470
what happened an A detailed explanation

00:00:17.470 --> 00:00:17.900
this.

00:00:26.360 --> 00:00:29.390
Require more.

00:00:30.620 --> 00:00:33.220
Knowledge about some of the

00:00:33.220 --> 00:00:35.470
inequalities that we haven't really

00:00:35.470 --> 00:00:37.100
covered yet.

00:00:38.170 --> 00:00:41.860
So here what you see is on the X

00:00:41.860 --> 00:00:42.325
dimension.

00:00:42.325 --> 00:00:43.950
We have set up value.

00:00:43.950 --> 00:00:45.965
This is the parameter that we left on

00:00:45.965 --> 00:00:46.817
the Y axis.

00:00:46.817 --> 00:00:48.830
We see the likelihood function.

00:00:50.700 --> 00:00:55.270
So this curve is reaching or like

00:00:55.270 --> 00:00:56.486
roller function, right?

00:00:56.486 --> 00:00:57.410
So this one.

00:00:58.040 --> 00:01:00.492
And this is the one that we hope to

00:01:00.492 --> 00:01:03.010
maximize an we hope to find a set of

00:01:03.010 --> 00:01:05.700
value at this point to maximize this.

00:01:06.550 --> 00:01:08.410
But in the case of mixture model, we

00:01:08.410 --> 00:01:10.840
cannot easily find the analytical

00:01:10.840 --> 00:01:12.250
solution to the problem.

00:01:12.250 --> 00:01:13.950
So we have to resolve a numerical

00:01:13.950 --> 00:01:14.240
algorithm.

00:01:14.240 --> 00:01:16.440
An EM algorithm is such an algorithm.

00:01:16.440 --> 00:01:18.190
It's a Hill climb algorithm that would

00:01:18.190 --> 00:01:22.360
mean you start with some random guess.

00:01:22.360 --> 00:01:24.675
Let's say you start from here.

00:01:24.675 --> 00:01:26.640
That's your starting point and then you

00:01:26.640 --> 00:01:32.009
try to improve this by moving this to

00:01:32.010 --> 00:01:34.190
another point where you can have a

00:01:34.190 --> 00:01:35.270
higher like recorder.

00:01:35.270 --> 00:01:37.120
So that's the idea of Hill climbing.

00:01:37.560 --> 00:01:40.970
Any in the MRI was the way we achieve

00:01:40.970 --> 00:01:42.980
this is to do two things.

00:01:42.980 --> 00:01:45.910
First will fix a lower bound of

00:01:45.910 --> 00:01:47.943
likelihood function, so this is the

00:01:47.943 --> 00:01:49.250
lower bound you can see here.

00:01:51.150 --> 00:01:54.430
An once we fit the lower bound we can

00:01:54.430 --> 00:01:57.790
then maximise the lower bound and of

00:01:57.790 --> 00:01:59.590
course the reason why this works is

00:01:59.590 --> 00:02:01.595
because the lower bound is much easier

00:02:01.595 --> 00:02:04.752
to optimize so we know our current gas

00:02:04.752 --> 00:02:08.452
is here an by maximizing the lower

00:02:08.452 --> 00:02:11.450
bound will move this point to the top

00:02:11.450 --> 00:02:12.140
two here.

00:02:13.120 --> 00:02:13.730
I.

00:02:14.540 --> 00:02:16.430
And that we can then map to the

00:02:16.430 --> 00:02:18.590
original like role function.

00:02:18.590 --> 00:02:19.490
We find this point.

00:02:20.060 --> 00:02:21.990
Be cause it's a lower bound, we are

00:02:21.990 --> 00:02:24.620
guaranteed to improve this gas.

00:02:25.260 --> 00:02:27.210
Right, because we improve our lower

00:02:27.210 --> 00:02:29.770
bound and then the original lighter

00:02:29.770 --> 00:02:32.440
Holder curve which is above this lower

00:02:32.440 --> 00:02:34.830
bound will definitely be improved as

00:02:34.830 --> 00:02:35.280
well.

00:02:35.870 --> 00:02:38.140
I so we already know it's improving the

00:02:38.140 --> 00:02:40.105
lower bound, so we definitely improve

00:02:40.105 --> 00:02:42.190
this original like record function

00:02:42.190 --> 00:02:45.689
which is above this lower bound.

00:02:47.420 --> 00:02:51.436
So in our example, the current gas is

00:02:51.436 --> 00:02:53.220
parameter value given by the current

00:02:53.220 --> 00:02:55.465
generation and then the next guest is

00:02:55.465 --> 00:02:57.540
the RE estimated parameter values.

00:02:57.540 --> 00:03:00.032
From this illustration you can see the

00:03:00.032 --> 00:03:02.820
next gas is always better than the

00:03:02.820 --> 00:03:04.860
current gas unless it has reached the

00:03:04.860 --> 00:03:06.913
maximum where it would be stuck there.

00:03:06.913 --> 00:03:08.150
So the two would be equal.

00:03:09.730 --> 00:03:14.370
So the E step is basically to compute

00:03:14.370 --> 00:03:16.380
this lower bound.

00:03:17.500 --> 00:03:19.430
And we don't direct it, just computed

00:03:19.430 --> 00:03:21.790
this likely or function, but we

00:03:21.790 --> 00:03:25.140
computed the latent variable values

00:03:25.140 --> 00:03:26.010
and.

00:03:26.010 --> 00:03:28.635
These are basically part of this lower

00:03:28.635 --> 00:03:28.917
bound.

00:03:28.917 --> 00:03:30.902
This helps determine the lower bound

00:03:30.902 --> 00:03:32.760
the M step on the other hand, is to

00:03:32.760 --> 00:03:34.202
maximize the lower bound.

00:03:34.202 --> 00:03:36.210
It allows us to move parameters to a

00:03:36.210 --> 00:03:36.780
new point.

00:03:37.370 --> 00:03:39.880
And that's why EML is gone.

00:03:39.880 --> 00:03:41.640
The little converge to a local maximum.

00:03:42.370 --> 00:03:44.730
Now, as you can imagine, when we have

00:03:44.730 --> 00:03:47.660
many local Maxima, we also have to

00:03:47.660 --> 00:03:50.200
repeat the EML with multiple times in

00:03:50.200 --> 00:03:52.129
order to figure out which one is the

00:03:52.130 --> 00:03:54.270
actual global maximum.

00:03:54.270 --> 00:03:56.090
And this actually in general is a

00:03:56.090 --> 00:03:58.420
difficult problem in numerical

00:03:58.420 --> 00:03:58.980
optimization.

00:03:58.980 --> 00:04:01.460
So here for example, how do we start

00:04:01.460 --> 00:04:02.770
from here?

00:04:02.770 --> 00:04:06.030
Then we gradually just climb up to this

00:04:06.030 --> 00:04:09.180
top, so that's not optimal, and we'd

00:04:09.180 --> 00:04:11.193
like to climb up all the way to here.

00:04:11.193 --> 00:04:13.185
So the only way to climb up to this

00:04:13.185 --> 00:04:13.409
here.

00:04:13.460 --> 00:04:15.770
This will start from somewhere here or

00:04:15.770 --> 00:04:16.170
here.

00:04:16.800 --> 00:04:18.070
Right so.

00:04:19.630 --> 00:04:22.176
In the EM algorithm, we generally would

00:04:22.176 --> 00:04:24.010
have to start from different points or

00:04:24.010 --> 00:04:26.260
have some other way to determine a good

00:04:26.260 --> 00:04:28.100
initial starting point.

00:04:29.130 --> 00:04:30.940
To summarize, in this lecture we

00:04:30.940 --> 00:04:32.990
introduce the EM algorithm.

00:04:32.990 --> 00:04:35.050
This is a general algorithm for

00:04:35.050 --> 00:04:35.600
computing.

00:04:35.600 --> 00:04:37.980
Maximum regular is made of all kinds of

00:04:37.980 --> 00:04:38.880
mixture models.

00:04:38.880 --> 00:04:41.100
So not just for our simple mixture

00:04:41.100 --> 00:04:43.870
model and so here climbing algorithm so

00:04:43.870 --> 00:04:45.565
can only converge it or local maximum,

00:04:45.565 --> 00:04:47.740
and it would depend on initial points.

00:04:49.670 --> 00:04:52.450
The general idea is that we will have

00:04:52.450 --> 00:04:54.810
two steps to improve the estimate of

00:04:54.810 --> 00:04:56.335
parameters in the E step.

00:04:56.335 --> 00:04:59.210
We roughly all augmenting our data by

00:04:59.210 --> 00:05:00.970
predicting values of useful hidden

00:05:00.970 --> 00:05:04.440
variables that we would use to simplify

00:05:04.440 --> 00:05:05.150
the estimation.

00:05:05.150 --> 00:05:08.420
In our case, this is the distribution

00:05:08.420 --> 00:05:09.810
that has been used to generate the

00:05:09.810 --> 00:05:10.160
world.

00:05:11.030 --> 00:05:13.420
In the end step, then would exploit

00:05:13.420 --> 00:05:15.660
such augmented data, which would make

00:05:15.660 --> 00:05:18.060
it easier to estimate the distribution

00:05:18.060 --> 00:05:20.679
to improve the estimate of parameters.

00:05:20.680 --> 00:05:23.540
Here improve is guaranteed in terms of

00:05:23.540 --> 00:05:24.740
the likelihood function.

00:05:24.740 --> 00:05:27.740
Note that it's not necessary that we

00:05:27.740 --> 00:05:30.820
will have a stable converged parameter

00:05:30.820 --> 00:05:32.795
values, even though the likelihood

00:05:32.795 --> 00:05:34.930
function is insured to increase.

00:05:34.930 --> 00:05:36.799
There are some properties that have to

00:05:36.800 --> 00:05:39.423
be satisfied in order for the

00:05:39.423 --> 00:05:41.250
parameters also too.

00:05:41.800 --> 00:05:44.660
Convert it to some stable value.

00:05:47.370 --> 00:05:49.660
Now he thought data augmentation is

00:05:49.660 --> 00:05:50.770
done probabilistically.

00:05:50.770 --> 00:05:52.630
That means we're not going to just say

00:05:52.630 --> 00:05:54.389
exactly what's the value of a hidden

00:05:54.390 --> 00:05:55.630
variable, but we're going to have a

00:05:55.630 --> 00:05:57.830
probability distribution over the

00:05:57.830 --> 00:05:59.890
possible values of these hidden

00:05:59.890 --> 00:06:03.346
variables, so this causes a split of

00:06:03.346 --> 00:06:07.120
counts of events probabilistically and

00:06:07.120 --> 00:06:09.260
in our case, will split the world

00:06:09.260 --> 00:06:11.040
counts between the two distributions.


