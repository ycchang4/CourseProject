WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-26T23:59:34.7918798Z by ClassTranscribe

00:00:00.300 --> 00:00:02.880
This lecture is about the latent aspect

00:00:02.880 --> 00:00:05.460
rating analysis or opinion mining and

00:00:05.460 --> 00:00:06.590
sentiment analysis.

00:00:14.700 --> 00:00:16.090
In this lecture, we're going to

00:00:16.090 --> 00:00:18.150
continue discussing opinion mining and

00:00:18.150 --> 00:00:19.410
sentiment analysis.

00:00:19.410 --> 00:00:21.379
In particular, we're going to

00:00:21.380 --> 00:00:22.500
introduce.

00:00:23.140 --> 00:00:25.510
Late in the aspect of rating analysis,

00:00:25.510 --> 00:00:28.246
which allows us to perform detailed

00:00:28.246 --> 00:00:30.790
analysis of reviews with overall

00:00:30.790 --> 00:00:31.340
ratings.

00:00:34.610 --> 00:00:36.030
First, motivation.

00:00:36.880 --> 00:00:39.740
Here are two reviews that you often see

00:00:39.740 --> 00:00:42.920
on the Internet about the Hotel and

00:00:42.920 --> 00:00:45.330
You see some overall ratings.

00:00:45.330 --> 00:00:47.880
In this case, both reviewers have given

00:00:47.880 --> 00:00:48.650
five stars.

00:00:49.240 --> 00:00:51.640
And of course there are also reviews

00:00:51.640 --> 00:00:52.560
that are in text.

00:00:53.650 --> 00:00:55.460
Now, if you just look at these reviews,

00:00:55.460 --> 00:00:58.040
it's not very clear whether a hotel is

00:00:58.040 --> 00:01:00.749
good for its location or for its

00:01:00.750 --> 00:01:03.302
service, and it's also unclear why are.

00:01:03.302 --> 00:01:04.940
If you are like this hotel.

00:01:06.260 --> 00:01:09.030
So what we want to do is to decompose

00:01:09.030 --> 00:01:10.420
this overall rating.

00:01:11.030 --> 00:01:13.620
Into ratings on different aspects such

00:01:13.620 --> 00:01:17.480
as value, rooms, location and service.

00:01:18.330 --> 00:01:20.970
So if we can decompose overrating two

00:01:20.970 --> 00:01:22.780
ratings on these different aspects.

00:01:23.400 --> 00:01:26.170
Then we can obtain more detailed

00:01:26.170 --> 00:01:28.260
understanding of the reviewers opinions

00:01:28.260 --> 00:01:29.640
about the hotel.

00:01:30.530 --> 00:01:32.350
And this would also allow us to rank

00:01:32.350 --> 00:01:34.890
hotels along different dimensions, such

00:01:34.890 --> 00:01:37.750
as valuable rooms, but in general such

00:01:37.750 --> 00:01:39.680
detailed understanding would reveal

00:01:39.680 --> 00:01:42.700
more information about the users,

00:01:42.700 --> 00:01:46.013
preferences, reviews, preferences and

00:01:46.013 --> 00:01:48.980
also we can understand better how

00:01:48.980 --> 00:01:51.940
reviewers view this hotel from

00:01:51.940 --> 00:01:53.020
different perspectives.

00:01:53.890 --> 00:01:57.760
Now, not only do we want to.

00:01:58.670 --> 00:02:00.510
Infer this aspect ratings.

00:02:00.510 --> 00:02:03.969
We also want to infer the aspect of

00:02:03.970 --> 00:02:06.910
weights, so some reviewers may care

00:02:06.910 --> 00:02:09.330
more about values as opposed to

00:02:09.330 --> 00:02:11.550
service, and that would be a case like

00:02:11.550 --> 00:02:13.589
what's shown on the left for the weight

00:02:13.590 --> 00:02:15.665
distribution where you can see a lot of

00:02:15.665 --> 00:02:17.180
weight is placed on value.

00:02:18.480 --> 00:02:20.990
But others might care more about

00:02:20.990 --> 00:02:22.890
service and therefore they might place

00:02:22.890 --> 00:02:24.780
more weight on service then value.

00:02:25.560 --> 00:02:27.010
Now, the reason why this is also

00:02:27.010 --> 00:02:29.390
important that is be cause do you think

00:02:29.390 --> 00:02:31.360
about a five star on value?

00:02:31.980 --> 00:02:35.289
It might still be very expensive if the

00:02:35.289 --> 00:02:37.940
reviewer cares a lot about service,

00:02:37.940 --> 00:02:38.360
right?

00:02:38.360 --> 00:02:41.120
For this kind of service, this price is

00:02:41.120 --> 00:02:44.420
good, so the reviewer might give it a

00:02:44.420 --> 00:02:45.070
five star.

00:02:45.070 --> 00:02:47.940
But if reviewer really cares about the

00:02:47.940 --> 00:02:50.790
value of the hotel, then the five star

00:02:50.790 --> 00:02:53.590
most likely would mean really cheaper

00:02:53.590 --> 00:02:54.335
prices.

00:02:54.335 --> 00:02:57.660
So in order to interpret the ratings on

00:02:57.660 --> 00:02:59.840
different aspects accurately, we also

00:02:59.840 --> 00:03:02.020
need to know these aspect weights.

00:03:02.610 --> 00:03:04.650
When they are combined together, we can

00:03:04.650 --> 00:03:07.550
have a more detailed understanding of

00:03:07.550 --> 00:03:08.300
the opinion.

00:03:08.300 --> 00:03:10.990
So the task here is to get these

00:03:10.990 --> 00:03:12.850
reviews and their overall ratings as

00:03:12.850 --> 00:03:16.746
input and then generate the both the

00:03:16.746 --> 00:03:19.008
aspect ratings, decomposed aspect

00:03:19.008 --> 00:03:22.330
ratings and the aspect of weights as

00:03:22.330 --> 00:03:22.870
output.

00:03:23.740 --> 00:03:27.020
And this is a problem called latent

00:03:27.020 --> 00:03:28.660
aspect rating analysis.

00:03:30.960 --> 00:03:34.950
So the task in general is given a set

00:03:34.950 --> 00:03:36.960
of review articles about the topic with

00:03:36.960 --> 00:03:37.940
overall ratings.

00:03:38.540 --> 00:03:41.130
An we hope to generate the three

00:03:41.130 --> 00:03:41.540
things.

00:03:41.540 --> 00:03:44.970
One is the major aspects comment on in

00:03:44.970 --> 00:03:45.620
the reviews.

00:03:46.260 --> 00:03:48.290
The second is the ratings on each

00:03:48.290 --> 00:03:51.820
aspect, such as value and room or

00:03:51.820 --> 00:03:52.270
service.

00:03:53.560 --> 00:03:56.730
And 3rd is the relative weights placed

00:03:56.730 --> 00:03:59.160
on different aspects by the reviewers,

00:03:59.160 --> 00:04:01.866
and this task has a lot of

00:04:01.866 --> 00:04:02.140
applications.

00:04:02.140 --> 00:04:04.766
If we can do this and we would enable a

00:04:04.766 --> 00:04:06.940
lot of applications, I just listed some

00:04:06.940 --> 00:04:07.720
here and later.

00:04:07.720 --> 00:04:09.090
I will show you some results.

00:04:09.680 --> 00:04:11.240
And for example, we can do opinion

00:04:11.240 --> 00:04:12.766
based and the ranking.

00:04:12.766 --> 00:04:16.420
We can generate a aspect level opinion

00:04:16.420 --> 00:04:17.100
summary.

00:04:17.100 --> 00:04:19.490
We can also analyze reviewers

00:04:19.490 --> 00:04:22.650
preferences, compare them or compare

00:04:22.650 --> 00:04:24.780
their preferences on different hotels.

00:04:25.380 --> 00:04:26.580
And we can do personalized

00:04:26.580 --> 00:04:27.840
recommendation of products.

00:04:29.410 --> 00:04:31.350
So of course the question is how can we

00:04:31.350 --> 00:04:32.530
solve this problem?

00:04:34.660 --> 00:04:37.600
Now, as in other cases of these

00:04:37.600 --> 00:04:39.580
advanced topics, we won't have time to

00:04:39.580 --> 00:04:41.510
really cover the technique in detail,

00:04:41.510 --> 00:04:43.510
but I'm going to give a press basic

00:04:43.510 --> 00:04:46.230
introduction to the technique developed

00:04:46.230 --> 00:04:47.790
for this problem.

00:04:47.790 --> 00:04:49.550
So first we're going to talk about how

00:04:49.550 --> 00:04:52.260
to solve the problem in two stages.

00:04:52.260 --> 00:04:54.640
Later, we're going to also mention that

00:04:54.640 --> 00:04:57.220
we can do this in the unified model.

00:04:57.220 --> 00:04:59.560
Now take this review with the overall

00:04:59.560 --> 00:05:00.560
reading as input.

00:05:00.560 --> 00:05:02.952
What we want to do is first we're going

00:05:02.952 --> 00:05:05.380
to segment the aspects.

00:05:05.840 --> 00:05:08.710
So we're going to figure out what words

00:05:08.710 --> 00:05:10.730
are talking about location in what words

00:05:10.730 --> 00:05:12.270
are talking about, the room conditioning,

00:05:12.270 --> 00:05:12.890
etc.

00:05:13.530 --> 00:05:16.720
So with this we would be able to obtain

00:05:16.720 --> 00:05:18.090
aspect segments.

00:05:18.640 --> 00:05:20.770
In particular, we're going to obtain

00:05:20.770 --> 00:05:23.490
the counts of all the words in each

00:05:23.490 --> 00:05:25.800
segment, and this is denoted by C

00:05:25.800 --> 00:05:28.020
supply of WND.

00:05:28.620 --> 00:05:30.760
This can be done by using seed words

00:05:30.760 --> 00:05:32.530
like location and room.

00:05:33.270 --> 00:05:36.605
Or price to retrieve the relevant the

00:05:36.605 --> 00:05:38.190
segments and then from those segments

00:05:38.190 --> 00:05:41.070
we can further mine correlated words.

00:05:41.690 --> 00:05:44.090
With these seed words and that would

00:05:44.090 --> 00:05:47.280
allow us to segment the text into

00:05:47.280 --> 00:05:48.010
segments.

00:05:48.920 --> 00:05:51.859
Discussing different aspects, but of

00:05:51.860 --> 00:05:53.940
course later as we would see, we can

00:05:53.940 --> 00:05:56.140
also use topic models to do the

00:05:56.140 --> 00:05:58.480
segmentation, But anyway, that's the

00:05:58.480 --> 00:06:02.990
first stage where we would obtain the

00:06:02.990 --> 00:06:04.760
counts of words in each segment.

00:06:05.350 --> 00:06:07.100
In the segmentation stage, which is called

00:06:07.100 --> 00:06:09.414
latent rating regression, we're going

00:06:09.414 --> 00:06:11.500
to use these words and their

00:06:11.500 --> 00:06:13.490
frequencies in different aspects to

00:06:13.490 --> 00:06:15.360
predict the overall rating, and this

00:06:15.360 --> 00:06:17.056
prediction happens in two stages.

00:06:17.056 --> 00:06:20.016
In the first stage, we're going to use

00:06:20.016 --> 00:06:23.780
the sentiment weights of these words in

00:06:23.780 --> 00:06:26.150
each aspect to predict the aspect

00:06:26.150 --> 00:06:26.580
rating.

00:06:26.580 --> 00:06:29.940
So, for example, if in the discussion

00:06:29.940 --> 00:06:32.290
of location using a word like amazing

00:06:32.290 --> 00:06:35.380
mentioned many times and it has a high

00:06:35.380 --> 00:06:35.820
weight.

00:06:36.020 --> 00:06:37.940
For example, here is 3.9.

00:06:37.940 --> 00:06:40.000
Then it would increase the aspect

00:06:40.000 --> 00:06:41.120
rating for location.

00:06:41.710 --> 00:06:44.093
But another word, like a far, which is

00:06:44.093 --> 00:06:47.020
a negative weight if it's mentioned

00:06:47.020 --> 00:06:49.460
many times and it will decrease the

00:06:49.460 --> 00:06:49.880
rating.

00:06:49.880 --> 00:06:52.769
So the aspect rating is assumed to be a

00:06:52.770 --> 00:06:54.790
weighted combination of these word

00:06:54.790 --> 00:06:57.173
frequencies where the weights are the

00:06:57.173 --> 00:06:59.420
sentiment weights on the words.

00:06:59.420 --> 00:07:02.270
Now of course these sentiment weights

00:07:02.270 --> 00:07:04.270
might be different for different

00:07:04.270 --> 00:07:04.900
aspects.

00:07:05.550 --> 00:07:09.520
So we have for each aspect a set of

00:07:09.520 --> 00:07:10.390
sentiment weights.

00:07:11.100 --> 00:07:13.060
As shown here, and that's denoted by

00:07:13.060 --> 00:07:16.220
beta sub I and W.

00:07:18.290 --> 00:07:20.890
In the second stage, or in a second

00:07:20.890 --> 00:07:23.150
step, we're going to assume that the

00:07:23.150 --> 00:07:25.240
overall rating is simply weighted

00:07:25.240 --> 00:07:27.910
combination of these aspect ratings.

00:07:27.910 --> 00:07:29.950
So we're going to assume we have aspect

00:07:29.950 --> 00:07:33.330
weights in order by of R sub of D.

00:07:33.940 --> 00:07:36.620
And this would be used to take a

00:07:36.620 --> 00:07:39.320
weighted average of the aspect ratings,

00:07:39.320 --> 00:07:41.919
which are denoted by our supply of the.

00:07:42.720 --> 00:07:44.700
And we can assume the overall rating is

00:07:44.700 --> 00:07:46.930
simply a weighted average of this

00:07:46.930 --> 00:07:47.850
aspect ratings.

00:07:48.470 --> 00:07:51.470
So this setup allows us to predict the

00:07:51.470 --> 00:07:54.540
overall rating based on the observed

00:07:54.540 --> 00:07:55.550
word frequencies.

00:07:55.550 --> 00:07:58.510
So on the left side you will see all

00:07:58.510 --> 00:08:00.460
these observed information, the arts,

00:08:00.460 --> 00:08:02.040
the and the count.

00:08:02.930 --> 00:08:05.570
But on the right side you see all the

00:08:05.570 --> 00:08:07.310
information that we're interested in is

00:08:07.310 --> 00:08:08.250
actually latent.

00:08:09.050 --> 00:08:11.010
So we hope to discover them.

00:08:12.270 --> 00:08:16.196
Now this is a typical case of

00:08:16.196 --> 00:08:18.110
generating model where we would embed

00:08:18.110 --> 00:08:20.290
the interesting variables in the

00:08:20.290 --> 00:08:21.030
generating model.

00:08:21.750 --> 00:08:23.720
And then we're going to set up a

00:08:23.720 --> 00:08:27.780
generation probability for the overall

00:08:27.780 --> 00:08:30.630
rating given the observed words.

00:08:31.330 --> 00:08:34.220
And then of course, then we can adjust

00:08:34.220 --> 00:08:36.820
these parameter values including

00:08:36.820 --> 00:08:40.130
betas, rs, alpha i.

00:08:40.750 --> 00:08:42.872
In order to maximize the probability of

00:08:42.872 --> 00:08:45.390
the data in this case, the conditional

00:08:45.390 --> 00:08:47.770
probability of the observed rating

00:08:47.770 --> 00:08:49.710
given the document.

00:08:51.320 --> 00:08:54.190
And so we have seen such cases before

00:08:54.190 --> 00:08:57.750
in, for example, PLSA, where we predict

00:08:57.750 --> 00:08:59.265
the text data.

00:08:59.265 --> 00:09:02.335
But here we predicting the rating and

00:09:02.335 --> 00:09:04.450
the parameters of course are also very

00:09:04.450 --> 00:09:04.706
different.

00:09:04.706 --> 00:09:06.990
But if you can see if we can uncover

00:09:06.990 --> 00:09:08.910
these parameters, that would be nice

00:09:08.910 --> 00:09:11.497
because also R of D is precisely the

00:09:11.497 --> 00:09:13.908
aspect ratings that we want to get, and

00:09:13.908 --> 00:09:15.970
these are decomposer ratings on

00:09:15.970 --> 00:09:19.522
different aspects of our sub ID is

00:09:19.522 --> 00:09:21.900
precisely the aspect weights that we

00:09:21.900 --> 00:09:22.820
hope to get.

00:09:23.310 --> 00:09:25.670
As a bi product that will also get the

00:09:25.670 --> 00:09:27.940
beta vector and these are the aspects

00:09:27.940 --> 00:09:29.980
of specifica sentiment, weights of

00:09:29.980 --> 00:09:32.510
words, so more formally.

00:09:33.340 --> 00:09:35.160
They thought we are modeling.

00:09:35.160 --> 00:09:36.980
Here is a set of review documents with

00:09:36.980 --> 00:09:38.000
overall ratings.

00:09:38.620 --> 00:09:41.710
And each review documents denoted by AT

00:09:41.710 --> 00:09:45.260
and overall rating is denoted by R

00:09:45.260 --> 00:09:48.260
sub D and these pre segmented into K as

00:09:48.260 --> 00:09:50.400
their segments and we're going to use C

00:09:50.400 --> 00:09:53.745
sub W and D and to denote the count of

00:09:53.745 --> 00:09:56.436
world W in aspect segment I.

00:09:56.436 --> 00:09:58.320
Of course it's zero if the world

00:09:58.320 --> 00:09:59.640
doesn't occur in the segment.

00:10:01.500 --> 00:10:03.330
Now the model is going to predict the

00:10:03.330 --> 00:10:04.620
rating based on the.

00:10:04.620 --> 00:10:07.230
So we are interested in the conditional

00:10:07.230 --> 00:10:11.170
probability of R sub T given D.

00:10:11.850 --> 00:10:13.960
And this model is set up as follows.

00:10:13.960 --> 00:10:17.016
So all of this is assumed to follow a

00:10:17.016 --> 00:10:19.690
normal distribution with a mean that

00:10:19.690 --> 00:10:22.965
denotes actually await the average of

00:10:22.965 --> 00:10:24.440
the aspect ratings.

00:10:24.440 --> 00:10:28.772
R sub of D as shown here is normal

00:10:28.772 --> 00:10:30.520
distribution has a variance of or

00:10:30.520 --> 00:10:31.070
square.

00:10:32.690 --> 00:10:34.150
Now of course, this is just what our

00:10:34.150 --> 00:10:35.800
assumption in the actual reading is not

00:10:35.800 --> 00:10:37.480
necessary generating this way.

00:10:37.480 --> 00:10:39.703
But as always when we make this

00:10:39.703 --> 00:10:41.630
assumption, we have a formal way to

00:10:41.630 --> 00:10:44.040
model the problem, and that allows us

00:10:44.040 --> 00:10:46.130
to compute interesting quantities.

00:10:46.710 --> 00:10:49.225
In this case, the aspect ratings and

00:10:49.225 --> 00:10:50.510
aspect of weights.

00:10:51.880 --> 00:10:54.720
Now the aspect rating as you see on the

00:10:54.720 --> 00:11:00.280
second line is assumed to be weighted

00:11:00.280 --> 00:11:02.230
sum of these weights where the weight

00:11:02.230 --> 00:11:03.660
is just sentiment wait.

00:11:04.830 --> 00:11:05.700
So.

00:11:06.760 --> 00:11:08.880
As I said, the overall rating is

00:11:08.880 --> 00:11:10.850
assumed to be a weighted average of

00:11:10.850 --> 00:11:11.880
aspect ratings.

00:11:15.130 --> 00:11:16.940
Now this alpha

00:11:18.050 --> 00:11:21.300
Values of a alpha sub of D

00:11:21.300 --> 00:11:23.600
together by our vector that depends on

00:11:23.600 --> 00:11:27.890
D is the document specific weights and

00:11:27.890 --> 00:11:30.310
we can assume this factor itself is

00:11:30.310 --> 00:11:32.730
drawn from another multivariate

00:11:32.730 --> 00:11:36.330
Gaussian distribution with mean denoted

00:11:36.330 --> 00:11:40.730
by a mule vector and covariance matrix

00:11:40.730 --> 00:11:42.130
Sigma, yeah.

00:11:42.970 --> 00:11:46.290
Now, so this means when we generate our

00:11:46.290 --> 00:11:47.700
overall rating, we're going to first

00:11:47.700 --> 00:11:48.470
draw.

00:11:49.570 --> 00:11:51.590
A set of other values from this

00:11:51.590 --> 00:11:53.140
multivariate Gaussian prior

00:11:53.140 --> 00:11:55.800
distribution and once we get these

00:11:55.800 --> 00:11:58.100
alpha values were going to use, then

00:11:58.100 --> 00:12:01.510
the weighted average of aspect ratings

00:12:01.510 --> 00:12:06.580
as the mean here to use the normal

00:12:06.580 --> 00:12:07.480
distribution.

00:12:08.280 --> 00:12:11.560
And to generate the overall rating.

00:12:13.820 --> 00:12:17.320
Now the aspect rating as I just said is

00:12:17.320 --> 00:12:19.240
the sum of the sentiment weights of

00:12:19.240 --> 00:12:20.380
words in their spectrum.

00:12:20.380 --> 00:12:22.769
Note that here the sentiment weights

00:12:22.770 --> 00:12:25.220
are specifically to aspects, so beta is

00:12:25.220 --> 00:12:26.500
indexed by I.

00:12:26.500 --> 00:12:27.790
And As for aspect.

00:12:28.490 --> 00:12:31.490
And that gives us way to model

00:12:31.490 --> 00:12:34.020
different segment of award.

00:12:36.170 --> 00:12:38.390
This is neither because of the same

00:12:38.390 --> 00:12:41.389
word might have positive sentiment for

00:12:41.390 --> 00:12:43.309
once back, but negative sentiment for

00:12:43.310 --> 00:12:44.470
another aspect.

00:12:45.430 --> 00:12:48.360
It's also useful to then see.

00:12:49.940 --> 00:12:53.120
What premise we have here, but I just

00:12:53.120 --> 00:12:57.230
said that the beta sub I W gives us a

00:12:57.230 --> 00:12:59.950
aspect specific sentiment of W.

00:12:59.950 --> 00:13:02.489
So obviously that's one of the

00:13:02.490 --> 00:13:04.760
important parameters, but in general we

00:13:04.760 --> 00:13:06.375
can see we have these parameters.

00:13:06.375 --> 00:13:09.760
The beta values that Delta and then the

00:13:09.760 --> 00:13:11.430
mu and Sigma.

00:13:12.440 --> 00:13:14.360
So next question is, how can we

00:13:14.360 --> 00:13:16.100
estimate these parameters and so we

00:13:16.100 --> 00:13:17.760
collectively denote all the parameters

00:13:17.760 --> 00:13:19.210
by Lambda here.

00:13:20.100 --> 00:13:21.980
Now we can, as usual, use

00:13:21.980 --> 00:13:24.160
The maximum likelihood is made and this

00:13:24.160 --> 00:13:26.840
will give us the settings of this

00:13:26.840 --> 00:13:28.350
premise that with the maximizer

00:13:28.350 --> 00:13:29.320
observed.

00:13:30.950 --> 00:13:33.910
Observer ratings condition on their

00:13:33.910 --> 00:13:35.460
respective reviews.

00:13:36.640 --> 00:13:39.510
And of course, this would then give us

00:13:39.510 --> 00:13:41.810
all the useful variables that will

00:13:41.810 --> 00:13:43.800
interest in computing.

00:13:45.320 --> 00:13:48.580
So now more specifically, we can now

00:13:48.580 --> 00:13:50.240
once we estimate the parameters, we can

00:13:50.240 --> 00:13:52.700
easily compute the abstract rating for

00:13:52.700 --> 00:13:55.760
aspect I or sub I of D and that's

00:13:55.760 --> 00:13:58.910
simply to take all the words that

00:13:58.910 --> 00:14:02.160
occurred in the segment I and then take

00:14:02.160 --> 00:14:03.900
their accounts and then multiply that

00:14:03.900 --> 00:14:06.020
by the sentiment weight of each word

00:14:06.020 --> 00:14:07.070
and take a sum.

00:14:07.710 --> 00:14:09.733
So of course this counter would be 04

00:14:09.733 --> 00:14:11.989
words that are not occurring in the

00:14:11.990 --> 00:14:13.710
aspect I, and that's why we can take

00:14:13.710 --> 00:14:15.100
some over all the words in the

00:14:15.100 --> 00:14:15.840
vocabulary.

00:14:16.830 --> 00:14:18.540
Now, what about the aspect weights?

00:14:18.540 --> 00:14:21.000
Alpha sub I of D?

00:14:21.000 --> 00:14:23.830
It's not part of our parameter, right?

00:14:23.830 --> 00:14:26.650
So we have to use Bayesian inference to

00:14:26.650 --> 00:14:27.290
compute it.

00:14:28.130 --> 00:14:31.690
And in this case we can use the maximum

00:14:31.690 --> 00:14:32.940
a posteriori.

00:14:34.410 --> 00:14:37.050
2 computer this alpha value.

00:14:37.050 --> 00:14:38.870
Basically we're going to maximize the

00:14:38.870 --> 00:14:41.320
product of the prior of our according

00:14:41.320 --> 00:14:43.450
to our assumed market valued Gaussian

00:14:43.450 --> 00:14:46.820
distribution and the likelihood in this

00:14:46.820 --> 00:14:49.150
case likely is the probability of

00:14:49.150 --> 00:14:51.550
generating this observed overall rating

00:14:51.550 --> 00:14:55.629
given this particular Alpha value and

00:14:55.630 --> 00:14:56.730
some other parameters.

00:14:56.730 --> 00:14:57.740
As you see here.

00:14:57.740 --> 00:14:59.680
So for more details about this model,

00:14:59.680 --> 00:15:01.970
you can read this paper cited here.


