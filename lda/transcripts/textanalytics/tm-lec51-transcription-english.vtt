WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-26T23:59:34.3522834Z by ClassTranscribe

00:00:00.300 --> 00:00:03.790
This lecture is about how to mine text

00:00:03.790 --> 00:00:06.230
data with social network as context.

00:00:15.130 --> 00:00:16.380
In this lecture, we're going to

00:00:16.380 --> 00:00:18.290
continue discussing contextual text

00:00:18.290 --> 00:00:18.820
mining.

00:00:18.820 --> 00:00:20.715
In particular, we're going to look at

00:00:20.715 --> 00:00:24.080
the social network of authors of text

00:00:24.080 --> 00:00:25.090
as context.

00:00:26.060 --> 00:00:28.290
So first some motivation for using

00:00:28.290 --> 00:00:31.830
network context for analysis of text.

00:00:32.850 --> 00:00:35.835
The context of a text article can form

00:00:35.835 --> 00:00:36.760
a network.

00:00:37.380 --> 00:00:39.650
For example, the authors of research

00:00:39.650 --> 00:00:42.060
articles might form a collaboration

00:00:42.060 --> 00:00:42.630
network.

00:00:44.000 --> 00:00:46.045
Or authors of social media content

00:00:46.045 --> 00:00:48.180
might also form social networks.

00:00:48.180 --> 00:00:49.690
For example, in Twitter.

00:00:49.690 --> 00:00:52.220
People might follow each other, or in

00:00:52.220 --> 00:00:52.770
Facebook.

00:00:52.770 --> 00:00:56.160
Some people might claim as friends of

00:00:56.160 --> 00:00:57.350
others etc.

00:00:58.030 --> 00:01:02.570
So such context connects the content.

00:01:03.260 --> 00:01:05.730
Often the others.

00:01:07.810 --> 00:01:09.940
Similarly, locations associated with

00:01:09.940 --> 00:01:12.690
text can also be connected to form

00:01:12.690 --> 00:01:15.040
geographical network, but in general

00:01:15.040 --> 00:01:18.031
you can imagine the meta data of the

00:01:18.031 --> 00:01:21.510
text data can form some kind of network

00:01:21.510 --> 00:01:23.020
if they have some relations.

00:01:24.430 --> 00:01:27.900
Now there is some benefit in jointly

00:01:27.900 --> 00:01:31.306
analyzing text and its social network

00:01:31.306 --> 00:01:34.710
context or network context in general.

00:01:35.530 --> 00:01:37.870
And that's because we can use network

00:01:37.870 --> 00:01:40.260
to impose some constraints on topics

00:01:40.260 --> 00:01:40.860
text.

00:01:41.600 --> 00:01:43.360
So for example, it's reasonable to

00:01:43.360 --> 00:01:47.560
assume that authors connected in

00:01:47.560 --> 00:01:49.730
collaboration network tend to write

00:01:49.730 --> 00:01:51.040
about the similar topics.

00:01:51.920 --> 00:01:55.030
So such heuristic can be used to guide

00:01:55.030 --> 00:01:56.510
us in analyzing topics.

00:01:59.610 --> 00:02:03.440
Text also can help characterize the

00:02:03.440 --> 00:02:06.440
content associated with each subnetwork

00:02:06.440 --> 00:02:09.470
and this is to say that

00:02:10.070 --> 00:02:14.040
both kinds of data, the network and

00:02:14.040 --> 00:02:15.550
text can help each other.

00:02:16.740 --> 00:02:18.750
So for example, the difference in

00:02:18.750 --> 00:02:22.160
opinions expressed in two subnetworks

00:02:22.160 --> 00:02:24.860
let's say two social networks can be

00:02:24.860 --> 00:02:27.050
revealed by doing this kind of joint

00:02:27.050 --> 00:02:27.790
analysis.

00:02:29.550 --> 00:02:32.130
So here we are going to briefly

00:02:32.130 --> 00:02:33.000
introduce.

00:02:35.090 --> 00:02:38.795
a model called Network supervised topic

00:02:38.795 --> 00:02:39.150
model.

00:02:40.260 --> 00:02:42.580
And, this in this slide we're going to

00:02:42.580 --> 00:02:44.600
give some general ideas, and then in

00:02:44.600 --> 00:02:46.190
the next slide will give some more

00:02:46.190 --> 00:02:47.000
details.

00:02:48.340 --> 00:02:50.960
But in general, in this part of the

00:02:50.960 --> 00:02:53.000
course we don't have enough time to

00:02:53.000 --> 00:02:56.760
cover these frontier topics in detail,

00:02:56.760 --> 00:02:58.840
but we provide references that would

00:02:58.840 --> 00:03:00.110
allow you to.

00:03:00.890 --> 00:03:02.880
To read more about the topic.

00:03:02.880 --> 00:03:04.080
To know the details.

00:03:05.490 --> 00:03:07.690
But it should still be useful to know

00:03:07.690 --> 00:03:10.300
the general ideas and to know what they

00:03:10.300 --> 00:03:12.920
can do to know when you might be able

00:03:12.920 --> 00:03:13.570
to use them.

00:03:15.490 --> 00:03:17.870
So the general idea of network

00:03:17.870 --> 00:03:20.800
supervised topic model modeling is

00:03:20.800 --> 00:03:21.660
the following.

00:03:21.660 --> 00:03:23.350
Let's start with.

00:03:24.460 --> 00:03:28.460
Viewing the regular topic models like

00:03:28.460 --> 00:03:32.460
PLSA or LDA as solving

00:03:32.460 --> 00:03:33.620
optimization problem.

00:03:33.620 --> 00:03:35.263
Of course, in this case the

00:03:35.263 --> 00:03:37.066
optimization objective function is the

00:03:37.066 --> 00:03:37.939
likelihood function.

00:03:37.940 --> 00:03:40.280
So we often use maximum likelihood

00:03:40.280 --> 00:03:42.970
estimator to obtain the parameters and

00:03:42.970 --> 00:03:45.820
these parameters would give us useful

00:03:45.820 --> 00:03:47.170
information that we want to

00:03:47.730 --> 00:03:49.310
obtained from text data.

00:03:49.880 --> 00:03:51.000
For example topics.

00:03:51.670 --> 00:03:54.260
So we want to maximize the probability

00:03:54.260 --> 00:03:56.280
of text data given the parameters

00:03:56.280 --> 00:03:58.990
generated denoted by Lambda here.

00:03:59.780 --> 00:04:02.497
Now the main idea of incorporating

00:04:02.497 --> 00:04:06.947
network is to say that to think about

00:04:06.947 --> 00:04:09.800
the constraints that can be imposed

00:04:09.800 --> 00:04:11.360
based on the network.

00:04:11.360 --> 00:04:14.083
In general, the idea is to use the

00:04:14.083 --> 00:04:16.040
network to impose some constraints on

00:04:16.040 --> 00:04:17.920
the model parameters Lambda here.

00:04:20.120 --> 00:04:23.360
For example, the text at adjacent nodes

00:04:23.360 --> 00:04:26.090
of the network can be assumed to cover

00:04:26.090 --> 00:04:27.250
similar topics.

00:04:27.250 --> 00:04:29.694
Indeed, in many cases they tend to

00:04:29.694 --> 00:04:31.470
cover similar topics.

00:04:33.100 --> 00:04:36.612
We so we may be able to smooth the

00:04:36.612 --> 00:04:40.340
topic distributions on the graph on the

00:04:40.340 --> 00:04:43.220
network so that adjacent nodes will

00:04:43.220 --> 00:04:46.316
have very similar topic distributions,

00:04:46.316 --> 00:04:47.570
so you.

00:04:48.250 --> 00:04:52.960
They will share common distribution of

00:04:52.960 --> 00:04:55.940
the topics or have just slight

00:04:55.940 --> 00:04:59.570
variations of the topic distributions

00:04:59.570 --> 00:05:00.790
or topic coverage.

00:05:02.090 --> 00:05:05.700
So technically what we can do is simply

00:05:05.700 --> 00:05:08.380
to add a network induced regularizers

00:05:08.380 --> 00:05:10.400
to the likelihood objective function as

00:05:10.400 --> 00:05:11.010
shown here.

00:05:11.010 --> 00:05:13.430
So instead of just optimizing the

00:05:13.430 --> 00:05:15.080
probability of text data given

00:05:15.080 --> 00:05:16.210
parameters, Lambda.

00:05:16.210 --> 00:05:17.875
We're going to optimize another

00:05:17.875 --> 00:05:18.830
function F.

00:05:19.580 --> 00:05:22.190
This function combines the likelyhood

00:05:22.190 --> 00:05:25.270
with a regularizer function called r

00:05:25.270 --> 00:05:27.870
here, and the regularizer is defined

00:05:27.870 --> 00:05:31.125
the on the parameters Lambda and the

00:05:31.125 --> 00:05:34.580
network and tells us basically what

00:05:34.580 --> 00:05:36.425
kind of parameters are preferred from

00:05:36.425 --> 00:05:38.780
network constraint perspective, so you

00:05:38.780 --> 00:05:41.370
can easy to see this is in effect

00:05:41.370 --> 00:05:42.030
implemented

00:05:42.030 --> 00:05:44.630
the idea of imposing a prior on the

00:05:44.630 --> 00:05:46.600
model parameters only that will not

00:05:46.600 --> 00:05:48.080
necessarily having a probabilistic

00:05:48.080 --> 00:05:48.520
model.

00:05:49.390 --> 00:05:51.110
But the idea is the same.

00:05:51.110 --> 00:05:54.300
We're going to combine the two in one

00:05:54.300 --> 00:05:55.590
single objective function.

00:05:57.650 --> 00:06:00.980
So the advantage of this idea is that

00:06:00.980 --> 00:06:04.390
it's quite general here the topic model

00:06:04.390 --> 00:06:06.610
can be any generative model for text.

00:06:07.180 --> 00:06:09.160
Right, it doesn't have to be PLSA or

00:06:09.160 --> 00:06:11.520
LDA or the current topic models.

00:06:12.700 --> 00:06:15.750
And similarly, the network can be also

00:06:15.750 --> 00:06:16.756
any network.

00:06:16.756 --> 00:06:19.190
Any graph that connects these text

00:06:19.190 --> 00:06:19.830
objects.

00:06:22.630 --> 00:06:25.660
This regularizer can also be any

00:06:25.660 --> 00:06:26.082
regularizer.

00:06:26.082 --> 00:06:28.520
We can be flexible in capturing

00:06:28.520 --> 00:06:31.050
different heuristics that we want to

00:06:31.050 --> 00:06:31.560
capture.

00:06:32.380 --> 00:06:35.390
And finally, the function F can also

00:06:35.390 --> 00:06:37.520
vary, so there can be many different

00:06:37.520 --> 00:06:38.450
with combined them.

00:06:38.450 --> 00:06:41.880
So this general idea is actually quite

00:06:41.880 --> 00:06:42.640
powerful.

00:06:42.640 --> 00:06:45.520
Office general approach to combining

00:06:45.520 --> 00:06:49.330
these different types of data in a

00:06:49.330 --> 00:06:51.890
single optimization framework.

00:06:52.500 --> 00:06:54.130
And this general idea clearly can be

00:06:54.130 --> 00:06:57.830
applied for many problems, but here in

00:06:57.830 --> 00:06:59.570
this paper referenced here.

00:07:00.150 --> 00:07:02.830
A particular instantiation, called

00:07:02.830 --> 00:07:06.270
NetPLSA was started in this case it's

00:07:06.270 --> 00:07:08.570
just an extension of PLSA to incorporate

00:07:08.570 --> 00:07:10.580
some simple constraints imposed by

00:07:10.580 --> 00:07:11.130
network.

00:07:11.880 --> 00:07:15.415
And the prior here is the neighbors on

00:07:15.415 --> 00:07:17.310
the network must have similar topic

00:07:17.310 --> 00:07:18.020
distribution.

00:07:18.020 --> 00:07:19.735
They must cover similar topics in

00:07:19.735 --> 00:07:21.390
similar ways, and that's basically what

00:07:21.390 --> 00:07:22.860
it says in English.

00:07:23.990 --> 00:07:26.070
So technically we just have a modified

00:07:26.070 --> 00:07:28.940
object function here as defined on both

00:07:28.940 --> 00:07:31.660
the text collection C and the network

00:07:31.660 --> 00:07:33.050
graph G here.

00:07:33.950 --> 00:07:36.037
And if you look at this formula and you

00:07:36.037 --> 00:07:39.000
can actually recognize some part fairly

00:07:39.000 --> 00:07:42.103
familiar, because are they should be

00:07:42.103 --> 00:07:44.370
fairly familiar to you by now.

00:07:44.370 --> 00:07:46.430
So can you recognize which part?

00:07:47.680 --> 00:07:50.730
Is the likelihood for the text data

00:07:50.730 --> 00:07:51.910
given by a topic model?

00:07:52.640 --> 00:07:54.540
If you look at it that you will see

00:07:54.540 --> 00:07:56.750
this part is precisely the PLSA log

00:07:56.750 --> 00:07:59.890
likelihood that we want to maximize

00:07:59.890 --> 00:08:02.240
when we estimate the parameters for PLSA

00:08:02.240 --> 00:08:02.790
alone.

00:08:04.030 --> 00:08:06.710
But the second equation shows some

00:08:06.710 --> 00:08:08.670
additional constraints on the

00:08:08.670 --> 00:08:09.320
parameters.

00:08:10.590 --> 00:08:12.670
And in particular we see here.

00:08:13.700 --> 00:08:17.315
It's to measure the difference between

00:08:17.315 --> 00:08:21.080
the topic coverage at node U and the

00:08:21.080 --> 00:08:21.840
V.

00:08:21.840 --> 00:08:24.850
The two adjacent nodes on the network.

00:08:25.400 --> 00:08:27.350
We want their distributions to be

00:08:27.350 --> 00:08:29.490
similar, so they here we are computing

00:08:29.490 --> 00:08:31.966
the square of their differences and we

00:08:31.966 --> 00:08:34.260
want to minimize this difference.

00:08:34.260 --> 00:08:36.160
And note that there is a negative sign

00:08:36.160 --> 00:08:37.730
in front of this sum.

00:08:37.730 --> 00:08:38.600
This whole Sum.

00:08:39.580 --> 00:08:40.300
Here.

00:08:42.380 --> 00:08:46.750
So this makes it possible to find the

00:08:46.750 --> 00:08:49.170
parameters that are

00:08:51.910 --> 00:08:56.930
that are both to maximize the PLSA log

00:08:56.930 --> 00:08:57.730
likelihood.

00:08:57.730 --> 00:08:59.650
That means the parameters will fit the

00:08:59.650 --> 00:09:03.770
data well and also to respect this

00:09:03.770 --> 00:09:05.830
constraint from the network.

00:09:06.590 --> 00:09:08.500
And this is an active sign that I just

00:09:08.500 --> 00:09:10.700
mentioned, because there's a negative

00:09:10.700 --> 00:09:12.860
sign when we maximize this objective

00:09:12.860 --> 00:09:16.090
function we will actually minimize this

00:09:16.090 --> 00:09:17.510
second term here.

00:09:19.660 --> 00:09:23.330
So if we look further in this picture,

00:09:23.330 --> 00:09:26.610
will see there is also weight of edge

00:09:27.300 --> 00:09:30.950
between u and v here and that's based on

00:09:30.950 --> 00:09:31.910
our network.

00:09:31.910 --> 00:09:34.099
If we have a weight that says these two

00:09:34.100 --> 00:09:37.360
nodes are strong collaborators of

00:09:37.360 --> 00:09:40.070
researchers, or these two are strong.

00:09:41.420 --> 00:09:44.140
Connections between two people in a

00:09:44.140 --> 00:09:45.750
social network and then they will have

00:09:45.750 --> 00:09:47.360
a high weight then that means it will

00:09:47.360 --> 00:09:50.020
be more important to make sure that

00:09:50.020 --> 00:09:52.330
their topic coverages are similar and

00:09:52.330 --> 00:09:53.840
that's basically what it says here.

00:09:54.810 --> 00:09:56.306
And then finally use your parameter

00:09:56.306 --> 00:09:56.743
Lambda.

00:09:56.743 --> 00:09:57.179
Here.

00:09:57.180 --> 00:09:59.795
This is a new parameter to control the

00:09:59.795 --> 00:10:01.450
influence of network constraints.

00:10:01.450 --> 00:10:03.620
We can see easily if Lambda is set to

00:10:03.620 --> 00:10:05.760
zero, we just go back to the standard

00:10:05.760 --> 00:10:06.455
PLSA.

00:10:06.455 --> 00:10:08.680
But when the lambda is set to a larger

00:10:08.680 --> 00:10:10.918
value, then we'll let the network

00:10:10.918 --> 00:10:14.610
influence the estimate models more so

00:10:14.610 --> 00:10:17.346
as you can see, the effect here is that

00:10:17.346 --> 00:10:19.580
we can do basically PLSA but we're

00:10:19.580 --> 00:10:22.300
going to also try to make the topic

00:10:22.300 --> 00:10:25.630
coverages on the two nodes that are.

00:10:26.190 --> 00:10:29.150
strongly connected to be similar and we

00:10:29.150 --> 00:10:30.640
ensure their coverages are more

00:10:30.640 --> 00:10:31.040
similar.

00:10:33.680 --> 00:10:36.250
So here are some sample results from

00:10:36.250 --> 00:10:39.110
that paper, and this slide shows the

00:10:39.110 --> 00:10:41.940
regular results of using PLSA and the

00:10:41.940 --> 00:10:45.480
data here is DBLP data.

00:10:45.480 --> 00:10:47.650
bibliographic data about the research

00:10:47.650 --> 00:10:48.310
articles.

00:10:48.860 --> 00:10:50.980
And the experiments have to do with

00:10:50.980 --> 00:10:53.560
using a four communities of

00:10:53.560 --> 00:10:54.420
publications.

00:10:54.420 --> 00:10:56.510
IR information retrieval.

00:10:56.510 --> 00:10:59.320
That means DM, stand for data mining,

00:10:59.320 --> 00:11:01.300
ML for machine learning and web.

00:11:01.300 --> 00:11:03.720
There are four communities of articles

00:11:03.720 --> 00:11:04.830
and we will.

00:11:06.340 --> 00:11:10.720
Hoping to see that the topic mining

00:11:10.720 --> 00:11:13.745
can help us uncover these four

00:11:13.745 --> 00:11:15.790
communities, but from these sample

00:11:15.790 --> 00:11:17.903
topics that you are seeing here that

00:11:17.903 --> 00:11:21.585
are generated by PLSA and PLSA is unable

00:11:21.585 --> 00:11:24.340
to generate the four communities that

00:11:24.340 --> 00:11:26.909
correspond to our intuition and the

00:11:26.910 --> 00:11:28.080
reason was because

00:11:28.080 --> 00:11:30.510
they are all mixed together and there

00:11:30.510 --> 00:11:33.083
are many words that are shared by these

00:11:33.083 --> 00:11:35.000
communities, so it's not that easy to

00:11:35.000 --> 00:11:36.619
use the four topics.

00:11:36.670 --> 00:11:37.640
to separate them.

00:11:37.640 --> 00:11:40.150
If we use more topics, perhaps will

00:11:40.150 --> 00:11:41.800
have more coherent topics.

00:11:42.880 --> 00:11:44.910
But what's interesting is that if we

00:11:44.910 --> 00:11:48.559
use the NetPLSA where the network the

00:11:48.560 --> 00:11:50.640
collaboration network in this case of

00:11:50.640 --> 00:11:53.500
others is used to impose constraints.

00:11:54.060 --> 00:11:56.310
And this, in this case we also use four

00:11:56.310 --> 00:12:00.260
topics, but NetPLSA would give much

00:12:00.260 --> 00:12:01.523
more meaningful topics.

00:12:01.523 --> 00:12:04.820
So here we see that these topics

00:12:04.820 --> 00:12:06.430
correspond well to the four

00:12:06.430 --> 00:12:07.145
communities.

00:12:07.145 --> 00:12:09.420
The first is the information retrieval

00:12:09.420 --> 00:12:11.060
second is data mining.

00:12:11.060 --> 00:12:13.090
Third is machine learning the fourth is

00:12:13.090 --> 00:12:13.780
web.

00:12:13.780 --> 00:12:16.550
So that separation was.

00:12:17.230 --> 00:12:19.850
Mostly because of the influence of

00:12:19.850 --> 00:12:21.550
network where we leverage the

00:12:21.550 --> 00:12:23.350
collaboration network information.

00:12:23.350 --> 00:12:26.480
Essentially the people that form

00:12:26.480 --> 00:12:28.980
a collaboration network would then be

00:12:28.980 --> 00:12:31.230
kind of assumed to write about similar

00:12:31.230 --> 00:12:33.250
topics and that's why we can have more

00:12:33.250 --> 00:12:34.416
coherent topics.

00:12:34.416 --> 00:12:36.660
And if you just listen to text data

00:12:36.660 --> 00:12:39.290
alone based on the Co occurrences you

00:12:39.290 --> 00:12:42.330
won't get such a coherent topics,

00:12:42.330 --> 00:12:45.980
even though a topic model PLSA or LDA

00:12:45.980 --> 00:12:47.000
also.

00:12:47.540 --> 00:12:49.840
should be able to pick up a Co

00:12:49.840 --> 00:12:51.640
occurring words, so in general the

00:12:51.640 --> 00:12:54.240
topics that they generate represent

00:12:54.240 --> 00:12:57.070
words that Co occur with each other.

00:12:57.940 --> 00:13:00.190
But still they cannot generate such

00:13:00.190 --> 00:13:03.930
coherent results as NetPLSA is showing

00:13:03.930 --> 00:13:07.000
that the network context is very useful

00:13:07.000 --> 00:13:07.380
here.

00:13:08.600 --> 00:13:11.560
A similar model could have been also

00:13:11.560 --> 00:13:13.910
used to characterize the content

00:13:13.910 --> 00:13:15.980
associated with each subnetwork of

00:13:15.980 --> 00:13:16.860
collaborations.

00:13:19.640 --> 00:13:22.510
So a more general view of text mining

00:13:22.510 --> 00:13:24.980
in the context of network is to treat

00:13:24.980 --> 00:13:28.885
text as living in the rich information

00:13:28.885 --> 00:13:30.180
network environment.

00:13:30.180 --> 00:13:32.600
That means we can connect all the

00:13:32.600 --> 00:13:34.937
related data together as a big network

00:13:34.937 --> 00:13:37.756
and text data out can be associated

00:13:37.756 --> 00:13:41.436
with a lot of structures in the

00:13:41.436 --> 00:13:42.520
network, for example text.

00:13:42.520 --> 00:13:45.727
It can be associated with the nodes of

00:13:45.727 --> 00:13:47.210
the network, and that's basically what

00:13:47.210 --> 00:13:49.970
we just discussed in the NetPLSA

00:13:51.030 --> 00:13:53.060
But text data can be associated with edges

00:13:53.060 --> 00:13:56.310
as well, or paths or even sub networks

00:13:56.310 --> 00:13:59.790
and such way to represent taxes are in

00:13:59.790 --> 00:14:01.880
the big environment of all the context.

00:14:01.880 --> 00:14:04.650
Information is very powerful because it

00:14:04.650 --> 00:14:07.030
allows us to analyze all the data, all

00:14:07.030 --> 00:14:09.080
the information together.

00:14:09.720 --> 00:14:12.539
And so, in general analysis of text

00:14:12.540 --> 00:14:14.850
there should be using the entire

00:14:14.850 --> 00:14:17.470
network of information that's related

00:14:17.470 --> 00:14:19.070
to the text data.

00:14:21.180 --> 00:14:23.835
So here's one suggested reading and this is

00:14:23.835 --> 00:14:26.430
the paper about the NetPLSA where

00:14:26.430 --> 00:14:27.986
you can find more details about the

00:14:27.986 --> 00:14:30.020
model and how to estimate such a model.


