WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-27T00:01:10.6259808Z by ClassTranscribe

00:00:00.300 --> 00:00:02.810
This lecture is about the generative

00:00:02.810 --> 00:00:04.500
probabilistic models for text

00:00:04.500 --> 00:00:05.000
clustering.

00:00:13.740 --> 00:00:15.670
In this lecture we can do continue

00:00:15.670 --> 00:00:17.880
discussing text clustering, and we're

00:00:17.880 --> 00:00:19.910
going to introduce generative

00:00:19.910 --> 00:00:22.950
probabilistic models as a way to do text

00:00:22.950 --> 00:00:23.580
clustering

00:00:25.650 --> 00:00:29.120
So this is the overall plan for

00:00:29.120 --> 00:00:31.360
covering text clustering in the previous

00:00:31.360 --> 00:00:33.030
lecture we have talked about what is

00:00:33.030 --> 00:00:34.863
text clustering and why text

00:00:34.863 --> 00:00:36.440
clustering is interesting.

00:00:37.120 --> 00:00:38.680
In this lecture we're going to talk

00:00:38.680 --> 00:00:40.880
about how to do text clustering, in

00:00:40.880 --> 00:00:42.530
general, as you see on this slide,

00:00:42.530 --> 00:00:44.880
there are two kinds of approaches.

00:00:44.880 --> 00:00:47.450
One is generating probabilistic models,

00:00:47.450 --> 00:00:49.770
which is the topic of this lecture, and

00:00:49.770 --> 00:00:51.640
later will also discuss similarity

00:00:51.640 --> 00:00:52.770
based approaches.

00:00:53.750 --> 00:00:57.000
So to talk about generative models for

00:00:57.000 --> 00:01:00.320
text clustering, it would be useful to

00:01:00.320 --> 00:01:04.040
revisit the topic mining problem using

00:01:04.040 --> 00:01:05.330
topic models.

00:01:06.220 --> 00:01:07.930
Because the two problems are very

00:01:07.930 --> 00:01:10.590
similar, so this is a slide that you

00:01:10.590 --> 00:01:13.690
have seen earlier in the lecture

00:01:13.690 --> 00:01:15.050
on topic model.

00:01:15.050 --> 00:01:19.010
Here we show that we have input of text

00:01:19.010 --> 00:01:22.820
collection C and number of topics K and

00:01:22.820 --> 00:01:25.350
vocabulary V, and we hope to generate

00:01:25.350 --> 00:01:27.640
as output two things.

00:01:27.640 --> 00:01:30.990
One is a set of topics denoted by Theta

00:01:30.990 --> 00:01:31.540
i's.

00:01:32.090 --> 00:01:33.989
Each is a word distribution and the

00:01:33.990 --> 00:01:36.770
other is a pi ij's and these are the

00:01:36.770 --> 00:01:40.710
probabilities that each document covers

00:01:40.710 --> 00:01:42.050
each topic.

00:01:42.050 --> 00:01:44.640
So this is a topic coverage and it's

00:01:44.640 --> 00:01:47.240
also visualized here on this slide you

00:01:47.240 --> 00:01:49.770
can see that this is what we can get by

00:01:49.770 --> 00:01:50.800
using a topic model.

00:01:51.420 --> 00:01:54.790
Now a main difference between this and

00:01:54.790 --> 00:01:58.035
text clustering problem is that here a

00:01:58.035 --> 00:02:01.400
document is assumed to possibly cover

00:02:01.400 --> 00:02:04.820
multiple topics, and indeed in general

00:02:04.820 --> 00:02:08.780
document will be covering more than one

00:02:08.780 --> 00:02:11.610
topic with non zero probabilities.

00:02:12.690 --> 00:02:16.990
In text clustering, however, we only

00:02:16.990 --> 00:02:20.220
allow a document to cover one topic.

00:02:20.220 --> 00:02:22.580
If we assume one topic is a cluster.

00:02:24.190 --> 00:02:24.930
So.

00:02:26.410 --> 00:02:28.850
That means if we change the topic

00:02:28.850 --> 00:02:31.200
definition just slightly by assuming

00:02:31.200 --> 00:02:33.660
that each document can only be

00:02:33.660 --> 00:02:35.990
generated by using precisely one topic.

00:02:37.070 --> 00:02:39.050
Then we'll have a definition of the

00:02:39.050 --> 00:02:39.980
clustering problem.

00:02:40.580 --> 00:02:41.420
As shown here.

00:02:42.090 --> 00:02:46.100
So here the output is changed so that

00:02:46.100 --> 00:02:48.210
we no longer have the detailed coverage

00:02:48.210 --> 00:02:51.200
distributions pi ij's, but instead will

00:02:51.200 --> 00:02:54.620
have cluster assignment decisions.

00:02:54.620 --> 00:02:59.620
An CI and CI is decision for the

00:02:59.620 --> 00:03:00.590
document i.

00:03:00.590 --> 00:03:04.390
And c sub i is going to take a

00:03:04.390 --> 00:03:06.750
value from one through K to indicate

00:03:06.750 --> 00:03:08.270
one of the K clusters.

00:03:09.120 --> 00:03:13.260
And basically tells us document Di

00:03:13.260 --> 00:03:14.830
is in which cluster.

00:03:15.690 --> 00:03:19.060
As illustrated here, we no longer have

00:03:19.060 --> 00:03:21.610
multiple topics covered in each

00:03:21.610 --> 00:03:24.040
document is precisely one topic,

00:03:24.040 --> 00:03:25.940
although which topic is still

00:03:25.940 --> 00:03:26.520
uncertain.

00:03:27.180 --> 00:03:30.520
There is also a connection with the.

00:03:31.100 --> 00:03:32.360
Problem of mining.

00:03:32.360 --> 00:03:34.250
One topic that we discussed earlier.

00:03:34.250 --> 00:03:36.830
So here again it's a slide that you

00:03:36.830 --> 00:03:37.940
have seen before.

00:03:38.540 --> 00:03:42.330
And here we hope to estimate a topic

00:03:42.330 --> 00:03:45.650
model or word distribution based on

00:03:45.650 --> 00:03:47.825
precisely one document, and that's when

00:03:47.825 --> 00:03:49.830
we assume that this document covers

00:03:49.830 --> 00:03:51.309
precisely one topic.

00:03:52.890 --> 00:03:54.229
But we can also consider some

00:03:54.230 --> 00:03:55.510
variations of the problem.

00:03:55.510 --> 00:03:58.120
For example, we can consider there are

00:03:58.120 --> 00:04:00.320
N documents, each covers different

00:04:00.320 --> 00:04:00.835
topic.

00:04:00.835 --> 00:04:03.936
So that's N documents and topics.

00:04:03.936 --> 00:04:06.268
Of course, in this case these documents

00:04:06.268 --> 00:04:08.513
are independent and these topics also

00:04:08.513 --> 00:04:08.887
independent.

00:04:08.887 --> 00:04:11.616
But we can further allow these

00:04:11.616 --> 00:04:13.920
documents share topics and then.

00:04:13.920 --> 00:04:16.640
We can also assume that we are going to

00:04:16.640 --> 00:04:18.485
assume there are fewer topics.

00:04:18.485 --> 00:04:19.885
The number of documents.

00:04:19.885 --> 00:04:22.662
So this document must share some

00:04:22.662 --> 00:04:23.230
topics.

00:04:24.000 --> 00:04:25.410
And if we have N documents for

00:04:25.410 --> 00:04:29.970
share k topics, then will again have

00:04:29.970 --> 00:04:32.000
precisely the document clustering

00:04:32.000 --> 00:04:32.500
problem.

00:04:34.240 --> 00:04:35.780
So because of these connections,

00:04:35.780 --> 00:04:37.650
naturally we can think about how to use

00:04:37.650 --> 00:04:39.420
a probabilistic generating model to

00:04:39.420 --> 00:04:41.430
solve the problem of text clustering.

00:04:43.330 --> 00:04:45.590
So the question now is what generating

00:04:45.590 --> 00:04:47.820
model can be used to do clustering.

00:04:48.700 --> 00:04:51.041
As in all cases of designing a

00:04:51.041 --> 00:04:52.868
generative model, we hope the

00:04:52.868 --> 00:04:55.910
generative model would adopt the output

00:04:55.910 --> 00:04:57.727
that we hope to generate, or the

00:04:57.727 --> 00:04:59.430
structure that we hope to model.

00:04:59.430 --> 00:05:01.450
So in this case it's a clustering

00:05:01.450 --> 00:05:02.585
structure.

00:05:02.585 --> 00:05:06.520
The topics and each document that

00:05:06.520 --> 00:05:09.710
covers one topic, and we hope to embed

00:05:09.710 --> 00:05:13.310
such such preferences in a generative

00:05:13.310 --> 00:05:13.750
model.

00:05:15.620 --> 00:05:16.899
But if you think about the main

00:05:16.900 --> 00:05:19.157
difference between this problem and the

00:05:19.157 --> 00:05:20.520
topic model that we talked about

00:05:20.520 --> 00:05:23.800
earlier and then you will see a main

00:05:23.800 --> 00:05:27.530
requirement is how can we force every

00:05:27.530 --> 00:05:30.240
document to be generated from precisely

00:05:30.240 --> 00:05:32.520
one topic instead of K topics?

00:05:33.550 --> 00:05:34.890
As in the topic model.

00:05:35.840 --> 00:05:39.710
So let's revisit the topic model again

00:05:39.710 --> 00:05:41.265
in more detail.

00:05:41.265 --> 00:05:44.550
So this is a detailed view of two

00:05:44.550 --> 00:05:46.890
component mixture model and when we

00:05:46.890 --> 00:05:49.140
have K components it looks similar.

00:05:49.840 --> 00:05:52.360
So here we see that when we generate a

00:05:52.360 --> 00:05:53.040
document.

00:05:53.770 --> 00:05:56.410
We generated each word independently.

00:05:57.380 --> 00:05:59.510
And we generated each word

00:05:59.510 --> 00:06:03.160
First make a choice between these

00:06:03.160 --> 00:06:05.860
distributions with decided to use one

00:06:05.860 --> 00:06:08.700
of them with probability.

00:06:09.580 --> 00:06:13.790
So P of theta one is the probability of

00:06:13.790 --> 00:06:15.980
choosing the distribution on the top.

00:06:17.880 --> 00:06:20.230
Now we first make this decision

00:06:20.230 --> 00:06:22.020
regarding which distribution should be

00:06:22.020 --> 00:06:24.560
used to generate the world, and then

00:06:24.560 --> 00:06:26.540
we're going to use this distribution to

00:06:26.540 --> 00:06:27.545
sample word.

00:06:27.545 --> 00:06:28.000
Now.

00:06:28.000 --> 00:06:30.780
Notice that in such a generative model.

00:06:31.490 --> 00:06:34.220
The decision on which distribution to

00:06:34.220 --> 00:06:37.690
use for each word is independent, so

00:06:37.690 --> 00:06:39.610
that means, for example, "the" here could

00:06:39.610 --> 00:06:41.100
have been generated from the second

00:06:41.100 --> 00:06:42.450
distribution.

00:06:42.450 --> 00:06:45.900
Theta two, whereas text is more

00:06:45.900 --> 00:06:48.200
likely generated from the first one on

00:06:48.200 --> 00:06:48.800
the top.

00:06:49.540 --> 00:06:52.430
That means the words in the document

00:06:52.430 --> 00:06:54.860
could have been generated in general

00:06:54.860 --> 00:06:56.780
from multiple distributions.

00:06:57.540 --> 00:07:00.640
Now this is not what we want to see

00:07:00.640 --> 00:07:01.895
for text clustering.

00:07:01.895 --> 00:07:04.070
For document clustering where we hope

00:07:04.070 --> 00:07:06.320
this document will be generated from

00:07:06.320 --> 00:07:07.530
precisely one topic.

00:07:09.440 --> 00:07:12.450
So now that means we need to modify the

00:07:12.450 --> 00:07:15.650
model, but how well, let's first think

00:07:15.650 --> 00:07:19.250
about why this model cannot be used for

00:07:19.250 --> 00:07:22.350
clustering, and I just say the reason

00:07:22.350 --> 00:07:23.460
is because.

00:07:23.460 --> 00:07:26.060
It has allowed multiple topics to

00:07:26.060 --> 00:07:28.160
contribute the words to the document.

00:07:28.740 --> 00:07:31.030
And that causes confusion because we're

00:07:31.030 --> 00:07:33.190
not going to know which cluster this

00:07:33.190 --> 00:07:35.040
document is from an it's more

00:07:35.040 --> 00:07:36.660
importantly, it's violating our

00:07:36.660 --> 00:07:39.100
assumption about the partitioning of

00:07:39.100 --> 00:07:40.490
documents in the clusters.

00:07:41.140 --> 00:07:43.540
If we really have one topic to

00:07:43.540 --> 00:07:45.620
correspond to one cluster of documents,

00:07:45.620 --> 00:07:48.871
then we would have a document to be

00:07:48.871 --> 00:07:50.630
generated from precisely one topic.

00:07:50.630 --> 00:07:53.486
That means all the words in the

00:07:53.486 --> 00:07:55.680
document must have been generated from

00:07:55.680 --> 00:07:57.920
precisely one distribution, and this

00:07:57.920 --> 00:08:00.660
is not true for such a topic model that

00:08:00.660 --> 00:08:02.740
we're seeing here, and that's why this

00:08:02.740 --> 00:08:06.140
cannot be used for clustering because

00:08:06.140 --> 00:08:09.220
it did not ensure that only one

00:08:09.220 --> 00:08:11.010
distribution has been used to generate.

00:08:11.150 --> 00:08:13.250
All the words in one document.

00:08:15.020 --> 00:08:17.520
So if you realize this problem, then we

00:08:17.520 --> 00:08:20.040
can naturally design alternative

00:08:20.040 --> 00:08:21.770
mixture model for doing clustering.

00:08:21.770 --> 00:08:24.490
So this is what you're seeing here and

00:08:24.490 --> 00:08:27.360
we again would have to make a decision

00:08:27.360 --> 00:08:29.125
regarding which is distributing to use

00:08:29.125 --> 00:08:31.456
to generate document, because the

00:08:31.456 --> 00:08:33.470
document that could potentially be

00:08:33.470 --> 00:08:35.320
generated from any of the K word

00:08:35.320 --> 00:08:36.539
distributions that we have.

00:08:37.750 --> 00:08:39.730
But this time, once we have made the

00:08:39.730 --> 00:08:42.110
decision to choose one of the topics,

00:08:42.110 --> 00:08:44.390
we're going to stay with this

00:08:44.390 --> 00:08:46.730
distribution to generate the all the

00:08:46.730 --> 00:08:48.380
words in the document.

00:08:49.940 --> 00:08:52.220
And that means once we have made the

00:08:52.220 --> 00:08:55.610
choice of the distribution for in

00:08:55.610 --> 00:08:57.070
generating the first word.

00:08:57.640 --> 00:08:59.979
We're going to stay with this

00:08:59.980 --> 00:09:02.870
decision in generating all the other

00:09:02.870 --> 00:09:03.990
words in the document.

00:09:04.800 --> 00:09:06.950
So in other words, we only make the

00:09:06.950 --> 00:09:08.110
choice once.

00:09:08.740 --> 00:09:09.400
for all.

00:09:10.420 --> 00:09:12.990
Basically we make the decision once for

00:09:12.990 --> 00:09:15.310
this document and stay with this to

00:09:15.310 --> 00:09:16.350
generate all the words.

00:09:18.410 --> 00:09:21.420
Similarly, if I had chosen the second

00:09:21.420 --> 00:09:23.320
distribution, theta sub two here, you

00:09:23.320 --> 00:09:25.230
can see we will stay with this one and

00:09:25.230 --> 00:09:27.870
then generate the entire document D.

00:09:28.790 --> 00:09:30.375
Now, if you compare this picture with

00:09:30.375 --> 00:09:33.290
the previous one, you will see the

00:09:33.290 --> 00:09:34.540
desicion of.

00:09:36.350 --> 00:09:39.650
Of using a particular distribution is

00:09:39.650 --> 00:09:42.486
made of just once for this document.

00:09:42.486 --> 00:09:44.310
In the case of document clustering.

00:09:44.310 --> 00:09:46.520
But in the case of topic model we have

00:09:46.520 --> 00:09:49.790
to make as many decisions as the number

00:09:49.790 --> 00:09:51.510
of words in the document because for

00:09:51.510 --> 00:09:53.340
each word we can make a potential

00:09:53.340 --> 00:09:55.580
different decision and that's the key

00:09:55.580 --> 00:09:57.150
difference between the two models.

00:09:58.160 --> 00:09:59.820
But this is obviously also a mixture

00:09:59.820 --> 00:10:01.630
model, so we can just group them

00:10:01.630 --> 00:10:05.203
together as one box to show that this

00:10:05.203 --> 00:10:05.549
is.

00:10:06.210 --> 00:10:08.390
Model that will give us a probability

00:10:08.390 --> 00:10:09.340
of a document.

00:10:10.380 --> 00:10:12.665
Now inside this model there's also

00:10:12.665 --> 00:10:14.480
this, which of choosing a different

00:10:14.480 --> 00:10:16.650
distribution and we don't observe that,

00:10:16.650 --> 00:10:18.100
so that's a mixture model.

00:10:19.110 --> 00:10:21.340
And of course, the main problem in

00:10:21.340 --> 00:10:23.580
document clustering is to infer.

00:10:24.760 --> 00:10:26.680
Which distribution has been used to

00:10:26.680 --> 00:10:29.090
generator a document and that would

00:10:29.090 --> 00:10:30.940
allow us to recover the cluster

00:10:30.940 --> 00:10:32.410
identity over document

00:10:37.690 --> 00:10:40.280
So it would be useful to think about

00:10:40.280 --> 00:10:42.700
the difference from the topic model, as

00:10:42.700 --> 00:10:44.770
I have also mentioned multiple times.

00:10:45.850 --> 00:10:48.600
There are many.

00:10:50.050 --> 00:10:51.030
Two differences.

00:10:51.030 --> 00:10:54.350
One is the choice of.

00:10:56.070 --> 00:10:59.080
Using a particular distribution is made

00:10:59.080 --> 00:11:01.140
just once for document clustering

00:11:01.140 --> 00:11:04.740
model, whereas in the topic model it's

00:11:04.740 --> 00:11:05.960
made multiple times.

00:11:06.770 --> 00:11:07.855
Four different words.

00:11:07.855 --> 00:11:11.450
The second is that word distribution

00:11:11.450 --> 00:11:15.140
here is going to be used to generate

00:11:15.140 --> 00:11:17.930
all the words for a document.

00:11:19.170 --> 00:11:22.800
But in the case of topic modeling, one

00:11:22.800 --> 00:11:24.510
distribution doesn't have to generate with

00:11:24.510 --> 00:11:26.670
all the words in a document.

00:11:26.670 --> 00:11:28.480
Multiple distribution could have been

00:11:28.480 --> 00:11:31.136
used to generate the words in the

00:11:31.136 --> 00:11:31.520
document.

00:11:34.440 --> 00:11:36.460
It's also think about the special case

00:11:36.460 --> 00:11:39.820
when one of the one of the probability of

00:11:39.820 --> 00:11:41.690
choosing a particular distribution is

00:11:41.690 --> 00:11:42.490
equal to 1.

00:11:43.420 --> 00:11:45.900
Now that just means we have no

00:11:45.900 --> 00:11:46.690
uncertainty now.

00:11:46.690 --> 00:11:49.680
We just stick with one particular

00:11:49.680 --> 00:11:50.380
distribution.

00:11:50.980 --> 00:11:54.080
Now in that case, clearly we will see

00:11:54.080 --> 00:11:56.300
this is no longer mixture model 'cause

00:11:56.300 --> 00:11:58.140
there's no certainty here and we're

00:11:58.140 --> 00:12:00.659
going to just use precise one of the

00:12:00.660 --> 00:12:02.532
distributions for generating a

00:12:02.532 --> 00:12:05.285
document, and we're going back to the

00:12:05.285 --> 00:12:09.070
case of estimating one word

00:12:09.070 --> 00:12:11.549
distribution based on one document.

00:12:12.790 --> 00:12:13.980
So that's the connection that we

00:12:13.980 --> 00:12:14.890
discussed earlier.

00:12:15.620 --> 00:12:17.310
But now you can see more clearly.

00:12:18.550 --> 00:12:21.270
So as more cases of using a generative

00:12:21.270 --> 00:12:23.450
model to solve a problem, we first look

00:12:23.450 --> 00:12:25.722
at theta and then think about how to

00:12:25.722 --> 00:12:26.450
design the model.

00:12:26.450 --> 00:12:28.730
But once we design model, the next step

00:12:28.730 --> 00:12:30.360
is to write down the likelihood

00:12:30.360 --> 00:12:30.840
function.

00:12:31.540 --> 00:12:33.680
And after that we can do is to look at the

00:12:33.680 --> 00:12:35.140
how to estimate the parameters.

00:12:36.110 --> 00:12:37.650
so in this case what's the

00:12:37.650 --> 00:12:39.590
likelihood function or it's going to be

00:12:39.590 --> 00:12:41.230
very similar to what we have seen

00:12:41.230 --> 00:12:43.350
before in topic models, but it will be

00:12:43.350 --> 00:12:44.130
also different.

00:12:45.110 --> 00:12:47.810
If you still recall what the likelihood

00:12:47.810 --> 00:12:50.270
function looks like in PLSA, then you

00:12:50.270 --> 00:12:52.460
realize that in general the probability

00:12:52.460 --> 00:12:54.820
of observing a data point from mixture

00:12:54.820 --> 00:12:56.990
model is going to be a sum over all the

00:12:56.990 --> 00:12:59.230
possibilities of generating the data.

00:13:00.270 --> 00:13:01.770
I in this case, so it's going to be

00:13:01.770 --> 00:13:04.030
some over these K topics because

00:13:04.030 --> 00:13:05.650
everyone can be used to generate the

00:13:05.650 --> 00:13:08.670
document and then inside the sum you

00:13:08.670 --> 00:13:10.850
can still recall what the formula looks

00:13:10.850 --> 00:13:13.560
like an it's going to be.

00:13:15.200 --> 00:13:19.268
A product of two probabilities and one

00:13:19.268 --> 00:13:21.300
is the probability of choosing a

00:13:21.300 --> 00:13:21.890
distribution.

00:13:21.890 --> 00:13:23.670
The other is the probability of

00:13:23.670 --> 00:13:25.740
observing a particular data point from

00:13:25.740 --> 00:13:26.610
that distribution.

00:13:27.530 --> 00:13:31.880
So if you are map, this formula is kind

00:13:31.880 --> 00:13:33.220
of formula to our problem.

00:13:33.220 --> 00:13:35.070
Here you will see the probability of

00:13:35.070 --> 00:13:39.130
observing a document D is basically a

00:13:39.130 --> 00:13:41.970
sum, in this case over two different

00:13:41.970 --> 00:13:42.790
distributions.

00:13:42.790 --> 00:13:44.860
Because we have a very simplified

00:13:44.860 --> 00:13:46.840
situation of just two clusters.

00:13:47.490 --> 00:13:49.500
And so in this case you can see it's a

00:13:49.500 --> 00:13:51.295
sum of two cases.

00:13:51.295 --> 00:13:53.290
In each case it's indeed the

00:13:53.290 --> 00:13:55.910
probability of choosing the.

00:13:56.810 --> 00:13:59.855
Choosing the world distribution.

00:13:59.855 --> 00:14:03.270
Is theta one or theta two right?

00:14:03.270 --> 00:14:06.330
And then it's this probability is

00:14:06.330 --> 00:14:08.480
multiplied by the probability of

00:14:08.480 --> 00:14:12.210
observing this document from this

00:14:12.210 --> 00:14:13.650
particular distribution.

00:14:16.300 --> 00:14:20.130
And if you further expand this

00:14:20.130 --> 00:14:22.437
probability of observing the whole

00:14:22.437 --> 00:14:25.150
document, we see that it's product of

00:14:25.150 --> 00:14:28.510
observing each word X sub i.

00:14:28.510 --> 00:14:30.855
Here we made the assumption that each

00:14:30.855 --> 00:14:33.414
word is generated independently, so the

00:14:33.414 --> 00:14:35.119
probability of the whole document is

00:14:35.120 --> 00:14:37.305
just a product of the probability of

00:14:37.305 --> 00:14:38.910
each word in the document.

00:14:39.950 --> 00:14:43.026
So this form should be very similar to

00:14:43.026 --> 00:14:45.860
the topic model, but it's also useful

00:14:45.860 --> 00:14:48.070
to think about the difference and for

00:14:48.070 --> 00:14:51.170
that purpose I am also copying the

00:14:51.170 --> 00:14:52.740
probability of.

00:14:53.390 --> 00:14:56.260
topic model with two components here.

00:14:56.260 --> 00:14:58.710
So here you can see at the formula

00:14:58.710 --> 00:15:00.900
looks very similar or in many ways they

00:15:00.900 --> 00:15:01.460
are similar.

00:15:02.410 --> 00:15:05.160
But there's also some difference.

00:15:06.000 --> 00:15:08.470
And in particular, the differences on

00:15:08.470 --> 00:15:11.570
the top you see for the mixture model,

00:15:11.570 --> 00:15:13.859
document clustering, we first take a

00:15:13.860 --> 00:15:15.720
product and then take a sum.

00:15:16.510 --> 00:15:18.820
And that's corresponding to our

00:15:18.820 --> 00:15:21.370
assumption of 1st make a choice of

00:15:21.370 --> 00:15:23.163
choosing one distribution and then stay

00:15:23.163 --> 00:15:25.810
with this distribution to generate all the words.

00:15:25.810 --> 00:15:28.320
And that's why we had the product

00:15:28.320 --> 00:15:29.410
inside the sum.

00:15:30.810 --> 00:15:32.710
The sum corresponds to the choice.

00:15:33.390 --> 00:15:33.730
right.

00:15:34.930 --> 00:15:37.250
Now in the topic model, we see that the

00:15:37.250 --> 00:15:40.030
sum is actually inside the product and

00:15:40.030 --> 00:15:42.270
that's be cause we generated each word

00:15:42.270 --> 00:15:43.230
independently.

00:15:44.640 --> 00:15:46.460
And that's why we have the product

00:15:46.460 --> 00:15:47.260
outside.

00:15:47.260 --> 00:15:49.690
But when we generate each each word, we

00:15:49.690 --> 00:15:51.620
have to make a decision regarding which

00:15:51.620 --> 00:15:52.580
distribution we use.

00:15:52.580 --> 00:15:55.140
So we have sum there for each word.

00:15:56.650 --> 00:15:58.915
But in general, ideas are all mixture

00:15:58.915 --> 00:16:01.350
models that we can estimate these models

00:16:01.350 --> 00:16:03.740
by using the EM algorithm as we will

00:16:03.740 --> 00:16:04.830
discuss more later.


