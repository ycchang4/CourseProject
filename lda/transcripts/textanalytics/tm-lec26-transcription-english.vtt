WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-26T23:59:27.5425716Z by ClassTranscribe

00:00:00.300 --> 00:00:02.420
This lecture is about the probabilistic

00:00:02.420 --> 00:00:05.020
latent semantic analysis or P LSA.

00:00:12.470 --> 00:00:14.420
In this lecture we're going to

00:00:14.420 --> 00:00:16.890
introduce probabilistic latent semantic

00:00:16.890 --> 00:00:18.950
analysis, often called the PLSA.

00:00:18.950 --> 00:00:22.240
Say this is the most basic topic model.

00:00:22.240 --> 00:00:25.290
Also, one of the most useful topic

00:00:25.290 --> 00:00:25.980
models.

00:00:25.980 --> 00:00:29.130
Now, this kind of models can in general

00:00:29.130 --> 00:00:32.780
be used to mine multiple topics from

00:00:32.780 --> 00:00:36.613
text documents, and PLSA is one of the

00:00:36.613 --> 00:00:39.320
most basic topic models for doing this,

00:00:39.320 --> 00:00:41.850
so let's first examine this problem in

00:00:41.850 --> 00:00:43.250
a little more detail.

00:00:43.740 --> 00:00:45.720
Here I show a sample article which is a

00:00:45.720 --> 00:00:47.920
blog article about Hurricane Katrina.

00:00:48.700 --> 00:00:51.230
An I showed some sample topics, for

00:00:51.230 --> 00:00:54.140
example government response, flooding

00:00:54.140 --> 00:00:56.640
of the city in new orlean's donation

00:00:56.640 --> 00:00:57.620
and the background.

00:00:59.130 --> 00:01:02.490
You can see in the article we use words

00:01:02.490 --> 00:01:04.140
from all these distributions.

00:01:04.920 --> 00:01:06.425
So we first for example.

00:01:06.425 --> 00:01:08.780
See there's a criticism of government

00:01:08.780 --> 00:01:10.680
response, and this is followed by the

00:01:10.680 --> 00:01:13.360
discussion of flooding of the city and

00:01:13.360 --> 00:01:14.335
donation, etc.

00:01:14.335 --> 00:01:17.040
We also see background words or mixed

00:01:17.040 --> 00:01:20.260
with them, so the goal of topic

00:01:20.260 --> 00:01:22.900
analysis here is try to decode these

00:01:22.900 --> 00:01:24.380
topics behind the text.

00:01:24.380 --> 00:01:26.940
So segment of the topics to figure out

00:01:26.940 --> 00:01:29.000
which words are from which distribution

00:01:29.000 --> 00:01:32.412
and to figure out the first one of

00:01:32.412 --> 00:01:33.297
these topics.

00:01:33.297 --> 00:01:35.190
So how do we know there's a topic about

00:01:35.190 --> 00:01:36.300
government response?

00:01:36.390 --> 00:01:37.795
There is a public about the flooding of

00:01:37.795 --> 00:01:38.270
the city.

00:01:38.920 --> 00:01:41.560
So these are the tasks of topical

00:01:41.560 --> 00:01:42.050
model.

00:01:42.720 --> 00:01:45.159
If we can discover these topics can

00:01:45.160 --> 00:01:48.190
color this words as you see here to

00:01:48.190 --> 00:01:50.280
separate the different topics, then you

00:01:50.280 --> 00:01:52.420
can do a lot of things such as

00:01:52.420 --> 00:01:55.210
summarization or segmentation of the

00:01:55.210 --> 00:01:59.120
topics, clustering of sentences, etc.

00:01:59.120 --> 00:02:02.190
So the formal definition of the problem

00:02:02.190 --> 00:02:04.320
of mining multiple topics from text is

00:02:04.320 --> 00:02:05.660
shown here, and this is actually a

00:02:05.660 --> 00:02:08.070
slide that you have seen in the earlier

00:02:08.070 --> 00:02:10.120
lecture, so the input is the

00:02:10.120 --> 00:02:11.960
collection, the number of topics and

00:02:11.960 --> 00:02:12.900
vocabulary set.

00:02:13.460 --> 00:02:15.400
And of course, the text data right?

00:02:15.400 --> 00:02:18.460
And then the output is of two kinds.

00:02:18.460 --> 00:02:20.190
One is the topic category

00:02:20.190 --> 00:02:23.340
characterization Seedies HCI is a water

00:02:23.340 --> 00:02:26.460
distribution and 2nd it's the topic

00:02:26.460 --> 00:02:27.646
coverage for each document.

00:02:27.646 --> 00:02:31.910
These are pie some ideas and they tell

00:02:31.910 --> 00:02:34.169
us which document covers which topic to

00:02:34.170 --> 00:02:34.950
what extent.

00:02:34.950 --> 00:02:37.690
So we hope to generate these as output

00:02:37.690 --> 00:02:39.540
because there are many useful

00:02:39.540 --> 00:02:41.470
applications if we can do that.

00:02:42.790 --> 00:02:46.460
So the idea of PLSA is actually very

00:02:46.460 --> 00:02:48.320
similar to the two component mixture

00:02:48.320 --> 00:02:50.125
model that we have already introduced.

00:02:50.125 --> 00:02:52.663
The only difference is that we're going

00:02:52.663 --> 00:02:54.240
to have more than two topics.

00:02:54.240 --> 00:02:56.790
Otherwise it's essentially the same.

00:02:57.590 --> 00:02:59.350
So here I illustrate how we can

00:02:59.350 --> 00:03:00.970
generate the text that I was multiple

00:03:00.970 --> 00:03:01.540
topics.

00:03:02.600 --> 00:03:07.050
And naturally, in all cases of

00:03:07.050 --> 00:03:09.440
probabilistic modeling, would want to

00:03:09.440 --> 00:03:11.320
figure out the likelihood function.

00:03:11.320 --> 00:03:13.430
So we will also ask the question what's

00:03:13.430 --> 00:03:16.110
the probability of observing a world W

00:03:16.110 --> 00:03:17.380
from such a mixture model?

00:03:18.080 --> 00:03:19.450
Now if you look at this picture and

00:03:19.450 --> 00:03:21.082
compare this with the picture that you

00:03:21.082 --> 00:03:22.940
have seen earlier, you will see the

00:03:22.940 --> 00:03:24.630
only difference is that we have added

00:03:24.630 --> 00:03:25.740
more topics here.

00:03:26.760 --> 00:03:30.120
So before we have just one topic

00:03:30.120 --> 00:03:33.620
besides the background topical, but now

00:03:33.620 --> 00:03:35.910
we have more topics.

00:03:35.910 --> 00:03:37.685
Specifically we have K topics.

00:03:37.685 --> 00:03:41.520
Now all these are topics that we assume

00:03:41.520 --> 00:03:44.259
that exist in the text data, so the

00:03:44.260 --> 00:03:46.410
consequences that our switch for

00:03:46.410 --> 00:03:48.656
choosing a topic now is multiway switch

00:03:48.656 --> 00:03:51.160
before it's just a two way switch.

00:03:51.160 --> 00:03:53.020
Going to think of as flipping a coin.

00:03:53.020 --> 00:03:54.910
But now we have multiple is.

00:03:54.910 --> 00:03:57.830
First we can flip a coin to decide

00:03:57.830 --> 00:03:59.361
whether we will talk about the

00:03:59.361 --> 00:03:59.733
background.

00:03:59.733 --> 00:04:01.219
So it's the background.

00:04:01.430 --> 00:04:05.250
Lambda sub B versus non background.

00:04:05.250 --> 00:04:08.540
So this one minus Lambda B gives us the

00:04:08.540 --> 00:04:11.270
probability of actually choosing a

00:04:11.270 --> 00:04:12.050
topic.

00:04:13.940 --> 00:04:14.950
And background help.

00:04:15.680 --> 00:04:17.860
After we have made this decision, we

00:04:17.860 --> 00:04:20.710
have to make another decision to choose

00:04:20.710 --> 00:04:23.920
one of these K distributions.

00:04:24.650 --> 00:04:25.870
So there's a key way.

00:04:25.870 --> 00:04:28.120
Switch here, and this is characterized

00:04:28.120 --> 00:04:30.320
by the pies, and there's someone.

00:04:31.330 --> 00:04:33.365
So this is just the different design

00:04:33.365 --> 00:04:34.750
of switches, a little bit more

00:04:34.750 --> 00:04:37.840
complicated, but once we decide which

00:04:37.840 --> 00:04:39.910
distribution to use, the rest is the

00:04:39.910 --> 00:04:40.220
same.

00:04:40.220 --> 00:04:42.270
We're going to generate the world by

00:04:42.270 --> 00:04:44.540
using one of these distributions,

00:04:44.540 --> 00:04:45.280
assume here.

00:04:46.170 --> 00:04:48.710
OK, so now let's look at this question

00:04:48.710 --> 00:04:50.625
about the like hold.

00:04:50.625 --> 00:04:53.500
So what's the probability of observing

00:04:53.500 --> 00:04:55.400
a word from such a distribution?

00:04:55.400 --> 00:04:56.480
What do you think?

00:04:57.120 --> 00:04:59.780
Now we've seen this problem many Times

00:04:59.780 --> 00:05:04.430
Now, and if you recall, it's generally

00:05:04.430 --> 00:05:06.210
a sum over all the different

00:05:06.210 --> 00:05:08.450
possibilities of generating the world.

00:05:08.450 --> 00:05:11.460
So let's first look at the how the

00:05:11.460 --> 00:05:13.700
world can be generated from the

00:05:13.700 --> 00:05:14.560
background model.

00:05:14.560 --> 00:05:16.709
The probability that the world is

00:05:16.709 --> 00:05:18.849
generated from the background model is

00:05:18.850 --> 00:05:21.363
Lambda multiplied by the probability of

00:05:21.363 --> 00:05:23.089
the world from the background model,

00:05:23.090 --> 00:05:23.560
right?

00:05:23.560 --> 00:05:25.170
Two things must happen.

00:05:25.170 --> 00:05:27.833
First, we have to have chosen the

00:05:27.833 --> 00:05:28.599
background model.

00:05:28.930 --> 00:05:31.450
And that's probability of Lambda sub B

00:05:31.450 --> 00:05:33.450
and then the second we must have

00:05:33.450 --> 00:05:35.410
actually obtained the world W from the

00:05:35.410 --> 00:05:37.990
background, and that's probability of W

00:05:37.990 --> 00:05:39.400
given sit out submit.

00:05:40.090 --> 00:05:43.186
OK, so similarly we can figure out the

00:05:43.186 --> 00:05:44.990
probability of observing the world from

00:05:44.990 --> 00:05:45.645
another topic.

00:05:45.645 --> 00:05:48.140
A topical theta sub k.

00:05:48.140 --> 00:05:50.810
Now notice that here's a product of

00:05:50.810 --> 00:05:52.970
three terms, and that's the cause.

00:05:52.970 --> 00:05:55.720
The choice of topic theta sub

00:05:55.720 --> 00:06:00.465
K only happens if two things happen.

00:06:00.465 --> 00:06:03.480
One is we decided not to talk about

00:06:03.480 --> 00:06:05.970
background, so that's probability 1

00:06:05.970 --> 00:06:07.310
minus Lambda sub b.

00:06:07.310 --> 00:06:09.780
Second, we also have to actually

00:06:09.780 --> 00:06:10.590
choose.

00:06:10.640 --> 00:06:11.225
Set us up.

00:06:11.225 --> 00:06:13.080
K among these K topics.

00:06:13.080 --> 00:06:15.140
So that's probability of serious up

00:06:15.140 --> 00:06:16.290
cake or pie.

00:06:16.910 --> 00:06:18.690
And similarly, the probability of

00:06:18.690 --> 00:06:20.640
generating the water from the second

00:06:20.640 --> 00:06:24.000
topic and his first topic popular like

00:06:24.000 --> 00:06:25.960
what you're seeing here and then.

00:06:25.960 --> 00:06:27.709
So in the end, the probability of

00:06:27.710 --> 00:06:29.750
observing the world is just a sum of

00:06:29.750 --> 00:06:30.900
all these cases.

00:06:32.380 --> 00:06:34.460
And I have to stress again, this is a

00:06:34.460 --> 00:06:37.570
very important formula to know because.

00:06:37.570 --> 00:06:39.729
This is really key to know to

00:06:39.730 --> 00:06:42.020
for understanding all the topic models and

00:06:42.020 --> 00:06:44.440
indeed a lot of mixture models, so make

00:06:44.440 --> 00:06:46.680
sure that you really understand the

00:06:46.680 --> 00:06:47.590
probability.

00:06:48.290 --> 00:06:52.860
Of W is indeed the some of these terms.

00:06:56.440 --> 00:07:00.077
So next, once we have the likelihood

00:07:00.077 --> 00:07:02.190
function, we would be interested in

00:07:02.190 --> 00:07:05.240
knowing the parameters right?

00:07:05.240 --> 00:07:06.900
So to estimate the parameters.

00:07:06.900 --> 00:07:09.670
But first let's put all these together

00:07:09.670 --> 00:07:11.245
to have the complete likelihood

00:07:11.245 --> 00:07:13.300
function for PLSA.

00:07:13.300 --> 00:07:15.200
Now the first line shows the

00:07:15.200 --> 00:07:17.345
probability of a word as illustrated on

00:07:17.345 --> 00:07:19.600
the previous slide and this is an

00:07:19.600 --> 00:07:21.200
important formula as I said.

00:07:21.870 --> 00:07:24.155
And so let's take a closer look at

00:07:24.155 --> 00:07:24.510
this.

00:07:24.510 --> 00:07:26.420
After that contains all the important

00:07:26.420 --> 00:07:27.005
parameters.

00:07:27.005 --> 00:07:29.260
So first we see Lambda sub b here.

00:07:29.260 --> 00:07:30.810
This represents the percentage of

00:07:30.810 --> 00:07:31.770
background words.

00:07:32.420 --> 00:07:35.100
That would believe exist in the text

00:07:35.100 --> 00:07:38.240
data and this can be unknown value that

00:07:38.240 --> 00:07:39.390
we set empirically.

00:07:41.090 --> 00:07:42.750
second, we see the background language

00:07:42.750 --> 00:07:44.820
model and typically we also assume this

00:07:44.820 --> 00:07:45.240
is known.

00:07:45.240 --> 00:07:47.820
We can use a large collection of text

00:07:47.820 --> 00:07:49.360
or use all the tests that we have

00:07:49.360 --> 00:07:51.250
available to estimate the water

00:07:51.250 --> 00:07:51.950
distribution.

00:07:52.700 --> 00:07:55.290
Now next in the rest of this formula.

00:07:56.390 --> 00:07:59.656
Excuse me, you see two interesting kinds

00:07:59.656 --> 00:08:00.369
of parameters.

00:08:00.370 --> 00:08:02.070
Those are the most important parameters

00:08:02.070 --> 00:08:06.350
that we are asked, so one is pies and

00:08:06.350 --> 00:08:09.790
these are the coverage of topic in the

00:08:09.790 --> 00:08:10.460
document.

00:08:11.160 --> 00:08:13.610
And the other is the word distributions

00:08:13.610 --> 00:08:15.410
that characterize all the topics.

00:08:17.930 --> 00:08:22.940
So the next line then is simply to plug

00:08:22.940 --> 00:08:25.829
this in to calculate the probability of

00:08:25.829 --> 00:08:26.103
document.

00:08:26.103 --> 00:08:28.310
This is again of the familiar form

00:08:28.310 --> 00:08:29.940
where you have some and you have

00:08:29.940 --> 00:08:32.180
account of world in the document and

00:08:32.180 --> 00:08:34.685
then log of a probability.

00:08:34.685 --> 00:08:37.760
Now it's a little bit more complicated

00:08:37.760 --> 00:08:39.630
than the two component because now we

00:08:39.630 --> 00:08:40.730
have more components.

00:08:40.730 --> 00:08:43.994
So the sum involves more terms and then

00:08:43.994 --> 00:08:46.390
this line is just the like holder for the

00:08:46.390 --> 00:08:48.910
whole collection and it's very similar.

00:08:49.000 --> 00:08:50.690
Just accounting for more documents in

00:08:50.690 --> 00:08:51.230
the collection.

00:08:52.370 --> 00:08:54.100
So what are the unknown primers?

00:08:54.100 --> 00:08:56.060
I already said there are two kinds on

00:08:56.060 --> 00:08:56.940
his coverage.

00:08:56.940 --> 00:08:58.350
One is awarded distributions.

00:08:59.040 --> 00:09:01.180
Again, it's a useful exercise for you

00:09:01.180 --> 00:09:03.890
to figure out exactly how many premise

00:09:03.890 --> 00:09:04.920
there on here.

00:09:05.660 --> 00:09:07.360
How many unknown parameters are there?

00:09:07.360 --> 00:09:09.970
Now trying to figure out that question

00:09:09.970 --> 00:09:11.960
would help you understand the model in

00:09:11.960 --> 00:09:14.395
more detail, and it would also allow

00:09:14.395 --> 00:09:16.140
you to understand what would be the

00:09:16.140 --> 00:09:18.665
output that we generate when we use PLSA

00:09:18.665 --> 00:09:20.910
to analyze text data, and these are

00:09:20.910 --> 00:09:22.609
precisely the unknown parameters.

00:09:24.380 --> 00:09:26.160
So after we have obtained the

00:09:26.160 --> 00:09:28.320
likelihood function shown here, the

00:09:28.320 --> 00:09:30.350
next is to worry about parameter

00:09:30.350 --> 00:09:30.990
estimation.

00:09:31.930 --> 00:09:33.300
And we can do the usual thing.

00:09:33.300 --> 00:09:34.730
Maximum likelihood estimator.

00:09:34.730 --> 00:09:35.880
So again, it's a constrained

00:09:35.880 --> 00:09:38.950
optimization problem like what we have

00:09:38.950 --> 00:09:40.990
seen before, only that we have a

00:09:40.990 --> 00:09:42.540
collection of text and we have more

00:09:42.540 --> 00:09:44.940
parameters to estimate and we still

00:09:44.940 --> 00:09:46.830
have two constraints, different

00:09:46.830 --> 00:09:48.390
constraint, two kinds of constraints.

00:09:48.390 --> 00:09:50.270
One is awarded distributions.

00:09:50.830 --> 00:09:53.890
All the words must have probabilities

00:09:53.890 --> 00:09:55.800
that sum to 141 distribution.

00:09:56.380 --> 00:09:58.460
The other is the topic coverage

00:09:58.460 --> 00:09:59.200
distribution.

00:09:59.200 --> 00:10:01.570
Anna Document will have to cover

00:10:01.570 --> 00:10:05.340
precisely these K topics, so the

00:10:05.340 --> 00:10:07.040
probability of covering each topical

00:10:07.040 --> 00:10:08.110
would have to sum to one.

00:10:09.120 --> 00:10:10.950
So at this point it's basically where

00:10:10.950 --> 00:10:12.750
they find applied math problem.

00:10:12.750 --> 00:10:13.980
You just need to figure out the

00:10:13.980 --> 00:10:15.960
solutions to optimization problem.

00:10:15.960 --> 00:10:18.140
There's a function with many variables

00:10:18.140 --> 00:10:19.880
and we need to just figure out the

00:10:19.880 --> 00:10:23.300
values of these variables to make the

00:10:23.300 --> 00:10:24.810
function which its maximum.


