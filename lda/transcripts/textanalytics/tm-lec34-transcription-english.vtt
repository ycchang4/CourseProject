WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-26T23:59:32.4422116Z by ClassTranscribe

00:00:00.300 --> 00:00:02.600
This lecture is about the similarity

00:00:02.600 --> 00:00:04.160
based approaches to text for

00:00:04.160 --> 00:00:04.740
clustering.

00:00:13.160 --> 00:00:14.400
In this lecture, we're going to

00:00:14.400 --> 00:00:16.760
continue the discussion of how to do a

00:00:16.760 --> 00:00:17.720
text clustering.

00:00:18.660 --> 00:00:20.680
In particular, we're going to cover a

00:00:20.680 --> 00:00:23.210
different kind of approaches than

00:00:23.210 --> 00:00:24.680
generative models.

00:00:25.380 --> 00:00:27.159
And that is similarity based

00:00:27.160 --> 00:00:28.000
approaches.

00:00:28.000 --> 00:00:31.344
So the general idea of similarity based

00:00:31.344 --> 00:00:35.136
clustering is to explicitly specify a

00:00:35.136 --> 00:00:37.485
similarity function to measure the

00:00:37.485 --> 00:00:40.110
similarity between 2:00 text objects.

00:00:40.110 --> 00:00:43.040
Now this is in contrast with a

00:00:43.040 --> 00:00:45.490
generative model where we implicitly

00:00:45.490 --> 00:00:47.190
define the clustering bias.

00:00:47.770 --> 00:00:49.675
By using a particular objective

00:00:49.675 --> 00:00:51.560
function like a likelihood function.

00:00:52.610 --> 00:00:54.370
The whole process is driven by

00:00:54.370 --> 00:00:56.750
optimizing the likeable, but here we

00:00:56.750 --> 00:01:00.110
explicitly provide a review of what we

00:01:00.110 --> 00:01:03.240
think are similar, and this is often

00:01:03.240 --> 00:01:05.960
very useful because then it allows us

00:01:05.960 --> 00:01:09.642
to inject any particular view of

00:01:09.642 --> 00:01:12.100
similarity into the clustering program.

00:01:12.100 --> 00:01:14.060
So once we have a similarity function,

00:01:14.060 --> 00:01:17.600
we can then aim at optimally

00:01:17.600 --> 00:01:20.490
partitioning to partitioning the data

00:01:20.490 --> 00:01:23.090
into clusters or into different groups.

00:01:23.830 --> 00:01:27.440
Anne, try to maximize the intragroup

00:01:27.440 --> 00:01:30.640
similarity and minimize the intergroup

00:01:30.640 --> 00:01:31.205
similarity.

00:01:31.205 --> 00:01:33.464
That is, to ensure the objects that are

00:01:33.464 --> 00:01:36.080
put in the same group to be similar,

00:01:36.080 --> 00:01:38.820
but the objects that are put into

00:01:38.820 --> 00:01:41.900
different groups to be not similar, and

00:01:41.900 --> 00:01:44.020
these are the general goals of

00:01:44.020 --> 00:01:44.575
clustering.

00:01:44.575 --> 00:01:47.200
And there's often a tradeoff between

00:01:47.200 --> 00:01:49.780
achieving both goals.

00:01:49.780 --> 00:01:51.720
Now, there are many different methods

00:01:51.720 --> 00:01:54.340
for doing similarity based clustering.

00:01:54.390 --> 00:01:56.870
In general, I think we can distinguish

00:01:56.870 --> 00:01:58.710
two strategies at high level.

00:01:58.710 --> 00:02:02.520
One is to progressively construct the

00:02:02.520 --> 00:02:03.870
hierarchy of clusters.

00:02:04.470 --> 00:02:06.620
And so this often leads to hierarchical

00:02:06.620 --> 00:02:10.060
clustering an we can further

00:02:10.060 --> 00:02:12.210
distinguishes two ways to construct the

00:02:12.210 --> 00:02:14.450
hierarchy depending on whether we

00:02:14.450 --> 00:02:16.700
started with the collection to divide

00:02:16.700 --> 00:02:19.580
the collection or start with individual

00:02:19.580 --> 00:02:22.180
objects and gradually group them

00:02:22.180 --> 00:02:22.575
together.

00:02:22.575 --> 00:02:24.960
So one is bottom up that can be called

00:02:24.960 --> 00:02:27.950
agglomerative, where we gradually Group

00:02:27.950 --> 00:02:29.910
A similar object into larger and larger

00:02:29.910 --> 00:02:32.760
clusters until we group everything

00:02:32.760 --> 00:02:33.240
together.

00:02:33.830 --> 00:02:35.950
The other is top down or divisive.

00:02:35.950 --> 00:02:37.925
In this case we gradually partitioning

00:02:37.925 --> 00:02:40.425
the whole data set into smaller and

00:02:40.425 --> 00:02:41.430
smaller clusters.

00:02:41.430 --> 00:02:44.160
The other general strategy is to start

00:02:44.160 --> 00:02:45.953
with the initial tentative clustering

00:02:45.953 --> 00:02:48.660
and then iteratively improve it and

00:02:48.660 --> 00:02:51.440
this often leads to a flat clustering.

00:02:51.440 --> 00:02:53.950
One example is K means.

00:02:54.610 --> 00:02:56.360
So as I just said, there are many

00:02:56.360 --> 00:02:58.580
different clustering methods available

00:02:58.580 --> 00:02:59.210
and.

00:03:00.390 --> 00:03:02.520
A full coverage of all these custom

00:03:02.520 --> 00:03:05.170
methods would be beyond the scope of

00:03:05.170 --> 00:03:05.950
this course.

00:03:06.740 --> 00:03:08.570
But here we can talk about the two

00:03:08.570 --> 00:03:11.080
representative methods and.

00:03:12.120 --> 00:03:13.330
In some detail.

00:03:13.880 --> 00:03:16.184
One is hierarchical agglomerative

00:03:16.184 --> 00:03:19.535
clustering or agency, the other is

00:03:19.535 --> 00:03:20.260
K-MEANS.

00:03:20.260 --> 00:03:22.102
So first let's look at the

00:03:22.102 --> 00:03:23.880
agglomerative hierarchical clustering.

00:03:24.670 --> 00:03:27.120
In this case, we are giving a

00:03:27.120 --> 00:03:29.120
similarity function calls to measure

00:03:29.120 --> 00:03:31.020
similarity between two objects and then

00:03:31.020 --> 00:03:33.250
we can gradually group similar objects

00:03:33.250 --> 00:03:35.850
together in a bottom up profession to

00:03:35.850 --> 00:03:38.940
form larger and larger groups, and they

00:03:38.940 --> 00:03:40.910
also form a hierarchy and then we can

00:03:40.910 --> 00:03:43.000
stop when some stopping criterions

00:03:43.000 --> 00:03:43.390
that.

00:03:44.070 --> 00:03:45.750
I could be either some number of

00:03:45.750 --> 00:03:48.900
classes has been achieved, or the

00:03:48.900 --> 00:03:50.669
threshold for similarity has been

00:03:50.670 --> 00:03:51.160
reached.

00:03:52.020 --> 00:03:54.383
There are different variations here and

00:03:54.383 --> 00:03:56.800
there mainly differ in the ways to

00:03:56.800 --> 00:03:59.160
computer group similarity based on the

00:03:59.160 --> 00:04:00.930
individual object similarity.

00:04:00.930 --> 00:04:04.060
So let's illustrate how can induce a

00:04:04.060 --> 00:04:06.846
structure based on just similarity.

00:04:06.846 --> 00:04:10.512
So start with all the text objects and

00:04:10.512 --> 00:04:13.510
we can then measure the similarity

00:04:13.510 --> 00:04:14.060
between them.

00:04:14.060 --> 00:04:16.370
Of course based on the provider

00:04:16.370 --> 00:04:18.690
similarity function and then we can see

00:04:18.690 --> 00:04:20.193
which pair has the highest similarity

00:04:20.193 --> 00:04:21.920
and then just group them together.

00:04:22.760 --> 00:04:25.960
And then was going to see which pair

00:04:25.960 --> 00:04:26.750
is.

00:04:28.170 --> 00:04:29.960
The next one to group.

00:04:30.630 --> 00:04:32.810
Maybe these two now have the highest

00:04:32.810 --> 00:04:33.550
similarity.

00:04:34.610 --> 00:04:36.520
And then we can gradually group them

00:04:36.520 --> 00:04:40.150
together in every time we're going to

00:04:40.150 --> 00:04:42.990
pick the highest similarity similarity

00:04:42.990 --> 00:04:44.030
pairs to group.

00:04:44.870 --> 00:04:46.400
This will give us a binary tree

00:04:46.400 --> 00:04:48.340
eventually to group everything

00:04:48.340 --> 00:04:48.880
together.

00:04:50.200 --> 00:04:53.420
Now depending our applications, we can

00:04:53.420 --> 00:04:55.580
use the whole hierarchy as structure

00:04:55.580 --> 00:04:58.189
for browsing for example, or we can

00:04:58.190 --> 00:05:00.350
choose the cut off at say come here to

00:05:00.350 --> 00:05:01.570
get four clusters.

00:05:02.860 --> 00:05:06.195
Or we can use the threshold to cut or

00:05:06.195 --> 00:05:08.790
we can cut at this high level to get

00:05:08.790 --> 00:05:10.070
just the two clusters.

00:05:10.660 --> 00:05:12.150
So this is a general idea.

00:05:13.140 --> 00:05:14.520
Now, if you think about how to

00:05:14.520 --> 00:05:16.130
implement this algorithm, you will

00:05:16.130 --> 00:05:18.920
realize that we have everything

00:05:18.920 --> 00:05:22.570
specified except for how to compute the

00:05:22.570 --> 00:05:23.830
group similarity.

00:05:23.830 --> 00:05:26.290
We are only given the similarity

00:05:26.290 --> 00:05:28.550
function or two objects, but as we

00:05:28.550 --> 00:05:30.870
group groups together we also need to

00:05:30.870 --> 00:05:32.565
assess the similarity between two

00:05:32.565 --> 00:05:32.980
groups.

00:05:33.730 --> 00:05:35.700
And there are also different ways to do

00:05:35.700 --> 00:05:37.630
that, and there's the three popular

00:05:37.630 --> 00:05:41.490
methods are single link complete link

00:05:41.490 --> 00:05:42.420
an average link?

00:05:43.210 --> 00:05:46.650
So given two groups and singling

00:05:46.650 --> 00:05:48.720
algorithm is going to define the group

00:05:48.720 --> 00:05:50.769
similarity as the similarity of the

00:05:50.770 --> 00:05:52.920
closest repair of the two groups.

00:05:52.920 --> 00:05:55.284
Complete Link defines the similarity of

00:05:55.284 --> 00:05:57.230
two groups as the similarity of the

00:05:57.230 --> 00:05:58.980
father sister pair.

00:05:59.620 --> 00:06:02.030
Average link defines the similarity as

00:06:02.030 --> 00:06:04.500
average of similarity of all the pairs

00:06:04.500 --> 00:06:05.660
of the two groups.

00:06:06.370 --> 00:06:09.500
So it's much easier to understand these

00:06:09.500 --> 00:06:11.765
methods by illustrating them.

00:06:11.765 --> 00:06:15.100
So here are two groups G1 and G2 with

00:06:15.100 --> 00:06:18.620
some objects in each group, and we know

00:06:18.620 --> 00:06:21.882
how to compute the similarity between

00:06:21.882 --> 00:06:22.970
two objects.

00:06:22.970 --> 00:06:25.280
But the question now is, how can we

00:06:25.280 --> 00:06:27.140
compute the similarity between the two

00:06:27.140 --> 00:06:27.450
groups?

00:06:28.370 --> 00:06:30.650
And then we can in general basis on the

00:06:30.650 --> 00:06:32.780
similarities of the objects in the two

00:06:32.780 --> 00:06:33.350
groups.

00:06:35.000 --> 00:06:38.300
So in terms of single link and we're

00:06:38.300 --> 00:06:39.973
just looking at the closest pair.

00:06:39.973 --> 00:06:43.140
So in this case these two pairs objects

00:06:43.140 --> 00:06:45.149
would define the similarity of the two

00:06:45.150 --> 00:06:45.740
groups.

00:06:47.170 --> 00:06:48.990
As long as they are very close orders,

00:06:48.990 --> 00:06:50.480
say the two groups are very.

00:06:51.630 --> 00:06:55.520
Close so it's optimistic view of

00:06:55.520 --> 00:06:56.260
similarity.

00:06:57.800 --> 00:06:59.800
The complete link, on the other hand,

00:06:59.800 --> 00:07:03.450
will be in some sense pessimistic and

00:07:03.450 --> 00:07:07.100
by taking the similarity of the two

00:07:07.100 --> 00:07:10.020
farthest appear as the similarity for

00:07:10.020 --> 00:07:11.070
the two groups.

00:07:12.330 --> 00:07:15.590
So we're going to make sure that if the

00:07:15.590 --> 00:07:16.720
two groups areÂ 

00:07:18.840 --> 00:07:21.620
having a high similarity, then every

00:07:21.620 --> 00:07:24.520
pair of the two the objects in the

00:07:24.520 --> 00:07:27.653
group will be insured to have

00:07:27.653 --> 00:07:28.299
high similarity.

00:07:29.240 --> 00:07:31.990
Every link is in between, so it takes

00:07:31.990 --> 00:07:33.960
average of all these pairs.

00:07:34.640 --> 00:07:37.790
Now, these different ways of computing

00:07:37.790 --> 00:07:39.605
group similarities will need to

00:07:39.605 --> 00:07:41.040
different clustering algorithms, and

00:07:41.040 --> 00:07:42.700
they will generally give different

00:07:42.700 --> 00:07:43.420
results.

00:07:45.220 --> 00:07:48.520
Now, so it's useful to take a look at

00:07:48.520 --> 00:07:50.530
their differences and to make

00:07:50.530 --> 00:07:51.230
comparison.

00:07:53.520 --> 00:07:55.790
Our first single link.

00:07:56.640 --> 00:07:59.080
Can be expected to generate the loose

00:07:59.080 --> 00:07:59.720
clusters.

00:07:59.720 --> 00:08:01.570
The reason is becausw.

00:08:01.570 --> 00:08:04.800
As long as two objects are very similar

00:08:04.800 --> 00:08:06.983
in the two groups, it would bring the

00:08:06.983 --> 00:08:07.930
two groups together.

00:08:09.130 --> 00:08:13.320
If you think about this is similar to

00:08:13.320 --> 00:08:15.880
having parties with people, then it

00:08:15.880 --> 00:08:19.287
just means two groups of two groups of

00:08:19.287 --> 00:08:22.150
people would be putting together as

00:08:22.150 --> 00:08:25.350
long as each group there is a person

00:08:25.350 --> 00:08:29.272
that is well connected with the other

00:08:29.272 --> 00:08:29.720
group.

00:08:29.720 --> 00:08:33.140
So the two leaders of the two groups

00:08:33.140 --> 00:08:36.387
can have a good relationship with each

00:08:36.387 --> 00:08:38.292
other and then they will bring together

00:08:38.292 --> 00:08:39.410
the two groups.

00:08:39.560 --> 00:08:41.290
In this case, the cluster is rules

00:08:41.290 --> 00:08:43.700
because there's no guarantee that other

00:08:43.700 --> 00:08:45.890
members of the two groups are actually

00:08:45.890 --> 00:08:47.060
very close to each other.

00:08:47.060 --> 00:08:50.120
Sometimes they may be very far away.

00:08:50.120 --> 00:08:52.240
Now in this case it's also based on

00:08:52.240 --> 00:08:53.740
individual decision, so it could be

00:08:53.740 --> 00:08:55.150
sensitive to outliers.

00:08:56.030 --> 00:08:59.340
The complete linker is in the opposite

00:08:59.340 --> 00:09:01.250
situation where we can expect the

00:09:01.250 --> 00:09:02.490
clusters to be tight.

00:09:03.270 --> 00:09:05.870
Anne, it's also based on individual

00:09:05.870 --> 00:09:07.530
decision, so it can be sensitive to

00:09:07.530 --> 00:09:08.290
outliers.

00:09:08.290 --> 00:09:10.700
Again, to continue the analogy to

00:09:10.700 --> 00:09:13.300
having a party of people then complete

00:09:13.300 --> 00:09:16.290
the link would mean when two groups

00:09:16.290 --> 00:09:18.750
come together they want to ensure that

00:09:18.750 --> 00:09:19.610
even the.

00:09:21.240 --> 00:09:24.880
Even the people that are unlikely to

00:09:24.880 --> 00:09:26.900
talk to each other would be comfortable

00:09:26.900 --> 00:09:29.325
with talking to each other, so ensure

00:09:29.325 --> 00:09:31.140
the whole class to be coherent.

00:09:31.820 --> 00:09:33.420
The average link, of course is in

00:09:33.420 --> 00:09:37.600
between and group decision, so it's

00:09:37.600 --> 00:09:40.010
going to be insensitive to outliers.

00:09:40.620 --> 00:09:42.870
In practice, which one is the best?

00:09:42.870 --> 00:09:44.780
Well, this will depend on the

00:09:44.780 --> 00:09:46.690
application and sometimes you need a

00:09:46.690 --> 00:09:49.760
loose classes and to aggressively on

00:09:49.760 --> 00:09:50.970
cluster objects together.

00:09:50.970 --> 00:09:52.410
Then maybe simple English good.

00:09:52.960 --> 00:09:56.260
But other times you might need a tight

00:09:56.260 --> 00:09:58.196
clusters, then completely completely

00:09:58.196 --> 00:10:00.140
better, but in general you have to

00:10:00.140 --> 00:10:02.140
empirically evaluate these methods for

00:10:02.140 --> 00:10:04.270
your application to know which one is

00:10:04.270 --> 00:10:04.540
better.

00:10:06.200 --> 00:10:08.730
Now let's look at another example of

00:10:08.730 --> 00:10:10.990
method for similarity based classroom

00:10:10.990 --> 00:10:11.950
in this case.

00:10:14.470 --> 00:10:17.050
Which is called K means clustering will

00:10:17.050 --> 00:10:19.440
represent each text object as a term

00:10:19.440 --> 00:10:22.270
vector and then assuming similarity

00:10:22.270 --> 00:10:24.050
function defined onto objects.

00:10:25.610 --> 00:10:27.310
Now we're going to start with some

00:10:27.310 --> 00:10:30.230
tentative clustering result by just

00:10:30.230 --> 00:10:32.630
selecting Kate randomly selected

00:10:32.630 --> 00:10:37.140
vectors as centroids of K clusters and

00:10:37.140 --> 00:10:40.080
treat them as sentence as they

00:10:40.080 --> 00:10:42.620
represent each cluster.

00:10:43.290 --> 00:10:44.010
So this is.

00:10:44.010 --> 00:10:45.860
This gives us the initial tentative

00:10:45.860 --> 00:10:46.390
classroom.

00:10:47.040 --> 00:10:48.390
And then we're going to iteratively

00:10:48.390 --> 00:10:50.590
improve it, and the process goes like

00:10:50.590 --> 00:10:51.185
this.

00:10:51.185 --> 00:10:53.180
And once we have these Central

00:10:53.180 --> 00:10:55.572
Eastside, we're going to assign a

00:10:55.572 --> 00:10:59.690
vector to the cluster hosts entry that

00:10:59.690 --> 00:11:03.470
is closest to the current vector.

00:11:03.470 --> 00:11:05.080
So basically we're going to measure the

00:11:05.080 --> 00:11:07.450
distance between this vector and each

00:11:07.450 --> 00:11:10.203
of the centroids, and see which one is

00:11:10.203 --> 00:11:12.935
closest to this one, and then just put

00:11:12.935 --> 00:11:16.090
this class this object into that

00:11:16.090 --> 00:11:16.660
cluster.

00:11:16.660 --> 00:11:18.600
Now this is to have tentative.

00:11:18.660 --> 00:11:20.950
Assignment of objects into clusters and

00:11:20.950 --> 00:11:23.365
we're going to partition or the objects

00:11:23.365 --> 00:11:26.040
into K clusters based on our tentative

00:11:26.040 --> 00:11:27.550
clustering centroids.

00:11:28.510 --> 00:11:30.600
And then we're going to recovery,

00:11:30.600 --> 00:11:32.760
compute the centroid based on the

00:11:32.760 --> 00:11:34.810
allocated objects in each cluster.

00:11:35.460 --> 00:11:36.600
And this is.

00:11:37.200 --> 00:11:40.270
To adjust the centroid and then we had

00:11:40.270 --> 00:11:41.980
repeated this process until the

00:11:41.980 --> 00:11:44.190
similarity based on objective function.

00:11:44.190 --> 00:11:47.325
In this case it's within cluster sum of

00:11:47.325 --> 00:11:50.520
squares converges an theoretically we

00:11:50.520 --> 00:11:53.640
can show that this process actually is

00:11:53.640 --> 00:11:56.029
going to minimize the within cluster

00:11:56.029 --> 00:11:58.676
sum of squares where define objective

00:11:58.676 --> 00:11:59.109
function.

00:11:59.700 --> 00:12:01.150
Given K clusters.

00:12:01.980 --> 00:12:04.439
So it can be also shown this process

00:12:04.440 --> 00:12:07.260
will converge to a local minimum.

00:12:07.260 --> 00:12:09.040
I think about this process for a

00:12:09.040 --> 00:12:09.350
moment.

00:12:09.350 --> 00:12:11.910
It might remind you the EM algorithm

00:12:11.910 --> 00:12:12.990
for mixture model.

00:12:13.780 --> 00:12:16.110
Indeed, this algorithm is very similar

00:12:16.110 --> 00:12:19.060
to the EM algorithm for the mixture

00:12:19.060 --> 00:12:20.230
model for clustering.

00:12:20.230 --> 00:12:22.260
So more specifically, we also

00:12:22.260 --> 00:12:23.790
initialize these.

00:12:24.830 --> 00:12:28.340
Predators in the EM algorithm, so the

00:12:28.340 --> 00:12:31.750
random inner inner initialization is

00:12:31.750 --> 00:12:32.250
similar.

00:12:34.080 --> 00:12:36.080
And then in the EML with them, you may

00:12:36.080 --> 00:12:37.970
recall that we're going to repeat

00:12:37.970 --> 00:12:41.900
eastep and M step to improved our

00:12:41.900 --> 00:12:42.865
primary destinations.

00:12:42.865 --> 00:12:45.340
In this case, we're going to improve

00:12:45.340 --> 00:12:48.660
the clustering result iteratively by

00:12:48.660 --> 00:12:52.163
also doing 2 steps, and in fact the two

00:12:52.163 --> 00:12:55.070
steps are very similar to EM algorithm.

00:12:55.630 --> 00:12:59.620
In that when we allocate vector into

00:12:59.620 --> 00:13:01.700
one of the clusters based on our

00:13:01.700 --> 00:13:04.175
tentative clustering, it's very similar

00:13:04.175 --> 00:13:06.480
to inferring the distribution that has

00:13:06.480 --> 00:13:07.920
been used with generally the document

00:13:07.920 --> 00:13:08.830
in the mixture model.

00:13:08.830 --> 00:13:11.280
So it's essentially similar to eastep.

00:13:11.280 --> 00:13:12.970
Also, what's the difference?

00:13:12.970 --> 00:13:16.720
While the differences here, we don't

00:13:16.720 --> 00:13:18.910
make a probabilistic on location as in

00:13:18.910 --> 00:13:20.130
the case of the step.

00:13:20.820 --> 00:13:22.167
But rather we make a choice.

00:13:22.167 --> 00:13:25.960
We're going to make a call if this data

00:13:25.960 --> 00:13:28.910
point is closest to cluster two that

00:13:28.910 --> 00:13:31.090
were going to say you are in class too.

00:13:31.090 --> 00:13:32.670
So there's no choice, and we're not

00:13:32.670 --> 00:13:35.423
going to say you are 70% belonging to

00:13:35.423 --> 00:13:36.049
class too.

00:13:37.080 --> 00:13:38.860
And so we're not going to have a

00:13:38.860 --> 00:13:40.360
probability, but we're going to just

00:13:40.360 --> 00:13:44.120
put one object into precisely one

00:13:44.120 --> 00:13:44.610
cluster.

00:13:45.230 --> 00:13:47.430
In the E step, however, we do a

00:13:47.430 --> 00:13:49.640
probabilistic location, so we split the

00:13:49.640 --> 00:13:50.200
counts.

00:13:51.000 --> 00:13:52.830
And we're not going to say exactly

00:13:52.830 --> 00:13:56.460
which distribution has been used to

00:13:56.460 --> 00:13:58.950
generate the data point.

00:13:58.950 --> 00:14:01.500
Now the next we're going to adjust the

00:14:01.500 --> 00:14:03.690
centroid, and this is very similar to M

00:14:03.690 --> 00:14:05.320
step where we re estimate the

00:14:05.320 --> 00:14:05.850
parameters.

00:14:05.850 --> 00:14:08.167
That's when we'll have a better

00:14:08.167 --> 00:14:10.020
estimate of the parameter.

00:14:10.020 --> 00:14:12.430
So here we have a better clustering

00:14:12.430 --> 00:14:15.142
result by adjusting the centroid.

00:14:15.142 --> 00:14:17.430
And note that the central is adjusted

00:14:17.430 --> 00:14:21.220
based on the average of the vectors in

00:14:21.220 --> 00:14:21.590
the.

00:14:21.890 --> 00:14:24.555
A cluster, so this is also similar to

00:14:24.555 --> 00:14:27.600
the M step, where we do counts pull

00:14:27.600 --> 00:14:29.960
together counter and normalize them, or

00:14:29.960 --> 00:14:31.270
the difference of course is also

00:14:31.270 --> 00:14:32.710
because of the difference in the

00:14:32.710 --> 00:14:35.730
instep, and we're not going to consider

00:14:35.730 --> 00:14:38.552
probabilities when we count the points

00:14:38.552 --> 00:14:41.554
in this case, for K means we're going

00:14:41.554 --> 00:14:44.108
to only count the objects allocated to

00:14:44.108 --> 00:14:47.040
this cluster, and this is only a subset

00:14:47.040 --> 00:14:48.840
of data points.

00:14:48.840 --> 00:14:50.689
But in the EM algorithm, we in

00:14:50.690 --> 00:14:52.769
principle consider all the data points.

00:14:53.240 --> 00:14:55.650
Based on probabilistic allocations.

00:14:56.640 --> 00:14:58.880
But in nature they are very similar and

00:14:58.880 --> 00:15:02.060
that's why it's also maximizing where

00:15:02.060 --> 00:15:03.499
defined objective function and it's

00:15:03.500 --> 00:15:06.390
guaranteed to convert converted local

00:15:06.390 --> 00:15:07.340
minimum.

00:15:07.340 --> 00:15:09.040
So to summarize our discussion of

00:15:09.040 --> 00:15:11.573
clustering methods, we first discussed

00:15:11.573 --> 00:15:13.440
the model based approaches, mainly the

00:15:13.440 --> 00:15:14.220
mixture model.

00:15:14.220 --> 00:15:16.070
And here we use is implicitly

00:15:16.070 --> 00:15:17.260
similarity function.

00:15:17.840 --> 00:15:22.570
To define the clustering bias, there's

00:15:22.570 --> 00:15:24.270
no explicit definer similarity

00:15:24.270 --> 00:15:24.765
function.

00:15:24.765 --> 00:15:27.439
The model defines clustering bias.

00:15:28.710 --> 00:15:30.950
And the clustering structure is built

00:15:30.950 --> 00:15:33.020
into a generated model.

00:15:33.020 --> 00:15:35.815
That's why we can use potentially a

00:15:35.815 --> 00:15:37.870
different model to recover different

00:15:37.870 --> 00:15:38.500
instruction.

00:15:39.600 --> 00:15:42.855
Complex generative models can be used

00:15:42.855 --> 00:15:45.520
to discover complex clustering

00:15:45.520 --> 00:15:46.250
structures.

00:15:46.250 --> 00:15:49.250
We did not talk about it, but we can

00:15:49.250 --> 00:15:51.480
easily design generated model to

00:15:51.480 --> 00:15:53.510
generate a hierarchical clusters.

00:15:54.690 --> 00:15:57.680
We can also use prior to further

00:15:57.680 --> 00:15:59.750
customize clustering algorithm to for

00:15:59.750 --> 00:16:02.910
example, control the topic of 1 cluster

00:16:02.910 --> 00:16:04.160
or multiple clusters.

00:16:04.830 --> 00:16:06.690
However, one disadvantage of this

00:16:06.690 --> 00:16:08.400
approach is that there is no easy way

00:16:08.400 --> 00:16:10.220
to direct or control the similarity

00:16:10.220 --> 00:16:10.700
measure.

00:16:11.570 --> 00:16:13.670
Sometimes we want to do that, but it's

00:16:13.670 --> 00:16:16.980
very hard to inject the such a explicit

00:16:16.980 --> 00:16:18.710
definition of similarity into such a

00:16:18.710 --> 00:16:19.190
model.

00:16:20.130 --> 00:16:22.030
We also talked about the similarity

00:16:22.030 --> 00:16:22.810
based approaches.

00:16:22.810 --> 00:16:24.640
These approaches are more flexible.

00:16:24.640 --> 00:16:27.820
Directly specify similarity functions.

00:16:28.800 --> 00:16:32.040
But one potential disadvantage is that

00:16:32.040 --> 00:16:34.200
their object function is not always

00:16:34.200 --> 00:16:35.020
very clear.

00:16:35.020 --> 00:16:37.870
The K means algorithm has a clearly

00:16:37.870 --> 00:16:39.740
defined the objective function, but

00:16:39.740 --> 00:16:41.620
it's also very similar to a model based

00:16:41.620 --> 00:16:42.210
approach.

00:16:42.890 --> 00:16:45.620
The hierarchical clustering algorithm,

00:16:45.620 --> 00:16:47.170
on the other hand, is.

00:16:49.050 --> 00:16:50.580
It's harder to.

00:16:51.270 --> 00:16:55.670
To specify the objective function so

00:16:55.670 --> 00:16:58.740
it's not clear what exactly is being

00:16:58.740 --> 00:16:59.600
optimized.

00:17:00.720 --> 00:17:03.162
Both approaches can and generate the

00:17:03.162 --> 00:17:05.680
term clusters and document clusters.

00:17:05.680 --> 00:17:08.480
An term clusters can be in general

00:17:08.480 --> 00:17:11.069
generated by representing each term

00:17:11.070 --> 00:17:13.020
with some text content.

00:17:13.020 --> 00:17:15.960
For example, take the context of each

00:17:15.960 --> 00:17:18.203
term as a representation of each term

00:17:18.203 --> 00:17:20.640
as we have done in paradigmatic

00:17:20.640 --> 00:17:22.010
relation learning.

00:17:22.010 --> 00:17:25.200
And then we can certainly cluster terms

00:17:25.200 --> 00:17:27.500
based on actually their tax

00:17:27.500 --> 00:17:28.440
representations.

00:17:28.990 --> 00:17:30.380
Of course, term clusters can be

00:17:30.380 --> 00:17:32.790
generated by using generative models as

00:17:32.790 --> 00:17:34.150
well as we have seen.


