WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-26T23:59:38.2429037Z by ClassTranscribe

00:00:00.300 --> 00:00:02.820
So this is indeed a general idea of the

00:00:02.820 --> 00:00:05.720
expectation maximization, or EM algorithm.

00:00:14.530 --> 00:00:17.300
So in all the EM algorithms, we

00:00:17.300 --> 00:00:19.860
introduce a hidden variable to help us

00:00:19.860 --> 00:00:21.460
solve the problem more easily.

00:00:21.460 --> 00:00:23.610
In our case, the hidden variable is a

00:00:23.610 --> 00:00:26.060
binary variable for each occurrence of

00:00:26.060 --> 00:00:26.470
word.

00:00:27.270 --> 00:00:29.800
And this binary variable would indicate

00:00:29.800 --> 00:00:31.650
whether the world has been generated

00:00:31.650 --> 00:00:33.950
from theta on Sunday or theater Super

00:00:33.950 --> 00:00:34.380
B.

00:00:35.140 --> 00:00:37.380
And here we show some possible values

00:00:37.380 --> 00:00:38.820
of these variables.

00:00:38.820 --> 00:00:40.580
For example for the it's from

00:00:40.580 --> 00:00:44.365
Background, Z value is 1 and text on

00:00:44.365 --> 00:00:47.830
the other hand is from the topic.

00:00:47.830 --> 00:00:52.290
Then it's 0 for Z etc.

00:00:53.150 --> 00:00:55.330
Now, of course we don't observe those Z

00:00:55.330 --> 00:00:55.770
values.

00:00:55.770 --> 00:00:58.900
We just imagine there are such a social

00:00:58.900 --> 00:01:01.950
values of Z attached to all the words.

00:01:02.810 --> 00:01:04.380
And that's why we call these hidden

00:01:04.380 --> 00:01:05.020
variables.

00:01:05.940 --> 00:01:07.590
Now the idea that we talked about

00:01:07.590 --> 00:01:10.350
before for predicting the word

00:01:10.350 --> 00:01:11.810
distribution that has been used with

00:01:11.810 --> 00:01:14.050
the general the world is it'll predict

00:01:14.050 --> 00:01:14.480
this.

00:01:16.360 --> 00:01:18.040
The value of this hidden variable.

00:01:18.740 --> 00:01:19.230
And

00:01:19.870 --> 00:01:20.480
So.

00:01:21.170 --> 00:01:23.680
The algorithm, the EM algorithm then

00:01:23.680 --> 00:01:24.950
would work as follows.

00:01:24.950 --> 00:01:27.635
First will initialize all the

00:01:27.635 --> 00:01:29.800
parameters with random values.

00:01:29.800 --> 00:01:33.485
In our case the parameters are mainly

00:01:33.485 --> 00:01:36.320
the probability of a word given by

00:01:36.320 --> 00:01:37.090
status update.

00:01:37.750 --> 00:01:39.590
So this is the initialization stage.

00:01:39.590 --> 00:01:41.780
It is initialized values would allow us

00:01:41.780 --> 00:01:44.420
to use Bayes rule to take a guess of

00:01:44.420 --> 00:01:45.575
these Z values.

00:01:45.575 --> 00:01:49.460
So will guess these values we can say

00:01:49.460 --> 00:01:52.340
for sure whether taxes from background

00:01:52.340 --> 00:01:55.020
or not, but we can have our guesses.

00:01:55.020 --> 00:01:57.480
This is given by this formula.

00:01:57.480 --> 00:01:58.900
It's called E-step.

00:01:59.580 --> 00:02:02.620
And so the algorithm would then try to

00:02:02.620 --> 00:02:04.760
use the E Step 2 gas.

00:02:04.760 --> 00:02:06.100
These Z values.

00:02:06.100 --> 00:02:08.960
After that it would then invoke another

00:02:08.960 --> 00:02:11.100
spec step called M-step.

00:02:12.070 --> 00:02:14.620
In this step we simply take advantage

00:02:14.620 --> 00:02:17.930
of the inferred values and then just

00:02:17.930 --> 00:02:20.460
group words that are in the same

00:02:20.460 --> 00:02:24.250
distribution like this from background,

00:02:24.250 --> 00:02:26.520
including this as well.

00:02:27.180 --> 00:02:30.973
We can then normalize the count to estimate

00:02:30.973 --> 00:02:34.105
the probabilities or to revise our

00:02:34.105 --> 00:02:35.490
estimate of the parameters.

00:02:36.480 --> 00:02:41.220
So let me also illustrate we can group

00:02:41.220 --> 00:02:44.840
the words that are believed to have

00:02:44.840 --> 00:02:47.440
come from Cedar sub D and as text

00:02:47.440 --> 00:02:49.540
mining algorithm for example and

00:02:49.540 --> 00:02:50.180
clustering.

00:02:51.040 --> 00:02:52.790
And we had group them together.

00:02:54.160 --> 00:02:57.730
To help us re estimate the parameters.

00:02:59.150 --> 00:03:01.929
That were interested in so these will

00:03:01.930 --> 00:03:05.230
help us re estimate these parameters.

00:03:05.230 --> 00:03:07.890
But note that before we just set these

00:03:07.890 --> 00:03:10.400
parameter values randomly, But with

00:03:10.400 --> 00:03:13.120
this guess we will have a somewhat

00:03:13.120 --> 00:03:14.700
improved estimate of this.

00:03:15.550 --> 00:03:17.460
Of course, we don't know exactly

00:03:17.460 --> 00:03:19.960
whether it's zero or one, so we're not

00:03:19.960 --> 00:03:23.740
going to really do the split in

00:03:23.740 --> 00:03:25.860
hardware, but rather we can do those

00:03:25.860 --> 00:03:27.750
soft split and this is what happened

00:03:27.750 --> 00:03:28.100
here.

00:03:28.930 --> 00:03:30.900
So we're going to adjust the count.

00:03:31.720 --> 00:03:34.630
By the probability that we believe this

00:03:34.630 --> 00:03:37.780
would has been generated by using the

00:03:37.780 --> 00:03:38.630
theta sub d.

00:03:39.730 --> 00:03:41.080
And you can see this.

00:03:41.080 --> 00:03:42.530
Where does this come from?

00:03:42.530 --> 00:03:46.122
Well, this has come from here right

00:03:46.122 --> 00:03:47.930
from the E step.

00:03:47.930 --> 00:03:50.800
So the EM algorithm with iteratively

00:03:50.800 --> 00:03:53.210
improve our initial estimate of

00:03:53.210 --> 00:03:56.030
parameters by using E-step first and

00:03:56.030 --> 00:03:57.276
then M step.

00:03:57.276 --> 00:04:00.900
the E step is to augment the data with

00:04:00.900 --> 00:04:02.710
additional information like Z.

00:04:03.320 --> 00:04:06.130
And the M step is to take advantage of

00:04:06.130 --> 00:04:08.090
the additional information to separate

00:04:08.090 --> 00:04:10.840
the data to split the data accounts and

00:04:10.840 --> 00:04:13.220
then collect the right data counts.

00:04:15.080 --> 00:04:16.960
re estimate our parameters.

00:04:17.780 --> 00:04:19.698
And then once we have a new generation

00:04:19.698 --> 00:04:21.970
of parameters, we're going to repeat

00:04:21.970 --> 00:04:22.267
this.

00:04:22.267 --> 00:04:25.300
We're going to use the E-step again to

00:04:25.300 --> 00:04:27.290
improve our estimate of the hidden

00:04:27.290 --> 00:04:30.180
variables, and then that would lead to

00:04:30.180 --> 00:04:33.123
another generation of re estimate the

00:04:33.123 --> 00:04:33.629
parameters.

00:04:34.180 --> 00:04:37.070
For the word distribution that we're

00:04:37.070 --> 00:04:37.790
interested in.

00:04:39.500 --> 00:04:43.360
OK, so as I said, the bridge between

00:04:43.360 --> 00:04:47.670
the two is really variable Z hidden

00:04:47.670 --> 00:04:50.290
variable, which indicates how likely

00:04:50.290 --> 00:04:53.980
this world is from the topic word

00:04:53.980 --> 00:04:55.450
distributions theta sub d.

00:04:56.740 --> 00:04:59.820
So this slide has a lot of content and

00:04:59.820 --> 00:05:02.780
you may need to pause the video to

00:05:02.780 --> 00:05:05.125
digest it, but this basically captured

00:05:05.125 --> 00:05:07.210
the essence of EM algorithm.

00:05:07.210 --> 00:05:10.380
Start with initial values that are

00:05:10.380 --> 00:05:11.780
often randomly set.

00:05:12.360 --> 00:05:15.070
And then we invoke E step followed by M

00:05:15.070 --> 00:05:18.840
step to get an improved setting of

00:05:18.840 --> 00:05:21.020
parameters, and then we repeat this.

00:05:21.020 --> 00:05:23.250
So this is a hill climbing algorithm

00:05:23.250 --> 00:05:24.940
that would gradually improve the

00:05:24.940 --> 00:05:27.630
estimate of parameters and as I will

00:05:27.630 --> 00:05:29.600
explain later, there's some guarantee

00:05:29.600 --> 00:05:31.520
for reaching a local maximum

00:05:32.180 --> 00:05:33.570
of the likelihood function.

00:05:35.210 --> 00:05:37.910
So let's take a look at the computation

00:05:37.910 --> 00:05:39.640
for specific case.

00:05:39.640 --> 00:05:42.990
So these formulas are the EM formulas

00:05:42.990 --> 00:05:45.745
that you see before, and you can also

00:05:45.745 --> 00:05:50.570
see there are superscripts here N to

00:05:50.570 --> 00:05:52.950
indicate the generation of parameters.

00:05:53.610 --> 00:05:54.300
I go here.

00:05:54.300 --> 00:05:56.070
For example, we have N + 1.

00:05:56.070 --> 00:05:59.690
That means we have improved parameters

00:05:59.690 --> 00:06:01.160
from here to.

00:06:01.160 --> 00:06:03.160
here we have improvement.

00:06:04.120 --> 00:06:06.185
So in this setting we have assumed that

00:06:06.185 --> 00:06:08.200
the two models have equal probabilities

00:06:08.200 --> 00:06:09.605
and the background model is known.

00:06:09.605 --> 00:06:11.840
So what are the relevant statistics?

00:06:11.840 --> 00:06:13.460
Well, these are the word counts.

00:06:14.030 --> 00:06:16.610
So assume we have just 4 words and

00:06:16.610 --> 00:06:18.820
their counts are like this and this is

00:06:18.820 --> 00:06:21.560
our background model that assigns high

00:06:21.560 --> 00:06:23.610
probabilities to common words like the.

00:06:25.810 --> 00:06:28.760
An in the first iteration you can

00:06:28.760 --> 00:06:29.780
picture what would happen.

00:06:29.780 --> 00:06:31.820
Well, we first we initialize all

00:06:31.820 --> 00:06:32.320
the values.

00:06:32.320 --> 00:06:34.680
So here this probability that we're

00:06:34.680 --> 00:06:36.470
interested in is normalized into an

00:06:36.470 --> 00:06:38.530
uniform distribution over all the

00:06:38.530 --> 00:06:38.920
words.

00:06:39.580 --> 00:06:41.660
And then the E step would give us a

00:06:41.660 --> 00:06:42.140
guess.

00:06:43.320 --> 00:06:45.915
Of the distribution that has been used

00:06:45.915 --> 00:06:49.522
to generate each word, we can see we

00:06:49.522 --> 00:06:50.730
have different probabilities for

00:06:50.730 --> 00:06:51.420
different words.

00:06:51.420 --> 00:06:54.819
Why that's be cause these words have

00:06:54.819 --> 00:06:56.286
different probabilities in the

00:06:56.286 --> 00:06:56.653
background.

00:06:56.653 --> 00:06:58.480
So even though the two distributions

00:06:58.480 --> 00:07:01.420
are equally likely, and then our

00:07:01.420 --> 00:07:03.450
initialization says uniform

00:07:03.450 --> 00:07:05.230
distribution because of the difference

00:07:05.230 --> 00:07:06.667
in the background world distribution,

00:07:06.667 --> 00:07:09.096
we have different guest probabilities.

00:07:09.096 --> 00:07:12.530
So these words are believed to be more

00:07:12.530 --> 00:07:14.390
likely from the topic.

00:07:15.420 --> 00:07:17.050
These, on the other hand, are less

00:07:17.050 --> 00:07:18.930
likely probably from background.

00:07:20.140 --> 00:07:23.100
So once we have the Z values, we know

00:07:23.100 --> 00:07:25.730
in the E step these probabilities

00:07:25.730 --> 00:07:28.446
would be used to adjust the counts.

00:07:28.446 --> 00:07:32.327
So 4 must be multiplied by this point

00:07:32.327 --> 00:07:36.460
three three in order to get the allocated

00:07:36.460 --> 00:07:38.020
counts toward the topic.

00:07:39.430 --> 00:07:41.820
And this is done by this

00:07:41.820 --> 00:07:42.940
multiplication.

00:07:43.640 --> 00:07:47.801
Note that if our guess says this is 100%.

00:07:47.801 --> 00:07:49.949
If this is 1.0

00:07:52.220 --> 00:07:55.956
Then we just get the full Council of

00:07:55.956 --> 00:07:57.890
this word for this topic.

00:07:57.890 --> 00:07:59.926
But in general, as I said, it's not

00:07:59.926 --> 00:08:02.186
going to be 1.0, so we're going to just

00:08:02.186 --> 00:08:04.480
get some percentage of the counts

00:08:04.480 --> 00:08:07.410
toward this topic, and then we simply

00:08:07.410 --> 00:08:08.790
normalize these counts.

00:08:09.440 --> 00:08:12.500
To have a new generation of practice

00:08:12.500 --> 00:08:14.760
mate so you can see, compare this with

00:08:14.760 --> 00:08:16.770
the old one which is here.

00:08:17.350 --> 00:08:20.060
So compare this with this one and will

00:08:20.060 --> 00:08:22.190
see at the probability is different.

00:08:22.190 --> 00:08:25.285
Not only that, we also see some words

00:08:25.285 --> 00:08:27.380
that are believed to have come from the

00:08:27.380 --> 00:08:27.710
topic.

00:08:27.710 --> 00:08:30.160
We have high probability like this one

00:08:30.160 --> 00:08:30.620
text.

00:08:32.410 --> 00:08:34.280
And of course, this new generation of

00:08:34.280 --> 00:08:35.900
parameters would allow us to further

00:08:35.900 --> 00:08:41.120
adjust the infer the latent variable or

00:08:41.120 --> 00:08:42.519
hidden variable values.

00:08:42.520 --> 00:08:44.710
So we have a new generation of values

00:08:44.710 --> 00:08:48.260
because of the E step based on the new

00:08:48.260 --> 00:08:49.760
generation of parameters.

00:08:51.040 --> 00:08:54.740
And this these new in further values of

00:08:54.740 --> 00:08:57.050
these will give us then another

00:08:57.050 --> 00:09:00.870
generation of the estimate of

00:09:00.870 --> 00:09:02.410
probabilities of the words.

00:09:03.260 --> 00:09:04.630
And so on so forth.

00:09:04.630 --> 00:09:06.740
So this is what would actually happen

00:09:06.740 --> 00:09:09.370
when we compute these probabilities

00:09:09.370 --> 00:09:11.360
using the EM algorithm.

00:09:11.360 --> 00:09:14.510
And as you can see in the last rule where

00:09:14.510 --> 00:09:17.600
we showed the log like code and the

00:09:17.600 --> 00:09:19.660
likelihood is increasing as we do the

00:09:19.660 --> 00:09:20.250
iteration.

00:09:20.870 --> 00:09:23.175
And note that these log likelihood is

00:09:23.175 --> 00:09:25.800
negative becausw the probability is

00:09:25.800 --> 00:09:27.330
between zero and one when you take

00:09:27.330 --> 00:09:29.350
logarithm, it becomes a negative value.

00:09:29.940 --> 00:09:32.510
What's also interesting is do not last

00:09:32.510 --> 00:09:35.620
column, and these are the inferred word

00:09:35.620 --> 00:09:39.430
split, and these are the probabilities

00:09:39.430 --> 00:09:42.830
that a word is believed to have come

00:09:42.830 --> 00:09:44.446
from one distribution.

00:09:44.446 --> 00:09:47.105
In this case the topic distribution,

00:09:47.105 --> 00:09:49.680
and you might wonder whether this would

00:09:49.680 --> 00:09:51.663
be also useful because our main goal is

00:09:51.663 --> 00:09:55.250
to estimate these word distribution

00:09:55.250 --> 00:09:55.550
right?

00:09:55.550 --> 00:09:57.420
So this is our primary goal.

00:09:57.420 --> 00:09:59.420
We hope to have a more discriminating

00:09:59.420 --> 00:10:00.540
world distribution.

00:10:00.830 --> 00:10:03.843
But the last column is also by product

00:10:03.843 --> 00:10:06.190
and this actually can also be very

00:10:06.190 --> 00:10:08.200
useful and you can think about that.

00:10:08.200 --> 00:10:10.069
And one use is to.

00:10:10.070 --> 00:10:12.385
For example, is made to what extent

00:10:12.385 --> 00:10:14.330
this document has covered background

00:10:14.330 --> 00:10:14.730
words.

00:10:15.990 --> 00:10:18.540
And this when we add this up or take

00:10:18.540 --> 00:10:21.410
the average will kind of know to what

00:10:21.410 --> 00:10:24.220
extent it has covered background versus

00:10:24.220 --> 00:10:26.100
content words that are not explained

00:10:26.100 --> 00:10:27.290
well by the background.


