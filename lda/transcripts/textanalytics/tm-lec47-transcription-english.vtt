WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-26T23:59:32.2998270Z by ClassTranscribe

00:00:00.300 --> 00:00:02.550
This lecture is a continued discussion

00:00:02.550 --> 00:00:05.250
of latent aspect rating analysis.

00:00:13.130 --> 00:00:15.670
Earlier we talked about how to solve

00:00:15.670 --> 00:00:18.610
the problem of Lara in two stages when

00:00:18.610 --> 00:00:21.380
we first do segmentation of different

00:00:21.380 --> 00:00:23.580
aspects and then we use a little

00:00:23.580 --> 00:00:25.720
regression model to learn the aspect

00:00:25.720 --> 00:00:27.590
ratings and letting the weights.

00:00:28.370 --> 00:00:31.390
Now, it's also possible to develop a

00:00:31.390 --> 00:00:33.610
unified generative model for solving

00:00:33.610 --> 00:00:36.529
this problem, and that is we not only

00:00:36.530 --> 00:00:39.035
modeling, we not only model the

00:00:39.035 --> 00:00:41.042
generation of overrating based on text,

00:00:41.042 --> 00:00:43.861
we also model the generation of text

00:00:43.861 --> 00:00:46.290
and so a natural solution would be to

00:00:46.290 --> 00:00:47.376
use topic model.

00:00:47.376 --> 00:00:50.540
So given an entity, we can assume there

00:00:50.540 --> 00:00:53.090
are aspects that are described by word

00:00:53.090 --> 00:00:53.860
distributions.

00:00:54.410 --> 00:00:57.230
Topics and then we can use a topic

00:00:57.230 --> 00:00:59.580
model to model the generation of the

00:00:59.580 --> 00:01:00.510
review text.

00:01:01.490 --> 00:01:03.260
Our assumed the words in the review

00:01:03.260 --> 00:01:06.150
text are drawn from these

00:01:06.150 --> 00:01:07.200
distributions.

00:01:07.890 --> 00:01:10.530
In the same way as we assumed for a

00:01:10.530 --> 00:01:12.210
generative model like PSA.

00:01:13.490 --> 00:01:17.260
And then we can then plug in the latent

00:01:17.260 --> 00:01:20.030
regression model to use the text to

00:01:20.030 --> 00:01:22.120
further predict the

00:01:22.820 --> 00:01:24.750
Overall rating and that means we first

00:01:24.750 --> 00:01:26.330
predict the aspect rating and then

00:01:26.330 --> 00:01:28.582
combine them with aspect weights to

00:01:28.582 --> 00:01:30.430
predict the overall rating.

00:01:30.430 --> 00:01:32.680
So this would give us a unified

00:01:32.680 --> 00:01:35.750
generative model where we model both

00:01:35.750 --> 00:01:38.438
the generation of text and the overall

00:01:38.438 --> 00:01:39.970
rating condition on text.

00:01:40.820 --> 00:01:43.640
So we don't have time to discuss this

00:01:43.640 --> 00:01:47.150
model in detail, as in many other cases

00:01:47.150 --> 00:01:48.895
in this part of the course where we

00:01:48.895 --> 00:01:51.180
discuss the cutting edge topics.

00:01:51.910 --> 00:01:54.140
But there is a reference site here

00:01:54.140 --> 00:01:55.990
where you can find more details.

00:01:57.020 --> 00:01:58.300
So now I'm going to show you some

00:01:58.300 --> 00:02:00.300
simple results that you can get by

00:02:00.300 --> 00:02:02.550
using this kind of generative models.

00:02:02.550 --> 00:02:04.920
First it's about rating decomposition.

00:02:04.920 --> 00:02:08.450
So here what you see are the decomposed

00:02:08.450 --> 00:02:11.840
ratings for three hotels that have the

00:02:11.840 --> 00:02:12.870
same overall rating.

00:02:12.870 --> 00:02:14.745
So if you just look at the overall

00:02:14.745 --> 00:02:15.810
rating you don't.

00:02:15.810 --> 00:02:17.450
You can't really tell much difference

00:02:17.450 --> 00:02:19.400
between these hotels, but by

00:02:19.400 --> 00:02:23.390
decomposing these ratings into aspect

00:02:23.390 --> 00:02:26.560
ratings we can see some hotels have

00:02:26.560 --> 00:02:27.930
higher ratings for some.

00:02:27.980 --> 00:02:30.680
Dimensions like value, but others might

00:02:30.680 --> 00:02:32.419
score better in other dimensions like

00:02:32.420 --> 00:02:34.850
location and so this can reveal

00:02:34.850 --> 00:02:37.830
detailed opinions at the aspect level.

00:02:38.630 --> 00:02:41.890
Now here, the ground truth is shown in the

00:02:41.890 --> 00:02:44.650
plans, so this also allows you to see

00:02:44.650 --> 00:02:46.346
whether the prediction is accurate.

00:02:46.346 --> 00:02:48.890
It's not always accurate, but it's

00:02:48.890 --> 00:02:51.840
mostly still reflecting some of the

00:02:51.840 --> 00:02:52.540
trends.

00:02:53.420 --> 00:02:55.916
The 2nd result is to compare different

00:02:55.916 --> 00:02:58.760
reviewers on the same hotel so the

00:02:58.760 --> 00:03:03.300
table shows the decompose ratings for

00:03:03.300 --> 00:03:05.390
two reviewers about same hotel again

00:03:05.390 --> 00:03:07.280
their high level overall ratings are

00:03:07.280 --> 00:03:07.893
the same.

00:03:07.893 --> 00:03:09.925
So if you just look at the overall

00:03:09.925 --> 00:03:12.590
ratings, you don't really get that much

00:03:12.590 --> 00:03:14.210
information about the difference

00:03:14.210 --> 00:03:15.310
between the two reviews.

00:03:15.310 --> 00:03:16.965
But after you decompose the ratings you

00:03:16.965 --> 00:03:19.760
can see clearly they have high scores

00:03:19.760 --> 00:03:21.100
on different dimensions.

00:03:21.100 --> 00:03:23.590
So this shows that the model can reveal

00:03:23.590 --> 00:03:24.400
differences in.

00:03:24.850 --> 00:03:27.030
Opinions of different reviewers and

00:03:27.030 --> 00:03:28.890
such a detailed understanding can help

00:03:28.890 --> 00:03:32.550
us understand better about reviews and

00:03:32.550 --> 00:03:35.830
also better about their feedback on the

00:03:35.830 --> 00:03:36.430
hotel.

00:03:36.430 --> 00:03:38.380
This is something very interesting

00:03:38.380 --> 00:03:39.830
because this is in some sense some

00:03:39.830 --> 00:03:42.250
byproduct in our problem formulation.

00:03:42.250 --> 00:03:44.060
We did not really have to do this, but

00:03:44.060 --> 00:03:46.360
the design of the generative model has

00:03:46.360 --> 00:03:48.930
this component and these are sentiment

00:03:48.930 --> 00:03:52.010
waits for words in different aspects.

00:03:52.830 --> 00:03:55.055
And you can see the highly weighted

00:03:55.055 --> 00:03:57.690
words versus the negatively lower

00:03:57.690 --> 00:04:00.230
weighted words here for each of the

00:04:00.230 --> 00:04:01.070
four dimensions.

00:04:01.070 --> 00:04:03.280
Value, rooms, location and cleanliness.

00:04:03.960 --> 00:04:06.673
I added the top words, cleared it,

00:04:06.673 --> 00:04:08.390
makes sense, and the bottom words also

00:04:08.390 --> 00:04:09.060
makes sense.

00:04:10.130 --> 00:04:12.580
So this shows that with this apology,

00:04:12.580 --> 00:04:14.540
we can also learn sentiment information

00:04:14.540 --> 00:04:15.790
directly from the data.

00:04:15.790 --> 00:04:18.170
Now this kind of laxing is very useful

00:04:18.170 --> 00:04:21.390
becausw in general a word like long,

00:04:21.390 --> 00:04:24.042
let's say, may have different the

00:04:24.042 --> 00:04:25.590
sentiment polarities for different

00:04:25.590 --> 00:04:26.220
context.

00:04:26.220 --> 00:04:28.939
So if I say the battery life of this

00:04:28.939 --> 00:04:30.990
laptop is long, then that's positive.

00:04:30.990 --> 00:04:34.344
But if I say the rebooting time for the

00:04:34.344 --> 00:04:36.415
laptop is long, that's bad, right?

00:04:36.415 --> 00:04:38.400
So even for reviews about the same

00:04:38.400 --> 00:04:41.350
product laptop, the word long

00:04:41.400 --> 00:04:44.615
Is ambiguous, it could mean positive or

00:04:44.615 --> 00:04:47.296
could be negative, but this kind of

00:04:47.296 --> 00:04:49.420
lexicon that we can learn by using this

00:04:49.420 --> 00:04:52.370
kind of generative models can show

00:04:52.370 --> 00:04:54.500
whether a word is positive for a

00:04:54.500 --> 00:04:56.710
particular aspect, so this is clearly

00:04:56.710 --> 00:04:59.592
very useful, and in fact such a lexicon

00:04:59.592 --> 00:05:02.830
can be directly used to tag other

00:05:02.830 --> 00:05:05.630
reviews about hotels or tag comments

00:05:05.630 --> 00:05:07.480
about the hotels in social media like

00:05:07.480 --> 00:05:08.000
tweets.

00:05:08.780 --> 00:05:11.930
And, what's also interesting that since

00:05:11.930 --> 00:05:13.960
this is an almost computer and

00:05:13.960 --> 00:05:18.130
supervised, assuming that the reviews

00:05:18.130 --> 00:05:20.460
with overall ratings are available, and

00:05:20.460 --> 00:05:22.580
then this can allow us to learn from

00:05:22.580 --> 00:05:24.410
potentially a large amount of data on

00:05:24.410 --> 00:05:26.680
the Internet to reach sentiment

00:05:26.680 --> 00:05:27.200
lexicon.

00:05:28.080 --> 00:05:30.225
And here are some results to validate

00:05:30.225 --> 00:05:31.440
the preference weights.

00:05:31.440 --> 00:05:34.280
Remember, the model can infer whether a

00:05:34.280 --> 00:05:36.320
reviewer cares more about service or

00:05:36.320 --> 00:05:37.070
the price.

00:05:37.070 --> 00:05:39.760
Now, how do we know whether the

00:05:39.760 --> 00:05:41.770
inferred weights are correct and this

00:05:41.770 --> 00:05:44.060
poses a very difficult challenge for

00:05:44.060 --> 00:05:44.800
evaluation.

00:05:45.360 --> 00:05:48.630
Now here we show some interesting way

00:05:48.630 --> 00:05:51.953
of evaluating result. what you here are

00:05:51.953 --> 00:05:53.839
the prices of hotels in different

00:05:53.840 --> 00:05:57.006
cities, and these are the prices of

00:05:57.006 --> 00:05:59.250
hotels that are favored by different

00:05:59.250 --> 00:06:00.500
groups of reviews.

00:06:00.500 --> 00:06:03.100
The top ten other reviewers with the

00:06:03.100 --> 00:06:08.060
highest inferred value to other aspect

00:06:08.060 --> 00:06:08.680
ratio.

00:06:09.470 --> 00:06:12.020
So for example, value versus location

00:06:12.020 --> 00:06:14.670
value versus room etc.

00:06:14.670 --> 00:06:17.090
But the top ten are the reviewers that

00:06:17.090 --> 00:06:19.070
have the highest ratios by this

00:06:19.070 --> 00:06:19.460
measure.

00:06:19.460 --> 00:06:22.320
And that means these reviewers tend to

00:06:22.320 --> 00:06:24.580
put a lot of weight on value as

00:06:24.580 --> 00:06:26.220
compared with other dimensions.

00:06:26.220 --> 00:06:28.555
That means they really emphasize on

00:06:28.555 --> 00:06:28.910
value.

00:06:30.350 --> 00:06:32.060
The bottom ten, on the other hand, are

00:06:32.060 --> 00:06:33.890
the reviews that have the lowest ratio.

00:06:33.890 --> 00:06:34.600
What does that mean?

00:06:34.600 --> 00:06:36.490
Well, that means these reviewers have

00:06:36.490 --> 00:06:38.850
put higher weights on other aspects

00:06:38.850 --> 00:06:43.082
than value, so those are people that

00:06:43.082 --> 00:06:45.550
care about the another dimension and

00:06:45.550 --> 00:06:47.600
they didn't care so much about the

00:06:47.600 --> 00:06:49.810
value in some sense by this, less

00:06:49.810 --> 00:06:51.610
compared with the top ten group.

00:06:52.350 --> 00:06:54.475
Now these ratios are computer based on

00:06:54.475 --> 00:06:56.820
the inferred weights from the model.

00:06:57.700 --> 00:06:59.700
So now you can see the average prices

00:06:59.700 --> 00:07:02.030
of hotels are favored by toptenreviews

00:07:02.030 --> 00:07:04.720
are indeed and much cheaper than those

00:07:04.720 --> 00:07:06.560
that are favored by the bottom 10.

00:07:07.270 --> 00:07:09.330
And this provides some

00:07:10.920 --> 00:07:14.240
Indirect way of validating the infer

00:07:14.240 --> 00:07:14.670
wait.

00:07:14.670 --> 00:07:16.020
It just means the weights are not

00:07:16.020 --> 00:07:18.360
random and they are actually meaningful

00:07:18.360 --> 00:07:20.774
here and in comparison with the average

00:07:20.774 --> 00:07:22.903
price in these three cities, you can

00:07:22.903 --> 00:07:24.930
actually the top ten tends to have

00:07:24.930 --> 00:07:27.780
below average price, whereas the bottom

00:07:27.780 --> 00:07:29.710
time where they care a lot about other

00:07:29.710 --> 00:07:31.560
things like service or room condition

00:07:31.560 --> 00:07:34.910
tend to have hotels that have higher

00:07:34.910 --> 00:07:36.780
prices than average.

00:07:36.780 --> 00:07:39.310
So with these results we can build a

00:07:39.310 --> 00:07:40.570
lot of interesting applications.

00:07:40.570 --> 00:07:42.320
For example, direct application would

00:07:42.320 --> 00:07:43.970
be the generator rated aspect, the

00:07:43.970 --> 00:07:44.490
summary.

00:07:44.560 --> 00:07:47.170
An because of the decomposition, we can

00:07:47.170 --> 00:07:49.505
now generate the summaries for each

00:07:49.505 --> 00:07:49.872
aspect.

00:07:49.872 --> 00:07:51.680
The positive sentence is negative

00:07:51.680 --> 00:07:53.279
sentences about each aspect.

00:07:53.930 --> 00:07:55.680
It's more informative than original

00:07:55.680 --> 00:07:57.885
review that has just overall rating and

00:07:57.885 --> 00:07:58.660
review test.

00:08:00.370 --> 00:08:02.840
Here are also mother results about the

00:08:02.840 --> 00:08:06.000
aspects discovered from reviews with

00:08:06.000 --> 00:08:06.615
low ratings.

00:08:06.615 --> 00:08:09.695
These are MP3 three reviews an these

00:08:09.695 --> 00:08:11.890
results show that the model can

00:08:11.890 --> 00:08:13.870
discover some interesting aspects

00:08:13.870 --> 00:08:16.260
commented on low overall ratings versus

00:08:16.260 --> 00:08:18.970
those high overall ratings, and they

00:08:18.970 --> 00:08:21.520
care more about the different aspects.

00:08:22.500 --> 00:08:24.130
Or they comment more on different

00:08:24.130 --> 00:08:24.880
aspects.

00:08:25.700 --> 00:08:27.530
So that can help us discover, for

00:08:27.530 --> 00:08:30.950
example, consumers trained in

00:08:30.950 --> 00:08:33.440
appreciating different features of

00:08:33.440 --> 00:08:34.000
product.

00:08:34.000 --> 00:08:37.880
For example, one might have discovered

00:08:37.880 --> 00:08:41.100
the trend that people tend to like large screens

00:08:41.100 --> 00:08:44.190
of cell phones or lightweight of laptop

00:08:44.190 --> 00:08:45.320
etc.

00:08:45.320 --> 00:08:49.960
And such knowledge can be useful for

00:08:49.960 --> 00:08:52.500
manufacturers to design their next

00:08:52.500 --> 00:08:53.740
generation of products.

00:08:55.350 --> 00:08:58.250
Here are some interesting results on

00:08:58.250 --> 00:09:00.750
analyzing users rating behavior.

00:09:00.750 --> 00:09:04.905
So what you see is average weights on

00:09:04.905 --> 00:09:07.330
different dimensions by different

00:09:07.330 --> 00:09:08.710
groups of reviewers.

00:09:09.360 --> 00:09:12.403
And on the left side you see the

00:09:12.403 --> 00:09:16.215
weights of reviews like the expensive

00:09:16.215 --> 00:09:17.285
hotels they give.

00:09:17.285 --> 00:09:20.160
The whole expensive hotels five stars

00:09:20.160 --> 00:09:22.880
and you can see their average weights

00:09:22.880 --> 00:09:25.006
tend to be more focused on service and

00:09:25.006 --> 00:09:27.470
that suggests that people might be

00:09:27.470 --> 00:09:29.070
expensive hotels because of good

00:09:29.070 --> 00:09:29.355
service.

00:09:29.355 --> 00:09:31.430
And that's not surprising as also

00:09:31.430 --> 00:09:33.230
another way to validate the inferred

00:09:33.230 --> 00:09:33.690
weights.

00:09:34.290 --> 00:09:36.990
But if you look at the right side where

00:09:36.990 --> 00:09:40.680
look at the column of five stars, these

00:09:40.680 --> 00:09:42.605
are the reviewers that like the cheaper

00:09:42.605 --> 00:09:44.805
hotels and they give cheaper hotels,

00:09:44.805 --> 00:09:47.640
five stars as we expected, and they put

00:09:47.640 --> 00:09:49.855
more weight on value and that's why

00:09:49.855 --> 00:09:51.170
they like the cheaper hotels.

00:09:52.490 --> 00:09:54.780
But if you look at the when they didn't

00:09:54.780 --> 00:09:56.790
like expensive hotels or cheaper hotels

00:09:56.790 --> 00:09:58.450
and you seal it tended to have more

00:09:58.450 --> 00:10:02.320
weights on the condition of the room

00:10:02.320 --> 00:10:03.980
cleanliness.

00:10:03.980 --> 00:10:07.760
So this shows that by using this model

00:10:07.760 --> 00:10:09.870
we can infer some information that's

00:10:09.870 --> 00:10:12.601
very hard to obtain, even if you read

00:10:12.601 --> 00:10:13.449
all the reviews.

00:10:13.449 --> 00:10:15.560
Even if you read all the reviews, it's

00:10:15.560 --> 00:10:19.020
very hard to infer such preferences or

00:10:19.020 --> 00:10:20.220
such emphasis.

00:10:20.780 --> 00:10:22.719
So this is a case where text mining

00:10:22.720 --> 00:10:24.710
algorithms can go beyond what humans

00:10:24.710 --> 00:10:27.070
can do to review interesting patterns

00:10:27.070 --> 00:10:29.250
in the data, and this of course can be

00:10:29.250 --> 00:10:29.760
very useful.

00:10:29.760 --> 00:10:31.880
You can compare different hotels,

00:10:31.880 --> 00:10:34.440
compare the opinions from different

00:10:34.440 --> 00:10:37.300
consumer groups in different locations,

00:10:37.300 --> 00:10:39.430
and of course the model is general.

00:10:39.430 --> 00:10:41.990
It can be applied to any reviews with

00:10:41.990 --> 00:10:44.520
overall ratings, so this is very useful

00:10:44.520 --> 00:10:46.763
technique that can support a lot of

00:10:46.763 --> 00:10:48.010
text mining applications.

00:10:49.910 --> 00:10:52.500
Finally, there is also some result on

00:10:52.500 --> 00:10:54.310
applying this model for personalized

00:10:54.310 --> 00:10:56.140
ranking or recommendation of entities.

00:10:57.690 --> 00:11:00.830
So because we can infer the reviewers

00:11:00.830 --> 00:11:02.530
weights on different dimensions, we can

00:11:02.530 --> 00:11:05.170
allow a user to actually say what do

00:11:05.170 --> 00:11:06.073
you care about.

00:11:06.073 --> 00:11:08.110
So, for example, if a query here that

00:11:08.110 --> 00:11:10.446
shows 90% of the way it should be on

00:11:10.446 --> 00:11:12.550
value and 10% on others.

00:11:12.550 --> 00:11:14.460
So that just means I don't care about

00:11:14.460 --> 00:11:16.290
other aspects, I just care about

00:11:16.290 --> 00:11:17.670
getting a cheap hotel.

00:11:17.670 --> 00:11:20.760
My emphasis is on the value dimension.

00:11:21.330 --> 00:11:23.825
Now what we can do is such a query is

00:11:23.825 --> 00:11:26.310
that we can use reviewers that we

00:11:26.310 --> 00:11:28.890
believe have a similar preference to

00:11:28.890 --> 00:11:30.690
recommend the hotels for you.

00:11:30.690 --> 00:11:33.555
How can we know that we can infer the

00:11:33.555 --> 00:11:35.790
weights of those reviewers on different

00:11:35.790 --> 00:11:36.130
aspects?

00:11:36.130 --> 00:11:38.965
We can find the reviewers whose weights

00:11:38.965 --> 00:11:41.522
or more precise whose inferred

00:11:41.522 --> 00:11:44.140
weights or similar to yours and then

00:11:44.140 --> 00:11:45.802
use those reviews to recommend the

00:11:45.802 --> 00:11:46.633
hotels for you.

00:11:46.633 --> 00:11:48.740
And this is what we call a personalized

00:11:48.740 --> 00:11:50.550
or rather query specific

00:11:50.550 --> 00:11:51.280
recommendation.

00:11:52.310 --> 00:11:54.010
The non personalized recommendation

00:11:54.010 --> 00:11:55.360
results are shown on the top.

00:11:55.930 --> 00:11:58.520
An you can see the top results

00:11:58.520 --> 00:12:00.320
generally have much higher price than

00:12:00.320 --> 00:12:03.840
the low Group, and that's because when

00:12:03.840 --> 00:12:06.240
reviewers cared more about the value as

00:12:06.240 --> 00:12:08.640
dictated by this query and they tend to

00:12:08.640 --> 00:12:12.250
really have favor low price hotels.

00:12:12.250 --> 00:12:15.350
So this is yet another application of

00:12:15.350 --> 00:12:16.980
this technique.

00:12:18.160 --> 00:12:20.106
And shows that by doing text mining we

00:12:20.106 --> 00:12:21.975
can understand the users better.

00:12:21.975 --> 00:12:23.785
And once we can end users better, we

00:12:23.785 --> 00:12:25.339
can serve these users better.

00:12:25.340 --> 00:12:27.180
So to summarize our discussion of

00:12:27.180 --> 00:12:29.140
opinion mining in general, this is a

00:12:29.140 --> 00:12:30.960
very important topic and with a lot of

00:12:30.960 --> 00:12:31.740
applications.

00:12:33.060 --> 00:12:36.980
And as a task sentiment analysis can be

00:12:36.980 --> 00:12:38.520
usually done by using just text

00:12:38.520 --> 00:12:40.320
categorization, but standard techniques

00:12:40.320 --> 00:12:42.280
tend not to be enough and so we need to

00:12:42.280 --> 00:12:44.220
have enriched feature representation.

00:12:44.910 --> 00:12:46.860
And we also need to consider the order

00:12:46.860 --> 00:12:48.925
of those categories and we talk about

00:12:48.925 --> 00:12:50.170
the ordinal regression.

00:12:50.900 --> 00:12:51.980
For solving this problem.

00:12:52.550 --> 00:12:54.290
We have also shown that generative

00:12:54.290 --> 00:12:56.450
models are powerful for mining latent

00:12:56.450 --> 00:12:58.860
user preferences, in particular in the

00:12:58.860 --> 00:13:01.080
generating model for letting the rating

00:13:01.080 --> 00:13:03.640
regression, we embed some interesting

00:13:03.640 --> 00:13:06.330
preference information and sentiment

00:13:06.330 --> 00:13:08.149
weights of words in the model.

00:13:08.150 --> 00:13:10.180
As a result, we can learn those useful

00:13:10.180 --> 00:13:12.260
information when fitting the model to

00:13:12.260 --> 00:13:12.760
the data.

00:13:13.680 --> 00:13:16.500
Most approaches have been proposed and

00:13:16.500 --> 00:13:18.290
evaluated for product reviews, and that

00:13:18.290 --> 00:13:21.016
was the cause in such a context of the

00:13:21.016 --> 00:13:22.730
opinion holder an opinion target or

00:13:22.730 --> 00:13:26.310
clear and they are easy to analyze and

00:13:26.310 --> 00:13:27.840
there of course also have a lot of

00:13:27.840 --> 00:13:30.440
practical applications, but opinion

00:13:30.440 --> 00:13:33.070
mining from news and social media is

00:13:33.070 --> 00:13:36.080
also important, but that's more

00:13:36.080 --> 00:13:38.630
difficult than analyzing review data,

00:13:38.630 --> 00:13:40.790
mainly because the opinion holders

00:13:40.790 --> 00:13:43.660
and opinion targets are all.

00:13:44.230 --> 00:13:46.890
implicit and so that calls for natural

00:13:46.890 --> 00:13:48.520
language processing techniques to

00:13:48.520 --> 00:13:49.900
uncover them accurately.

00:13:50.830 --> 00:13:52.520
So here are some suggested readings,

00:13:52.520 --> 00:13:54.940
the first 2.

00:13:55.560 --> 00:13:57.970
are small books that are excellent

00:13:57.970 --> 00:14:00.220
reviews of this topic where you can

00:14:00.220 --> 00:14:02.470
find a lot of discussion about the

00:14:02.470 --> 00:14:04.420
other variations of the problem and

00:14:04.420 --> 00:14:06.845
techniques proposal for solving the

00:14:06.845 --> 00:14:07.150
problem.

00:14:08.030 --> 00:14:10.900
The next two papers are about the

00:14:10.900 --> 00:14:12.810
generative models for letting the

00:14:12.810 --> 00:14:14.280
aspect rating analysis.

00:14:14.280 --> 00:14:16.327
The first one is about solving the

00:14:16.327 --> 00:14:18.780
problem using two stages and the second

00:14:18.780 --> 00:14:21.350
one is about the unified model where

00:14:21.350 --> 00:14:23.050
topic model is integrated with the

00:14:23.050 --> 00:14:24.180
regression model.

00:14:24.180 --> 00:14:27.030
To solve the problem using a unified

00:14:27.030 --> 00:14:27.480
model.


