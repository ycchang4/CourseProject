WEBVTT Kind: captions; Language: en-US

NOTE
Created on 2021-02-27T00:00:27.7507216Z by ClassTranscribe

00:00:00.300 --> 00:00:02.940
This lecture is about an overview of

00:00:02.940 --> 00:00:05.690
statistical language models which cover

00:00:05.690 --> 00:00:07.880
probabilistic topic models as special

00:00:07.880 --> 00:00:08.370
cases.

00:00:16.100 --> 00:00:17.990
In this lecture we're going to give an

00:00:17.990 --> 00:00:20.250
overview of statistical language

00:00:20.250 --> 00:00:21.260
models.

00:00:21.260 --> 00:00:23.780
These models are general models that

00:00:23.780 --> 00:00:26.230
cover probabilistic topic models as

00:00:26.230 --> 00:00:27.230
special cases.

00:00:28.060 --> 00:00:29.810
So first, what is the statistical

00:00:29.810 --> 00:00:30.670
language model?

00:00:31.670 --> 00:00:33.250
A statistical language model is

00:00:33.250 --> 00:00:35.759
basically the probability distribution

00:00:35.760 --> 00:00:37.720
over word sequences.

00:00:37.720 --> 00:00:40.515
So, for example, we might have a

00:00:40.515 --> 00:00:41.510
distribution that gives

00:00:41.510 --> 00:00:45.110
"Today is Wednesday" a probability of 0.001

00:00:46.530 --> 00:00:47.350
It might give

00:00:47.350 --> 00:00:49.850
"Today Wednesday is" which is a non

00:00:49.850 --> 00:00:52.160
grammatical sentence very, very small

00:00:52.160 --> 00:00:53.730
probability as shown here.

00:00:54.470 --> 00:00:56.330
And similarly another sentence, "The

00:00:56.330 --> 00:00:58.250
eigenvalue is positive", might get a

00:00:58.250 --> 00:01:01.020
probability of 0.00001

00:01:02.170 --> 00:01:04.520
So as you can see, such a distribution

00:01:04.520 --> 00:01:06.390
clearly is context dependent.

00:01:07.040 --> 00:01:09.140
It depends on the context of

00:01:09.140 --> 00:01:09.790
discussion.

00:01:09.790 --> 00:01:12.110
Some word sequences might have higher

00:01:12.110 --> 00:01:13.550
probabilities than others.

00:01:14.320 --> 00:01:16.640
But the same sequence of words might

00:01:16.640 --> 00:01:18.250
have a different probability in a

00:01:18.250 --> 00:01:19.140
different context.

00:01:20.400 --> 00:01:22.560
And so this suggests that such a

00:01:22.560 --> 00:01:24.450
distribution can actually characterize

00:01:24.450 --> 00:01:24.960
topic.

00:01:26.870 --> 00:01:28.740
Such a model can also be regarded

00:01:28.740 --> 00:01:31.360
as a probabilistic mechanism for

00:01:31.360 --> 00:01:32.600
generating text.

00:01:33.750 --> 00:01:37.880
And that just means we can view text

00:01:37.880 --> 00:01:41.702
data as data observed from such a

00:01:41.702 --> 00:01:42.010
model.

00:01:42.010 --> 00:01:45.217
For this reason, we also call such a

00:01:45.217 --> 00:01:46.680
model generative model.

00:01:48.000 --> 00:01:49.210
So now

00:01:50.430 --> 00:01:52.900
Given a model, we can then sample

00:01:52.900 --> 00:01:54.190
sequences of words.

00:01:54.190 --> 00:01:56.640
So for example, based on the

00:01:56.640 --> 00:01:59.110
distribution that I have shown here on

00:01:59.110 --> 00:02:01.935
this slide, we might, let's say, sample

00:02:01.935 --> 00:02:04.110
a sequence like "today is Wednesday"

00:02:04.110 --> 00:02:05.970
because it has a relatively high

00:02:05.970 --> 00:02:09.020
probability, we might often get such a

00:02:09.020 --> 00:02:09.340
sequence.

00:02:10.020 --> 00:02:12.190
We might also get "the eigenvalue is

00:02:12.190 --> 00:02:15.080
positive", sometimes with a smaller

00:02:15.080 --> 00:02:15.840
probability.

00:02:16.720 --> 00:02:18.450
Very, very occasionally,

00:02:18.450 --> 00:02:19.760
we might get "today

00:02:19.760 --> 00:02:22.360
Wednesday is" because the probability is

00:02:22.360 --> 00:02:23.200
so small.

00:02:24.100 --> 00:02:25.940
So in general, in order to characterize

00:02:25.940 --> 00:02:27.960
such a distribution, we must specify

00:02:27.960 --> 00:02:30.850
probability values for all these

00:02:30.850 --> 00:02:33.370
different sequences of words.

00:02:33.370 --> 00:02:35.700
Obviously it's impossible to specify

00:02:35.700 --> 00:02:38.760
that, because it's impossible to

00:02:38.760 --> 00:02:40.792
enumerate all the possible sequences of

00:02:40.792 --> 00:02:41.220
words.

00:02:42.440 --> 00:02:46.580
So in practice we will have to simplify

00:02:46.580 --> 00:02:48.480
the model in some way.

00:02:49.200 --> 00:02:50.840
So the simplest language model is

00:02:50.840 --> 00:02:52.660
called a unigram language model.

00:02:52.660 --> 00:02:56.550
In such a case, we simply assume that

00:02:56.550 --> 00:03:00.240
text is generated by generating each

00:03:00.240 --> 00:03:01.980
word independently.

00:03:02.880 --> 00:03:05.280
Now, in general, the words may not be

00:03:05.280 --> 00:03:07.460
generated independently, but after we

00:03:07.460 --> 00:03:08.790
make this assumption, we can

00:03:08.790 --> 00:03:10.730
significantly simplify the language

00:03:10.730 --> 00:03:11.170
model.

00:03:12.140 --> 00:03:13.770
Basically now the probability of a

00:03:13.770 --> 00:03:16.330
sequence of words w_1 through

00:03:16.330 --> 00:03:19.075
w_n would be just a product of

00:03:19.075 --> 00:03:19.520
each.

00:03:19.520 --> 00:03:21.740
The probability of each word.

00:03:24.740 --> 00:03:27.270
So for such a model we have as many

00:03:27.270 --> 00:03:29.510
parameters as the number of words in

00:03:29.510 --> 00:03:30.430
our vocabulary.

00:03:30.430 --> 00:03:32.986
So here we assume we have N words, so

00:03:32.986 --> 00:03:36.010
we have N probabilities, one for each

00:03:36.010 --> 00:03:37.550
word, and they sum to one.

00:03:39.980 --> 00:03:42.220
So now we can assume our text is a

00:03:42.220 --> 00:03:44.740
sample drawn according to this word

00:03:44.740 --> 00:03:45.510
distribution.

00:03:46.140 --> 00:03:47.970
That just means we're gonna draw

00:03:49.390 --> 00:03:51.440
a word each time and then eventually

00:03:51.440 --> 00:03:52.410
we'll get a text.

00:03:53.610 --> 00:03:55.970
So for example now again.

00:03:57.690 --> 00:04:00.500
We can try to sample words according to

00:04:00.500 --> 00:04:01.400
a distribution.

00:04:01.400 --> 00:04:04.810
We might get Wednesday often or today

00:04:04.810 --> 00:04:07.800
often and some other words like

00:04:07.800 --> 00:04:09.840
eigenvalue might have a small

00:04:09.840 --> 00:04:11.270
probability, etc.

00:04:11.270 --> 00:04:16.200
Now, with this we actually can also

00:04:16.200 --> 00:04:19.760
compute the probability of every

00:04:19.760 --> 00:04:22.680
sequence, even though our model only

00:04:22.680 --> 00:04:25.070
specifies the probabilities of words.

00:04:25.870 --> 00:04:27.700
This is because of the independence

00:04:27.700 --> 00:04:28.880
assumption.

00:04:28.880 --> 00:04:31.290
So specifically we can compute the

00:04:31.290 --> 00:04:33.120
probability of "today is Wednesday".

00:04:33.900 --> 00:04:36.330
Because it's just a product of the

00:04:36.330 --> 00:04:39.510
probability of today, probability of is

00:04:39.510 --> 00:04:41.180
and probably Wednesday.

00:04:41.930 --> 00:04:44.110
For example, I showed some fake numbers

00:04:44.110 --> 00:04:46.160
here and we might then multiply these

00:04:46.160 --> 00:04:47.920
numbers together to get the probability

00:04:47.920 --> 00:04:48.940
of "today is Wednesday".

00:04:49.540 --> 00:04:52.390
So as you can see, with N

00:04:52.390 --> 00:04:55.300
probabilities, one for each word, we

00:04:55.300 --> 00:04:57.940
actually can characterize the

00:04:57.940 --> 00:05:00.550
probability distribution over all kinds

00:05:00.550 --> 00:05:04.390
of sequences of words, and so this is a

00:05:04.390 --> 00:05:06.010
very simple model.

00:05:06.010 --> 00:05:08.750
Ignore the word order, so it may not

00:05:08.750 --> 00:05:11.410
be effective for some problems such as

00:05:11.410 --> 00:05:13.830
speech recognition, where you may care

00:05:13.830 --> 00:05:15.350
about the order of words.

00:05:15.350 --> 00:05:17.945
But it turns out to be quite sufficient

00:05:17.945 --> 00:05:19.710
for many tasks that involve

00:05:19.760 --> 00:05:22.470
topic analysis, and that's also what

00:05:22.470 --> 00:05:23.820
we're interested in here.

00:05:24.480 --> 00:05:26.550
So when we have a model, we generally

00:05:26.550 --> 00:05:30.350
have two problems that we can think

00:05:30.350 --> 00:05:30.740
about.

00:05:30.740 --> 00:05:34.199
One is given a model.

00:05:34.200 --> 00:05:36.870
How likely we'll observe certain kind of

00:05:36.870 --> 00:05:37.770
data points.

00:05:38.360 --> 00:05:39.830
That is, we're interested in the

00:05:39.830 --> 00:05:41.060
sampling process.

00:05:41.780 --> 00:05:44.622
The other is the estimation process and

00:05:44.622 --> 00:05:48.085
that is to figure out the parameters of

00:05:48.085 --> 00:05:51.480
the model given some observed data, and

00:05:51.480 --> 00:05:52.860
we're going to talk about that in a

00:05:52.860 --> 00:05:53.380
moment.

00:05:53.380 --> 00:05:54.800
Let's first talk about the sampling.

00:05:55.510 --> 00:05:59.700
So here I show two examples of word

00:05:59.700 --> 00:06:01.380
distributions or unigram language

00:06:01.380 --> 00:06:02.410
models.

00:06:02.410 --> 00:06:04.370
The first one has higher probabilities

00:06:04.370 --> 00:06:07.900
for words,Â  text, mining, association,

00:06:07.900 --> 00:06:08.690
etc.

00:06:10.000 --> 00:06:13.076
Now this signals a topic about text

00:06:13.076 --> 00:06:15.620
mining, because when we sample words

00:06:15.620 --> 00:06:18.690
from such a distribution we tend to see

00:06:18.690 --> 00:06:21.379
words that often occur in text mining

00:06:21.380 --> 00:06:21.970
context.

00:06:23.620 --> 00:06:26.310
So in this case, if we ask the question

00:06:26.310 --> 00:06:28.660
about what is the probability of

00:06:28.660 --> 00:06:31.040
generating a particular document, then

00:06:31.040 --> 00:06:35.190
we likely will see text that looks like

00:06:35.190 --> 00:06:37.400
a text mining paper of course.

00:06:38.590 --> 00:06:38.810
...

00:06:39.480 --> 00:06:41.610
The text that we generated by drawing

00:06:41.610 --> 00:06:43.759
words from this distribution is

00:06:43.760 --> 00:06:46.253
unlikely coherent, although the

00:06:46.253 --> 00:06:49.300
probability of generating a text mining

00:06:49.300 --> 00:06:52.800
paper publishing in the top conference

00:06:52.800 --> 00:06:54.560
is non zero.

00:06:54.560 --> 00:06:57.240
Assuming that no word has a zero

00:06:57.240 --> 00:06:59.190
probability in the distribution and

00:06:59.190 --> 00:07:00.960
that just means we can essentially

00:07:00.960 --> 00:07:03.479
generate all kinds of text documents,

00:07:03.480 --> 00:07:06.125
including very meaningful text

00:07:06.125 --> 00:07:06.600
documents.

00:07:07.720 --> 00:07:09.880
The second distribution show on the

00:07:09.880 --> 00:07:13.270
bottom has different words that with

00:07:13.270 --> 00:07:13.850
higher probability.

00:07:13.850 --> 00:07:15.680
Food, nutrition and

00:07:15.680 --> 00:07:17.226
healthy, diet etc.

00:07:17.226 --> 00:07:19.870
So this clearly indicates a different

00:07:19.870 --> 00:07:21.600
topic and in this case it's probably

00:07:21.600 --> 00:07:22.410
about health.

00:07:23.090 --> 00:07:25.260
So if we sample words from such

00:07:25.260 --> 00:07:28.110
distribution, then the probability of

00:07:28.110 --> 00:07:30.183
observing a text mining paper would be

00:07:30.183 --> 00:07:31.580
very very small.

00:07:32.720 --> 00:07:34.750
On the other hand, the probability of

00:07:34.750 --> 00:07:36.720
observing a text that looks like a

00:07:36.720 --> 00:07:39.410
food nutrition paper would be high,

00:07:39.410 --> 00:07:40.530
relatively higher.

00:07:41.420 --> 00:07:43.980
So that just means given a particular

00:07:43.980 --> 00:07:46.140
distribution, different text will

00:07:46.140 --> 00:07:47.700
have different probabilities.

00:07:48.360 --> 00:07:50.050
Now let's look at the estimation

00:07:50.050 --> 00:07:50.552
problem.

00:07:50.552 --> 00:07:53.010
Now, in this case, we're going to

00:07:53.010 --> 00:07:54.910
assume that we have observed data.

00:07:54.910 --> 00:07:56.819
We know exactly what the text data

00:07:56.820 --> 00:07:57.286
looks like.

00:07:57.286 --> 00:07:58.870
In this case, let's assume we have a

00:07:58.870 --> 00:07:59.930
text mining paper.

00:07:59.930 --> 00:08:04.010
In fact, it's abstract of the paper, so

00:08:04.010 --> 00:08:07.900
the total number of words is 100, and

00:08:07.900 --> 00:08:10.250
I've shown some counts of individual

00:08:10.250 --> 00:08:11.090
words here.

00:08:12.440 --> 00:08:15.700
If we ask the question, what is the

00:08:15.700 --> 00:08:19.070
most likely language model that has

00:08:19.070 --> 00:08:22.180
been used to generate this text data,

00:08:22.180 --> 00:08:24.880
assuming that the text is observed from

00:08:24.880 --> 00:08:27.300
some language model, what's our best

00:08:27.300 --> 00:08:29.069
guess of this language model?

00:08:30.630 --> 00:08:32.670
OK, so the problem now is just the

00:08:32.670 --> 00:08:35.210
estimated probabilities of these words

00:08:35.210 --> 00:08:36.600
as I've shown here.

00:08:37.470 --> 00:08:38.500
So what do you think?

00:08:38.500 --> 00:08:39.670
What would be your guess?

00:08:40.610 --> 00:08:41.460
Would you guess

00:08:42.430 --> 00:08:44.420
text that has a very very small

00:08:44.420 --> 00:08:46.660
probability or relatively large

00:08:46.660 --> 00:08:47.370
probability?

00:08:48.280 --> 00:08:49.610
What about the query?

00:08:50.190 --> 00:08:52.430
Your guess probably will be dependent

00:08:52.430 --> 00:08:55.180
on how many times we have observed this

00:08:55.180 --> 00:08:57.180
word in the text data, right?

00:08:58.340 --> 00:09:00.520
And if you think about it for a moment,

00:09:00.520 --> 00:09:03.490
and if you like many others, you would

00:09:03.490 --> 00:09:06.840
have guessed that text has a

00:09:06.840 --> 00:09:08.940
probability of 10 out of 100.

00:09:08.940 --> 00:09:12.480
Because I've observed text 10 times in

00:09:12.480 --> 00:09:15.580
the text that has a total of 100 words.

00:09:16.910 --> 00:09:19.570
And similarly, mining has five out of

00:09:19.570 --> 00:09:20.060
100.

00:09:20.790 --> 00:09:22.920
And query as a relatively small

00:09:22.920 --> 00:09:25.070
probability, just observd once.

00:09:25.070 --> 00:09:26.500
So it's one out of 100.

00:09:27.980 --> 00:09:30.990
Right, so that, intuitively, is a

00:09:30.990 --> 00:09:33.180
reasonable guess, but the question is

00:09:33.180 --> 00:09:33.470
Is this

00:09:33.470 --> 00:09:36.020
our best guess or best estimate of the

00:09:36.020 --> 00:09:36.560
parameters?

00:09:37.720 --> 00:09:39.520
Of course, in order to answer this

00:09:39.520 --> 00:09:41.830
question we have to define what we mean

00:09:41.830 --> 00:09:42.960
by best.

00:09:44.100 --> 00:09:46.340
In this case, it turns out that our

00:09:46.340 --> 00:09:50.990
guesses are indeed the best in some

00:09:50.990 --> 00:09:52.730
sense, and this is called maximum

00:09:52.730 --> 00:09:53.930
likelihood estimate.

00:09:54.560 --> 00:09:57.610
And it's the best in that it would

00:09:57.610 --> 00:10:00.210
give our observed data the maximum

00:10:00.210 --> 00:10:00.950
probability.

00:10:01.820 --> 00:10:03.850
Meaning that if you change the estimate

00:10:03.850 --> 00:10:06.250
somehow even slightly, then the

00:10:06.250 --> 00:10:08.510
probability of the observed text data

00:10:08.510 --> 00:10:10.040
will be somewhat smaller.

00:10:10.640 --> 00:10:12.450
And this is called a maximum likelihood

00:10:12.450 --> 00:10:12.800
estimate.


