{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cff2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open(\"textanalytics.txt\") as f:\n",
    "    for line in f:\n",
    "        sentences+=line.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64bad1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['In',\n",
       "  '@',\n",
       "  'lecture',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'overview',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'analytics'],\n",
       " ['First,',\n",
       "  \"let's\",\n",
       "  'define',\n",
       "  '@',\n",
       "  'term',\n",
       "  'text',\n",
       "  'mining',\n",
       "  '@',\n",
       "  '@',\n",
       "  'term',\n",
       "  'text',\n",
       "  'analytics'],\n",
       " ['The',\n",
       "  'title',\n",
       "  '@',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  'called',\n",
       "  'Text',\n",
       "  'Mining',\n",
       "  '@',\n",
       "  'Analytics,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'terms',\n",
       "  'text',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'text',\n",
       "  'analytics',\n",
       "  '@',\n",
       "  'actually',\n",
       "  'roughly',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'going',\n",
       "  '@',\n",
       "  '@',\n",
       "  'distinguish',\n",
       "  'them,',\n",
       "  '@',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'interchangeably'],\n",
       " ['But',\n",
       "  '@',\n",
       "  'reason',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'chosen',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'terms',\n",
       "  '@',\n",
       "  '@',\n",
       "  'title',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'subtle',\n",
       "  'difference',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'phrases',\n",
       "  'literally'],\n",
       " ['Mining',\n",
       "  'emphasizes',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'process,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'gives',\n",
       "  '@',\n",
       "  '@',\n",
       "  'algorithmic',\n",
       "  'view',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problem'],\n",
       " ['Analytics',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'hand',\n",
       "  'emphasizes',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'result',\n",
       "  '@',\n",
       "  'having',\n",
       "  '@',\n",
       "  'problem',\n",
       "  '@',\n",
       "  'mind'],\n",
       " [\"We're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'solve',\n",
       "  '@',\n",
       "  'problem'],\n",
       " ['But',\n",
       "  'again,',\n",
       "  '@',\n",
       "  'I',\n",
       "  'said,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'treat',\n",
       "  '@',\n",
       "  '@',\n",
       "  'terms',\n",
       "  'roughly',\n",
       "  '@',\n",
       "  'same,',\n",
       "  '@',\n",
       "  'I',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  'literature',\n",
       "  '@',\n",
       "  'probably',\n",
       "  '@',\n",
       "  'find',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['So',\n",
       "  \"we're\",\n",
       "  '@',\n",
       "  'going',\n",
       "  '@',\n",
       "  '@',\n",
       "  'distinguish',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'course'],\n",
       " ['Both',\n",
       "  'text',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'text',\n",
       "  'analytics',\n",
       "  'mean',\n",
       "  '@',\n",
       "  '@',\n",
       "  'want',\n",
       "  '@',\n",
       "  'turn',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'high',\n",
       "  'quality',\n",
       "  'information',\n",
       "  '@',\n",
       "  'actionable',\n",
       "  'knowledge'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'cases',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problem',\n",
       "  '@',\n",
       "  'dealing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'hope',\n",
       "  '@',\n",
       "  'turn',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'raw',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'distinguish',\n",
       "  '@',\n",
       "  'different',\n",
       "  'results',\n",
       "  '@',\n",
       "  '@',\n",
       "  'high',\n",
       "  'quality',\n",
       "  'information,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'actionable',\n",
       "  'knowledge'],\n",
       " ['Now,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'boundary',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'clear,',\n",
       "  '@',\n",
       "  'I',\n",
       "  '@',\n",
       "  'want',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'little',\n",
       "  'bit',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'angles',\n",
       "  '@',\n",
       "  '@',\n",
       "  'result',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  'high',\n",
       "  'quality',\n",
       "  'information',\n",
       "  '@',\n",
       "  'refer',\n",
       "  '@',\n",
       "  '@',\n",
       "  'concise',\n",
       "  'information',\n",
       "  '@',\n",
       "  '@',\n",
       "  'topic,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'easier',\n",
       "  '@',\n",
       "  'humans',\n",
       "  '@',\n",
       "  'digest',\n",
       "  '@',\n",
       "  '@',\n",
       "  'raw',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'face',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'reviews',\n",
       "  '@',\n",
       "  '@',\n",
       "  'product'],\n",
       " ['The',\n",
       "  '@',\n",
       "  'concise',\n",
       "  'form',\n",
       "  '@',\n",
       "  '@',\n",
       "  'information',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'concise',\n",
       "  'summary',\n",
       "  '@',\n",
       "  '@',\n",
       "  'major',\n",
       "  'opinions',\n",
       "  '@',\n",
       "  '@',\n",
       "  'features',\n",
       "  '@',\n",
       "  '@',\n",
       "  'product'],\n",
       " ['Positive',\n",
       "  'about,',\n",
       "  \"let's\",\n",
       "  'say,',\n",
       "  'battery',\n",
       "  'life',\n",
       "  '@',\n",
       "  '@',\n",
       "  'laptop'],\n",
       " ['Now,',\n",
       "  '@',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'results',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  '@',\n",
       "  'help',\n",
       "  'people',\n",
       "  'digest',\n",
       "  'text',\n",
       "  'data,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'minimize',\n",
       "  '@',\n",
       "  'human',\n",
       "  'effort',\n",
       "  '@',\n",
       "  'consuming',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sense'],\n",
       " ['The', '@', 'kind', '@', 'output', '@', 'actionable', 'knowledge'],\n",
       " ['Here',\n",
       "  '@',\n",
       "  'emphasize',\n",
       "  '@',\n",
       "  'utility',\n",
       "  '@',\n",
       "  '@',\n",
       "  'information',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  'discover',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data'],\n",
       " [\"It's\",\n",
       "  'actionable',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  'decision',\n",
       "  'problem,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'actions',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'able',\n",
       "  '@',\n",
       "  'determine',\n",
       "  '@',\n",
       "  'product',\n",
       "  '@',\n",
       "  '@',\n",
       "  'appealing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'all,',\n",
       "  '@',\n",
       "  'better',\n",
       "  'choice',\n",
       "  '@',\n",
       "  '@',\n",
       "  'shopping',\n",
       "  'decision'],\n",
       " ['Now,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'outcome',\n",
       "  '@',\n",
       "  '@',\n",
       "  'called',\n",
       "  'actionable',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  'consumer',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'decision',\n",
       "  '@',\n",
       "  'act',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case,',\n",
       "  'text',\n",
       "  'mining',\n",
       "  'supplies',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  'optimal',\n",
       "  'decision',\n",
       "  'making'],\n",
       " ['But',\n",
       "  'again,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'clearly',\n",
       "  'distinguished,',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"don't\",\n",
       "  'necessarily',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'distinction'],\n",
       " ['Text',\n",
       "  'mining',\n",
       "  '@',\n",
       "  '@',\n",
       "  'related',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'essential',\n",
       "  'component',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  'systems'],\n",
       " ['Now',\n",
       "  'text',\n",
       "  'retrieval',\n",
       "  'refers',\n",
       "  '@',\n",
       "  'finding',\n",
       "  'relevant',\n",
       "  'information',\n",
       "  '@',\n",
       "  '@',\n",
       "  'large',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['So',\n",
       "  \"I've\",\n",
       "  'taught',\n",
       "  '@',\n",
       "  'separate',\n",
       "  'MOOC',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval',\n",
       "  '@',\n",
       "  'search',\n",
       "  'engines,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'discussed',\n",
       "  '@',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval'],\n",
       " ['If',\n",
       "  '@',\n",
       "  '@',\n",
       "  'taken',\n",
       "  '@',\n",
       "  'MOOC,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'find',\n",
       "  '@',\n",
       "  'overlap',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'background',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval',\n",
       "  '@',\n",
       "  'understanding',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'topics',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'taken',\n",
       "  '@',\n",
       "  'MOOC',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'fine,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'analytics',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'repeat',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'key',\n",
       "  'concepts',\n",
       "  '@',\n",
       "  '@',\n",
       "  'relevant',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  'high',\n",
       "  'level,',\n",
       "  'let',\n",
       "  '@',\n",
       "  '@',\n",
       "  'explain',\n",
       "  '@',\n",
       "  'relation',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining'],\n",
       " ['Text',\n",
       "  'retrieval',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ways:',\n",
       "  'First,',\n",
       "  'text',\n",
       "  'retrieval',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'pre-processor',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining,',\n",
       "  'meaning',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'turn',\n",
       "  'big',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'relatively',\n",
       "  'small',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'relevant',\n",
       "  'text',\n",
       "  'data,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"what's\",\n",
       "  'needed',\n",
       "  '@',\n",
       "  'solving',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'problem'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sense,',\n",
       "  'text',\n",
       "  'retrieval',\n",
       "  '@',\n",
       "  'helps',\n",
       "  'minimize',\n",
       "  'human',\n",
       "  'effort'],\n",
       " ['Text',\n",
       "  'retrieval',\n",
       "  '@',\n",
       "  '@',\n",
       "  'needed',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  'provenance',\n",
       "  '@',\n",
       "  '@',\n",
       "  'roughly',\n",
       "  'corresponds',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interpretation',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'turning',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'actionable',\n",
       "  'knowledge'],\n",
       " ['Once',\n",
       "  '@',\n",
       "  'find',\n",
       "  '@',\n",
       "  'patterns',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'actionable',\n",
       "  'knowledge,',\n",
       "  '@',\n",
       "  'generally',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'verify',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  'looking',\n",
       "  '@',\n",
       "  '@',\n",
       "  'original',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'users',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval',\n",
       "  'support',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'original',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'interpret',\n",
       "  '@',\n",
       "  'pattern,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'better',\n",
       "  'understand',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  'verify',\n",
       "  '@',\n",
       "  '@',\n",
       "  'pattern',\n",
       "  '@',\n",
       "  '@',\n",
       "  'reliable'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'high',\n",
       "  'level',\n",
       "  'introduction',\n",
       "  '@',\n",
       "  '@',\n",
       "  'concept',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  '@',\n",
       "  '@',\n",
       "  'relation',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'retrieval'],\n",
       " ['Next,',\n",
       "  \"let's\",\n",
       "  'talk',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'special',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'data'],\n",
       " ['Now',\n",
       "  \"it's\",\n",
       "  'interesting',\n",
       "  '@',\n",
       "  'view',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'data',\n",
       "  'generated',\n",
       "  '@',\n",
       "  'humans',\n",
       "  '@',\n",
       "  'subjective',\n",
       "  'sensors'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'slide',\n",
       "  'shows',\n",
       "  '@',\n",
       "  'analogy',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'non',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'humans',\n",
       "  '@',\n",
       "  'subjective',\n",
       "  'sensors',\n",
       "  '@',\n",
       "  'physical',\n",
       "  'sensors',\n",
       "  '@',\n",
       "  '@',\n",
       "  'network',\n",
       "  'sensor',\n",
       "  '@',\n",
       "  'thermometer'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'general,',\n",
       "  '@',\n",
       "  'sensor',\n",
       "  '@',\n",
       "  'monitor',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world',\n",
       "  '@',\n",
       "  '@',\n",
       "  'way',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sense',\n",
       "  '@',\n",
       "  'signal',\n",
       "  '@',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'report',\n",
       "  '@',\n",
       "  'signal',\n",
       "  '@',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'forms,',\n",
       "  '@',\n",
       "  'example,',\n",
       "  '@',\n",
       "  'thermometer',\n",
       "  '@',\n",
       "  'watch',\n",
       "  '@',\n",
       "  'temperature',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'report',\n",
       "  '@',\n",
       "  'temperature',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'format'],\n",
       " ['Similarly',\n",
       "  '@',\n",
       "  'geo',\n",
       "  'sensor',\n",
       "  '@',\n",
       "  'sense',\n",
       "  '@',\n",
       "  'location',\n",
       "  '@',\n",
       "  '@',\n",
       "  'report',\n",
       "  '@',\n",
       "  'location',\n",
       "  'specification,',\n",
       "  '@',\n",
       "  'example',\n",
       "  '@',\n",
       "  '@',\n",
       "  'form',\n",
       "  '@',\n",
       "  'longitude',\n",
       "  'value',\n",
       "  '@',\n",
       "  'lattitude',\n",
       "  'value'],\n",
       " ['Network',\n",
       "  'sensor',\n",
       "  '@',\n",
       "  'monitor',\n",
       "  'network',\n",
       "  'traffic',\n",
       "  '@',\n",
       "  'activities',\n",
       "  '@',\n",
       "  '@',\n",
       "  'network',\n",
       "  '@',\n",
       "  'report',\n",
       "  '@',\n",
       "  'digital',\n",
       "  'format',\n",
       "  '@',\n",
       "  'data'],\n",
       " ['Similarly,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  'humans',\n",
       "  '@',\n",
       "  'subjective',\n",
       "  'sensors',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observe',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world',\n",
       "  '@',\n",
       "  '@',\n",
       "  'perspective,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'humans',\n",
       "  '@',\n",
       "  'express',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observed',\n",
       "  '@',\n",
       "  '@',\n",
       "  'form',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sense',\n",
       "  'human',\n",
       "  '@',\n",
       "  'actually',\n",
       "  '@',\n",
       "  'subjective',\n",
       "  'sensor',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sense',\n",
       "  \"what's\",\n",
       "  'happening',\n",
       "  '@',\n",
       "  '@',\n",
       "  'world',\n",
       "  '@',\n",
       "  '@',\n",
       "  'express',\n",
       "  \"what's\",\n",
       "  'observed',\n",
       "  '@',\n",
       "  '@',\n",
       "  'form',\n",
       "  '@',\n",
       "  'data,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['Now',\n",
       "  'looking',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'way',\n",
       "  '@',\n",
       "  '@',\n",
       "  'advantage',\n",
       "  '@',\n",
       "  '@',\n",
       "  'able',\n",
       "  '@',\n",
       "  'integrate',\n",
       "  '@',\n",
       "  'kinds',\n",
       "  '@',\n",
       "  'data',\n",
       "  'together,',\n",
       "  '@',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  'needed',\n",
       "  '@',\n",
       "  '@',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'problems'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'looking',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  'problem',\n",
       "  '@',\n",
       "  'data',\n",
       "  'mining,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'dealing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'world',\n",
       "  '@',\n",
       "  '@',\n",
       "  'related',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problem'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  'dealing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'non',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  'non',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'usually',\n",
       "  'produced',\n",
       "  '@',\n",
       "  'physical',\n",
       "  'sensors'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'non',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'formats',\n",
       "  '-',\n",
       "  'numerical',\n",
       "  'data',\n",
       "  '@',\n",
       "  'categorical',\n",
       "  '@',\n",
       "  'relational',\n",
       "  'data',\n",
       "  '@',\n",
       "  'multimedia',\n",
       "  'data',\n",
       "  'like',\n",
       "  '@',\n",
       "  'video',\n",
       "  '@',\n",
       "  'speech'],\n",
       " ['So,',\n",
       "  '@',\n",
       "  'non',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'important',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problems'],\n",
       " ['But',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'important,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contain',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'semantic',\n",
       "  'content',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contain',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  'users,',\n",
       "  'especially',\n",
       "  'preferences',\n",
       "  '@',\n",
       "  'opinions',\n",
       "  '@',\n",
       "  'users'],\n",
       " ['So,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'treating',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'data',\n",
       "  'observed',\n",
       "  '@',\n",
       "  'human',\n",
       "  'sensors,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'treat',\n",
       "  '@',\n",
       "  '@',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'framework'],\n",
       " ['So,',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'problem',\n",
       "  '@',\n",
       "  'basically',\n",
       "  '@',\n",
       "  'turn',\n",
       "  '@',\n",
       "  'data,',\n",
       "  'turn',\n",
       "  '@',\n",
       "  '@',\n",
       "  'data',\n",
       "  '@',\n",
       "  'actionable',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'advantage',\n",
       "  '@',\n",
       "  'change',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world,',\n",
       "  '@',\n",
       "  'course,',\n",
       "  '@',\n",
       "  'better'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'means',\n",
       "  '@',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'problem',\n",
       "  '@',\n",
       "  'basically',\n",
       "  'taking',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'data',\n",
       "  '@',\n",
       "  'input',\n",
       "  '@',\n",
       "  'giving',\n",
       "  'actionable',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  'output'],\n",
       " ['Inside',\n",
       "  '@',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'module',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'number',\n",
       "  '@',\n",
       "  'different',\n",
       "  'kinds',\n",
       "  '@',\n",
       "  'mining',\n",
       "  'algorithms',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'kinds',\n",
       "  '@',\n",
       "  'data',\n",
       "  '@',\n",
       "  'generally',\n",
       "  'need',\n",
       "  'different',\n",
       "  'algorithms',\n",
       "  '@',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'data'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  'video',\n",
       "  'data',\n",
       "  '@',\n",
       "  'require',\n",
       "  'computer',\n",
       "  'vision',\n",
       "  '@',\n",
       "  'understand',\n",
       "  'video',\n",
       "  'content',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'facilitate',\n",
       "  '@',\n",
       "  '@',\n",
       "  'effective',\n",
       "  'mining',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'general',\n",
       "  'algorithms',\n",
       "  '@',\n",
       "  '@',\n",
       "  'applicable',\n",
       "  '@',\n",
       "  '@',\n",
       "  'kinds',\n",
       "  '@',\n",
       "  'data,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'algorithms',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'data',\n",
       "  '@',\n",
       "  'generally',\n",
       "  'want',\n",
       "  '@',\n",
       "  '@',\n",
       "  'develop',\n",
       "  'special',\n",
       "  'algorithms'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  'cover',\n",
       "  'specialized',\n",
       "  'algorithms',\n",
       "  '@',\n",
       "  '@',\n",
       "  'particularly',\n",
       "  'useful',\n",
       "  '@',\n",
       "  'mining',\n",
       "  'text',\n",
       "  'data'],\n",
       " [],\n",
       " ['So',\n",
       "  'looking',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  'problem',\n",
       "  '@',\n",
       "  'closely,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problem',\n",
       "  '@',\n",
       "  'similar',\n",
       "  '@',\n",
       "  'general',\n",
       "  'data',\n",
       "  'mining,',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"we'll\",\n",
       "  '@',\n",
       "  'focusing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['And',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  'algorithms',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  '@',\n",
       "  'turn',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'actionable',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  '(the)',\n",
       "  'real',\n",
       "  'world'],\n",
       " ['Especially',\n",
       "  '@',\n",
       "  'decision',\n",
       "  'making',\n",
       "  '@',\n",
       "  '@',\n",
       "  'completing',\n",
       "  '@',\n",
       "  'tasks',\n",
       "  '@',\n",
       "  'require',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'support,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world',\n",
       "  'problems',\n",
       "  '@',\n",
       "  'data',\n",
       "  'mining,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'kinds',\n",
       "  '@',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'non',\n",
       "  'textual'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  'picture',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'include',\n",
       "  'non',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'reason,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'concerned',\n",
       "  '@',\n",
       "  'joint',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'text',\n",
       "  '@',\n",
       "  'non',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'course',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'focus',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'touch',\n",
       "  '@',\n",
       "  '@',\n",
       "  'join',\n",
       "  '@',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'non-text',\n",
       "  'data'],\n",
       " ['With',\n",
       "  '@',\n",
       "  'problem',\n",
       "  'definition',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'landscape',\n",
       "  '@',\n",
       "  '@',\n",
       "  'topics',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  'analytics'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'slide',\n",
       "  'shows',\n",
       "  '@',\n",
       "  'process',\n",
       "  '@',\n",
       "  'generating',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'detail'],\n",
       " ['Most',\n",
       "  'specifically,',\n",
       "  'human',\n",
       "  'sensor',\n",
       "  '@',\n",
       "  'human',\n",
       "  'observer',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'world',\n",
       "  '@',\n",
       "  '@',\n",
       "  'perspective'],\n",
       " ['Different',\n",
       "  'people',\n",
       "  '@',\n",
       "  '@',\n",
       "  'looking',\n",
       "  '@',\n",
       "  '@',\n",
       "  'world',\n",
       "  '@',\n",
       "  'different',\n",
       "  'angles',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'pay',\n",
       "  'attention',\n",
       "  '@',\n",
       "  'different',\n",
       "  'things'],\n",
       " ['The',\n",
       "  '@',\n",
       "  'person',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'time',\n",
       "  '@',\n",
       "  '@',\n",
       "  'pay',\n",
       "  'attention',\n",
       "  '@',\n",
       "  'different',\n",
       "  'aspects',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observed',\n",
       "  'world'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'human',\n",
       "  'sensor',\n",
       "  '@',\n",
       "  'perceive',\n",
       "  '@',\n",
       "  'world',\n",
       "  '@',\n",
       "  '@',\n",
       "  'perspective'],\n",
       " ['And', '@', 'human'],\n",
       " [],\n",
       " [],\n",
       " ['The',\n",
       "  'sensor',\n",
       "  '@',\n",
       "  '@',\n",
       "  'form',\n",
       "  '@',\n",
       "  'view',\n",
       "  '@',\n",
       "  '@',\n",
       "  'world',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'called',\n",
       "  '@',\n",
       "  'observed',\n",
       "  'world'],\n",
       " ['Of',\n",
       "  'course',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  '@',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'perspective',\n",
       "  '@',\n",
       "  '@',\n",
       "  'person',\n",
       "  '@',\n",
       "  'taken'],\n",
       " ['This', '@', '@', '@', 'biased', '@'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'observable',\n",
       "  'world',\n",
       "  '@',\n",
       "  '@',\n",
       "  'represented',\n",
       "  '@',\n",
       "  '@',\n",
       "  'example',\n",
       "  'entity',\n",
       "  'relation',\n",
       "  'graphs',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  'way,',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  'representation',\n",
       "  'language'],\n",
       " ['But',\n",
       "  '@',\n",
       "  'general,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'basically',\n",
       "  '@',\n",
       "  '@',\n",
       "  'person',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mind',\n",
       "  '@',\n",
       "  '@',\n",
       "  'world,',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"don't\",\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'exactly',\n",
       "  '@',\n",
       "  'looks',\n",
       "  'like,',\n",
       "  '@',\n",
       "  'course'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  'human',\n",
       "  '@',\n",
       "  'express',\n",
       "  '@',\n",
       "  '@',\n",
       "  'person',\n",
       "  '@',\n",
       "  'observed',\n",
       "  '@',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language',\n",
       "  '@',\n",
       "  '@',\n",
       "  'English,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'result',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['Of',\n",
       "  'course,',\n",
       "  '@',\n",
       "  'person',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'language',\n",
       "  '@',\n",
       "  'express',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observed'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'case,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'mixed',\n",
       "  'languages',\n",
       "  '@',\n",
       "  'different',\n",
       "  'languages'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'main',\n",
       "  'goal',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'actually',\n",
       "  '@',\n",
       "  'revert',\n",
       "  '@',\n",
       "  'process',\n",
       "  '@',\n",
       "  'generating',\n",
       "  'test',\n",
       "  'data'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'hope',\n",
       "  '@',\n",
       "  '@',\n",
       "  'able',\n",
       "  '@',\n",
       "  'uncover',\n",
       "  '@',\n",
       "  'aspect',\n",
       "  '@',\n",
       "  '@',\n",
       "  'process'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'specifically',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mining,',\n",
       "  '@',\n",
       "  'example,',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  'language'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'means',\n",
       "  '@',\n",
       "  'looking',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'English,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'able',\n",
       "  '@',\n",
       "  'discover',\n",
       "  '@',\n",
       "  '@',\n",
       "  'English'],\n",
       " [],\n",
       " [],\n",
       " ['Some', 'usage', '@', 'English'],\n",
       " [],\n",
       " [],\n",
       " ['Some', 'patterns', '@', 'English'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '1',\n",
       "  'type',\n",
       "  '@',\n",
       "  'mining',\n",
       "  'problems',\n",
       "  '@',\n",
       "  '@',\n",
       "  'result',\n",
       "  '@',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  'language',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ways'],\n",
       " ['If',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'picture,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  '\"Observed',\n",
       "  'World\"'],\n",
       " ['As',\n",
       "  'so,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'content',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data'],\n",
       " [\"We're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'try',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'essence',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['Or',\n",
       "  'extracting',\n",
       "  'high',\n",
       "  'quality',\n",
       "  'information',\n",
       "  '@',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'aspect',\n",
       "  '@',\n",
       "  '@',\n",
       "  'world',\n",
       "  '@',\n",
       "  \"we're\",\n",
       "  'interested',\n",
       "  '@'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'said',\n",
       "  '@',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'person',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'entity,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'regarded',\n",
       "  '@',\n",
       "  'mining',\n",
       "  'content',\n",
       "  '@',\n",
       "  'describe',\n",
       "  '@',\n",
       "  'observed',\n",
       "  'world',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"user's\",\n",
       "  'mind,',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"person's\",\n",
       "  'mind'],\n",
       " ['If',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'imagine',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observer',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'infer',\n",
       "  '@',\n",
       "  'properties',\n",
       "  '@',\n",
       "  '@',\n",
       "  'person'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'properties',\n",
       "  '@',\n",
       "  'include',\n",
       "  '@',\n",
       "  'mood',\n",
       "  '@',\n",
       "  '@',\n",
       "  'person',\n",
       "  '@',\n",
       "  'sentiment',\n",
       "  '@',\n",
       "  '@',\n",
       "  'person'],\n",
       " ['And',\n",
       "  'note',\n",
       "  '@',\n",
       "  '@',\n",
       "  'distinguish',\n",
       "  '@',\n",
       "  'observed',\n",
       "  '@',\n",
       "  'world',\n",
       "  '@',\n",
       "  '@',\n",
       "  'person',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'describe',\n",
       "  '@',\n",
       "  '@',\n",
       "  'person',\n",
       "  '@',\n",
       "  'observed',\n",
       "  '@',\n",
       "  '@',\n",
       "  'objective',\n",
       "  'way,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'description',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'subject',\n",
       "  '@',\n",
       "  'sentiment,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  'imagine',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'contain',\n",
       "  '@',\n",
       "  'factual',\n",
       "  'descriptions',\n",
       "  '@',\n",
       "  '@',\n",
       "  'world',\n",
       "  'plus',\n",
       "  '@',\n",
       "  'subjective',\n",
       "  'comments,',\n",
       "  '@',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'possible',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  '@',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observer'],\n",
       " ['Finally,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'picture',\n",
       "  '@',\n",
       "  '@',\n",
       "  'left',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'picture,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'certainly',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world,',\n",
       "  'right?',\n",
       "  'So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'infer',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world',\n",
       "  'variables,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'called',\n",
       "  'predictive',\n",
       "  'analytics'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'want',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  'value',\n",
       "  '@',\n",
       "  'certain',\n",
       "  'interesting',\n",
       "  'variables'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'picture',\n",
       "  'basically',\n",
       "  'covered',\n",
       "  'multiple',\n",
       "  'types',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  '@',\n",
       "  'general'],\n",
       " ['When',\n",
       "  '@',\n",
       "  'infer',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world',\n",
       "  'variables,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'results',\n",
       "  '@',\n",
       "  'mining',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'intermediate',\n",
       "  'results',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'prediction'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'content',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'generate',\n",
       "  '@',\n",
       "  'summary',\n",
       "  '@',\n",
       "  'content,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'summary',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  'variables',\n",
       "  '@',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'course,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'generated',\n",
       "  '@',\n",
       "  '@',\n",
       "  'original',\n",
       "  'text',\n",
       "  'data,',\n",
       "  '@',\n",
       "  'I',\n",
       "  'want',\n",
       "  '@',\n",
       "  'emphasize',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'processing',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'generate',\n",
       "  '@',\n",
       "  'features',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  '@',\n",
       "  'prediction,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'important'],\n",
       " ['And',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'results',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mining',\n",
       "  'tasks,',\n",
       "  'including',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'content',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'mining',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observer',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'helpful',\n",
       "  '@',\n",
       "  'prediction'],\n",
       " ['In',\n",
       "  'fact,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'non-text',\n",
       "  'data,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'non-text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'help',\n",
       "  'prediction'],\n",
       " ['And', '@', 'course,', '@', 'depends', '@', '@', 'problem'],\n",
       " ['In',\n",
       "  'general,',\n",
       "  'non-text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'important',\n",
       "  '@',\n",
       "  '@',\n",
       "  'prediction',\n",
       "  'tasks'],\n",
       " ['For', 'example,', '@', '@', 'want', '@', 'predict', '@', 'stocks'],\n",
       " ['Stock',\n",
       "  'prices',\n",
       "  '@',\n",
       "  'changes',\n",
       "  '@',\n",
       "  'stock',\n",
       "  'prices',\n",
       "  'based',\n",
       "  '@',\n",
       "  'discussion',\n",
       "  '@',\n",
       "  '@',\n",
       "  'news',\n",
       "  'articles',\n",
       "  '@',\n",
       "  '@',\n",
       "  'social',\n",
       "  'media,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'example',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world',\n",
       "  'variables'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case,',\n",
       "  'obviously',\n",
       "  '@',\n",
       "  'historical',\n",
       "  'stock',\n",
       "  'price',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'important',\n",
       "  '@',\n",
       "  '@',\n",
       "  'prediction,',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"that's\",\n",
       "  'example',\n",
       "  '@',\n",
       "  'non-text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  '@',\n",
       "  '@',\n",
       "  'prediction',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'combine',\n",
       "  '@',\n",
       "  'kinds',\n",
       "  '@',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'prediction'],\n",
       " ['Now',\n",
       "  'non-text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  '@',\n",
       "  'analyzing',\n",
       "  'text',\n",
       "  '@',\n",
       "  'supplying',\n",
       "  'context'],\n",
       " ['When',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'looking',\n",
       "  '@',\n",
       "  '@',\n",
       "  'content',\n",
       "  '@',\n",
       "  'opinions',\n",
       "  'expressed',\n",
       "  '@',\n",
       "  'text'],\n",
       " ['But', 'text', 'data', 'generally', '@', '@', 'context', 'associated'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  'time,',\n",
       "  '@',\n",
       "  'location,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'associated',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  'context',\n",
       "  'information'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'provide',\n",
       "  'interesting',\n",
       "  'angles',\n",
       "  '@',\n",
       "  'analyzing',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'partition',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'different',\n",
       "  'time',\n",
       "  'periods',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'availability',\n",
       "  '@',\n",
       "  'time'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  'analyze',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'time',\n",
       "  'period',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'comparison'],\n",
       " ['Similarly,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'partition',\n",
       "  'text',\n",
       "  'data',\n",
       "  'based',\n",
       "  '@',\n",
       "  'locations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'metadata',\n",
       "  \"that's\",\n",
       "  'associated',\n",
       "  '@',\n",
       "  'form',\n",
       "  'interesting',\n",
       "  'comparison',\n",
       "  'scenarios'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sense,',\n",
       "  'non-text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'actually',\n",
       "  'provide',\n",
       "  'interesting',\n",
       "  'angles',\n",
       "  '@',\n",
       "  'perspectives',\n",
       "  '@',\n",
       "  'text',\n",
       "  'analysis,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  'sensitive',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  'content',\n",
       "  '@',\n",
       "  '@',\n",
       "  'language',\n",
       "  'usage',\n",
       "  '@',\n",
       "  '@',\n",
       "  'opinions',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observer',\n",
       "  '@',\n",
       "  '@',\n",
       "  'authors',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['We',\n",
       "  '@',\n",
       "  'analyze',\n",
       "  '@',\n",
       "  'sentiment',\n",
       "  '@',\n",
       "  'different',\n",
       "  'context,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'fairly',\n",
       "  'general',\n",
       "  'landscape',\n",
       "  '@',\n",
       "  '@',\n",
       "  'topics',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'analytics'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'course',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'selectively',\n",
       "  'cover',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'topics'],\n",
       " ['We', 'actually', 'hope', '@', 'cover', '@', '@', '@', 'general', 'topics'],\n",
       " ['First,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'going',\n",
       "  '@',\n",
       "  'cover',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  '@',\n",
       "  'briefly',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'understanding',\n",
       "  'text',\n",
       "  'data,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'determines',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'represent',\n",
       "  'text',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining'],\n",
       " ['Second,',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'associations',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'word',\n",
       "  'associations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'form',\n",
       "  '@',\n",
       "  'useful',\n",
       "  'lexical',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  'language'],\n",
       " ['Third,',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  'topic',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'analysis,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'way',\n",
       "  '@',\n",
       "  'analyze',\n",
       "  'content',\n",
       "  '@',\n",
       "  'text,',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  'way',\n",
       "  '@',\n",
       "  'analyzing',\n",
       "  'content'],\n",
       " [\"It's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining'],\n",
       " ['And',\n",
       "  '@',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  'opinion',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'sentiment',\n",
       "  'analysis'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'regarded',\n",
       "  '@',\n",
       "  '@',\n",
       "  'example',\n",
       "  '@',\n",
       "  'mining',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observer'],\n",
       " ['And',\n",
       "  'finally,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'going',\n",
       "  '@',\n",
       "  'cover',\n",
       "  '@',\n",
       "  'text',\n",
       "  'based',\n",
       "  'prediction',\n",
       "  'problems',\n",
       "  '@',\n",
       "  '@',\n",
       "  'try',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world',\n",
       "  'variable',\n",
       "  'based',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'slide',\n",
       "  '@',\n",
       "  'serves',\n",
       "  '@',\n",
       "  '@',\n",
       "  'road',\n",
       "  'map',\n",
       "  '@',\n",
       "  '@',\n",
       "  'course'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  '@',\n",
       "  'outline',\n",
       "  '@',\n",
       "  '@',\n",
       "  'topics',\n",
       "  '@',\n",
       "  '@',\n",
       "  'cover',\n",
       "  '@',\n",
       "  '@',\n",
       "  'rest',\n",
       "  '@',\n",
       "  '@',\n",
       "  'course'],\n",
       " [],\n",
       " ['This', 'lecture', '@', '@', '@', 'natural聽language', 'content', 'analysis'],\n",
       " ['Natural',\n",
       "  'language',\n",
       "  'content',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  '@',\n",
       "  'foundation',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining'],\n",
       " ['So', '@', '@', 'going', '@', '@', 'talk', '@', '@'],\n",
       " ['And', '@', 'particular,', 'natural', 'language', 'processing'],\n",
       " ['@',\n",
       "  '@',\n",
       "  'factor',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'represent',\n",
       "  'text',\n",
       "  'data?',\n",
       "  'And',\n",
       "  '@',\n",
       "  'determines',\n",
       "  '@',\n",
       "  'algorithms',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'analyze',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data'],\n",
       " [\"We're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'basic',\n",
       "  'concepts',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language',\n",
       "  '@'],\n",
       " [\"I'm\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'explain',\n",
       "  '@',\n",
       "  'concepts',\n",
       "  '@',\n",
       "  '@',\n",
       "  'simple',\n",
       "  'example',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'seeing',\n",
       "  '@'],\n",
       " ['A', 'dog', '@', 'chasing', '@', 'boy', '@', '@', 'playground'],\n",
       " ['Now', '@', '@', '@', '@', 'simple', 'sentence'],\n",
       " ['When',\n",
       "  '@',\n",
       "  'read',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence,',\n",
       "  '@',\n",
       "  \"don't\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'meaning',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  'computer',\n",
       "  '@',\n",
       "  '@',\n",
       "  'understand',\n",
       "  '@',\n",
       "  'sentence,',\n",
       "  '@',\n",
       "  'computer',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'steps'],\n",
       " ['First,',\n",
       "  '@',\n",
       "  'computer',\n",
       "  'needs',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['In',\n",
       "  'English',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'easy',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'space',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'computer',\n",
       "  '@',\n",
       "  'need',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'categories',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words,',\n",
       "  'syntactical',\n",
       "  'categories'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'example',\n",
       "  'Dog',\n",
       "  '@',\n",
       "  '@',\n",
       "  'noun,',\n",
       "  'chasing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'verb,',\n",
       "  'boy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'noun,',\n",
       "  'etc'],\n",
       " ['And', '@', '@', 'called', '@', 'lexical', 'analysis'],\n",
       " ['In',\n",
       "  'particular',\n",
       "  'tagging',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'syntactic',\n",
       "  'categories',\n",
       "  '@',\n",
       "  'called',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'speech',\n",
       "  'tagging'],\n",
       " ['After',\n",
       "  'that,',\n",
       "  '@',\n",
       "  'computer',\n",
       "  '@',\n",
       "  'needs',\n",
       "  '@',\n",
       "  'figure',\n",
       "  '@',\n",
       "  '@',\n",
       "  'relation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['So', 'A', '@', '@', 'dog', '@', 'form', '@', 'noun', 'phrase'],\n",
       " ['On', '@', 'playground', '@', '@', '@', 'prepositional', 'phrase,', 'etc'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'certain',\n",
       "  'way',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'connected',\n",
       "  '@',\n",
       "  '@',\n",
       "  'order',\n",
       "  '@',\n",
       "  'generate',\n",
       "  '@',\n",
       "  'meaning'],\n",
       " ['Some', '@', 'combinations', '@', '@', '@', 'sense'],\n",
       " ['And', '@'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['This', '@', 'called', 'syntactic', 'parsing'],\n",
       " ['Or',\n",
       "  'syntactical',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  'parsing',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'sentence'],\n",
       " ['The', 'outcome', '@', 'parse', 'tree', '@', \"you're\", 'seeing', '@'],\n",
       " ['That',\n",
       "  'tells',\n",
       "  '@',\n",
       "  '@',\n",
       "  'structure',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interpret',\n",
       "  '@',\n",
       "  'sentence'],\n",
       " ['But', '@', '@', '@', 'semantics', '@'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'order',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'meeting',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'map',\n",
       "  '@',\n",
       "  'phrases',\n",
       "  '@',\n",
       "  '@',\n",
       "  'structures',\n",
       "  '@',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world',\n",
       "  'entities',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mind'],\n",
       " ['So',\n",
       "  'dog',\n",
       "  '@',\n",
       "  '@',\n",
       "  'concept',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'A',\n",
       "  'boy,',\n",
       "  '@',\n",
       "  'concept',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know'],\n",
       " ['So',\n",
       "  'connecting',\n",
       "  '@',\n",
       "  'phrases',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'understanding'],\n",
       " ['For',\n",
       "  'computer,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'formally',\n",
       "  'represent',\n",
       "  '@',\n",
       "  'entities',\n",
       "  '@',\n",
       "  '@',\n",
       "  'symbols'],\n",
       " ['So',\n",
       "  'dog',\n",
       "  'd1',\n",
       "  'means',\n",
       "  'd1',\n",
       "  '@',\n",
       "  '@',\n",
       "  'dog,',\n",
       "  'boy',\n",
       "  'b1',\n",
       "  'means',\n",
       "  'b1',\n",
       "  'refers',\n",
       "  '@',\n",
       "  '@',\n",
       "  'boy,',\n",
       "  'etc'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'represented',\n",
       "  '@',\n",
       "  'chasing',\n",
       "  'action',\n",
       "  '@',\n",
       "  '@',\n",
       "  'predicate'],\n",
       " ['So',\n",
       "  'chasing',\n",
       "  '@',\n",
       "  'predic',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'arguments,',\n",
       "  'd1,',\n",
       "  'b1',\n",
       "  '@',\n",
       "  'p1,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'playground,',\n",
       "  'right?',\n",
       "  'So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'formal',\n",
       "  'representation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'semantics',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence'],\n",
       " ['Once',\n",
       "  '@',\n",
       "  'reach',\n",
       "  '@',\n",
       "  'level',\n",
       "  '@',\n",
       "  'understanding,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'inferences'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'assume',\n",
       "  \"there's\",\n",
       "  '@',\n",
       "  'rule',\n",
       "  '@',\n",
       "  'says',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'chased',\n",
       "  '@',\n",
       "  '@',\n",
       "  'person',\n",
       "  '@',\n",
       "  '@',\n",
       "  'scared,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'infer',\n",
       "  '@',\n",
       "  'boy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'scared'],\n",
       " ['This',\n",
       "  '@',\n",
       "  '@',\n",
       "  'inferred',\n",
       "  'meaning',\n",
       "  'based',\n",
       "  '@',\n",
       "  '@',\n",
       "  'additional',\n",
       "  'knowledge'],\n",
       " ['And', 'finally,', '@', '@', '@', '@', '@', 'infer'],\n",
       " [],\n",
       " [],\n",
       " ['@',\n",
       "  '@',\n",
       "  'infer',\n",
       "  '@',\n",
       "  'This',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  'requesting',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'person',\n",
       "  '@',\n",
       "  'said',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  'saying',\n",
       "  '@',\n",
       "  'sentence'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'understanding',\n",
       "  '@',\n",
       "  'purpose',\n",
       "  '@',\n",
       "  'saying',\n",
       "  '@',\n",
       "  'sentence,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'called',\n",
       "  'SPEECH',\n",
       "  'Act',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  'pragmatic',\n",
       "  'analysis'],\n",
       " ['Which', 'refers', '@', '@', 'use', '@', 'language'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case,',\n",
       "  'person',\n",
       "  'saying',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'reminding',\n",
       "  '@',\n",
       "  'person',\n",
       "  '@',\n",
       "  'bring',\n",
       "  '@',\n",
       "  '@',\n",
       "  'dog'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'means',\n",
       "  '@',\n",
       "  'saying',\n",
       "  '@',\n",
       "  'sentence,',\n",
       "  '@',\n",
       "  'person',\n",
       "  'actually',\n",
       "  'takes',\n",
       "  '@',\n",
       "  'action'],\n",
       " ['So', '@', 'action', '@', '@', '@', '@', '@', 'request'],\n",
       " ['Now,',\n",
       "  '@',\n",
       "  'slide',\n",
       "  'clearly',\n",
       "  'shows',\n",
       "  '@',\n",
       "  '@',\n",
       "  'order',\n",
       "  '@',\n",
       "  '@',\n",
       "  'understand',\n",
       "  '@',\n",
       "  'sentence,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'things',\n",
       "  '@',\n",
       "  '@',\n",
       "  'computer',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['In',\n",
       "  'general,',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'hard',\n",
       "  '@',\n",
       "  '@',\n",
       "  'computer',\n",
       "  '@',\n",
       "  '@',\n",
       "  'everything,',\n",
       "  'especially',\n",
       "  '@',\n",
       "  '@',\n",
       "  'wanted',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'correctly'],\n",
       " ['This', '@', '@', '@', 'difficult'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'main',\n",
       "  'reason',\n",
       "  '@',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'difficult',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  'designed',\n",
       "  '@',\n",
       "  '@',\n",
       "  'human',\n",
       "  'communications',\n",
       "  'efficient'],\n",
       " ['As',\n",
       "  '@',\n",
       "  'result,',\n",
       "  '@',\n",
       "  'example,',\n",
       "  '@',\n",
       "  'omit',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'common',\n",
       "  'sense',\n",
       "  'knowledge'],\n",
       " ['Because', '@', 'assume', '@', '@'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'knowledge,',\n",
       "  \"there's\",\n",
       "  '@',\n",
       "  'need',\n",
       "  '@',\n",
       "  'encode',\n",
       "  '@',\n",
       "  'knowledge'],\n",
       " ['And', '@', 'makes', 'communication', 'efficient'],\n",
       " ['We',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'ambiguities,',\n",
       "  'like',\n",
       "  'ambiguities',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'assume',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ability',\n",
       "  '@',\n",
       "  'disambiguate',\n",
       "  '@',\n",
       "  'word,',\n",
       "  '@',\n",
       "  \"there's\",\n",
       "  '@',\n",
       "  'problem',\n",
       "  '@',\n",
       "  'having',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  'mean,',\n",
       "  'possibly',\n",
       "  'different',\n",
       "  'things',\n",
       "  '@',\n",
       "  'different',\n",
       "  'context'],\n",
       " ['Yet,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'computer',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'difficult',\n",
       "  '@',\n",
       "  '@',\n",
       "  'computer',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'common',\n",
       "  'sense',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  'do,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'computer',\n",
       "  '@',\n",
       "  '@',\n",
       "  'confused',\n",
       "  'indeed,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'makes',\n",
       "  '@',\n",
       "  'hard',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing'],\n",
       " ['Indeed,',\n",
       "  '@',\n",
       "  'makes',\n",
       "  '@',\n",
       "  '@',\n",
       "  'hard',\n",
       "  '@',\n",
       "  '@',\n",
       "  'step',\n",
       "  '@',\n",
       "  '@',\n",
       "  'slide',\n",
       "  '@',\n",
       "  'I',\n",
       "  'showed',\n",
       "  '@',\n",
       "  'earlier'],\n",
       " ['Ambiguity',\n",
       "  '@',\n",
       "  '@',\n",
       "  'main',\n",
       "  'killer,',\n",
       "  'meaning',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'step',\n",
       "  '@',\n",
       "  '@',\n",
       "  'multiple',\n",
       "  'choices',\n",
       "  '@',\n",
       "  '@',\n",
       "  'computer',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'decide',\n",
       "  \"what's\",\n",
       "  '@',\n",
       "  'right',\n",
       "  'choice',\n",
       "  '@',\n",
       "  '@',\n",
       "  'decision',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'difficult,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'moment'],\n",
       " ['And', '@', 'general', '@', 'need', 'common', 'sense', 'reasoning'],\n",
       " ['In',\n",
       "  'order',\n",
       "  '@',\n",
       "  'fully',\n",
       "  'understand',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language',\n",
       "  '@',\n",
       "  'computers',\n",
       "  'today',\n",
       "  \"don't\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'that,',\n",
       "  '@',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'hard',\n",
       "  '@',\n",
       "  'computers',\n",
       "  '@',\n",
       "  'precisely',\n",
       "  'understanding',\n",
       "  'natural',\n",
       "  'language',\n",
       "  '@',\n",
       "  '@',\n",
       "  'point'],\n",
       " ['@', '@', '@', '@', 'specific', 'examples', '@', 'challenges'],\n",
       " ['Think',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'level',\n",
       "  'ambiguity',\n",
       "  '@',\n",
       "  'word',\n",
       "  'like',\n",
       "  'design',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'noun',\n",
       "  '@',\n",
       "  '@',\n",
       "  'verb,',\n",
       "  '@',\n",
       "  \"we've\",\n",
       "  'got',\n",
       "  'ambiguous',\n",
       "  '@',\n",
       "  '@',\n",
       "  'speech',\n",
       "  'tag'],\n",
       " ['Root', '@', '@', 'multiple', 'meanings'],\n",
       " ['It',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mathematical',\n",
       "  'sense,',\n",
       "  'like',\n",
       "  '@',\n",
       "  'square',\n",
       "  'root'],\n",
       " ['Or', '@', '@', '@', '@', 'root', '@', '@', 'plant'],\n",
       " ['Syntactic',\n",
       "  'ambiguity',\n",
       "  'refers',\n",
       "  '@',\n",
       "  'different',\n",
       "  'interpretations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  'terms',\n",
       "  '@',\n",
       "  'structures'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'example,',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  '@',\n",
       "  'actually',\n",
       "  '@',\n",
       "  'interpreted',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ways'],\n",
       " ['So', '@', '@'],\n",
       " ['The', 'ordinary', 'meaning', '@', '@', '@', '@', 'getting'],\n",
       " ['As',\n",
       "  '@',\n",
       "  'talking',\n",
       "  '@',\n",
       "  '@',\n",
       "  'topic',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  'processing',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'possible',\n",
       "  'interpretation,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'language',\n",
       "  'processing',\n",
       "  '@',\n",
       "  'natural'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  \"don't\",\n",
       "  'generally',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problem,',\n",
       "  '@',\n",
       "  'imagine',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'computer',\n",
       "  '@',\n",
       "  'determine',\n",
       "  '@',\n",
       "  'structure,',\n",
       "  '@',\n",
       "  'computer',\n",
       "  '@',\n",
       "  'actually',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'choice',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['Another',\n",
       "  'classic',\n",
       "  'example',\n",
       "  '@',\n",
       "  '@',\n",
       "  'man',\n",
       "  'saw',\n",
       "  '@',\n",
       "  'boy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'telescope'],\n",
       " ['This',\n",
       "  'ambiguity',\n",
       "  'lies',\n",
       "  '@',\n",
       "  '@',\n",
       "  'question',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'telescope'],\n",
       " ['This聽',\n",
       "  '@',\n",
       "  'called',\n",
       "  '@',\n",
       "  'prepositional',\n",
       "  'phrase',\n",
       "  'attachment',\n",
       "  'ambiguity',\n",
       "  'meaning'],\n",
       " [],\n",
       " [],\n",
       " ['@',\n",
       "  '@',\n",
       "  'attach',\n",
       "  '@',\n",
       "  'prepositional',\n",
       "  'phrase',\n",
       "  '@',\n",
       "  '@',\n",
       "  'telescope?',\n",
       "  'Should',\n",
       "  '@',\n",
       "  'modify',\n",
       "  '@',\n",
       "  'boy',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'modifying',\n",
       "  'saw,',\n",
       "  '@',\n",
       "  'verb?',\n",
       "  'Another',\n",
       "  'problem',\n",
       "  'Anaphora',\n",
       "  'resolution',\n",
       "  '\"John',\n",
       "  'persuaded',\n",
       "  'Bill',\n",
       "  '@',\n",
       "  'buy',\n",
       "  '@',\n",
       "  'TV',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['\"',\n",
       "  'Does',\n",
       "  '@',\n",
       "  'referred',\n",
       "  '@',\n",
       "  'John',\n",
       "  '@',\n",
       "  'Bill?',\n",
       "  'Pre',\n",
       "  'supposition',\n",
       "  '@',\n",
       "  '@',\n",
       "  'difficulty'],\n",
       " ['He',\n",
       "  '@',\n",
       "  'quit',\n",
       "  'Smoking',\n",
       "  'implies',\n",
       "  '@',\n",
       "  '@',\n",
       "  'smoked',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'need',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  'order',\n",
       "  '@',\n",
       "  'understand',\n",
       "  '@',\n",
       "  'languages'],\n",
       " ['Because',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problems,',\n",
       "  '@',\n",
       "  'state',\n",
       "  '@',\n",
       "  '@',\n",
       "  'art',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'perfectly,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'simplest',\n",
       "  '@',\n",
       "  '@',\n",
       "  'speech',\n",
       "  'tagging,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'solve',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problem'],\n",
       " ['The',\n",
       "  'accuracy',\n",
       "  '@',\n",
       "  'I',\n",
       "  'listed',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '97%',\n",
       "  '@',\n",
       "  '@',\n",
       "  'taken',\n",
       "  '@',\n",
       "  '@',\n",
       "  'studies',\n",
       "  'earlier,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'studies',\n",
       "  'obviously',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'datasets,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'numbers',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'meaningful',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'data',\n",
       "  'set',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'evaluation,',\n",
       "  '@',\n",
       "  'I',\n",
       "  '@',\n",
       "  '@',\n",
       "  'numbers',\n",
       "  '@',\n",
       "  'need',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sense',\n",
       "  '@',\n",
       "  '@',\n",
       "  'accuracy',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'things',\n",
       "  'like',\n",
       "  '@'],\n",
       " ['It',\n",
       "  \"doesn't\",\n",
       "  'mean',\n",
       "  '@',\n",
       "  '@',\n",
       "  'data',\n",
       "  'set',\n",
       "  '@',\n",
       "  '@',\n",
       "  'accuracy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'precisely',\n",
       "  '97%'],\n",
       " ['But',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'speech',\n",
       "  'tagging',\n",
       "  'fairly',\n",
       "  'well,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'perfectly'],\n",
       " ['Parsing',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'difficult,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'partial',\n",
       "  'parsing,',\n",
       "  'meaning',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'phrases',\n",
       "  'correct,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probably',\n",
       "  'achieve',\n",
       "  '90%',\n",
       "  '@',\n",
       "  'better',\n",
       "  'accuracy'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'complete',\n",
       "  'parse',\n",
       "  'tree',\n",
       "  'correctly',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'difficult'],\n",
       " ['For',\n",
       "  'semantic',\n",
       "  'analysis,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'aspects',\n",
       "  '@',\n",
       "  'semantic',\n",
       "  'analysis,',\n",
       "  'particularly',\n",
       "  'extraction',\n",
       "  '@',\n",
       "  'entities',\n",
       "  '@',\n",
       "  'relations'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  'recognizing',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'person,',\n",
       "  \"that's\",\n",
       "  'the聽',\n",
       "  'location,',\n",
       "  '@',\n",
       "  'person',\n",
       "  '@',\n",
       "  '@',\n",
       "  'person',\n",
       "  'met',\n",
       "  '@',\n",
       "  '@',\n",
       "  'place',\n",
       "  'etc'],\n",
       " ['@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'sense',\n",
       "  'disambiguation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'extent'],\n",
       " ['We',\n",
       "  '@',\n",
       "  'figure',\n",
       "  '@',\n",
       "  '@',\n",
       "  'occurrence',\n",
       "  '@',\n",
       "  'root',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  'refers',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mathematical',\n",
       "  'sense',\n",
       "  'etc'],\n",
       " ['Sentiment',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  '@',\n",
       "  'aspect',\n",
       "  '@',\n",
       "  'semantic',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['That',\n",
       "  'means',\n",
       "  '@',\n",
       "  '@',\n",
       "  'tag',\n",
       "  '@',\n",
       "  'sentences',\n",
       "  'general',\n",
       "  'positive',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  'talking',\n",
       "  '@',\n",
       "  'product'],\n",
       " ['Or', 'talking', '@', '@', 'person'],\n",
       " ['Influence,',\n",
       "  'however,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'hard',\n",
       "  '@',\n",
       "  '@',\n",
       "  'generally',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'big',\n",
       "  'domain,',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'feasible',\n",
       "  '@',\n",
       "  '@',\n",
       "  'limited',\n",
       "  'domain'],\n",
       " ['And',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  'generally',\n",
       "  'difficult',\n",
       "  'problem',\n",
       "  '@',\n",
       "  'artificial',\n",
       "  'intelligence'],\n",
       " ['Speech',\n",
       "  'Act',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'difficult,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'properly',\n",
       "  '@',\n",
       "  '@',\n",
       "  'specialized',\n",
       "  'cases',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'human'],\n",
       " ['To', 'annotate', '@', 'data', '@', '@', 'computer', '@', 'learn', '@'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'slides',\n",
       "  '@',\n",
       "  'shows',\n",
       "  '@',\n",
       "  'computers',\n",
       "  '@',\n",
       "  'far',\n",
       "  '@',\n",
       "  '@',\n",
       "  'able',\n",
       "  '@',\n",
       "  'understand',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'precisely,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'explains',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  'problem',\n",
       "  '@',\n",
       "  'difficult,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'rely',\n",
       "  '@',\n",
       "  'mechanical',\n",
       "  'approaces',\n",
       "  '@',\n",
       "  'computational',\n",
       "  'methods',\n",
       "  '@',\n",
       "  'understand',\n",
       "  '@',\n",
       "  'language',\n",
       "  'precisely'],\n",
       " ['Therefore,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'today',\n",
       "  'particular',\n",
       "  'statistical',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'methods,',\n",
       "  '@'],\n",
       " ['Statistical',\n",
       "  'analysis',\n",
       "  'methods',\n",
       "  '@',\n",
       "  'try',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'meaning',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  '@',\n",
       "  'possible'],\n",
       " ['And',\n",
       "  'later',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'actually',\n",
       "  '@',\n",
       "  '@',\n",
       "  'algorithms',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'extract',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  'text,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'fully',\n",
       "  'understand',\n",
       "  '@',\n",
       "  'meaning',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'sentences',\n",
       "  'precisely'],\n",
       " [],\n",
       " ['So,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'specific',\n",
       "  'examples',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"can't\",\n",
       "  '@',\n",
       "  'today,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'speech',\n",
       "  'tagging',\n",
       "  '@',\n",
       "  '@',\n",
       "  'easy',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'percent',\n",
       "  'correctly'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'example:',\n",
       "  '\"He',\n",
       "  'turned',\n",
       "  '@',\n",
       "  '@',\n",
       "  'highway\"',\n",
       "  'versus',\n",
       "  '\"He',\n",
       "  'turned',\n",
       "  '@',\n",
       "  '@',\n",
       "  'fan\"',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '\"off\"\\'s',\n",
       "  'actually',\n",
       "  '@',\n",
       "  'somewhat',\n",
       "  'different',\n",
       "  'syntactic',\n",
       "  'categories'],\n",
       " ['And',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'difficult',\n",
       "  '@',\n",
       "  '@',\n",
       "  'complete',\n",
       "  'parsing',\n",
       "  'correct'],\n",
       " ['Again,',\n",
       "  '@',\n",
       "  'example',\n",
       "  '\"A',\n",
       "  'man',\n",
       "  'saw',\n",
       "  '@',\n",
       "  'boy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'telescope\"',\n",
       "  '@',\n",
       "  'actually',\n",
       "  '@',\n",
       "  '@',\n",
       "  'difficult',\n",
       "  '@',\n",
       "  'parse',\n",
       "  'depending',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['Precise', 'deep', 'semantic', 'analysis', '@', '@', '@', 'hard'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  'define',\n",
       "  '@',\n",
       "  'meaning',\n",
       "  '@',\n",
       "  '\"own\"',\n",
       "  'precisely',\n",
       "  '@',\n",
       "  '@',\n",
       "  'difficult',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  'like',\n",
       "  '\"John',\n",
       "  'owns',\n",
       "  '@',\n",
       "  'restaurant\"'],\n",
       " ['So', '@', 'state', '@', '@', 'art', '@', '@', 'summarized', '@', 'follows'],\n",
       " ['Robust',\n",
       "  '@',\n",
       "  'general',\n",
       "  'NLP',\n",
       "  'tends',\n",
       "  '@',\n",
       "  '@',\n",
       "  'shallow,',\n",
       "  '@',\n",
       "  'deep',\n",
       "  'understanding',\n",
       "  '@',\n",
       "  '@',\n",
       "  'scale',\n",
       "  '@'],\n",
       " ['For',\n",
       "  '@',\n",
       "  'reason,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'course,',\n",
       "  '@',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  '@',\n",
       "  'cover',\n",
       "  'are,',\n",
       "  '@',\n",
       "  'general,',\n",
       "  'shallow',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  'analyzing',\n",
       "  'text',\n",
       "  'data,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'generally',\n",
       "  'based',\n",
       "  '@',\n",
       "  'statistical',\n",
       "  'analysis,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'robust,',\n",
       "  '@',\n",
       "  'general,',\n",
       "  'and,',\n",
       "  'And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'category',\n",
       "  '@',\n",
       "  'shallow',\n",
       "  'analysis,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  '@',\n",
       "  'advantage',\n",
       "  '@',\n",
       "  '@',\n",
       "  'able',\n",
       "  '@',\n",
       "  '@',\n",
       "  'applied',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language',\n",
       "  '@',\n",
       "  '@',\n",
       "  'topic'],\n",
       " ['But',\n",
       "  '@',\n",
       "  'downside',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"don't\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'deeper',\n",
       "  'understanding',\n",
       "  '@',\n",
       "  'text'],\n",
       " ['For',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'rely',\n",
       "  '@',\n",
       "  'deeper',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'analysis',\n",
       "  'techniques'],\n",
       " ['That',\n",
       "  'typically',\n",
       "  '@',\n",
       "  'require',\n",
       "  'human',\n",
       "  'effort',\n",
       "  '@',\n",
       "  'annotate',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'examples',\n",
       "  '@',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  \"we'd\",\n",
       "  'like',\n",
       "  '@',\n",
       "  'do,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'computers',\n",
       "  '@',\n",
       "  'use',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  'learn',\n",
       "  '@',\n",
       "  '@',\n",
       "  'training',\n",
       "  'examples',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'task'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'practical',\n",
       "  'applications',\n",
       "  '@',\n",
       "  'generally',\n",
       "  'combine',\n",
       "  '@',\n",
       "  '@',\n",
       "  'kinds',\n",
       "  '@',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  'statistical',\n",
       "  '@',\n",
       "  'methods',\n",
       "  '@',\n",
       "  'backbone',\n",
       "  '@',\n",
       "  '@',\n",
       "  'basis,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'applied',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'use',\n",
       "  'humans',\n",
       "  '@',\n",
       "  'annotate',\n",
       "  '@',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  'supervised',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'tasks',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'can,',\n",
       "  'especially',\n",
       "  '@',\n",
       "  '@',\n",
       "  'important',\n",
       "  'tasks'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'bring',\n",
       "  'humans',\n",
       "  'in,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'loop',\n",
       "  '@',\n",
       "  'analyze,',\n",
       "  'fix,',\n",
       "  '@',\n",
       "  'analyze',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'precisely'],\n",
       " ['But',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  'cover',\n",
       "  '@',\n",
       "  'general',\n",
       "  'statistical',\n",
       "  'approaches',\n",
       "  '@',\n",
       "  'generally',\n",
       "  \"don't\",\n",
       "  'require',\n",
       "  '@',\n",
       "  'human',\n",
       "  'effort'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'practically',\n",
       "  '@',\n",
       "  'useful',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'deeper',\n",
       "  'analysis',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  'require',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'human',\n",
       "  'effort',\n",
       "  '@',\n",
       "  'annotate',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'summarize',\n",
       "  '@',\n",
       "  'lecture,',\n",
       "  '@',\n",
       "  'main',\n",
       "  'points',\n",
       "  '@',\n",
       "  '@',\n",
       "  'away',\n",
       "  'are,',\n",
       "  '@',\n",
       "  'NLP',\n",
       "  '@',\n",
       "  '@',\n",
       "  'foundation',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining'],\n",
       " ['So',\n",
       "  'obviously',\n",
       "  '@',\n",
       "  'better',\n",
       "  '@',\n",
       "  '@',\n",
       "  'understand',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data,',\n",
       "  '@',\n",
       "  'better',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining'],\n",
       " ['Computers',\n",
       "  'today',\n",
       "  '@',\n",
       "  'far',\n",
       "  '@',\n",
       "  '@',\n",
       "  'able',\n",
       "  '@',\n",
       "  'understand',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language'],\n",
       " ['Deeper',\n",
       "  'NLP',\n",
       "  'requires',\n",
       "  'common',\n",
       "  'sense',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  'inferences,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'working',\n",
       "  '@',\n",
       "  '@',\n",
       "  'limited',\n",
       "  'domains'],\n",
       " ['Not', 'feasible', '@', 'large', 'scale', 'text', 'mining'],\n",
       " ['Shallow',\n",
       "  'NLP',\n",
       "  'based',\n",
       "  '@',\n",
       "  'statistical',\n",
       "  'methods',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'large',\n",
       "  'scale'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'main',\n",
       "  'topic',\n",
       "  '@',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'generally',\n",
       "  'applicable',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'applications'],\n",
       " ['They', '@', '@', '@', 'sense', '@', '@', 'useful', 'techniques'],\n",
       " ['In', 'practice,', '@', 'use', 'statistical', 'NLP', '@', '@', 'basis'],\n",
       " ['And', '@', '@', 'humans', '@', 'help', '@', 'needed', '@', '@', 'ways'],\n",
       " [],\n",
       " ['This', 'lecture', '@', '@', '@', 'text', 'representation'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'lecture',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'discuss',\n",
       "  'text',\n",
       "  'representation'],\n",
       " ['And',\n",
       "  'discuss',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  '@',\n",
       "  'allow',\n",
       "  '@',\n",
       "  '@',\n",
       "  'represent',\n",
       "  'text',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'ways'],\n",
       " [\"Let's\", '@', '@', 'look', '@', '@', 'example', 'sentence', '@'],\n",
       " ['We', '@', 'represent', '@', 'sentence', '@', '@', 'different', 'ways'],\n",
       " ['1st'],\n",
       " ['We',\n",
       "  '@',\n",
       "  '@',\n",
       "  'represent',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'string',\n",
       "  '@',\n",
       "  'characters'],\n",
       " ['This',\n",
       "  '@',\n",
       "  'true',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'languages',\n",
       "  '@',\n",
       "  '@',\n",
       "  'store',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'computer'],\n",
       " ['When',\n",
       "  '@',\n",
       "  'store',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'string',\n",
       "  '@',\n",
       "  'characters,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  'way',\n",
       "  '@',\n",
       "  'representing',\n",
       "  'text,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'approach',\n",
       "  '@',\n",
       "  'represent',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['But',\n",
       "  'unfortunately,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'representation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  '@',\n",
       "  'semantic',\n",
       "  'analysis,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'needed',\n",
       "  '@',\n",
       "  '@',\n",
       "  'applications',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining'],\n",
       " ['The', 'reason', '@', '@', \"we're\", '@', '@', 'recognizing', 'words'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'string',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'spaces',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ASCII',\n",
       "  'symbols'],\n",
       " ['We', '@', '@', 'count', '@'],\n",
       " [],\n",
       " [],\n",
       " [\"what's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'frequent',\n",
       "  'character',\n",
       "  '@',\n",
       "  'English',\n",
       "  'text,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'correlation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'characters,',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"can't\",\n",
       "  '@',\n",
       "  'analyze',\n",
       "  'semantics'],\n",
       " ['Yet',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  'way',\n",
       "  '@',\n",
       "  'representing',\n",
       "  'text,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  '@',\n",
       "  'represent',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'text'],\n",
       " ['If',\n",
       "  '@',\n",
       "  'try',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'little',\n",
       "  'bit',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'segmentation'],\n",
       " ['Then',\n",
       "  '@',\n",
       "  '@',\n",
       "  'obtain',\n",
       "  '@',\n",
       "  'representation',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'form',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sequence',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'identify',\n",
       "  'words',\n",
       "  'like:',\n",
       "  'a,',\n",
       "  'dog,',\n",
       "  'is,',\n",
       "  'chasing,',\n",
       "  'etc'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  'level',\n",
       "  '@',\n",
       "  'representation,',\n",
       "  '@',\n",
       "  'certainly',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'things,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mainly',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'basic',\n",
       "  'units',\n",
       "  '@',\n",
       "  'human',\n",
       "  'communication',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'powerful'],\n",
       " ['By',\n",
       "  'identifying',\n",
       "  'words',\n",
       "  '@',\n",
       "  'can,',\n",
       "  '@',\n",
       "  'example,',\n",
       "  'easily',\n",
       "  'count',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'frequent',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'document',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collection,',\n",
       "  'etc'],\n",
       " ['And', '@', 'words', '@', '@', '@', '@', 'form', 'topics'],\n",
       " ['When',\n",
       "  '@',\n",
       "  'combine',\n",
       "  'related',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'positive,',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'negative,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentiment',\n",
       "  'analysis'],\n",
       " ['So',\n",
       "  'representing',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sequence',\n",
       "  '@',\n",
       "  'words',\n",
       "  'opens',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  'analysis',\n",
       "  'possibilities'],\n",
       " ['However,',\n",
       "  '@',\n",
       "  'level',\n",
       "  '@',\n",
       "  'representation',\n",
       "  '@',\n",
       "  'slightly',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  'string',\n",
       "  '@',\n",
       "  'characters,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'languages',\n",
       "  '@',\n",
       "  '@',\n",
       "  'Chinese,',\n",
       "  \"it's\",\n",
       "  'actually',\n",
       "  '@',\n",
       "  '@',\n",
       "  'easy',\n",
       "  '@',\n",
       "  'identify',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'boundaries,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'language',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sequence',\n",
       "  '@',\n",
       "  'characters',\n",
       "  '@',\n",
       "  '@',\n",
       "  'space',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'rely',\n",
       "  '@',\n",
       "  '@',\n",
       "  'special',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  'identify',\n",
       "  'words'],\n",
       " ['In',\n",
       "  '@',\n",
       "  '@',\n",
       "  'language,',\n",
       "  '@',\n",
       "  'course,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mistakes',\n",
       "  '@',\n",
       "  'segmenting',\n",
       "  'words'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'sequence',\n",
       "  '@',\n",
       "  'words',\n",
       "  'representation',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'robust',\n",
       "  '@',\n",
       "  'string',\n",
       "  '@',\n",
       "  'characters'],\n",
       " ['But',\n",
       "  '@',\n",
       "  'English',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'easy',\n",
       "  '@',\n",
       "  'obtain',\n",
       "  '@',\n",
       "  'level',\n",
       "  '@',\n",
       "  'representation,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'time'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'add',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'speech',\n",
       "  'tags'],\n",
       " ['Now,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'that,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'count',\n",
       "  '@',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'frequent',\n",
       "  'nouns',\n",
       "  '@',\n",
       "  '@',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'nouns',\n",
       "  '@',\n",
       "  'associated',\n",
       "  '@',\n",
       "  '@',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'verbs,',\n",
       "  'etc'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'opens',\n",
       "  '@',\n",
       "  '@',\n",
       "  'little',\n",
       "  'bit',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  'opportunities',\n",
       "  '@',\n",
       "  '@',\n",
       "  'analysis'],\n",
       " ['Note',\n",
       "  '@',\n",
       "  'I',\n",
       "  'use',\n",
       "  '@',\n",
       "  'plus',\n",
       "  'sign',\n",
       "  'here,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'representing',\n",
       "  'text',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sequence',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'speech',\n",
       "  'tags'],\n",
       " ['We',\n",
       "  \"don't\",\n",
       "  'necessarily',\n",
       "  'replace',\n",
       "  '@',\n",
       "  'original',\n",
       "  'word',\n",
       "  'sequence',\n",
       "  'representation',\n",
       "  'Instead,',\n",
       "  '@',\n",
       "  'add',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'additional',\n",
       "  'way',\n",
       "  '@',\n",
       "  'representing',\n",
       "  'text',\n",
       "  'data,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'data',\n",
       "  '@',\n",
       "  'represented',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sequence',\n",
       "  '@',\n",
       "  'words,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sequence',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'speech',\n",
       "  'tags'],\n",
       " ['This',\n",
       "  'enriches',\n",
       "  '@',\n",
       "  'representation',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'thus,',\n",
       "  'also,',\n",
       "  'enables',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  'analysis'],\n",
       " ['If',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"we'll\",\n",
       "  '@',\n",
       "  'parsing',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  'obtain',\n",
       "  '@',\n",
       "  'syntactic',\n",
       "  'structure'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  'open',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  'analysis',\n",
       "  'of,',\n",
       "  '@',\n",
       "  'example,',\n",
       "  '@',\n",
       "  'writing',\n",
       "  'styles,',\n",
       "  '@',\n",
       "  'correcting',\n",
       "  'grammar',\n",
       "  'mistakes'],\n",
       " ['If',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'semantic',\n",
       "  'analysis,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'able',\n",
       "  '@',\n",
       "  'recognize',\n",
       "  'dog',\n",
       "  '@',\n",
       "  'animal',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'recognize',\n",
       "  'boy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'person',\n",
       "  '@',\n",
       "  'playground',\n",
       "  '@',\n",
       "  '@',\n",
       "  'location'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'analyze',\n",
       "  '@',\n",
       "  'relations,',\n",
       "  '@',\n",
       "  'example,',\n",
       "  'dog',\n",
       "  '@',\n",
       "  'chasing',\n",
       "  '@',\n",
       "  'boy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'boy',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'playground'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'add',\n",
       "  '@',\n",
       "  'entities',\n",
       "  '@',\n",
       "  'relations',\n",
       "  '@',\n",
       "  'entity-relation',\n",
       "  'recognition'],\n",
       " ['At', '@', 'level,', '@', '@', '@', '@', '@', '@', 'interesting', 'things'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'count',\n",
       "  'easily',\n",
       "  '@',\n",
       "  '@',\n",
       "  'frequent',\n",
       "  'person',\n",
       "  \"that's\",\n",
       "  'mentioned',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collection',\n",
       "  '@',\n",
       "  'news',\n",
       "  'articles,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mention',\n",
       "  '@',\n",
       "  'person,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mention',\n",
       "  '@',\n",
       "  '@',\n",
       "  'person,',\n",
       "  'etc'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  'representation',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'related',\n",
       "  '@',\n",
       "  '@',\n",
       "  'Knowledge',\n",
       "  'Graph',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'heard',\n",
       "  '@'],\n",
       " ['That',\n",
       "  'Google',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'semantic',\n",
       "  'way',\n",
       "  '@',\n",
       "  'representing',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['However,',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'robust',\n",
       "  '@',\n",
       "  'sequence',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'syntactic',\n",
       "  'analysis,',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'easy',\n",
       "  '@',\n",
       "  'identify',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entities',\n",
       "  '@',\n",
       "  '@',\n",
       "  'right',\n",
       "  'types,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mistakes,',\n",
       "  '@',\n",
       "  'relations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'harder',\n",
       "  '@',\n",
       "  'find',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mistakes'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'makes',\n",
       "  '@',\n",
       "  'level',\n",
       "  '@',\n",
       "  'representation',\n",
       "  '@',\n",
       "  'robust,',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'useful'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'logical',\n",
       "  'representation',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'predicates',\n",
       "  '@',\n",
       "  '@',\n",
       "  'inference',\n",
       "  'rules'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'inference',\n",
       "  'rules',\n",
       "  '@',\n",
       "  '@',\n",
       "  'infer',\n",
       "  'interesting,',\n",
       "  'derived',\n",
       "  'facts',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text'],\n",
       " ['So',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  'useful,',\n",
       "  '@',\n",
       "  'unfortunately',\n",
       "  '@',\n",
       "  '@',\n",
       "  'level',\n",
       "  '@',\n",
       "  'representation',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'robust',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mistakes,',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"can't\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'time',\n",
       "  '@',\n",
       "  '@',\n",
       "  'kinds',\n",
       "  '@',\n",
       "  'sentences'],\n",
       " ['And',\n",
       "  'finally,',\n",
       "  'speech',\n",
       "  'acts',\n",
       "  '@',\n",
       "  'added',\n",
       "  '@',\n",
       "  '@',\n",
       "  'level',\n",
       "  '@',\n",
       "  'representation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'intent',\n",
       "  '@',\n",
       "  'saying',\n",
       "  '@',\n",
       "  'sentence'],\n",
       " ['So', '@', '@', 'case', '@', '@', '@', '@', 'request'],\n",
       " ['So',\n",
       "  'knowing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'allow',\n",
       "  '@',\n",
       "  '@',\n",
       "  'analyze',\n",
       "  'more,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  'things',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observer',\n",
       "  'order'],\n",
       " ['Author',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence,',\n",
       "  \"what's\",\n",
       "  '@',\n",
       "  'intention',\n",
       "  '@',\n",
       "  'saying',\n",
       "  'that?',\n",
       "  'What',\n",
       "  'scenarios,',\n",
       "  '@',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'actions',\n",
       "  '@',\n",
       "  '@',\n",
       "  'made?',\n",
       "  'So',\n",
       "  '@',\n",
       "  '@'],\n",
       " [],\n",
       " [],\n",
       " ['Another', 'level', '@', 'analysis', '@', '@', '@', '@', 'interesting'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'picture',\n",
       "  'shows',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'down,',\n",
       "  '@',\n",
       "  'generally',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sophisticated',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['And',\n",
       "  'unfortunately,',\n",
       "  '@',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  'require',\n",
       "  '@',\n",
       "  'human',\n",
       "  'effort'],\n",
       " ['And', '@', '@', '@', 'accurate'],\n",
       " ['That', 'means', '@', '@', 'mistakes'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'analyze',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'levels',\n",
       "  '@',\n",
       "  '@',\n",
       "  'represented,',\n",
       "  'deeper',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  'language,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'tolerate',\n",
       "  '@',\n",
       "  'errors'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'means',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'necessary',\n",
       "  '@',\n",
       "  'combine',\n",
       "  '@',\n",
       "  'deep',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  'shallow',\n",
       "  'analysis',\n",
       "  'based',\n",
       "  'on,',\n",
       "  '@',\n",
       "  'example',\n",
       "  'sequence',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['On',\n",
       "  '@',\n",
       "  'right',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'arrow',\n",
       "  'points',\n",
       "  'down,',\n",
       "  '@',\n",
       "  'indicate',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'representation',\n",
       "  '@',\n",
       "  'text,',\n",
       "  \"it's\",\n",
       "  'closer',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  'representation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mind,',\n",
       "  '@',\n",
       "  'need',\n",
       "  '@',\n",
       "  'solving',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'problems'],\n",
       " ['Now,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'desirable',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'represent',\n",
       "  'text',\n",
       "  '@',\n",
       "  '@',\n",
       "  'level',\n",
       "  '@',\n",
       "  'knowledge,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'easily',\n",
       "  'extract',\n",
       "  '@',\n",
       "  'knowledge'],\n",
       " [\"That's\", '@', 'purpose', '@', 'text', 'mining'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'trade',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'deeper',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'errors,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'direct',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'extracted',\n",
       "  '@',\n",
       "  'text',\n",
       "  '@',\n",
       "  '@',\n",
       "  'shallow',\n",
       "  'analysis,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'robust'],\n",
       " ['But',\n",
       "  \"wouldn't\",\n",
       "  'actually',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'necessary',\n",
       "  'deeper',\n",
       "  'representation',\n",
       "  '@',\n",
       "  'knowledge'],\n",
       " ['I',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'generated',\n",
       "  '@',\n",
       "  'humans',\n",
       "  '@',\n",
       "  '@',\n",
       "  'meant',\n",
       "  '@',\n",
       "  '@',\n",
       "  'consumed',\n",
       "  '@',\n",
       "  'humans,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'result',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'text',\n",
       "  'mining,',\n",
       "  'humans',\n",
       "  'play',\n",
       "  '@',\n",
       "  '@',\n",
       "  'important',\n",
       "  'role'],\n",
       " ['They', '@', '@', '@', '@', 'loop'],\n",
       " ['Meaning',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'optimize',\n",
       "  '@',\n",
       "  'collaboration',\n",
       "  '@',\n",
       "  'humans',\n",
       "  '@',\n",
       "  'computers'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sense,',\n",
       "  \"it's\",\n",
       "  'OK',\n",
       "  '@',\n",
       "  'computers',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'able',\n",
       "  '@',\n",
       "  '@',\n",
       "  'completely',\n",
       "  'accurate',\n",
       "  'representation',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  'patterns',\n",
       "  '@',\n",
       "  '@',\n",
       "  'extracted',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interpreted',\n",
       "  '@',\n",
       "  'humans,',\n",
       "  '@',\n",
       "  'humans',\n",
       "  '@',\n",
       "  'guide',\n",
       "  '@',\n",
       "  'computers',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'accurate',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  'annotating',\n",
       "  '@',\n",
       "  'data',\n",
       "  '@',\n",
       "  'providing',\n",
       "  'features',\n",
       "  '@',\n",
       "  'guide',\n",
       "  '@',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'programs',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'work',\n",
       "  '@',\n",
       "  'effectively'],\n",
       " [],\n",
       " ['So,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'explained,',\n",
       "  'different',\n",
       "  'textual',\n",
       "  'representation',\n",
       "  'tends',\n",
       "  '@',\n",
       "  'enable',\n",
       "  'different',\n",
       "  'analysis'],\n",
       " ['In',\n",
       "  'particular,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'gradually',\n",
       "  'add',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'deeper',\n",
       "  'analysis',\n",
       "  'results',\n",
       "  '@',\n",
       "  'represent',\n",
       "  'text',\n",
       "  'data,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'open',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  'representation',\n",
       "  'opportunities',\n",
       "  '@',\n",
       "  '@',\n",
       "  'analysis',\n",
       "  'capacities'],\n",
       " ['So', '@', 'table', 'summarizes', '@', '@', '@', '@', 'seen'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'column',\n",
       "  'shows',\n",
       "  '@',\n",
       "  'text',\n",
       "  'recognition,',\n",
       "  '@',\n",
       "  'second',\n",
       "  'visualizes',\n",
       "  '@',\n",
       "  'generality',\n",
       "  '@',\n",
       "  '@',\n",
       "  'representation,',\n",
       "  'meaning',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'representation',\n",
       "  'accurate',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'them,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'column',\n",
       "  'shows',\n",
       "  '@',\n",
       "  'enabled',\n",
       "  'analysis',\n",
       "  'techniques'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'final',\n",
       "  'column',\n",
       "  'shows',\n",
       "  '@',\n",
       "  'examples',\n",
       "  '@',\n",
       "  'application',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'achieved',\n",
       "  '@',\n",
       "  '@',\n",
       "  'level',\n",
       "  '@',\n",
       "  'representation'],\n",
       " ['So', \"let's\", '@', '@', 'look', '@', '@'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'string',\n",
       "  'text',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'processed',\n",
       "  '@',\n",
       "  '@',\n",
       "  'stream',\n",
       "  'processing',\n",
       "  'algorithms,',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'robust,',\n",
       "  \"it's\",\n",
       "  'general'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  'applications',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'level'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  'compression',\n",
       "  '@',\n",
       "  'text',\n",
       "  \"doesn't\",\n",
       "  'necessarily',\n",
       "  'need',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'word',\n",
       "  'boundaries'],\n",
       " ['Although', 'knowing', 'word', 'boundaries', '@', 'actually', '@', 'help'],\n",
       " ['Word',\n",
       "  'based',\n",
       "  'representation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'important',\n",
       "  'level',\n",
       "  '@',\n",
       "  'representation'],\n",
       " [\"It's\", '@', 'general', '@', 'relatively', 'robust'],\n",
       " ['It',\n",
       "  '@',\n",
       "  'enable',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'analysis',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'relation',\n",
       "  'analysis,',\n",
       "  'topic',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  'sentiment',\n",
       "  'analysis,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'applications',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'enabled',\n",
       "  '@',\n",
       "  '@',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'analysis'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  'Thesaurus',\n",
       "  'discovery',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  'related',\n",
       "  'words',\n",
       "  '@',\n",
       "  'topic',\n",
       "  '@',\n",
       "  'opinion',\n",
       "  'related',\n",
       "  'applications',\n",
       "  '@',\n",
       "  'abundant,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'example,',\n",
       "  '@',\n",
       "  'people',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interested',\n",
       "  '@',\n",
       "  'knowing',\n",
       "  '@',\n",
       "  'major',\n",
       "  'topics',\n",
       "  'covered',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collection',\n",
       "  '@',\n",
       "  'text'],\n",
       " ['And', '@', '@', '@', '@', 'case'],\n",
       " ['In',\n",
       "  'research',\n",
       "  'literature,',\n",
       "  '@',\n",
       "  'scientist',\n",
       "  'want',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'important',\n",
       "  'research',\n",
       "  'topics',\n",
       "  'today',\n",
       "  '@',\n",
       "  'customer',\n",
       "  'service',\n",
       "  'people',\n",
       "  '@',\n",
       "  'want',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'major',\n",
       "  'complaints',\n",
       "  '@',\n",
       "  '@',\n",
       "  'customers',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'email',\n",
       "  'messages'],\n",
       " ['And',\n",
       "  'business',\n",
       "  'intelligence',\n",
       "  'people',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interested',\n",
       "  '@',\n",
       "  'understanding',\n",
       "  'consumers',\n",
       "  'opinions',\n",
       "  '@',\n",
       "  '@',\n",
       "  'products',\n",
       "  '@',\n",
       "  'competitors',\n",
       "  'products',\n",
       "  '@',\n",
       "  'figure',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'winning',\n",
       "  'features',\n",
       "  '@',\n",
       "  '@',\n",
       "  'products'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'applications',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'enabled',\n",
       "  '@',\n",
       "  '@',\n",
       "  'representation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'level'],\n",
       " ['Now',\n",
       "  'moving',\n",
       "  'down,',\n",
       "  \"we'll\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'gradually',\n",
       "  'add',\n",
       "  'additional',\n",
       "  'representations'],\n",
       " ['By',\n",
       "  'adding',\n",
       "  'syntactic',\n",
       "  'structures',\n",
       "  '@',\n",
       "  '@',\n",
       "  'enable,',\n",
       "  'Of',\n",
       "  'course,',\n",
       "  'syntactic',\n",
       "  'graph',\n",
       "  'analysis'],\n",
       " ['We',\n",
       "  '@',\n",
       "  'use',\n",
       "  'graph',\n",
       "  'mining',\n",
       "  'algorithms',\n",
       "  '@',\n",
       "  'analyze',\n",
       "  'Syntactic',\n",
       "  'graphs'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'applications',\n",
       "  '@',\n",
       "  'related',\n",
       "  '@',\n",
       "  '@',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'representation'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  'stylistic',\n",
       "  'analysis',\n",
       "  'generally',\n",
       "  'requires',\n",
       "  'syntactical',\n",
       "  'representation'],\n",
       " ['Syntactical', 'structure', 'representation'],\n",
       " ['We',\n",
       "  '@',\n",
       "  '@',\n",
       "  'generate',\n",
       "  '@',\n",
       "  'structure',\n",
       "  'based',\n",
       "  'feature',\n",
       "  'features',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'features',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'classify',\n",
       "  'text',\n",
       "  'objects',\n",
       "  '@',\n",
       "  'different',\n",
       "  'categories'],\n",
       " ['By',\n",
       "  'looking',\n",
       "  '@',\n",
       "  '@',\n",
       "  'structures,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'classification',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'accurate'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'want',\n",
       "  '@',\n",
       "  'classify',\n",
       "  'articles',\n",
       "  '@',\n",
       "  'different',\n",
       "  'categories',\n",
       "  'corresponding',\n",
       "  '@',\n",
       "  'different',\n",
       "  'authors',\n",
       "  'want',\n",
       "  '@',\n",
       "  'figure',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'K',\n",
       "  'authors',\n",
       "  '@',\n",
       "  'actually',\n",
       "  'written',\n",
       "  '@',\n",
       "  'article'],\n",
       " ['Then',\n",
       "  '@',\n",
       "  'generally',\n",
       "  'need',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'syntactic',\n",
       "  'structures'],\n",
       " ['When',\n",
       "  '@',\n",
       "  'add',\n",
       "  'entities',\n",
       "  '@',\n",
       "  'relations,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'enable',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  'graph',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  'information',\n",
       "  'network',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  'enable',\n",
       "  'applications',\n",
       "  '@',\n",
       "  'entities,',\n",
       "  '@',\n",
       "  'example,',\n",
       "  'discovery',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  'opinions',\n",
       "  '@',\n",
       "  '@',\n",
       "  'real',\n",
       "  'world',\n",
       "  'energy',\n",
       "  'entity'],\n",
       " ['You',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'level',\n",
       "  'representation',\n",
       "  '@',\n",
       "  'integrate',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entity',\n",
       "  '@',\n",
       "  'scattered',\n",
       "  'sources'],\n",
       " ['Finally,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'add',\n",
       "  'logic',\n",
       "  'predicates',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'enable',\n",
       "  'logic',\n",
       "  'inference',\n",
       "  'ofcourse,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  '@',\n",
       "  'integrative',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  'scattered',\n",
       "  'knowledge'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'add',\n",
       "  'ontology',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'extracted',\n",
       "  'information',\n",
       "  '@',\n",
       "  'text',\n",
       "  '@',\n",
       "  '@',\n",
       "  'inferences'],\n",
       " ['A',\n",
       "  'good',\n",
       "  'example',\n",
       "  '@',\n",
       "  'application',\n",
       "  '@',\n",
       "  '@',\n",
       "  'enabled',\n",
       "  '@',\n",
       "  '@',\n",
       "  'level',\n",
       "  '@',\n",
       "  'representation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'intelligent',\n",
       "  'knowledge',\n",
       "  'assistant',\n",
       "  '@',\n",
       "  'biologists'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'intended',\n",
       "  'program',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  'biologists',\n",
       "  'manage',\n",
       "  '@',\n",
       "  '@',\n",
       "  'relevant',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  'literature',\n",
       "  '@',\n",
       "  '@',\n",
       "  'research',\n",
       "  'problem,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'understanding',\n",
       "  'functions',\n",
       "  '@',\n",
       "  'genes'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'computer',\n",
       "  '@',\n",
       "  '@',\n",
       "  'inferences',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'hypothesis',\n",
       "  '@',\n",
       "  'biologists',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interesting,',\n",
       "  '@',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'gene',\n",
       "  '@',\n",
       "  '@',\n",
       "  'certain',\n",
       "  'function',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'intelligent',\n",
       "  'program',\n",
       "  '@',\n",
       "  'read',\n",
       "  '@',\n",
       "  'literature',\n",
       "  '@',\n",
       "  'extract',\n",
       "  '@',\n",
       "  'relevant',\n",
       "  'facts'],\n",
       " ['Doing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'information',\n",
       "  'extraction',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'logical',\n",
       "  'system',\n",
       "  '@',\n",
       "  'actually',\n",
       "  'track',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  'answers',\n",
       "  '@',\n",
       "  'researchers',\n",
       "  'questioning',\n",
       "  '@',\n",
       "  '@',\n",
       "  'genes',\n",
       "  '@',\n",
       "  'related',\n",
       "  '@',\n",
       "  '@',\n",
       "  'functions'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'order',\n",
       "  '@',\n",
       "  'support',\n",
       "  '@',\n",
       "  'level',\n",
       "  '@',\n",
       "  'application,',\n",
       "  '@',\n",
       "  'need',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'far',\n",
       "  '@',\n",
       "  'logical',\n",
       "  'representation'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  'covering',\n",
       "  'techniques',\n",
       "  'mainly',\n",
       "  'based',\n",
       "  '@',\n",
       "  'word',\n",
       "  'based',\n",
       "  'representation'],\n",
       " ['These',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  'robust',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'widely',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'applications'],\n",
       " ['In',\n",
       "  'fact,',\n",
       "  '@',\n",
       "  'virtually',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'mining',\n",
       "  'applications',\n",
       "  '@',\n",
       "  'need',\n",
       "  '@',\n",
       "  'level',\n",
       "  '@',\n",
       "  'representation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  'support',\n",
       "  'analysis',\n",
       "  '@',\n",
       "  'texting',\n",
       "  '@',\n",
       "  'level'],\n",
       " ['But',\n",
       "  'obviously',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'levels',\n",
       "  '@',\n",
       "  '@',\n",
       "  'combined',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'combined',\n",
       "  '@',\n",
       "  'order',\n",
       "  '@',\n",
       "  'support',\n",
       "  'sophisticated',\n",
       "  'applications'],\n",
       " ['So', '@', 'summarize,', '@', '@', '@', 'major', 'takeaway', 'points'],\n",
       " ['Text',\n",
       "  'representation',\n",
       "  'determines',\n",
       "  '@',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'mining',\n",
       "  'algorithms',\n",
       "  '@',\n",
       "  '@',\n",
       "  'applied'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'multiple',\n",
       "  'ways',\n",
       "  '@',\n",
       "  'represent',\n",
       "  'text',\n",
       "  '-',\n",
       "  'strings,',\n",
       "  'words,',\n",
       "  'syntactic',\n",
       "  'structures',\n",
       "  '@',\n",
       "  '@',\n",
       "  'relation',\n",
       "  'graphs,',\n",
       "  'logical',\n",
       "  'predicates,',\n",
       "  'etc'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'different',\n",
       "  'representations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  'combined',\n",
       "  '@',\n",
       "  'real',\n",
       "  'applications',\n",
       "  '@',\n",
       "  '@',\n",
       "  'extent',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'accurately,',\n",
       "  '@',\n",
       "  'application',\n",
       "  '@',\n",
       "  'syntactic',\n",
       "  'structures',\n",
       "  '@',\n",
       "  '@',\n",
       "  'stick',\n",
       "  '@',\n",
       "  'partial',\n",
       "  'structures',\n",
       "  'extracted',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'recognize',\n",
       "  '@',\n",
       "  'entities',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'great'],\n",
       " ['So', '@', 'general', '@', 'want', '@', '@', '@', '@', '@', '@', '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'different',\n",
       "  'levels',\n",
       "  '@',\n",
       "  'combined',\n",
       "  'together,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'enable',\n",
       "  'richer',\n",
       "  'analysis'],\n",
       " ['More', 'powerful', 'analysis'],\n",
       " ['This',\n",
       "  'course,',\n",
       "  'however,',\n",
       "  'focuses',\n",
       "  '@',\n",
       "  'word',\n",
       "  'based',\n",
       "  'representation'],\n",
       " ['Such', 'techniques', '@', '@', '@', 'advantages'],\n",
       " ['First,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  'robust,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'applicable',\n",
       "  '@',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language'],\n",
       " [\"That's\",\n",
       "  '@',\n",
       "  'big',\n",
       "  'advantage',\n",
       "  '@',\n",
       "  '@',\n",
       "  'approaches',\n",
       "  '@',\n",
       "  'rely',\n",
       "  '@',\n",
       "  '@',\n",
       "  'fragile',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'techniques'],\n",
       " ['Secondly,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'require',\n",
       "  '@',\n",
       "  'manual',\n",
       "  'effort',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'require',\n",
       "  '@',\n",
       "  'manual',\n",
       "  'effort'],\n",
       " ['So',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  'important',\n",
       "  'benefit,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'means',\n",
       "  '@',\n",
       "  '@',\n",
       "  'apply',\n",
       "  'directly',\n",
       "  '@',\n",
       "  '@',\n",
       "  'application'],\n",
       " ['Third,',\n",
       "  '@',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  'actually',\n",
       "  'surprisingly',\n",
       "  'powerful',\n",
       "  '@',\n",
       "  'effective',\n",
       "  '@',\n",
       "  '@',\n",
       "  'applications'],\n",
       " ['Although', '@', 'all,', '@', 'course,', '@', 'I', '@', 'explained'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'effective,',\n",
       "  'partly',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'invented',\n",
       "  '@',\n",
       "  'humans',\n",
       "  '@',\n",
       "  'basic',\n",
       "  'units',\n",
       "  '@',\n",
       "  'communications'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'actually',\n",
       "  '@',\n",
       "  'sufficient',\n",
       "  '@',\n",
       "  'representing',\n",
       "  '@',\n",
       "  'kinds',\n",
       "  '@',\n",
       "  'semantics'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'makes',\n",
       "  '@',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'word',\n",
       "  'based',\n",
       "  'representation',\n",
       "  '@',\n",
       "  'powerful'],\n",
       " ['And',\n",
       "  'finally',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'based',\n",
       "  'representation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'techniques',\n",
       "  'enabled',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'representation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'combined',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sophisticated',\n",
       "  'approaches'],\n",
       " ['So', \"they're\", '@', 'competing', '@', '@', '@'],\n",
       " [],\n",
       " ['This',\n",
       "  'lecture',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'Word',\n",
       "  'Association',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'analysis'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'lecture',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'associations',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'text'],\n",
       " ['This',\n",
       "  '@',\n",
       "  '@',\n",
       "  'example',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  'natural',\n",
       "  'language',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data'],\n",
       " [\"Here's\", '@', 'outline'],\n",
       " ['We',\n",
       "  '@',\n",
       "  'gooing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'Association',\n",
       "  '@',\n",
       "  '@',\n",
       "  'explain',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  '@',\n",
       "  'relations',\n",
       "  '@',\n",
       "  'useful',\n",
       "  '@',\n",
       "  'finally',\n",
       "  '@',\n",
       "  '@',\n",
       "  'going',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  'ideas',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'associations'],\n",
       " ['In',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'relations,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'basic'],\n",
       " ['One',\n",
       "  '@',\n",
       "  'called',\n",
       "  '@',\n",
       "  'paradigmatic',\n",
       "  'relation,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'syntagmatic',\n",
       "  'relations'],\n",
       " ['A&amp;B',\n",
       "  '@',\n",
       "  'paradigmatic',\n",
       "  'relation',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'substituted',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['That',\n",
       "  'means,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'paradigmatic',\n",
       "  'relation',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'semantic',\n",
       "  'class',\n",
       "  '@',\n",
       "  'syntactic',\n",
       "  'class,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  'replace',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'affecting',\n",
       "  '@',\n",
       "  'understanding',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence'],\n",
       " ['That', 'means', '@', '@', '@', '@', '@', 'valid', 'sentence'],\n",
       " ['For', 'example,', 'cat', '@', 'dog'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'paradigmatic',\n",
       "  'relation',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'class',\n",
       "  '@',\n",
       "  'animal'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'general,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'replace',\n",
       "  'cat',\n",
       "  '@',\n",
       "  'dog',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence,',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'valid',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sense',\n",
       "  '@'],\n",
       " ['Similarly,', 'Monday', '@', 'Tuesday', '@', 'paradigmatic', 'relation'],\n",
       " ['The',\n",
       "  'second',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'relation',\n",
       "  '@',\n",
       "  'called',\n",
       "  'syntagmatic',\n",
       "  'relation'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'case,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'relation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'combined',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['So',\n",
       "  'A&amp;B',\n",
       "  '@',\n",
       "  'syntagmatic',\n",
       "  'relation',\n",
       "  'If',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'combined',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence'],\n",
       " ['That', 'means', '@', '@', 'words', '@', 'semantically', 'related'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'example,',\n",
       "  'Cat',\n",
       "  '@',\n",
       "  'sit',\n",
       "  '@',\n",
       "  'related',\n",
       "  '@',\n",
       "  '@',\n",
       "  'cat',\n",
       "  '@',\n",
       "  'sit',\n",
       "  '@'],\n",
       " ['Similarly,',\n",
       "  'car',\n",
       "  '@',\n",
       "  'drive',\n",
       "  '@',\n",
       "  'related',\n",
       "  'semantically,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'combined',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'convey',\n",
       "  'meaning'],\n",
       " ['However,',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  'replace',\n",
       "  'cat',\n",
       "  '@',\n",
       "  'sit',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  'car',\n",
       "  '@',\n",
       "  'drive',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'valid',\n",
       "  'sentence'],\n",
       " ['Meaning',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'that,',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'somewhat',\n",
       "  'meaningless'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  '@',\n",
       "  'paradigmatic',\n",
       "  'relation',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'relations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'fact',\n",
       "  '@',\n",
       "  'fundamental,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'generalized',\n",
       "  '@',\n",
       "  'capture',\n",
       "  'basic',\n",
       "  'relations',\n",
       "  '@',\n",
       "  'units',\n",
       "  '@',\n",
       "  'arbitrary',\n",
       "  'sequences'],\n",
       " ['And',\n",
       "  'definitely',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'generalized',\n",
       "  '@',\n",
       "  'describe',\n",
       "  'relations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'items',\n",
       "  '@',\n",
       "  '@',\n",
       "  'language'],\n",
       " ['So',\n",
       "  'A&amp;B',\n",
       "  \"don't\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'phrases',\n",
       "  'example'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'complex',\n",
       "  'phrases',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'noun',\n",
       "  'phrase'],\n",
       " ['If',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  'problem',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sequence',\n",
       "  'mining,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  'units',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sequence',\n",
       "  'data,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  'paradigmatic',\n",
       "  'relation',\n",
       "  '@',\n",
       "  'relations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'applied',\n",
       "  '@',\n",
       "  'units',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  'occur',\n",
       "  '@',\n",
       "  'similar',\n",
       "  'locations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sequence',\n",
       "  '@',\n",
       "  'data',\n",
       "  'elements',\n",
       "  '@',\n",
       "  'general'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'occur',\n",
       "  '@',\n",
       "  'similar',\n",
       "  'locations',\n",
       "  'relative',\n",
       "  '@',\n",
       "  '@',\n",
       "  'neighbors',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sequence'],\n",
       " ['Syntagmatic',\n",
       "  'relation',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'hand,',\n",
       "  '@',\n",
       "  'related',\n",
       "  '@',\n",
       "  'co-occurring',\n",
       "  'elements',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sequence'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'complementary',\n",
       "  '@',\n",
       "  'basically',\n",
       "  'relations',\n",
       "  '@',\n",
       "  'words,',\n",
       "  '@',\n",
       "  \"we're\",\n",
       "  'interested',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  '@',\n",
       "  'automatically',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['Discovering', '@', 'world', 'relations', '@', '@', 'applications'],\n",
       " ['First,',\n",
       "  '@',\n",
       "  'relations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'directly',\n",
       "  'useful',\n",
       "  '@',\n",
       "  'improving',\n",
       "  'accuracy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'NLP',\n",
       "  'tasks,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'knowledge',\n",
       "  '@',\n",
       "  '@',\n",
       "  'language'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'synonyms,',\n",
       "  '@',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'tasks'],\n",
       " ['And',\n",
       "  'grammar',\n",
       "  'learning',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  'If',\n",
       "  '@',\n",
       "  '@',\n",
       "  'learn',\n",
       "  'paradigmatic',\n",
       "  'relations,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'form',\n",
       "  'classes',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['Syntactic', 'classes', '@', 'example'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'learn',\n",
       "  'syntagmatic',\n",
       "  'relations,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'able',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'rules',\n",
       "  '@',\n",
       "  'putting',\n",
       "  '@',\n",
       "  '@',\n",
       "  'larger',\n",
       "  'expression',\n",
       "  'based',\n",
       "  '@',\n",
       "  'component',\n",
       "  'expressions'],\n",
       " ['So', \"we'll\", 'learn', '@', 'structure', '@', '@', '@', '@', '@', '@', '@'],\n",
       " ['Word',\n",
       "  'relations',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  '@',\n",
       "  '@',\n",
       "  'applications',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval',\n",
       "  '@',\n",
       "  'mining'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  'search',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  'word',\n",
       "  'associations',\n",
       "  '@',\n",
       "  'modify',\n",
       "  '@',\n",
       "  'query'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'introduce',\n",
       "  'additional',\n",
       "  'related',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'query',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'query',\n",
       "  '@',\n",
       "  'effective'],\n",
       " [\"It's\", '@', 'called', 'query', 'expansion'],\n",
       " ['Or',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  'related',\n",
       "  'words',\n",
       "  '@',\n",
       "  'suggest',\n",
       "  'related',\n",
       "  'queries',\n",
       "  '@',\n",
       "  '@',\n",
       "  'user',\n",
       "  '@',\n",
       "  'explore',\n",
       "  '@',\n",
       "  'information',\n",
       "  'space'],\n",
       " ['Another',\n",
       "  'application',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  'word',\n",
       "  'associations',\n",
       "  '@',\n",
       "  'automatically',\n",
       "  'construct',\n",
       "  '@',\n",
       "  'topic',\n",
       "  'map',\n",
       "  '@',\n",
       "  'browsing',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'nodes',\n",
       "  '@',\n",
       "  'associations',\n",
       "  '@',\n",
       "  'edge'],\n",
       " ['The',\n",
       "  'user',\n",
       "  '@',\n",
       "  'navigate',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'find',\n",
       "  'information',\n",
       "  '@',\n",
       "  '@',\n",
       "  'information',\n",
       "  'space'],\n",
       " ['Finally,',\n",
       "  '@',\n",
       "  'word',\n",
       "  'associations',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compare',\n",
       "  '@',\n",
       "  'summarize',\n",
       "  'opinions'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interested',\n",
       "  '@',\n",
       "  'understanding',\n",
       "  'positive',\n",
       "  '@',\n",
       "  'negative',\n",
       "  'opinions',\n",
       "  '@',\n",
       "  'iPhone',\n",
       "  '6'],\n",
       " ['In',\n",
       "  'order',\n",
       "  '@',\n",
       "  '@',\n",
       "  'that,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'strongly',\n",
       "  'associated',\n",
       "  '@',\n",
       "  '@',\n",
       "  'feature',\n",
       "  'word',\n",
       "  'like',\n",
       "  '@',\n",
       "  'battery',\n",
       "  '@',\n",
       "  'positive',\n",
       "  'versus',\n",
       "  'negative',\n",
       "  'reviews'],\n",
       " ['Such',\n",
       "  '@',\n",
       "  'syntagmatic',\n",
       "  'relations',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'detailed',\n",
       "  'opinions',\n",
       "  '@',\n",
       "  '@',\n",
       "  'product'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'discover',\n",
       "  '@',\n",
       "  'associations',\n",
       "  'automatically?',\n",
       "  'Now,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'intuitions',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@'],\n",
       " [\"Let's\", '@', 'look', '@', '@', 'paradigmatic', 'relation'],\n",
       " ['Here',\n",
       "  '@',\n",
       "  'essentially',\n",
       "  '@',\n",
       "  '@',\n",
       "  'advantage',\n",
       "  '@',\n",
       "  'similar',\n",
       "  'context'],\n",
       " ['So', '@', '@', '@', '@', 'simple', 'sentences', '@', 'cat', '@', 'dog'],\n",
       " ['You', '@', '@', '@', 'generally', 'occur', '@', 'similar', 'context'],\n",
       " ['And',\n",
       "  'that,',\n",
       "  '@',\n",
       "  'all,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'definition',\n",
       "  '@',\n",
       "  'paradigmatic',\n",
       "  'relation'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'right',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'I',\n",
       "  'extracted',\n",
       "  'explicitly',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'cat',\n",
       "  '@',\n",
       "  'dog',\n",
       "  '@',\n",
       "  '@',\n",
       "  'small',\n",
       "  'sample',\n",
       "  '@',\n",
       "  'text',\n",
       "  'data'],\n",
       " ['So',\n",
       "  'I',\n",
       "  '@',\n",
       "  'taken',\n",
       "  'away',\n",
       "  'cat',\n",
       "  '@',\n",
       "  'dog',\n",
       "  '@',\n",
       "  '@',\n",
       "  'corresponding',\n",
       "  'sentences',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'perspectives',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  'occur',\n",
       "  '@',\n",
       "  '@',\n",
       "  'left',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['So', '@', '@', '@', '@', 'left', 'context'],\n",
       " ['What', 'words', 'occur', '@', '@', '@', 'cat,', 'cat', '@', 'dog'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  'clearly',\n",
       "  'dog',\n",
       "  '@',\n",
       "  'cat',\n",
       "  '@',\n",
       "  'similar',\n",
       "  'left',\n",
       "  'context'],\n",
       " ['You',\n",
       "  'generally',\n",
       "  '@',\n",
       "  '@',\n",
       "  'cat',\n",
       "  '@',\n",
       "  '@',\n",
       "  'cat,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'dog',\n",
       "  '@',\n",
       "  '@',\n",
       "  'dog'],\n",
       " ['So', '@', 'makes', '@', 'similar', '@', '@', 'left', 'context'],\n",
       " ['Similarly,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'occur',\n",
       "  '@',\n",
       "  'cat',\n",
       "  '@',\n",
       "  'dog,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'right',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'similar',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case,',\n",
       "  '@',\n",
       "  'course',\n",
       "  \"it's\",\n",
       "  'extreme',\n",
       "  'case',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'In',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['Of', 'course', '@', '@', 'follow', 'cat', '@', 'dog'],\n",
       " ['You', '@', '@', '@', 'look', '@', '@', 'general', 'context'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'improve',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentences',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'suggesting',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'discover',\n",
       "  'paradigmatic',\n",
       "  'relation',\n",
       "  '@',\n",
       "  'looking',\n",
       "  '@',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  'following',\n",
       "  'questions,',\n",
       "  '@',\n",
       "  'similar',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'cat',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'dog?',\n",
       "  'In',\n",
       "  'contrast,',\n",
       "  '@',\n",
       "  'similar',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'cat',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'computer?',\n",
       "  'Now',\n",
       "  'intuitively,',\n",
       "  '@',\n",
       "  'imagine',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'Cat',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'dog',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'similar',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'cat',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'computer,',\n",
       "  '@',\n",
       "  'means',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case,',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  'value',\n",
       "  '@',\n",
       "  '@',\n",
       "  'high'],\n",
       " ['Between',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'cat',\n",
       "  '@',\n",
       "  'dog,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'second',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  'contexts',\n",
       "  '@',\n",
       "  'cat',\n",
       "  '@',\n",
       "  'computer',\n",
       "  '@',\n",
       "  '@',\n",
       "  'low',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'having',\n",
       "  'paradigmatic',\n",
       "  'relationship'],\n",
       " ['And', '@', 'imagine', '@', 'words', 'occur', '@', 'computer'],\n",
       " ['In',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  'occur',\n",
       "  '@',\n",
       "  'cat'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'basic',\n",
       "  'idea',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  'paradigmatic',\n",
       "  'relation'],\n",
       " ['What',\n",
       "  '@',\n",
       "  '@',\n",
       "  'syntagmatic',\n",
       "  'relation?',\n",
       "  'Here',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'going',\n",
       "  '@',\n",
       "  'explore',\n",
       "  '@',\n",
       "  'correlated',\n",
       "  'occurrences',\n",
       "  '@',\n",
       "  'based',\n",
       "  '@',\n",
       "  '@',\n",
       "  'definition',\n",
       "  '@',\n",
       "  'syntagmatic',\n",
       "  'relation'],\n",
       " ['Here', '@', '@', '@', '@', 'sample', '@', 'text'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interested',\n",
       "  '@',\n",
       "  'knowing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'correlated',\n",
       "  '@',\n",
       "  '@',\n",
       "  'verb',\n",
       "  'eats'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'eat?',\n",
       "  'And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'right',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'slide',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"I've\",\n",
       "  'taken',\n",
       "  'away',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'eats'],\n",
       " [\"I've\",\n",
       "  'taken',\n",
       "  'away',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'left',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'world',\n",
       "  '@',\n",
       "  '@',\n",
       "  'right,',\n",
       "  'In',\n",
       "  '@',\n",
       "  'sentence'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ask',\n",
       "  '@',\n",
       "  'question',\n",
       "  '@',\n",
       "  'words',\n",
       "  'tend',\n",
       "  '@',\n",
       "  'occur',\n",
       "  '@',\n",
       "  '@',\n",
       "  'left',\n",
       "  '@',\n",
       "  'eat',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  'tend',\n",
       "  '@',\n",
       "  'occur',\n",
       "  '@',\n",
       "  '@',\n",
       "  'right',\n",
       "  '@',\n",
       "  'eat?',\n",
       "  'Now',\n",
       "  'thinking',\n",
       "  '@',\n",
       "  '@',\n",
       "  'question',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'discover',\n",
       "  'Syntagmatic',\n",
       "  'relations'],\n",
       " ['Because',\n",
       "  'syntagmatic',\n",
       "  'relation',\n",
       "  'essentially',\n",
       "  'captures',\n",
       "  '@',\n",
       "  'correlations'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'important',\n",
       "  'question',\n",
       "  '@',\n",
       "  'ask',\n",
       "  '@',\n",
       "  'syntagmatic',\n",
       "  'relation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'eats',\n",
       "  'occurs,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  'occur?',\n",
       "  'So',\n",
       "  '@',\n",
       "  'question',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  'co-occur',\n",
       "  '@',\n",
       "  '@',\n",
       "  'eats,',\n",
       "  'meaning',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'eat,',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"don't\",\n",
       "  '@',\n",
       "  'eat,',\n",
       "  'probably',\n",
       "  '@',\n",
       "  \"don't\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'intuition',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'discover',\n",
       "  'syntagmatic',\n",
       "  'relations'],\n",
       " ['Now',\n",
       "  'again,',\n",
       "  'consider',\n",
       "  'example-',\n",
       "  'How',\n",
       "  'helpful',\n",
       "  '@',\n",
       "  '@',\n",
       "  'occurrence',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'predicting',\n",
       "  'occurrence',\n",
       "  '@',\n",
       "  'meat?',\n",
       "  'knowing',\n",
       "  '@',\n",
       "  'eats',\n",
       "  'occurs',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  'generally',\n",
       "  'help',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  'Whether',\n",
       "  'meat',\n",
       "  '@',\n",
       "  'occurs',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'eats',\n",
       "  'occur',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'increase',\n",
       "  '@',\n",
       "  'chance',\n",
       "  '@',\n",
       "  'meat',\n",
       "  '@',\n",
       "  '@',\n",
       "  'occur'],\n",
       " ['In',\n",
       "  'contrast,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'question',\n",
       "  '@',\n",
       "  '@',\n",
       "  'bottom,',\n",
       "  '@',\n",
       "  'helpful',\n",
       "  '@',\n",
       "  'occurrence',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'predicting',\n",
       "  '@',\n",
       "  'occurrence',\n",
       "  '@',\n",
       "  'text?',\n",
       "  'Because',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'text',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'related,',\n",
       "  '@',\n",
       "  'knowing',\n",
       "  '@',\n",
       "  'eats',\n",
       "  'occurred',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  \"doesn't\",\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  'text',\n",
       "  '@',\n",
       "  'occurs',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contrast',\n",
       "  '@',\n",
       "  '@',\n",
       "  'question',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'meat'],\n",
       " ['This',\n",
       "  '@',\n",
       "  'helps',\n",
       "  'explain',\n",
       "  '@',\n",
       "  'intuition',\n",
       "  '@',\n",
       "  '@',\n",
       "  'methods',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  'syntagmatic',\n",
       "  'relation'],\n",
       " ['Mainly',\n",
       "  '@',\n",
       "  'need',\n",
       "  '@',\n",
       "  'capture',\n",
       "  '@',\n",
       "  'correlation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'occurrences',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'summarize,',\n",
       "  '@',\n",
       "  'general',\n",
       "  'ideas',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  'word',\n",
       "  'associations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'following'],\n",
       " ['For',\n",
       "  'paradigmatically',\n",
       "  'relation',\n",
       "  '@',\n",
       "  'represent',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'context',\n",
       "  'similarity'],\n",
       " ['We',\n",
       "  '@',\n",
       "  'gonna',\n",
       "  'assume',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'high',\n",
       "  'context',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  '@',\n",
       "  'paradigmatic',\n",
       "  'relation'],\n",
       " ['For',\n",
       "  'syntagmatic',\n",
       "  'relation,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'count',\n",
       "  '@',\n",
       "  '@',\n",
       "  'times',\n",
       "  '@',\n",
       "  'words',\n",
       "  'occur',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence,',\n",
       "  'paragraph',\n",
       "  '@',\n",
       "  '@',\n",
       "  'document',\n",
       "  '@'],\n",
       " ['And',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'compare',\n",
       "  '@',\n",
       "  'Co',\n",
       "  'occurrences',\n",
       "  '@',\n",
       "  '@',\n",
       "  'individual',\n",
       "  'occurrences'],\n",
       " [\"We're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'assume',\n",
       "  'words',\n",
       "  '@',\n",
       "  'high',\n",
       "  'co-occurrences,',\n",
       "  '@',\n",
       "  'relatively',\n",
       "  'low',\n",
       "  'individual',\n",
       "  'occurrences',\n",
       "  '@',\n",
       "  '@',\n",
       "  'syntagmatic',\n",
       "  'relations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  'occur',\n",
       "  'together,',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"don't\",\n",
       "  'usually',\n",
       "  'occur',\n",
       "  '@'],\n",
       " ['Note',\n",
       "  '@',\n",
       "  '@',\n",
       "  'paradigmatic',\n",
       "  'relation',\n",
       "  '@',\n",
       "  'syntagmatic',\n",
       "  'relation,',\n",
       "  '@',\n",
       "  'actually',\n",
       "  'closely',\n",
       "  'related'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'paradigmatically',\n",
       "  'related',\n",
       "  'words',\n",
       "  'tend',\n",
       "  '@',\n",
       "  '@',\n",
       "  'syntagmatic',\n",
       "  'relation',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  '@',\n",
       "  'associated',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'suggests',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'join',\n",
       "  '@',\n",
       "  'discovery',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'relations'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'general',\n",
       "  'ideas',\n",
       "  '@',\n",
       "  '@',\n",
       "  'implemented',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'ways,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'course',\n",
       "  \"won't\",\n",
       "  'cover',\n",
       "  '@',\n",
       "  '@',\n",
       "  'them,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'cover',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'methods',\n",
       "  '@',\n",
       "  'effective',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  '@',\n",
       "  'relations'],\n",
       " [],\n",
       " ['This', 'lecture', '@', '@', '@', 'paradigmatic', 'relation', 'discovery'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'lecture',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'discover',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'word',\n",
       "  'Association',\n",
       "  'called',\n",
       "  'paradigmatic',\n",
       "  'relations'],\n",
       " ['By',\n",
       "  'definition,',\n",
       "  '2',\n",
       "  'words',\n",
       "  '@',\n",
       "  'paradigmatically',\n",
       "  'related',\n",
       "  '@',\n",
       "  '@',\n",
       "  'share',\n",
       "  'similar',\n",
       "  'contexts'],\n",
       " ['Namely,', '@', 'occur', '@', 'similar', 'positions', '@', 'text'],\n",
       " ['So',\n",
       "  'naturally,',\n",
       "  '@',\n",
       "  'idea',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  '@',\n",
       "  'relation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'try',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contexts'],\n",
       " ['So', \"here's\", '@', 'example', '@', 'context', '@', 'word', 'Cat'],\n",
       " ['Here', 'I', '@', 'taken', '@', 'word', 'cat', '@', '@', '@', 'context'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'seeing',\n",
       "  '@',\n",
       "  'remaining',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentences',\n",
       "  '@',\n",
       "  'contain',\n",
       "  'cat'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'thing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'like',\n",
       "  '@',\n",
       "  'dog'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  'like',\n",
       "  '@',\n",
       "  'capture',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'try',\n",
       "  '@',\n",
       "  'assess',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'cat',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'like',\n",
       "  'dog'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'question',\n",
       "  'is,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'formally',\n",
       "  'represent',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'define',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  'function?',\n",
       "  'So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'note',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  'actually',\n",
       "  'contains',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['So', '@', '@', '@', 'regarded', '@', '@', 'pseudo', 'document'],\n",
       " ['An', 'imaginary', 'document'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'ways',\n",
       "  '@',\n",
       "  'looking',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  'occurs',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'cat'],\n",
       " ['We', '@', '@'],\n",
       " ['We', '@', '@', '@', 'context', 'left1', 'context'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  'like',\n",
       "  'my,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'big,',\n",
       "  'a,',\n",
       "  'the,',\n",
       "  'etc'],\n",
       " ['These',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'occur',\n",
       "  '@',\n",
       "  '@',\n",
       "  'left',\n",
       "  '@',\n",
       "  '@',\n",
       "  'world',\n",
       "  'cat'],\n",
       " ['So', '@', '@', '@', 'cat,', '@', 'cat', 'big', 'cat'],\n",
       " ['@', 'cat', 'etc'],\n",
       " ['Similarly,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collect',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'occur',\n",
       "  'right',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'cat'],\n",
       " ['We', '@', '@', '@', 'context', 'right1'],\n",
       " ['And', '@', '@', '@', 'words', 'eats,', 'ate,', 'is,', 'has,', 'etc'],\n",
       " ['Or',\n",
       "  '@',\n",
       "  'generally,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'window',\n",
       "  '@',\n",
       "  'text',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'cat'],\n",
       " ['Here',\n",
       "  \"let's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'window',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'world',\n",
       "  'cat'],\n",
       " ['We',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  'Window8',\n",
       "  'Now',\n",
       "  '@',\n",
       "  'course,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'left',\n",
       "  '@',\n",
       "  '@',\n",
       "  'right,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'bag',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  'represent',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['Now,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'based',\n",
       "  'representation',\n",
       "  '@',\n",
       "  'actually',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  'way',\n",
       "  '@',\n",
       "  'define',\n",
       "  '@',\n",
       "  'perspective',\n",
       "  '@',\n",
       "  'measuring',\n",
       "  '@',\n",
       "  'similarity'],\n",
       " ['Because',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  'left1,',\n",
       "  '@',\n",
       "  \"we'll\",\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'share',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'left',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'ignore',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  'context'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'gives',\n",
       "  '@',\n",
       "  '@',\n",
       "  'perspective',\n",
       "  '@',\n",
       "  'measure',\n",
       "  '@',\n",
       "  'similarity'],\n",
       " ['And',\n",
       "  'similarly,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'right1',\n",
       "  'context',\n",
       "  '@',\n",
       "  'capture',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  '@',\n",
       "  'perspective'],\n",
       " ['Using',\n",
       "  '@',\n",
       "  'left1',\n",
       "  '@',\n",
       "  'right1,',\n",
       "  'ofcourse',\n",
       "  '@',\n",
       "  'allow',\n",
       "  '@',\n",
       "  '@',\n",
       "  'capture',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'strict',\n",
       "  'criteria'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'general,',\n",
       "  'context',\n",
       "  '@',\n",
       "  'contain',\n",
       "  'adjacent',\n",
       "  'words',\n",
       "  'like',\n",
       "  'eats',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'non-adjacent',\n",
       "  'words',\n",
       "  'like',\n",
       "  'Saturday,',\n",
       "  'Tuesday',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'flexibility',\n",
       "  '@',\n",
       "  'allows',\n",
       "  '@',\n",
       "  '@',\n",
       "  'measure',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'ways'],\n",
       " ['Sometimes',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'want',\n",
       "  '@',\n",
       "  'capture',\n",
       "  'similarity',\n",
       "  'based',\n",
       "  '@',\n",
       "  'general',\n",
       "  'content',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'loosely',\n",
       "  'related',\n",
       "  'paradigmatic',\n",
       "  'relations,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  'immediately',\n",
       "  '@',\n",
       "  '@',\n",
       "  'left',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'right',\n",
       "  '@',\n",
       "  '@',\n",
       "  'world,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'likely',\n",
       "  '@',\n",
       "  'capture',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'related',\n",
       "  '@',\n",
       "  '@',\n",
       "  'syntactical',\n",
       "  'categories',\n",
       "  '@',\n",
       "  'semantics'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'general',\n",
       "  'idea',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  'paradigmatic',\n",
       "  'relations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'measure',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  'cat',\n",
       "  '@',\n",
       "  'dog',\n",
       "  'based',\n",
       "  '@',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contexts'],\n",
       " ['In',\n",
       "  'general,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'combine',\n",
       "  '@',\n",
       "  'kinds',\n",
       "  '@',\n",
       "  'views',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  'function',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  'combination',\n",
       "  '@',\n",
       "  'similarities',\n",
       "  '@',\n",
       "  'different',\n",
       "  'contexts'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'assign',\n",
       "  'weights',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'similarities',\n",
       "  '@',\n",
       "  'allow',\n",
       "  '@',\n",
       "  '@',\n",
       "  'focus',\n",
       "  '@',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'context,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'naturally',\n",
       "  'application',\n",
       "  'specific,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'main',\n",
       "  'idea',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  'paradigmatically',\n",
       "  'related',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['So',\n",
       "  'next,',\n",
       "  \"let's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'exactly',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  'functions'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'answer',\n",
       "  '@',\n",
       "  'question',\n",
       "  \"it's\",\n",
       "  'useful',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  'bag',\n",
       "  '@',\n",
       "  'words',\n",
       "  'representation',\n",
       "  '@',\n",
       "  'vectors',\n",
       "  '@',\n",
       "  '@',\n",
       "  'vector',\n",
       "  'space',\n",
       "  'model'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'familiar',\n",
       "  '@',\n",
       "  'information',\n",
       "  'retrieval',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval',\n",
       "  'techniques',\n",
       "  '@',\n",
       "  'realize',\n",
       "  '@',\n",
       "  'vector',\n",
       "  'space',\n",
       "  'model',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'frequently',\n",
       "  '@',\n",
       "  'modeling',\n",
       "  'documents',\n",
       "  '@',\n",
       "  'queries',\n",
       "  '@',\n",
       "  'search'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'find',\n",
       "  '@',\n",
       "  'convenient',\n",
       "  '@',\n",
       "  'model',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  'paradigmatically',\n",
       "  'relation',\n",
       "  'discovery'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'idea',\n",
       "  '@',\n",
       "  '@',\n",
       "  'approach',\n",
       "  '@',\n",
       "  '@',\n",
       "  'view',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'vocabulary',\n",
       "  '@',\n",
       "  'defining',\n",
       "  '@',\n",
       "  'dimension',\n",
       "  '@',\n",
       "  'high',\n",
       "  'dimensional',\n",
       "  'space',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'N',\n",
       "  'words',\n",
       "  '@',\n",
       "  'total',\n",
       "  '@',\n",
       "  '@',\n",
       "  'vocabulary'],\n",
       " ['Then', '@', '@', 'N', 'dimensions', '@', 'illustrated', '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'frequency',\n",
       "  'vector',\n",
       "  'representing',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'eats',\n",
       "  'occured',\n",
       "  '@',\n",
       "  'times',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context,',\n",
       "  'ate',\n",
       "  'occurred',\n",
       "  '@',\n",
       "  'times',\n",
       "  'etc'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'vector',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'placed',\n",
       "  '@',\n",
       "  '@',\n",
       "  'vector',\n",
       "  'space',\n",
       "  'model'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'general,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'represent',\n",
       "  '@',\n",
       "  'pseudo',\n",
       "  'document',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'cat',\n",
       "  '@',\n",
       "  '@',\n",
       "  'vector'],\n",
       " ['d1'],\n",
       " ['An',\n",
       "  '@',\n",
       "  'word',\n",
       "  'dog',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'context,',\n",
       "  '@',\n",
       "  'd2'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'measure',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'vectors'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'viewing',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'vector',\n",
       "  'space',\n",
       "  'model,',\n",
       "  '@',\n",
       "  'convert',\n",
       "  '@',\n",
       "  'problem',\n",
       "  '@',\n",
       "  'paradigmatic',\n",
       "  'relations',\n",
       "  'discovery',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problem',\n",
       "  '@',\n",
       "  'computing',\n",
       "  '@',\n",
       "  'vectors',\n",
       "  '@',\n",
       "  '@',\n",
       "  'similarity'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'questions',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'address',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'vector,',\n",
       "  '@',\n",
       "  'is,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'xi',\n",
       "  '@',\n",
       "  'yi?',\n",
       "  'And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'question',\n",
       "  'is,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'similarity?',\n",
       "  'Now',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'approaches',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'solve',\n",
       "  '@',\n",
       "  'problem,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'developed',\n",
       "  '@',\n",
       "  'information',\n",
       "  'retrieval'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'shown',\n",
       "  '@',\n",
       "  'work',\n",
       "  '@',\n",
       "  '@',\n",
       "  'matching',\n",
       "  '@',\n",
       "  'query',\n",
       "  'vector',\n",
       "  '@',\n",
       "  '@',\n",
       "  'document',\n",
       "  'vector,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'adapt',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ideas',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  'context',\n",
       "  'documents',\n",
       "  '@',\n",
       "  '@',\n",
       "  'purpose',\n",
       "  '@'],\n",
       " ['So',\n",
       "  \"let's\",\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'possible',\n",
       "  'approach,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'try',\n",
       "  '@',\n",
       "  'measure',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  'context',\n",
       "  'based',\n",
       "  '@',\n",
       "  '@',\n",
       "  'expected',\n",
       "  'overlap',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'EOWC'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'idea',\n",
       "  '@',\n",
       "  '@',\n",
       "  'represent',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'award',\n",
       "  'vector',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'weight',\n",
       "  '@',\n",
       "  '@',\n",
       "  'equal',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  'randomly',\n",
       "  'picked',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'document',\n",
       "  'vector',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word'],\n",
       " ['So', '@', '@', 'words'],\n",
       " ['xi',\n",
       "  '@',\n",
       "  'defined',\n",
       "  '@',\n",
       "  '@',\n",
       "  'normalized',\n",
       "  'count',\n",
       "  '@',\n",
       "  'word',\n",
       "  'wi',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interpreted',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'actually',\n",
       "  'pick',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  'd1',\n",
       "  '@',\n",
       "  '@',\n",
       "  'randomly',\n",
       "  'pick',\n",
       "  '@',\n",
       "  'word'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  \"xi's\",\n",
       "  '@',\n",
       "  'sum',\n",
       "  '@',\n",
       "  '1',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'normalized',\n",
       "  'frequencies'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'means',\n",
       "  '@',\n",
       "  'vector',\n",
       "  '@',\n",
       "  'actually',\n",
       "  'probability',\n",
       "  'distribution',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['So,', '@', 'vector', 'd2', '@', '@', '@', 'computed', '@', '@', '@', 'way'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  'distributions',\n",
       "  'representing',\n",
       "  '@',\n",
       "  'contexts'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'addresses',\n",
       "  '@',\n",
       "  'problem',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'vectors?',\n",
       "  'Next,',\n",
       "  \"let's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'define',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  '@',\n",
       "  'approach'],\n",
       " ['Well,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'simply',\n",
       "  'define',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  '@',\n",
       "  'dot',\n",
       "  'product',\n",
       "  '@',\n",
       "  '@',\n",
       "  'vectors',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'defined',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sum',\n",
       "  '@',\n",
       "  '@',\n",
       "  'products',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'corresponding',\n",
       "  'elements',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'vectors'],\n",
       " ['Now',\n",
       "  \"it's\",\n",
       "  'interesting',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  'function',\n",
       "  'actually',\n",
       "  '@',\n",
       "  '@',\n",
       "  'nice',\n",
       "  'interpretation'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'dot',\n",
       "  'product',\n",
       "  'infact',\n",
       "  'gives',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  'randomly',\n",
       "  'picked',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contexts',\n",
       "  '@',\n",
       "  'identical',\n",
       "  '@',\n",
       "  'means',\n",
       "  '@',\n",
       "  '@',\n",
       "  'try',\n",
       "  '@',\n",
       "  'pick',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'try',\n",
       "  '@',\n",
       "  'pick',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ask',\n",
       "  '@',\n",
       "  'question,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'identical?',\n",
       "  'If',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contexts',\n",
       "  '@',\n",
       "  '@',\n",
       "  'similar,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'expect',\n",
       "  '@',\n",
       "  '@',\n",
       "  'frequently',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  'picked',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contexts',\n",
       "  '@',\n",
       "  'identical'],\n",
       " ['If',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  '@',\n",
       "  '@',\n",
       "  'chance',\n",
       "  '@',\n",
       "  'seeing',\n",
       "  'identical',\n",
       "  'words',\n",
       "  '@',\n",
       "  'picked',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contexts',\n",
       "  '@',\n",
       "  '@',\n",
       "  'small'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'intuitively',\n",
       "  'makes',\n",
       "  'sense',\n",
       "  '@',\n",
       "  'measuring',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  'contexts'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  'want',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'exact',\n",
       "  'formulas',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interpreted',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  'randomly',\n",
       "  'picked',\n",
       "  'words',\n",
       "  '@',\n",
       "  'identical'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'stay',\n",
       "  '@',\n",
       "  '@',\n",
       "  'formula',\n",
       "  '@',\n",
       "  'check',\n",
       "  \"what's\",\n",
       "  'inside',\n",
       "  '@',\n",
       "  'sum',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'see,',\n",
       "  'basically',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  'gives',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  \"we'll\",\n",
       "  '@',\n",
       "  'overlap',\n",
       "  '@',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'word,',\n",
       "  'wi',\n",
       "  '@',\n",
       "  '@',\n",
       "  'xi',\n",
       "  'gives',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  'pick',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'word',\n",
       "  '@',\n",
       "  'd1',\n",
       "  '@',\n",
       "  'yi',\n",
       "  'gives',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  'picking',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  'd2',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'pick',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contexts',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'identical',\n",
       "  'pick'],\n",
       " ['Alright,', '@', \"that's\", '@', 'possible', 'approach'],\n",
       " ['EOWC', 'expected', 'overlap', '@', 'words', '@', 'context'],\n",
       " ['Now,',\n",
       "  '@',\n",
       "  'always,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'like',\n",
       "  '@',\n",
       "  'assess',\n",
       "  '@',\n",
       "  '@',\n",
       "  'approach',\n",
       "  '@',\n",
       "  '@',\n",
       "  'work',\n",
       "  '@'],\n",
       " ['Now,',\n",
       "  '@',\n",
       "  'course,',\n",
       "  'ultimately',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'test',\n",
       "  '@',\n",
       "  'approach',\n",
       "  '@',\n",
       "  'real',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'gives',\n",
       "  '@',\n",
       "  '@',\n",
       "  'semantically',\n",
       "  'related',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'paradigmatic',\n",
       "  'relations'],\n",
       " ['But',\n",
       "  'analytically,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'analyze',\n",
       "  '@',\n",
       "  'formula',\n",
       "  'little',\n",
       "  'bit'],\n",
       " ['So',\n",
       "  'first,',\n",
       "  '@',\n",
       "  'I',\n",
       "  'said,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sense',\n",
       "  'right?',\n",
       "  '@',\n",
       "  '@',\n",
       "  'formula',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'higher',\n",
       "  'score',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'overlap',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contexts'],\n",
       " ['So', \"that's\", 'exactly', '@', '@', 'want'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  'analyze',\n",
       "  '@',\n",
       "  'formula',\n",
       "  '@',\n",
       "  'carefully,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'potential',\n",
       "  'problems'],\n",
       " ['And', 'specifically', '@', '@', '@', 'potential', 'problems'],\n",
       " ['First',\n",
       "  '@',\n",
       "  '@',\n",
       "  'favor',\n",
       "  'matching',\n",
       "  '@',\n",
       "  'frequent',\n",
       "  'term',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'matching',\n",
       "  '@',\n",
       "  'distinct',\n",
       "  'terms,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'dot',\n",
       "  'product,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'element',\n",
       "  '@',\n",
       "  '@',\n",
       "  'high',\n",
       "  'value',\n",
       "  '@',\n",
       "  '@',\n",
       "  'element',\n",
       "  '@',\n",
       "  'shared',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contributes',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  '@',\n",
       "  'overall',\n",
       "  'sum'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'score',\n",
       "  'higher',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'vectors',\n",
       "  'actually',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'overlap',\n",
       "  '@',\n",
       "  'different',\n",
       "  'terms,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'term',\n",
       "  '@',\n",
       "  '@',\n",
       "  'relatively',\n",
       "  'low',\n",
       "  'frequency'],\n",
       " ['So', '@', '@', '@', '@', 'desirable'],\n",
       " ['Of',\n",
       "  'course,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'desirable',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'cases,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  '@',\n",
       "  'intuitively',\n",
       "  'prefer',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  '@',\n",
       "  'match',\n",
       "  '@',\n",
       "  'different',\n",
       "  'terms',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'confidence',\n",
       "  '@',\n",
       "  'saying',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'occur',\n",
       "  '@',\n",
       "  'similar',\n",
       "  'context'],\n",
       " ['If',\n",
       "  '@',\n",
       "  '@',\n",
       "  'rely',\n",
       "  '@',\n",
       "  '@',\n",
       "  'term',\n",
       "  '@',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  'little',\n",
       "  'bit',\n",
       "  'questionable'],\n",
       " ['It', '@', '@', '@', 'robust'],\n",
       " ['The',\n",
       "  'second',\n",
       "  'problem',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'treats',\n",
       "  '@',\n",
       "  'word',\n",
       "  'equally,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'match',\n",
       "  '@',\n",
       "  'word',\n",
       "  'like',\n",
       "  'the,',\n",
       "  '@',\n",
       "  'match',\n",
       "  'was,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'matching',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'like',\n",
       "  'eats'],\n",
       " ['But',\n",
       "  'intuitively',\n",
       "  '@',\n",
       "  'know',\n",
       "  'matching',\n",
       "  '@',\n",
       "  \"isn't\",\n",
       "  '@',\n",
       "  'surprising',\n",
       "  '@',\n",
       "  '@',\n",
       "  'occurs',\n",
       "  'everywhere,',\n",
       "  '@',\n",
       "  'matching',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'strong',\n",
       "  'evidence',\n",
       "  '@',\n",
       "  'matching',\n",
       "  '@',\n",
       "  'word',\n",
       "  'like',\n",
       "  'eats',\n",
       "  '@',\n",
       "  \"doesn't\",\n",
       "  'occur',\n",
       "  'frequently'],\n",
       " ['So', '@', '@', '@', 'problem', '@', '@', 'approach'],\n",
       " ['In',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lecture,',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'address',\n",
       "  '@',\n",
       "  'problems'],\n",
       " [],\n",
       " ['In',\n",
       "  '@',\n",
       "  'lecture,',\n",
       "  '@',\n",
       "  'continue',\n",
       "  'discussing',\n",
       "  'paradigmatic',\n",
       "  'relation',\n",
       "  'discovery'],\n",
       " ['Earlier,',\n",
       "  '@',\n",
       "  'introduced',\n",
       "  '@',\n",
       "  'method',\n",
       "  'called',\n",
       "  'expected',\n",
       "  'overlap',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'method,',\n",
       "  '@',\n",
       "  'represent',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'vector',\n",
       "  '@',\n",
       "  'represents',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'measure',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'DOT',\n",
       "  'product'],\n",
       " ['Which',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interpreted',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  'randomly',\n",
       "  'pick',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contexts',\n",
       "  '@',\n",
       "  'identical,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'discuss',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problems',\n",
       "  '@',\n",
       "  '@',\n",
       "  'method'],\n",
       " ['The',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'favors',\n",
       "  'matching',\n",
       "  '@',\n",
       "  'frequent',\n",
       "  'term',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'matching',\n",
       "  '@',\n",
       "  'distinct',\n",
       "  'terms'],\n",
       " ['It', '@', '@', '@', 'emphasis', '@', 'matching', '@', 'term', '@', '@'],\n",
       " ['The', 'second', '@', '@', '@', 'treats', '@', 'word', 'equally'],\n",
       " ['Even',\n",
       "  '@',\n",
       "  'common',\n",
       "  'word',\n",
       "  'like',\n",
       "  \"'the'\",\n",
       "  '@',\n",
       "  'contribute',\n",
       "  'equally',\n",
       "  '@',\n",
       "  'content',\n",
       "  'word',\n",
       "  'like',\n",
       "  \"'eats'\"],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'going',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'solve',\n",
       "  '@',\n",
       "  'problems'],\n",
       " ['Most',\n",
       "  'specifically,',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'introduce',\n",
       "  '@',\n",
       "  'retrieval',\n",
       "  'heuristics',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'heuristics',\n",
       "  '@',\n",
       "  'effectively',\n",
       "  'solve',\n",
       "  '@',\n",
       "  'problems,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problems',\n",
       "  '@',\n",
       "  'occur',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval',\n",
       "  '@',\n",
       "  '@',\n",
       "  'match',\n",
       "  '@',\n",
       "  'query',\n",
       "  'vector',\n",
       "  '@',\n",
       "  'document',\n",
       "  'vector'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'address',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problem,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'sub',\n",
       "  'linear',\n",
       "  'transformation',\n",
       "  '@',\n",
       "  'term',\n",
       "  'frequency'],\n",
       " ['That',\n",
       "  'is,',\n",
       "  '@',\n",
       "  \"don't\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'raw',\n",
       "  'frequency',\n",
       "  'count',\n",
       "  '@',\n",
       "  'term',\n",
       "  '@',\n",
       "  'represent',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['We',\n",
       "  '@',\n",
       "  'transform',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'form',\n",
       "  '@',\n",
       "  \"wouldn't\",\n",
       "  'emphasize',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'raw',\n",
       "  'frequency'],\n",
       " ['To',\n",
       "  'address',\n",
       "  '@',\n",
       "  'second',\n",
       "  'problem,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'weight',\n",
       "  '@',\n",
       "  'rare',\n",
       "  'terms'],\n",
       " ['That',\n",
       "  'is,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'reward',\n",
       "  'matching',\n",
       "  '@',\n",
       "  'rare',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'heuristic',\n",
       "  '@',\n",
       "  'called',\n",
       "  'IDF',\n",
       "  'term',\n",
       "  'weighting',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval'],\n",
       " ['IDF', 'stands', '@', 'inverse', 'document', 'frequency'],\n",
       " ['So',\n",
       "  '@',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'heuristics',\n",
       "  '@',\n",
       "  '@',\n",
       "  'detail'],\n",
       " ['First,', \"let's\", 'talk', '@', '@', 'TF', 'transformation'],\n",
       " ['That',\n",
       "  'is,',\n",
       "  '@',\n",
       "  'convert',\n",
       "  '@',\n",
       "  'raw',\n",
       "  'count',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'document',\n",
       "  '@',\n",
       "  '@',\n",
       "  'weight',\n",
       "  '@',\n",
       "  'reflects',\n",
       "  '@',\n",
       "  'belief',\n",
       "  '@',\n",
       "  '@',\n",
       "  'important',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'document'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'denoted',\n",
       "  '@',\n",
       "  'TF',\n",
       "  '@',\n",
       "  'W&amp;D',\n",
       "  '@',\n",
       "  'shown',\n",
       "  '@',\n",
       "  '@',\n",
       "  'Y',\n",
       "  'axis'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ways',\n",
       "  '@',\n",
       "  'map',\n",
       "  'that,',\n",
       "  '@',\n",
       "  \"let's\",\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'simple',\n",
       "  'way',\n",
       "  '@',\n",
       "  'mapping'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'case,',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'say,',\n",
       "  '@',\n",
       "  'non',\n",
       "  'zero',\n",
       "  'counts',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mapped',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['And', '@', 'zero', 'count', '@', '@', 'mapped', '@', '0'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mapping,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'frequencies',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mapped',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'values,',\n",
       "  'zero',\n",
       "  '@',\n",
       "  'one,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mapping',\n",
       "  'function',\n",
       "  '@',\n",
       "  'shown',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'flat',\n",
       "  'line',\n",
       "  '@'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  'naive',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ignored',\n",
       "  '@',\n",
       "  'frequency',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['However,',\n",
       "  '@',\n",
       "  'actually',\n",
       "  '@',\n",
       "  '@',\n",
       "  'advantage',\n",
       "  '@',\n",
       "  'emphasizing',\n",
       "  'matching',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'allow',\n",
       "  '@',\n",
       "  'frequent',\n",
       "  'word',\n",
       "  '@',\n",
       "  'dominate',\n",
       "  '@',\n",
       "  'matching'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'approach',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'taken',\n",
       "  'earlier',\n",
       "  '@',\n",
       "  '@',\n",
       "  'expected',\n",
       "  'overlap',\n",
       "  'account',\n",
       "  'approach',\n",
       "  '@',\n",
       "  '@',\n",
       "  'linear',\n",
       "  'transformation'],\n",
       " ['We', 'basically', '@', 'Y', '@', '@', '@', '@', 'X'],\n",
       " ['So', '@', 'use', '@', 'raw', 'count', '@', 'representation'],\n",
       " ['And', '@', 'created', '@', 'problem', '@', '@', '@', 'talked', '@'],\n",
       " ['Namely',\n",
       "  '@',\n",
       "  'answers',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'matching',\n",
       "  '@',\n",
       "  'frequent',\n",
       "  'term'],\n",
       " ['Matching', '@', 'frequent', 'term', '@', 'contribute', '@', 'lot'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  'transformations',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'extremes'],\n",
       " ['And', '@', 'generally', 'form', '@', 'sub', 'linear', 'transformation'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'example,',\n",
       "  '@',\n",
       "  'possibility',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'logarithm',\n",
       "  '@',\n",
       "  '@',\n",
       "  'raw',\n",
       "  'count,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'curve',\n",
       "  '@',\n",
       "  'looks',\n",
       "  'like',\n",
       "  'this,',\n",
       "  'right?',\n",
       "  'That',\n",
       "  \"you're\",\n",
       "  'seeing',\n",
       "  '@'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'case,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'high',\n",
       "  'frequency',\n",
       "  'counts,',\n",
       "  '@',\n",
       "  'high',\n",
       "  'counts',\n",
       "  '@',\n",
       "  'penalized',\n",
       "  '@',\n",
       "  'little',\n",
       "  'bit',\n",
       "  'right?',\n",
       "  'So',\n",
       "  '@',\n",
       "  'curve',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sub',\n",
       "  'linear',\n",
       "  'curve,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'brings',\n",
       "  '@',\n",
       "  '@',\n",
       "  'weight',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'high',\n",
       "  'counts'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'want,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'prevents',\n",
       "  '@',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'terms',\n",
       "  '@',\n",
       "  'dominating',\n",
       "  '@',\n",
       "  'scoring',\n",
       "  'function'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  'transformation',\n",
       "  'called',\n",
       "  '@',\n",
       "  'BM25',\n",
       "  'transformation',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'shown',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'effective',\n",
       "  '@',\n",
       "  'retrieval',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'transformation',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'form',\n",
       "  '@'],\n",
       " ['Looks', 'like', '@'],\n",
       " ['I',\n",
       "  'saw',\n",
       "  \"it's\",\n",
       "  '(K',\n",
       "  '+',\n",
       "  '1)',\n",
       "  '*',\n",
       "  'X',\n",
       "  '/(',\n",
       "  'X',\n",
       "  '+',\n",
       "  'K)',\n",
       "  '@',\n",
       "  'K',\n",
       "  '@',\n",
       "  '@',\n",
       "  'parameter'],\n",
       " ['X', '@', '@', 'count,', '@', 'raw', 'count', '@', 'word'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'transformation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'actually',\n",
       "  'kind',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'extreme',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'extreme',\n",
       "  '@',\n",
       "  'varying',\n",
       "  'K'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'upper',\n",
       "  'bound',\n",
       "  'K',\n",
       "  '+1',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'puts',\n",
       "  '@',\n",
       "  '@',\n",
       "  'strict',\n",
       "  'constraint',\n",
       "  '@',\n",
       "  'high',\n",
       "  'frequency',\n",
       "  'terms,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'weight',\n",
       "  '@',\n",
       "  '@',\n",
       "  'exceed',\n",
       "  'K+1'],\n",
       " ['As', '@', 'vary', 'K,', '@', '@', '@', 'simulate', '@', '@', 'extremes'],\n",
       " ['So', '@', 'case', '@', 'set', '@', 'zero'],\n",
       " ['We', 'roughly', '@', '@', '01', 'vector'],\n",
       " ['Whereas',\n",
       "  '@',\n",
       "  '@',\n",
       "  'set',\n",
       "  '@',\n",
       "  'key',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'large',\n",
       "  'value,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'behave',\n",
       "  '@',\n",
       "  'like',\n",
       "  '@',\n",
       "  'linear',\n",
       "  'transformation'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'transformation',\n",
       "  'function',\n",
       "  '@',\n",
       "  '@',\n",
       "  'far',\n",
       "  '@',\n",
       "  '@',\n",
       "  'effective',\n",
       "  'transformation',\n",
       "  'function',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'makes',\n",
       "  'sense',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problem',\n",
       "  'set',\n",
       "  '@'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'solve',\n",
       "  '@',\n",
       "  'problem',\n",
       "  '@',\n",
       "  '@',\n",
       "  'emphasizing',\n",
       "  '@',\n",
       "  'frequently',\n",
       "  'frequent',\n",
       "  'term'],\n",
       " ['Now',\n",
       "  \"let's\",\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'second',\n",
       "  'problem,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'penalize',\n",
       "  'popular',\n",
       "  'terms'],\n",
       " ['Matching',\n",
       "  \"'the'\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'surprising',\n",
       "  '@',\n",
       "  \"'the'\",\n",
       "  'occurs',\n",
       "  'everywhere,',\n",
       "  '@',\n",
       "  'matching',\n",
       "  \"'eats'\",\n",
       "  '@',\n",
       "  'account',\n",
       "  '@',\n",
       "  'lot'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'address',\n",
       "  '@',\n",
       "  'problem?',\n",
       "  'In',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'IDF',\n",
       "  'weighting',\n",
       "  \"that's\",\n",
       "  'commonly',\n",
       "  '@',\n",
       "  '@',\n",
       "  'retrieval'],\n",
       " ['IDF', 'stands', '@', 'inverse', 'document', 'frequency'],\n",
       " ['Document',\n",
       "  'frequency',\n",
       "  'means',\n",
       "  '@',\n",
       "  'count',\n",
       "  '@',\n",
       "  '@',\n",
       "  'total',\n",
       "  'number',\n",
       "  '@',\n",
       "  'documents',\n",
       "  '@',\n",
       "  'contain',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'word'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'IDF',\n",
       "  'measure',\n",
       "  '@',\n",
       "  'defined',\n",
       "  '@',\n",
       "  '@',\n",
       "  'logarithm',\n",
       "  'function',\n",
       "  '@',\n",
       "  '@',\n",
       "  'number',\n",
       "  '@',\n",
       "  'documents',\n",
       "  '@',\n",
       "  'match',\n",
       "  '@',\n",
       "  'term,',\n",
       "  '@',\n",
       "  'document',\n",
       "  'frequency'],\n",
       " ['So',\n",
       "  'K',\n",
       "  '@',\n",
       "  '@',\n",
       "  'number',\n",
       "  '@',\n",
       "  'documents',\n",
       "  'containing',\n",
       "  'word',\n",
       "  '@',\n",
       "  'document',\n",
       "  'frequency',\n",
       "  '@',\n",
       "  'M',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'total',\n",
       "  'number',\n",
       "  '@',\n",
       "  'documents',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collection'],\n",
       " ['The',\n",
       "  'IDF',\n",
       "  'function',\n",
       "  '@',\n",
       "  'giving',\n",
       "  '@',\n",
       "  'higher',\n",
       "  'value',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lower',\n",
       "  'K,',\n",
       "  'meaning',\n",
       "  '@',\n",
       "  '@',\n",
       "  'rewards',\n",
       "  '@',\n",
       "  'rare',\n",
       "  'term'],\n",
       " ['And', '@', 'maximum', 'value', '@', 'log', '@', 'M', '+', '1'],\n",
       " [\"That's\", '@', '@', 'word', 'occurs', '@', '@', '@', '@', 'context'],\n",
       " ['So',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'rare',\n",
       "  'term,',\n",
       "  '@',\n",
       "  'rarest',\n",
       "  'term',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collection'],\n",
       " ['The',\n",
       "  'lowest',\n",
       "  'value',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'K',\n",
       "  'reaches',\n",
       "  '@',\n",
       "  'maximum,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'M'],\n",
       " ['That', '@', '@', '@', '@', 'low', 'value', 'close', '@', '0', '@', 'fact'],\n",
       " ['Right', '@', '@'],\n",
       " ['This',\n",
       "  '@',\n",
       "  'course',\n",
       "  'measure',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'search',\n",
       "  '@',\n",
       "  '@',\n",
       "  'naturally',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collection'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'case,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collection?',\n",
       "  'We',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collect',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collection',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'say,',\n",
       "  '@',\n",
       "  'word',\n",
       "  \"that's\",\n",
       "  'popular',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collection',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'low',\n",
       "  'IDF'],\n",
       " ['Because',\n",
       "  'depending',\n",
       "  '@',\n",
       "  '@',\n",
       "  'data',\n",
       "  'set,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'construct',\n",
       "  '@',\n",
       "  'context',\n",
       "  'vectors',\n",
       "  '@',\n",
       "  'different',\n",
       "  'ways,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'end,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'term',\n",
       "  '@',\n",
       "  '@',\n",
       "  'frequently',\n",
       "  '@',\n",
       "  '@',\n",
       "  'original',\n",
       "  'data',\n",
       "  'set,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'frequently',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collected',\n",
       "  'context',\n",
       "  'documents'],\n",
       " ['So', '@', '@', '@', 'add', '@', 'heuristics', '@', 'improve', '@'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Our', 'similarity', 'function'],\n",
       " [\"Here's\",\n",
       "  '@',\n",
       "  'way,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ways',\n",
       "  '@',\n",
       "  '@',\n",
       "  'possible'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'reasonable',\n",
       "  'way',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'adapt',\n",
       "  '@',\n",
       "  'BM25',\n",
       "  'retrieval',\n",
       "  'model',\n",
       "  '@',\n",
       "  'paradigmatic',\n",
       "  'relation',\n",
       "  'mining'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'define',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  'define',\n",
       "  '@',\n",
       "  'document',\n",
       "  'vector'],\n",
       " ['As',\n",
       "  'containing',\n",
       "  'elements',\n",
       "  'representing',\n",
       "  'normalized',\n",
       "  'BM',\n",
       "  '25',\n",
       "  'values'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'normalization',\n",
       "  'function',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sum',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'normalize',\n",
       "  '@',\n",
       "  'weight',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sum',\n",
       "  '@',\n",
       "  '@',\n",
       "  'weights',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['This',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ensure',\n",
       "  '@',\n",
       "  '@',\n",
       "  'x(i)',\n",
       "  '@',\n",
       "  'sum',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'vector'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'similar',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'vector',\n",
       "  '@',\n",
       "  'actually',\n",
       "  '@',\n",
       "  'similar',\n",
       "  '@',\n",
       "  'word',\n",
       "  'distribution',\n",
       "  '@',\n",
       "  '@',\n",
       "  'exercise',\n",
       "  '@',\n",
       "  'sum',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['Now', '@', 'weight', '@', 'BM25', '@', '@', 'word', '@', 'defined', '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compare',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'old',\n",
       "  'definition',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'normalized',\n",
       "  'count'],\n",
       " ['On',\n",
       "  '@',\n",
       "  'one,',\n",
       "  'right?',\n",
       "  'So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'document',\n",
       "  'length',\n",
       "  '@',\n",
       "  '@',\n",
       "  'total',\n",
       "  'count',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  'document'],\n",
       " ['And', \"that's\", '@', '@', '@', '@'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'BM',\n",
       "  '25',\n",
       "  'transformation,',\n",
       "  '@',\n",
       "  'introduced',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['First,',\n",
       "  '@',\n",
       "  'course,',\n",
       "  '@',\n",
       "  'extra',\n",
       "  'occurrence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'count',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'achieve',\n",
       "  '@',\n",
       "  'sub',\n",
       "  'linear',\n",
       "  'normalization'],\n",
       " ['But', '@', '@', '@', '@', 'introduce', '@', 'parameter', 'K', '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'parameter',\n",
       "  '@',\n",
       "  'generally',\n",
       "  'non',\n",
       "  'negetive',\n",
       "  'number,',\n",
       "  '@',\n",
       "  'zero',\n",
       "  '@',\n",
       "  '@',\n",
       "  'possible'],\n",
       " ['This',\n",
       "  'controls',\n",
       "  '@',\n",
       "  'upper',\n",
       "  'bound',\n",
       "  '@',\n",
       "  '@',\n",
       "  'kinds',\n",
       "  'controls',\n",
       "  '@',\n",
       "  'choose',\n",
       "  'To',\n",
       "  '@',\n",
       "  'extent',\n",
       "  '@',\n",
       "  'simulates',\n",
       "  '@',\n",
       "  'linear',\n",
       "  'transformation'],\n",
       " ['And', '@', '@', '@', '1', 'parameter'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'parameter',\n",
       "  '@',\n",
       "  'b',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'zero',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'parameter',\n",
       "  '@',\n",
       "  'control聽',\n",
       "  'length',\n",
       "  'normalization'],\n",
       " ['And,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  'normalizing',\n",
       "  'formula',\n",
       "  '@',\n",
       "  'average',\n",
       "  'document',\n",
       "  'length',\n",
       "  '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'computed',\n",
       "  '@',\n",
       "  'taking',\n",
       "  '@',\n",
       "  'average',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lengths',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'documents',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collection'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'case,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lengths',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  'documents',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'considering'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'average',\n",
       "  'documents',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'constant',\n",
       "  '@',\n",
       "  '@',\n",
       "  'given',\n",
       "  'collection,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'actually',\n",
       "  '@',\n",
       "  '@',\n",
       "  'affecting',\n",
       "  '@',\n",
       "  'effect',\n",
       "  '@',\n",
       "  '@',\n",
       "  'parameter',\n",
       "  'B',\n",
       "  '@'],\n",
       " ['Because', '@', '@', '@', 'constant'],\n",
       " ['But',\n",
       "  'I',\n",
       "  'kept',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  'constant',\n",
       "  \"that's\",\n",
       "  'useful',\n",
       "  '@',\n",
       "  'retrieval,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'stabilized',\n",
       "  'interpretation',\n",
       "  '@',\n",
       "  'parameter',\n",
       "  'b'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  'purpose',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'constant,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'affecting',\n",
       "  '@',\n",
       "  'length',\n",
       "  'formalization',\n",
       "  '@',\n",
       "  '@',\n",
       "  'parameter',\n",
       "  'B'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  'definition,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'new',\n",
       "  'way',\n",
       "  '@',\n",
       "  'define',\n",
       "  '@',\n",
       "  'document',\n",
       "  'vectors',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'vector',\n",
       "  'D2',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'way'],\n",
       " ['The',\n",
       "  'difference',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'high',\n",
       "  'frequency',\n",
       "  'terms',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'somewhat',\n",
       "  'lower',\n",
       "  'weights',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  'control',\n",
       "  '@',\n",
       "  'influence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'high',\n",
       "  'frequency',\n",
       "  'terms'],\n",
       " ['Now', '@', 'IDF', '@', '@', 'added', '@', '@', '@', 'scoring', 'function'],\n",
       " ['That',\n",
       "  'means',\n",
       "  \"we'll\",\n",
       "  'introduce',\n",
       "  'weight',\n",
       "  '@',\n",
       "  'matching',\n",
       "  '@',\n",
       "  'term'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'recall',\n",
       "  '@',\n",
       "  'sum',\n",
       "  'indicates',\n",
       "  '@',\n",
       "  '@',\n",
       "  'possible',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'overlap',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contexts'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'Xi',\n",
       "  '@',\n",
       "  'Yi',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  'picking',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'contexts,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'indicates',\n",
       "  '@',\n",
       "  'likely',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'match',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word'],\n",
       " ['Now',\n",
       "  'IDF',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'importance',\n",
       "  '@',\n",
       "  'matching',\n",
       "  '@',\n",
       "  'word'],\n",
       " ['A',\n",
       "  'common',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'worth',\n",
       "  '@',\n",
       "  '@',\n",
       "  'rare',\n",
       "  'word,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'emphasize',\n",
       "  '@',\n",
       "  '@',\n",
       "  'matching',\n",
       "  'rare',\n",
       "  'words',\n",
       "  '@'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'modification,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'new',\n",
       "  'function',\n",
       "  '@',\n",
       "  'likely',\n",
       "  'address',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problems'],\n",
       " ['Now',\n",
       "  'interestingly',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'approach',\n",
       "  '@',\n",
       "  'discover',\n",
       "  'syntagmatic',\n",
       "  'relations'],\n",
       " ['In',\n",
       "  'general,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'represent',\n",
       "  '@',\n",
       "  'term',\n",
       "  'vector',\n",
       "  '@',\n",
       "  'represent',\n",
       "  '@',\n",
       "  'sorry',\n",
       "  '@',\n",
       "  'represent',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'term',\n",
       "  'vector,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'likely',\n",
       "  '@',\n",
       "  '@',\n",
       "  'terms',\n",
       "  '@',\n",
       "  'higher',\n",
       "  'weights',\n",
       "  '@',\n",
       "  '@',\n",
       "  'terms',\n",
       "  '@',\n",
       "  'lower',\n",
       "  'weights',\n",
       "  'depending',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'assign',\n",
       "  'weights',\n",
       "  '@',\n",
       "  '@',\n",
       "  'terms,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'able',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'weights',\n",
       "  '@',\n",
       "  'discover',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'strongly',\n",
       "  'associated',\n",
       "  '@',\n",
       "  '@',\n",
       "  'candidate',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['So',\n",
       "  \"let's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'term',\n",
       "  'vector',\n",
       "  '@',\n",
       "  '@',\n",
       "  'detail',\n",
       "  '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'Xi,',\n",
       "  'defined',\n",
       "  '@',\n",
       "  '@',\n",
       "  'normalized',\n",
       "  'weight',\n",
       "  '@',\n",
       "  'BM',\n",
       "  '25'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'weight',\n",
       "  '@',\n",
       "  '@',\n",
       "  'reflects',\n",
       "  '@',\n",
       "  'frequently',\n",
       "  '@',\n",
       "  'word',\n",
       "  'occurs',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['But',\n",
       "  '@',\n",
       "  \"can't\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'frequent',\n",
       "  'term',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'correlated',\n",
       "  '@',\n",
       "  '@',\n",
       "  'candidate',\n",
       "  'word'],\n",
       " ['Because',\n",
       "  '@',\n",
       "  'common',\n",
       "  'words',\n",
       "  'like',\n",
       "  \"'the'\",\n",
       "  '@',\n",
       "  'occur',\n",
       "  'frequently',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  'apply',\n",
       "  'IDF',\n",
       "  'weighting',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'here,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'weight',\n",
       "  '@',\n",
       "  'terms',\n",
       "  'based',\n",
       "  '@',\n",
       "  'IDF',\n",
       "  'That',\n",
       "  'means',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'common,',\n",
       "  'like',\n",
       "  \"'the'\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'penalized'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'highest',\n",
       "  'weighted',\n",
       "  'terms',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'common',\n",
       "  'terms',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lower',\n",
       "  'IDFs'],\n",
       " ['Instead,',\n",
       "  '@',\n",
       "  'terms',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'terms',\n",
       "  '@',\n",
       "  '@',\n",
       "  'frequent',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'frequently',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collection'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'clearly',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  'occur',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'candidate',\n",
       "  'word,',\n",
       "  '@',\n",
       "  'example,',\n",
       "  'cat'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'reason,',\n",
       "  '@',\n",
       "  'highly',\n",
       "  'weighted',\n",
       "  'terms',\n",
       "  '@',\n",
       "  '@',\n",
       "  'IDF',\n",
       "  'weighted',\n",
       "  'vector',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'assumed',\n",
       "  '@',\n",
       "  '@',\n",
       "  'candidate',\n",
       "  '@',\n",
       "  'Syntagmatic',\n",
       "  'relations'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'course,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'bi-product',\n",
       "  '@',\n",
       "  '@',\n",
       "  'approach',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  'paradigmatic',\n",
       "  'relations'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lecture,',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'discover',\n",
       "  'Syntagmatic',\n",
       "  'relations'],\n",
       " ['But',\n",
       "  '@',\n",
       "  'clearly',\n",
       "  'shows',\n",
       "  '@',\n",
       "  'relation',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  '@',\n",
       "  '@',\n",
       "  'relations'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'discussed,',\n",
       "  'discovered',\n",
       "  '@',\n",
       "  '@',\n",
       "  'joint',\n",
       "  'manner',\n",
       "  '@',\n",
       "  'leveraging',\n",
       "  '@',\n",
       "  'associations'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'summarize,',\n",
       "  '@',\n",
       "  'main',\n",
       "  'idea',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  'paradigmatic',\n",
       "  'relations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'collect',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  '@',\n",
       "  'candidate',\n",
       "  'word',\n",
       "  '@',\n",
       "  'form',\n",
       "  '@',\n",
       "  'pseudo',\n",
       "  'document,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'typically',\n",
       "  'represented',\n",
       "  '@',\n",
       "  '@',\n",
       "  'bag',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'similarity',\n",
       "  '@',\n",
       "  '@',\n",
       "  'corresponding',\n",
       "  'context',\n",
       "  'documents',\n",
       "  '@',\n",
       "  '@',\n",
       "  'candidate',\n",
       "  'words'],\n",
       " ['An',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'highly',\n",
       "  'similar',\n",
       "  'word',\n",
       "  'pairs',\n",
       "  '@',\n",
       "  'treat',\n",
       "  '@',\n",
       "  '@',\n",
       "  'having',\n",
       "  'paradigmatic',\n",
       "  'relations'],\n",
       " ['These', '@', '@', 'words', '@', 'share', 'similar', 'context'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'ways',\n",
       "  '@',\n",
       "  'implement',\n",
       "  '@',\n",
       "  'general',\n",
       "  'idea',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'approaches'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'specifically,',\n",
       "  '@',\n",
       "  'talked',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval',\n",
       "  'models',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'design',\n",
       "  'effective',\n",
       "  'similarity',\n",
       "  'function',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'paradigmatic',\n",
       "  'relations'],\n",
       " ['More',\n",
       "  'specifically,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'BM25',\n",
       "  '@',\n",
       "  'IDF',\n",
       "  'weighting',\n",
       "  '@',\n",
       "  'discover',\n",
       "  'paradigmatic',\n",
       "  'relation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'approaches',\n",
       "  '@',\n",
       "  'represent',\n",
       "  '@',\n",
       "  'state',\n",
       "  '@',\n",
       "  '@',\n",
       "  'art',\n",
       "  '@',\n",
       "  'text',\n",
       "  'retrieval',\n",
       "  'techniques'],\n",
       " ['Finally,',\n",
       "  'Syntagmatic',\n",
       "  'relations',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'discovered',\n",
       "  '@',\n",
       "  '@',\n",
       "  'bi-product',\n",
       "  '@',\n",
       "  '@',\n",
       "  'discover',\n",
       "  'paradigmatic',\n",
       "  'relations'],\n",
       " [],\n",
       " ['This', 'lecture', '@', '@', '@', 'syntagmatic', 'relation', 'discovery'],\n",
       " ['An', 'entropy'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'lecture,',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'continue',\n",
       "  'talking',\n",
       "  '@',\n",
       "  'word',\n",
       "  'Association',\n",
       "  'mining'],\n",
       " ['In',\n",
       "  'particular,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'discover',\n",
       "  'syntagmatic',\n",
       "  'relations'],\n",
       " ['And',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'start',\n",
       "  '@',\n",
       "  '@',\n",
       "  'introduction',\n",
       "  '@',\n",
       "  'entropy,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'basis',\n",
       "  '@',\n",
       "  'designing',\n",
       "  '@',\n",
       "  'measures',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  '@',\n",
       "  'relations'],\n",
       " ['By',\n",
       "  'definition,',\n",
       "  'Syntagmatic',\n",
       "  'relations',\n",
       "  'hold',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'correlated',\n",
       "  'Co',\n",
       "  'occurrences'],\n",
       " ['That',\n",
       "  'means',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'occurs',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context,',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'occurrence',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'specific',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ask',\n",
       "  '@',\n",
       "  'question',\n",
       "  '@',\n",
       "  'eats',\n",
       "  'occurs,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  'occur'],\n",
       " ['Now', 'looking', '@', '@', 'sentence', '@', '@', '@', 'left'],\n",
       " ['We',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'occur',\n",
       "  '@',\n",
       "  '@',\n",
       "  'eats',\n",
       "  'like',\n",
       "  '@',\n",
       "  'cat,',\n",
       "  'dog',\n",
       "  '@',\n",
       "  'fish',\n",
       "  '@',\n",
       "  'right'],\n",
       " ['But',\n",
       "  '@',\n",
       "  'I',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'right',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['The',\n",
       "  'question',\n",
       "  '@',\n",
       "  'is,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  'occur?',\n",
       "  'To',\n",
       "  '@',\n",
       "  'left',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'right'],\n",
       " ['Right,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'force',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'associated',\n",
       "  '@',\n",
       "  'eats'],\n",
       " ['If',\n",
       "  '@',\n",
       "  '@',\n",
       "  'associated',\n",
       "  '@',\n",
       "  'eats,',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  'occur',\n",
       "  '@',\n",
       "  '@',\n",
       "  'context',\n",
       "  '@',\n",
       "  'eats'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'specifically,',\n",
       "  '@',\n",
       "  'prediction',\n",
       "  'problem',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'segment,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence,',\n",
       "  'paragraph',\n",
       "  '@',\n",
       "  '@',\n",
       "  'document,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'I',\n",
       "  'asked',\n",
       "  '@',\n",
       "  'question',\n",
       "  '@',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'word',\n",
       "  'present',\n",
       "  '@',\n",
       "  'absent',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment'],\n",
       " ['Right',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ask',\n",
       "  '@',\n",
       "  'question',\n",
       "  '@',\n",
       "  '@',\n",
       "  'world',\n",
       "  'W',\n",
       "  '@',\n",
       "  'present',\n",
       "  '@',\n",
       "  'absent',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment'],\n",
       " ['Now,',\n",
       "  \"what's\",\n",
       "  'interesting',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'actually',\n",
       "  'easier',\n",
       "  '@',\n",
       "  'it,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['If',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  'shown',\n",
       "  'here,',\n",
       "  'meet,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'Unicorn'],\n",
       " ['Which',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  'easier',\n",
       "  '@',\n",
       "  'predict?',\n",
       "  'Now,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'moment,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conclude',\n",
       "  '@'],\n",
       " ['The',\n",
       "  '@',\n",
       "  'easier',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  '@',\n",
       "  'tends',\n",
       "  '@',\n",
       "  'occur',\n",
       "  'everywhere,',\n",
       "  '@',\n",
       "  'I',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'semtence'],\n",
       " ['Unicorn', '@', '@', 'relatively', 'easy'],\n",
       " ['Because', 'Unicorn', '@', 'rare,', '@', '@', 'rare'],\n",
       " ['And', 'I', '@', 'bet', '@', '@', \"doesn't\", 'occur', '@', '@', 'sentence'],\n",
       " ['But',\n",
       "  'meat',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'terms',\n",
       "  '@',\n",
       "  'frequency,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'makes',\n",
       "  '@',\n",
       "  'hard',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  'possible',\n",
       "  '@',\n",
       "  '@',\n",
       "  'occurs',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment',\n",
       "  '@',\n",
       "  'accurately'],\n",
       " ['But', '@', '@', '@', '@', 'occur', '@', '@', 'segment'],\n",
       " ['So', '@', \"let's\", 'start', '@', 'problem', '@', 'formally'],\n",
       " ['Alright,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problem',\n",
       "  '@',\n",
       "  '@',\n",
       "  'formally',\n",
       "  'defined',\n",
       "  '@',\n",
       "  'predicting',\n",
       "  '@',\n",
       "  'value',\n",
       "  '@',\n",
       "  '@',\n",
       "  'binary',\n",
       "  'random',\n",
       "  'variable'],\n",
       " ['Here', '@', 'denoted', '@', 'X', 'sub', 'w,', 'w', 'denotes', '@', 'word'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  '@',\n",
       "  'associated',\n",
       "  '@',\n",
       "  'precisely',\n",
       "  '@',\n",
       "  'word'],\n",
       " ['When',\n",
       "  '@',\n",
       "  'value',\n",
       "  '@',\n",
       "  '@',\n",
       "  'variable',\n",
       "  '@',\n",
       "  '1,',\n",
       "  '@',\n",
       "  'means',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  'present'],\n",
       " ['When',\n",
       "  \"it's\",\n",
       "  'zero,',\n",
       "  '@',\n",
       "  'means',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  'absent,',\n",
       "  '@',\n",
       "  'naturally',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'zero',\n",
       "  '@',\n",
       "  'sum',\n",
       "  '@',\n",
       "  '1'],\n",
       " ['Because',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'present',\n",
       "  '@',\n",
       "  'absent',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment'],\n",
       " [\"There's\", '@', '@', 'choice'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'intuition',\n",
       "  '@',\n",
       "  'discussed',\n",
       "  'earlier',\n",
       "  '@',\n",
       "  '@',\n",
       "  'formally',\n",
       "  'stated',\n",
       "  '@',\n",
       "  'follows'],\n",
       " ['The',\n",
       "  '@',\n",
       "  'random',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  'is,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'difficult',\n",
       "  '@',\n",
       "  'prediction',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'question',\n",
       "  'is,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'quantitatively',\n",
       "  'measure',\n",
       "  '@',\n",
       "  'randomness',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  'like',\n",
       "  'X',\n",
       "  'sub',\n",
       "  'w,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'quantify',\n",
       "  '@',\n",
       "  'randomness',\n",
       "  '@',\n",
       "  '@',\n",
       "  'variable?',\n",
       "  'And',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'need',\n",
       "  '@',\n",
       "  'measure',\n",
       "  'called',\n",
       "  'entropy'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'measure',\n",
       "  'introduced',\n",
       "  '@',\n",
       "  'information',\n",
       "  'theory',\n",
       "  '@',\n",
       "  'measure',\n",
       "  '@',\n",
       "  'randomness',\n",
       "  '@',\n",
       "  'X'],\n",
       " ['There',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'connection',\n",
       "  '@',\n",
       "  '@',\n",
       "  'information',\n",
       "  'here,',\n",
       "  '@',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'scope',\n",
       "  '@',\n",
       "  '@',\n",
       "  'course'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'purpose',\n",
       "  '@',\n",
       "  '@',\n",
       "  'treat',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  'function',\n",
       "  '@',\n",
       "  '@',\n",
       "  'function',\n",
       "  'defined',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'case',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'binary',\n",
       "  'random',\n",
       "  'variable,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'definition',\n",
       "  '@',\n",
       "  '@',\n",
       "  'easily',\n",
       "  'generalized',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  '@',\n",
       "  'multiple',\n",
       "  'values'],\n",
       " ['Now', '@', 'function', 'form', 'looks', 'like', '@'],\n",
       " [\"There's\",\n",
       "  '@',\n",
       "  'sum',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'possible',\n",
       "  'values',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  'inside',\n",
       "  '@',\n",
       "  'sum,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'value',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'product',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  'equals',\n",
       "  '@',\n",
       "  'value',\n",
       "  '@',\n",
       "  'log',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability'],\n",
       " ['And', 'note', '@', '@', '@', '@', '@', 'negative', 'sign', '@'],\n",
       " ['Now,',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  'negative',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mathematically',\n",
       "  'proved'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'expand',\n",
       "  '@',\n",
       "  'sum',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'equation',\n",
       "  'looks',\n",
       "  'like',\n",
       "  '@',\n",
       "  'second',\n",
       "  '@',\n",
       "  'I',\n",
       "  'explicitly',\n",
       "  'plugged',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'values',\n",
       "  'zero',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '0',\n",
       "  'log',\n",
       "  '@',\n",
       "  '0,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'generally',\n",
       "  'find',\n",
       "  '@',\n",
       "  '@',\n",
       "  'zero',\n",
       "  '@',\n",
       "  'log',\n",
       "  '@',\n",
       "  '0',\n",
       "  '@',\n",
       "  'undefined'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  'function',\n",
       "  '@',\n",
       "  '@',\n",
       "  'function',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'value',\n",
       "  '@',\n",
       "  'different',\n",
       "  'distributions',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'clear',\n",
       "  '@',\n",
       "  'clearly',\n",
       "  'depends',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  'taking',\n",
       "  '@',\n",
       "  'value',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'zero'],\n",
       " ['If',\n",
       "  '@',\n",
       "  'plotted',\n",
       "  '@',\n",
       "  'function',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  '@',\n",
       "  'equal',\n",
       "  '@',\n",
       "  '1',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'function',\n",
       "  'looks',\n",
       "  'like',\n",
       "  '@'],\n",
       " ['At',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ends,',\n",
       "  'That',\n",
       "  'means',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  'X',\n",
       "  '=',\n",
       "  '1',\n",
       "  '@',\n",
       "  '@',\n",
       "  'small',\n",
       "  '@',\n",
       "  '@',\n",
       "  'large,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  'function',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lower',\n",
       "  'value',\n",
       "  '@',\n",
       "  \"it's\"],\n",
       " ['5', '@', '@', 'middle', '@', '@', 'reaches', '@', 'maximum'],\n",
       " ['Now,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'plot',\n",
       "  '@',\n",
       "  'function',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  'X',\n",
       "  '@',\n",
       "  'taking',\n",
       "  '@',\n",
       "  'value',\n",
       "  '@',\n",
       "  '0',\n",
       "  '@',\n",
       "  '@',\n",
       "  'function',\n",
       "  '@',\n",
       "  '@',\n",
       "  'exactly',\n",
       "  '@',\n",
       "  '@',\n",
       "  'curve',\n",
       "  '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'imagine',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  'symmetric',\n",
       "  '@',\n",
       "  'completely',\n",
       "  'symmetric'],\n",
       " ['So', '@', 'interesting', 'question'],\n",
       " ['You',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'X?',\n",
       "  'Does',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  'reached',\n",
       "  'maximum',\n",
       "  '@',\n",
       "  'minimum',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  'special',\n",
       "  'cases'],\n",
       " ['For',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  '@',\n",
       "  '@',\n",
       "  'takes',\n",
       "  '@',\n",
       "  'value',\n",
       "  '@',\n",
       "  'one,',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  '@',\n",
       "  'Is',\n",
       "  'equally',\n",
       "  'likely',\n",
       "  'taking',\n",
       "  '@',\n",
       "  'value',\n",
       "  '@',\n",
       "  '1',\n",
       "  '@',\n",
       "  '0'],\n",
       " ['In', '@', 'case,', '@', 'probability', '@', 'X', '=', '1', '@'],\n",
       " ['5'],\n",
       " ['Now,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'higher',\n",
       "  'entropy?',\n",
       "  \"It's\",\n",
       "  'easier',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problem',\n",
       "  '@',\n",
       "  'thinking',\n",
       "  '@',\n",
       "  'simple',\n",
       "  'example'],\n",
       " ['Using',\n",
       "  'coin',\n",
       "  'tossing,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'experiment',\n",
       "  'like',\n",
       "  '@',\n",
       "  'tossing',\n",
       "  '@',\n",
       "  'coin,',\n",
       "  '@',\n",
       "  'gives',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  '@',\n",
       "  '@',\n",
       "  'represent',\n",
       "  '@',\n",
       "  'result'],\n",
       " ['It',\n",
       "  '@',\n",
       "  '@',\n",
       "  'head',\n",
       "  '@',\n",
       "  'tail,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'define',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  'X',\n",
       "  'sub',\n",
       "  'coin',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'coin',\n",
       "  'shows',\n",
       "  '@',\n",
       "  '@',\n",
       "  'head,',\n",
       "  \"it's\",\n",
       "  'zero',\n",
       "  '@',\n",
       "  '@',\n",
       "  'coin',\n",
       "  'shows',\n",
       "  '@',\n",
       "  '@',\n",
       "  'tail'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  'indicates',\n",
       "  '@',\n",
       "  'difficult',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  'outcome',\n",
       "  '@',\n",
       "  '@',\n",
       "  'coin',\n",
       "  '@',\n",
       "  'coin',\n",
       "  'tossing'],\n",
       " ['So', '@', '@', 'think', '@', '@', '@', 'cases'],\n",
       " ['One', '@', '@', 'fair', 'coin,', \"it's\", 'completely', 'fair'],\n",
       " ['The',\n",
       "  'coin',\n",
       "  'shows',\n",
       "  '@',\n",
       "  '@',\n",
       "  'head',\n",
       "  'hotel',\n",
       "  'equally',\n",
       "  'likely,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  'be,',\n",
       "  '1/2',\n",
       "  'right',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'equal',\n",
       "  '@',\n",
       "  '1/2'],\n",
       " ['Another',\n",
       "  'extreme',\n",
       "  'case',\n",
       "  '@',\n",
       "  'completely',\n",
       "  'biased',\n",
       "  'coin,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'coin',\n",
       "  '@',\n",
       "  'shows',\n",
       "  '@',\n",
       "  '@',\n",
       "  'head,',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'completely',\n",
       "  'biased',\n",
       "  'coin'],\n",
       " ['Now',\n",
       "  \"let's\",\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropies',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'cases,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'plug',\n",
       "  '@',\n",
       "  '@',\n",
       "  'values',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropies,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'follows',\n",
       "  '@',\n",
       "  '@',\n",
       "  'fair',\n",
       "  'coin',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  'reaches',\n",
       "  '@',\n",
       "  'maximum,',\n",
       "  \"that's\",\n",
       "  '@'],\n",
       " ['For',\n",
       "  '@',\n",
       "  'completely',\n",
       "  'biased',\n",
       "  'coin',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '0',\n",
       "  '@',\n",
       "  '@',\n",
       "  'intuitively',\n",
       "  'makes',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'sense',\n",
       "  '@',\n",
       "  '@',\n",
       "  'fair',\n",
       "  'coin',\n",
       "  '@',\n",
       "  '@',\n",
       "  'difficult',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  '@',\n",
       "  'completely',\n",
       "  'biased',\n",
       "  'coin',\n",
       "  '@',\n",
       "  '@',\n",
       "  'easy',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'head',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'head',\n",
       "  '@',\n",
       "  '@',\n",
       "  'time',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'shown',\n",
       "  '@',\n",
       "  '@',\n",
       "  'curve',\n",
       "  '@',\n",
       "  'follows'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'fair',\n",
       "  'coin',\n",
       "  'corresponds',\n",
       "  '@',\n",
       "  '@',\n",
       "  'middle',\n",
       "  'point,',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'uncertain'],\n",
       " ['The',\n",
       "  'completely',\n",
       "  'biased',\n",
       "  'coin',\n",
       "  'corresponds',\n",
       "  '@',\n",
       "  '@',\n",
       "  'end',\n",
       "  'point'],\n",
       " ['We', '@', '@', 'probability', '@', '1'],\n",
       " ['0', '@', '@', 'entropy', '@', '0'],\n",
       " ['So',\n",
       "  '@',\n",
       "  \"let's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'word',\n",
       "  'prediction'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'problem,',\n",
       "  \"let's\",\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problem',\n",
       "  'right,',\n",
       "  '@',\n",
       "  'predicted',\n",
       "  '@',\n",
       "  'W',\n",
       "  '@',\n",
       "  'present',\n",
       "  '@',\n",
       "  'absolutely',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment'],\n",
       " ['Again,', 'think', '@', '@', '@', 'words'],\n",
       " ['Particularly,', 'think', '@', '@', 'entropies'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  'assume',\n",
       "  'high',\n",
       "  'entropy',\n",
       "  'words',\n",
       "  '@',\n",
       "  'harder',\n",
       "  '@',\n",
       "  'predict'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'quantitative',\n",
       "  'way',\n",
       "  '@',\n",
       "  'tell',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  'harder',\n",
       "  '@',\n",
       "  'predict'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words,',\n",
       "  'meat,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'Unicorn',\n",
       "  '@'],\n",
       " ['An',\n",
       "  '@',\n",
       "  'clearly',\n",
       "  '@',\n",
       "  'expect',\n",
       "  '@',\n",
       "  'meat',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'high',\n",
       "  'entropy,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'OR',\n",
       "  'Unicorn'],\n",
       " ['In',\n",
       "  'fact,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'the,',\n",
       "  \"It's\",\n",
       "  'close',\n",
       "  '@',\n",
       "  '0,聽',\n",
       "  '@',\n",
       "  '@',\n",
       "  'occurs',\n",
       "  '@'],\n",
       " ['So,',\n",
       "  \"it's\",\n",
       "  'like',\n",
       "  '@',\n",
       "  'completed',\n",
       "  'biased',\n",
       "  'coin,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  '0'],\n",
       " [],\n",
       " ['This',\n",
       "  'lecture',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'syntagmatic',\n",
       "  'relation',\n",
       "  'discovery',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'lecture,',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'continue',\n",
       "  '@',\n",
       "  'discussion',\n",
       "  '@',\n",
       "  'word',\n",
       "  'association',\n",
       "  'mining',\n",
       "  '@',\n",
       "  'analysis'],\n",
       " [\"We're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'useful',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  'syntagmatic',\n",
       "  'relations'],\n",
       " ['Earlier',\n",
       "  '@',\n",
       "  'talked',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'capture',\n",
       "  '@',\n",
       "  'easy',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  'absence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'address',\n",
       "  '@',\n",
       "  'different',\n",
       "  'scenario',\n",
       "  '@',\n",
       "  '@',\n",
       "  'assume',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'text',\n",
       "  'segment'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'question',\n",
       "  'is,',\n",
       "  'suppose',\n",
       "  '@',\n",
       "  'know',\n",
       "  'eats',\n",
       "  'occured',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  'absence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'like',\n",
       "  'meat?',\n",
       "  'And',\n",
       "  '@',\n",
       "  'particular',\n",
       "  '@',\n",
       "  'want',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'helped',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  'meat'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'frame',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mean',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interested',\n",
       "  '@',\n",
       "  'knowing',\n",
       "  '@',\n",
       "  'knowing',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'reduce',\n",
       "  'uncertainty',\n",
       "  '@',\n",
       "  '@',\n",
       "  'meat',\n",
       "  '@',\n",
       "  'reduce',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  'corresponding',\n",
       "  '@',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  'absence',\n",
       "  '@',\n",
       "  'meat'],\n",
       " ['We',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ask',\n",
       "  '@',\n",
       "  'question,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  '@',\n",
       "  'absence',\n",
       "  '@',\n",
       "  'eats?',\n",
       "  'Would',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  'absence',\n",
       "  '@',\n",
       "  'meat'],\n",
       " ['So', '@', 'questions', '@', '@', 'addressed', '@', '@'],\n",
       " ['Another', 'concept,', 'called', '@', 'conditional', 'entropy'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'explain',\n",
       "  '@',\n",
       "  'concept,',\n",
       "  \"let's\",\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'scenario',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  'indicating',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'like',\n",
       "  'meat',\n",
       "  'occurs',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'occur',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  'function',\n",
       "  '@',\n",
       "  'looks',\n",
       "  'like',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'slide'],\n",
       " ['I',\n",
       "  'suppose',\n",
       "  '@',\n",
       "  'know',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'present,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'value',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  '@',\n",
       "  'denotes',\n",
       "  'eats'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  'change',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  'absence',\n",
       "  '@',\n",
       "  'meat'],\n",
       " ['Given', '@', '@', 'know', 'eats', 'occured', '@', '@', 'context'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'result,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'replace',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  '@',\n",
       "  'corresponding',\n",
       "  'conditional',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  'function,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy'],\n",
       " ['So', '@', 'equation', '@', '@'],\n",
       " ['Would', '@'],\n",
       " ['The',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  'conditioned',\n",
       "  '@',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  'eats'],\n",
       " ['Right?',\n",
       "  'So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'essentially',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  'function',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'seen',\n",
       "  'before,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'condition'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'tells',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'meat',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'known',\n",
       "  'eats',\n",
       "  'occurring',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'course,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'define',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'scenario',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"don't\",\n",
       "  '@',\n",
       "  'eats'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  'eats',\n",
       "  '@',\n",
       "  '@',\n",
       "  'occur',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'capture',\n",
       "  '@',\n",
       "  'uncertainty',\n",
       "  '@',\n",
       "  'meat',\n",
       "  '@',\n",
       "  '@',\n",
       "  'content',\n",
       "  '@',\n",
       "  '@',\n",
       "  'condition'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'putting',\n",
       "  'different',\n",
       "  'scenarios',\n",
       "  'together,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'complete',\n",
       "  'definition',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'follows'],\n",
       " ['Basically'],\n",
       " [\"We're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'consider',\n",
       "  '@',\n",
       "  'scenarios',\n",
       "  '@',\n",
       "  '@',\n",
       "  'value',\n",
       "  '@',\n",
       "  'eats',\n",
       "  'zero',\n",
       "  '@',\n",
       "  'one,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'gives',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'equal',\n",
       "  '@',\n",
       "  '0',\n",
       "  '@',\n",
       "  '1'],\n",
       " ['Basically,',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'present',\n",
       "  '@',\n",
       "  'absent,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'meat',\n",
       "  '@',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'scenario'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'expand',\n",
       "  '@',\n",
       "  'entropy,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'following',\n",
       "  'equation'],\n",
       " ['Where',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'involvement',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'probabilities'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'general,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'discrete',\n",
       "  'random',\n",
       "  'variables',\n",
       "  'X&amp;Y',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['The',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'larger',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'variable',\n",
       "  'X,',\n",
       "  '@',\n",
       "  'basically',\n",
       "  '@',\n",
       "  '@',\n",
       "  'upper',\n",
       "  'bound',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy'],\n",
       " ['That',\n",
       "  'means',\n",
       "  '@',\n",
       "  'knowing',\n",
       "  '@',\n",
       "  'information',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment,',\n",
       "  '@',\n",
       "  \"won't\",\n",
       "  '@',\n",
       "  'able',\n",
       "  '@',\n",
       "  'increase',\n",
       "  '@',\n",
       "  'uncertainty'],\n",
       " ['We',\n",
       "  '@',\n",
       "  '@',\n",
       "  'reduce',\n",
       "  'uncertainty,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'intuitively',\n",
       "  'makes',\n",
       "  'sense',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'information,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@'],\n",
       " ['Make',\n",
       "  '@',\n",
       "  'prediction',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'hurt',\n",
       "  '@',\n",
       "  'prediction',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case'],\n",
       " ['Now',\n",
       "  \"what's\",\n",
       "  'interesting',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  \"what's\",\n",
       "  '@',\n",
       "  'minimum',\n",
       "  'possible',\n",
       "  'value',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  '@',\n",
       "  'maximum',\n",
       "  'value',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'X'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'minimum?',\n",
       "  'So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think?',\n",
       "  'I',\n",
       "  'hope',\n",
       "  '@',\n",
       "  '@',\n",
       "  'reach',\n",
       "  '@',\n",
       "  'conclusion',\n",
       "  '@',\n",
       "  '@',\n",
       "  'minimum',\n",
       "  'possible',\n",
       "  'value',\n",
       "  '@',\n",
       "  '@',\n",
       "  '0',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'situation',\n",
       "  '@',\n",
       "  'achieve',\n",
       "  '@'],\n",
       " ['So',\n",
       "  \"let's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'capture',\n",
       "  'syntagmatic',\n",
       "  'relations'],\n",
       " ['Now,',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  'gives',\n",
       "  '@',\n",
       "  'directly',\n",
       "  '@',\n",
       "  'way',\n",
       "  '@',\n",
       "  'measure',\n",
       "  '@',\n",
       "  'association',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['Because',\n",
       "  '@',\n",
       "  'tells',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'extent',\n",
       "  '@',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'given',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  'absence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'intuition',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'capturing',\n",
       "  'syntagmatic',\n",
       "  'relations,',\n",
       "  \"it's\",\n",
       "  'useful',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'special',\n",
       "  'case',\n",
       "  'listed',\n",
       "  'here,',\n",
       "  '@',\n",
       "  'is,',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'given',\n",
       "  '@'],\n",
       " ['So,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'listed',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'middle'],\n",
       " ['So', \"it's\", '@'],\n",
       " ['So', '@', '@', '@', 'value', '@', 'this?', 'Now'],\n",
       " ['This',\n",
       "  'means',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'meat',\n",
       "  'occurs',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'hope',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  '@',\n",
       "  'meat',\n",
       "  'occurs',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sentence'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  '@',\n",
       "  'zero',\n",
       "  '@',\n",
       "  \"there's\",\n",
       "  '@',\n",
       "  'uncertain',\n",
       "  '@',\n",
       "  'anymore',\n",
       "  'Once',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'occurs',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'answer',\n",
       "  '@',\n",
       "  '@',\n",
       "  'prediction'],\n",
       " ['So', '@', '@', '0'],\n",
       " ['And',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  'reaches',\n",
       "  '@',\n",
       "  'minimum'],\n",
       " ['So', '@', \"let's\", 'look', '@', '@', '@', 'cases'],\n",
       " ['So', '@', '@', '@', 'case', '@'],\n",
       " ['Knowing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'trying',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  'meat',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  'knowing',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'trying',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  'meat'],\n",
       " ['Which',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  'smaller?',\n",
       "  'Note',\n",
       "  '@',\n",
       "  '@',\n",
       "  'smaller',\n",
       "  'entropy',\n",
       "  'means',\n",
       "  'easier',\n",
       "  '@',\n",
       "  'prediction'],\n",
       " ['Which',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  'higher?',\n",
       "  'Which',\n",
       "  '@',\n",
       "  '@',\n",
       "  'smaller?',\n",
       "  'If',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'uncertainty,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  \"doesn't\",\n",
       "  '@',\n",
       "  'tell',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'meat,',\n",
       "  '@',\n",
       "  'knowing',\n",
       "  '@',\n",
       "  'occurrence',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"doesn't\",\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'reduce',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'match,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'stays',\n",
       "  '@',\n",
       "  'fairly',\n",
       "  'close',\n",
       "  '@',\n",
       "  '@',\n",
       "  'original',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'meat'],\n",
       " ['Whereas',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  'eats,',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'related',\n",
       "  '@',\n",
       "  'meet,',\n",
       "  '@',\n",
       "  'knowing',\n",
       "  'presence',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'absence',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'predict',\n",
       "  'wether',\n",
       "  'meat',\n",
       "  'occurs',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'reduce',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'meat,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'expect',\n",
       "  '@',\n",
       "  'second',\n",
       "  'term,',\n",
       "  'namely,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'smaller',\n",
       "  'entropy'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'means',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'stronger',\n",
       "  'association',\n",
       "  '@',\n",
       "  'meat',\n",
       "  '@',\n",
       "  'eats'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  '@',\n",
       "  'w',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'meat',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'reach',\n",
       "  '@',\n",
       "  'minimum',\n",
       "  '@',\n",
       "  '@',\n",
       "  '0?',\n",
       "  'And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'kind',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'reach',\n",
       "  '@',\n",
       "  'maximum?',\n",
       "  'Well,',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'W',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'related',\n",
       "  '@',\n",
       "  'meat'],\n",
       " ['like',\n",
       "  'the,',\n",
       "  '@',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'close',\n",
       "  '@',\n",
       "  '@',\n",
       "  'maximum,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'meat',\n",
       "  '@'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'suggests',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'mining',\n",
       "  'syntagmatic',\n",
       "  'relations'],\n",
       " ['The', 'algorithm', '@', 'look', '@', 'follows'],\n",
       " ['For',\n",
       "  '@',\n",
       "  'word',\n",
       "  'W1,',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'enumerate',\n",
       "  '@',\n",
       "  'overall',\n",
       "  '@',\n",
       "  'words',\n",
       "  'W2,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'W1',\n",
       "  'given',\n",
       "  'W2'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'thought',\n",
       "  '@',\n",
       "  '@',\n",
       "  'candidate',\n",
       "  'words',\n",
       "  '@',\n",
       "  'ascending',\n",
       "  'order',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'want',\n",
       "  '@',\n",
       "  'favor',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'small',\n",
       "  'entropy,',\n",
       "  'meaning',\n",
       "  '@',\n",
       "  '@',\n",
       "  'helps',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  'target',\n",
       "  'word',\n",
       "  'W1,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ranked',\n",
       "  '@',\n",
       "  'candidate',\n",
       "  'words',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'potential',\n",
       "  'syntagmatic',\n",
       "  'relations',\n",
       "  '@',\n",
       "  'W1'],\n",
       " ['Note',\n",
       "  '@',\n",
       "  '@',\n",
       "  'need',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'threshold',\n",
       "  '@',\n",
       "  'find',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['The',\n",
       "  'threshold',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'number',\n",
       "  '@',\n",
       "  '@',\n",
       "  'candidates',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'absolute',\n",
       "  'value',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  'allow',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'strongly',\n",
       "  'correlated',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'particular',\n",
       "  'word',\n",
       "  'W1',\n",
       "  '@'],\n",
       " ['But',\n",
       "  '@',\n",
       "  'algorithm',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'strongest',\n",
       "  'K',\n",
       "  'syntagmatic',\n",
       "  'relations',\n",
       "  '@',\n",
       "  'entire',\n",
       "  'collection'],\n",
       " ['Because',\n",
       "  '@',\n",
       "  'order',\n",
       "  '@',\n",
       "  '@',\n",
       "  'that,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ensure',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropies',\n",
       "  'are聽',\n",
       "  'comparable',\n",
       "  '@',\n",
       "  'different',\n",
       "  'words'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  'discovering',\n",
       "  'Syntagmatic',\n",
       "  'relations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'target',\n",
       "  'word',\n",
       "  'like',\n",
       "  'W1,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'need',\n",
       "  '@',\n",
       "  'compare',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropies',\n",
       "  'For',\n",
       "  'W1',\n",
       "  'given',\n",
       "  'different',\n",
       "  'words'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  '@',\n",
       "  'comparable',\n",
       "  'right?',\n",
       "  'So',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'W1',\n",
       "  'given',\n",
       "  'W2',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'W1',\n",
       "  'given聽',\n",
       "  'W3',\n",
       "  '@',\n",
       "  'comparable'],\n",
       " ['They', '@', 'measure', '@', 'hard', '@', '@', '@', 'predict', 'W1'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'pairs',\n",
       "  '@',\n",
       "  '@',\n",
       "  'share',\n",
       "  'W2',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'condition',\n",
       "  '@',\n",
       "  '@',\n",
       "  'try',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  'W1&amp;W3,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropies',\n",
       "  '@',\n",
       "  'actually',\n",
       "  '@',\n",
       "  'comperable'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  'question,',\n",
       "  'why?',\n",
       "  'So',\n",
       "  'Why',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'comparable?',\n",
       "  'Well,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'upper',\n",
       "  'bounds,',\n",
       "  'right?',\n",
       "  'So',\n",
       "  '@',\n",
       "  'upper',\n",
       "  'bounds',\n",
       "  '@',\n",
       "  'precisely',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'W1',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'W3'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'upper',\n",
       "  'bounds,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compare',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'way'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'address',\n",
       "  '@',\n",
       "  'problem?',\n",
       "  'Later',\n",
       "  \"we'll\",\n",
       "  'discuss',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  '@',\n",
       "  'solve',\n",
       "  '@',\n",
       "  'problem'],\n",
       " [],\n",
       " ['This',\n",
       "  'lecture',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'syntagmatic',\n",
       "  'relation',\n",
       "  'discovery',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'lecture,',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'continue',\n",
       "  'discussing',\n",
       "  'syntagmatic',\n",
       "  'relation',\n",
       "  'discovery'],\n",
       " ['In',\n",
       "  'particular,',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'talk',\n",
       "  '@',\n",
       "  '@',\n",
       "  'concept,',\n",
       "  '@',\n",
       "  'information',\n",
       "  'theory,',\n",
       "  'called',\n",
       "  'mutual',\n",
       "  'information'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'discover',\n",
       "  'syntagmatic',\n",
       "  'relations?',\n",
       "  'Before',\n",
       "  '@',\n",
       "  'talked',\n",
       "  '@',\n",
       "  '@',\n",
       "  'problem',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  'computed',\n",
       "  '@',\n",
       "  'different',\n",
       "  'pairs',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'comparable,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'makes',\n",
       "  '@',\n",
       "  'hard',\n",
       "  '@',\n",
       "  'discover',\n",
       "  'strong',\n",
       "  'syntagmatic',\n",
       "  'relations',\n",
       "  'globally',\n",
       "  '@',\n",
       "  'corpus'],\n",
       " ['So',\n",
       "  '@',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'introduce',\n",
       "  'mutual',\n",
       "  'information,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'concept',\n",
       "  '@',\n",
       "  'information',\n",
       "  'theory',\n",
       "  '@',\n",
       "  'allows',\n",
       "  '@',\n",
       "  'to,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sense,',\n",
       "  'normalize',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['@', '@', 'comparable', '@', 'different', 'pairs'],\n",
       " ['In',\n",
       "  'particular,',\n",
       "  'mutual',\n",
       "  'information,',\n",
       "  'denoted',\n",
       "  '@',\n",
       "  'I(X;Y),',\n",
       "  'measures',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  'reduction',\n",
       "  '@',\n",
       "  'X',\n",
       "  'obtained',\n",
       "  '@',\n",
       "  'knowing',\n",
       "  'Y'],\n",
       " ['More',\n",
       "  'specifically',\n",
       "  '@',\n",
       "  'question',\n",
       "  \"we're\",\n",
       "  'interested',\n",
       "  '@',\n",
       "  'here,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'reduction',\n",
       "  '@',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'X',\n",
       "  '@',\n",
       "  '@',\n",
       "  'obtain',\n",
       "  '@',\n",
       "  'knowing',\n",
       "  'Y'],\n",
       " ['So',\n",
       "  'mathematically,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'defined',\n",
       "  '@',\n",
       "  '@',\n",
       "  'difference',\n",
       "  '@',\n",
       "  '@',\n",
       "  'original',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'X',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'X',\n",
       "  'given',\n",
       "  'Y'],\n",
       " ['And', '@', '@', '@', '@', '@', '@', '@', '@'],\n",
       " ['It',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'defined',\n",
       "  '@',\n",
       "  '@',\n",
       "  'reduction',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'Y,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'knowing',\n",
       "  'X'],\n",
       " ['Normally',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropies',\n",
       "  'H(X|Y)',\n",
       "  '@',\n",
       "  'H(Y|X)',\n",
       "  '@',\n",
       "  '@',\n",
       "  'equal'],\n",
       " ['But',\n",
       "  'interestingly,',\n",
       "  '@',\n",
       "  'reduction',\n",
       "  '@',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'knowing',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'actually',\n",
       "  'equal,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'quantity',\n",
       "  '@',\n",
       "  'called',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  'denoted',\n",
       "  '@',\n",
       "  'I',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'function',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interesting',\n",
       "  'properties'],\n",
       " ['First,', \"it's\", '@', 'non', 'negative'],\n",
       " ['This',\n",
       "  '@',\n",
       "  'easy',\n",
       "  '@',\n",
       "  'understand',\n",
       "  'becausw',\n",
       "  '@',\n",
       "  'original',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'going',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lower',\n",
       "  '@',\n",
       "  '@',\n",
       "  'possibly',\n",
       "  'reduced',\n",
       "  'conditional',\n",
       "  'entropy'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'words,',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'exceed',\n",
       "  '@',\n",
       "  'original',\n",
       "  'entropy'],\n",
       " ['Knowing',\n",
       "  '@',\n",
       "  'information',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'potentially,',\n",
       "  '@',\n",
       "  \"won't\",\n",
       "  'hurt',\n",
       "  '@',\n",
       "  '@',\n",
       "  'predicting',\n",
       "  'X'],\n",
       " ['The',\n",
       "  'second',\n",
       "  'property',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  'symmetric',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  '@',\n",
       "  'symmetrical'],\n",
       " ['Mutual', 'information', '@'],\n",
       " ['The',\n",
       "  '@',\n",
       "  'property',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'reaches',\n",
       "  '@',\n",
       "  'minimum',\n",
       "  'zero',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variables',\n",
       "  '@',\n",
       "  'completely',\n",
       "  'independent'],\n",
       " ['That',\n",
       "  'means',\n",
       "  'knowing',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  \"doesn't\",\n",
       "  'tell',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'property',\n",
       "  '@',\n",
       "  '@',\n",
       "  'verified',\n",
       "  '@',\n",
       "  'simply',\n",
       "  'looking',\n",
       "  '@',\n",
       "  '@',\n",
       "  'equation',\n",
       "  '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'reaches',\n",
       "  '0',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'X',\n",
       "  'given',\n",
       "  'Y',\n",
       "  '@',\n",
       "  'exactly',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'original',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'X'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'means',\n",
       "  'knowing',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'help',\n",
       "  '@',\n",
       "  'all,',\n",
       "  '@',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  'X&amp;Y',\n",
       "  '@',\n",
       "  'completely',\n",
       "  'independent'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  'fix',\n",
       "  'X',\n",
       "  '@',\n",
       "  'rank',\n",
       "  'different',\n",
       "  'Ys',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'order',\n",
       "  '@',\n",
       "  'ranking',\n",
       "  'based',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'function',\n",
       "  '@',\n",
       "  'H',\n",
       "  '@',\n",
       "  'X',\n",
       "  '@',\n",
       "  'fixed',\n",
       "  '@',\n",
       "  'X',\n",
       "  '@',\n",
       "  'fixed'],\n",
       " ['So',\n",
       "  'ranking',\n",
       "  'based',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  '@',\n",
       "  'exactly',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ranking',\n",
       "  'based',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'X',\n",
       "  'given',\n",
       "  'Y'],\n",
       " ['But',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  'allows',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compare',\n",
       "  'different',\n",
       "  'pairs',\n",
       "  '@',\n",
       "  'X&amp;Y,',\n",
       "  '@',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  '@',\n",
       "  'general',\n",
       "  '@',\n",
       "  'useful'],\n",
       " ['So',\n",
       "  \"let's\",\n",
       "  'examine',\n",
       "  '@',\n",
       "  'intuition',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  '@',\n",
       "  'syntagmatic',\n",
       "  'relation',\n",
       "  'mining'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'question',\n",
       "  '@',\n",
       "  'ask',\n",
       "  '@',\n",
       "  'syntactic',\n",
       "  'relation',\n",
       "  'mining',\n",
       "  '@',\n",
       "  '@',\n",
       "  'eats',\n",
       "  'occurs,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  'occur?',\n",
       "  'So',\n",
       "  '@',\n",
       "  'question',\n",
       "  '@',\n",
       "  '@',\n",
       "  'framed',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  'question,',\n",
       "  '@',\n",
       "  'is,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'higher',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  '@',\n",
       "  'eats'],\n",
       " ['So',\n",
       "  \"we're\",\n",
       "  'going',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'that,',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  'basically',\n",
       "  '@',\n",
       "  'based',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'intuition',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'strongly',\n",
       "  'associated',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'tend',\n",
       "  '@',\n",
       "  '@',\n",
       "  'high',\n",
       "  'mutual',\n",
       "  'information,',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'related'],\n",
       " ['We',\n",
       "  '@',\n",
       "  'lower',\n",
       "  'mutual',\n",
       "  'information,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'I',\n",
       "  '@',\n",
       "  '@',\n",
       "  'example',\n",
       "  '@'],\n",
       " ['The',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  'meats,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'meats',\n",
       "  '@',\n",
       "  'eats',\n",
       "  'cause',\n",
       "  'major',\n",
       "  'information',\n",
       "  '@',\n",
       "  'symmetric',\n",
       "  '@',\n",
       "  'expected',\n",
       "  '@',\n",
       "  '@',\n",
       "  'higher',\n",
       "  '@',\n",
       "  'The',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['Because', 'knowing', '@', \"doesn't\", '@', 'help', '@', 'predict', 'eats'],\n",
       " ['Similarly',\n",
       "  'knowing',\n",
       "  'eats',\n",
       "  \"doesn't\",\n",
       "  'help',\n",
       "  '@',\n",
       "  'predicting',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'easily',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  'between聽',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'largest',\n",
       "  '@',\n",
       "  '@',\n",
       "  'equal',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'info'],\n",
       " ['The', 'entropy', '@', '@', 'word'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  'reduction',\n",
       "  '@',\n",
       "  'maximum',\n",
       "  '@',\n",
       "  'knowing',\n",
       "  '@',\n",
       "  '@',\n",
       "  'allow',\n",
       "  '@',\n",
       "  '@',\n",
       "  'predict',\n",
       "  '@',\n",
       "  '@',\n",
       "  'completely',\n",
       "  '@',\n",
       "  '@',\n",
       "  'conditional',\n",
       "  'entropy',\n",
       "  '@',\n",
       "  'zero'],\n",
       " ['Therefore', '@', 'mutual', 'information', 'reaches', '@', 'maximum'],\n",
       " [\"It's\",\n",
       "  'going',\n",
       "  '@',\n",
       "  '@',\n",
       "  'larger',\n",
       "  '@',\n",
       "  '@',\n",
       "  'equal',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'words,',\n",
       "  'picking',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word,',\n",
       "  '@',\n",
       "  'computing',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word,',\n",
       "  '@',\n",
       "  \"won't\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  'larger',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  '@',\n",
       "  'eats',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['So',\n",
       "  '@',\n",
       "  \"let's\",\n",
       "  'think',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information'],\n",
       " ['Now,', '@', 'order', '@', '@', 'that,', '@', '@'],\n",
       " ['use',\n",
       "  '@',\n",
       "  'different',\n",
       "  'form',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mathematically',\n",
       "  'write',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  '@',\n",
       "  '@',\n",
       "  'form',\n",
       "  'shown',\n",
       "  '@',\n",
       "  '@',\n",
       "  'slide,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'essentially',\n",
       "  '@',\n",
       "  '@',\n",
       "  'formula',\n",
       "  '@',\n",
       "  'computes',\n",
       "  \"what's\",\n",
       "  'called',\n",
       "  'KL-divergences',\n",
       "  '@',\n",
       "  'callback',\n",
       "  'labeler',\n",
       "  'divergance'],\n",
       " ['This',\n",
       "  '@',\n",
       "  '@',\n",
       "  'term',\n",
       "  '@',\n",
       "  'information',\n",
       "  'theory',\n",
       "  '@',\n",
       "  'measures',\n",
       "  '@',\n",
       "  'divergance',\n",
       "  '@',\n",
       "  '@',\n",
       "  'distributions'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'formula,',\n",
       "  \"it's\",\n",
       "  '@',\n",
       "  'sum',\n",
       "  '@',\n",
       "  '@',\n",
       "  'combinations',\n",
       "  '@',\n",
       "  'different',\n",
       "  'values',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variables,',\n",
       "  '@',\n",
       "  'inside',\n",
       "  '@',\n",
       "  'sum',\n",
       "  'mainly',\n",
       "  \"we're\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'comparison',\n",
       "  '@',\n",
       "  '2',\n",
       "  'joint',\n",
       "  'distributions'],\n",
       " ['The', 'numerator', '@', '@', 'joint', 'actual', 'observed'],\n",
       " ['Join', '@', 'distribution', '@', '@', '@', 'random', 'variables'],\n",
       " ['The',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'denominator',\n",
       "  '@',\n",
       "  '@',\n",
       "  'interpreted',\n",
       "  '@',\n",
       "  '@',\n",
       "  'expected',\n",
       "  'joint',\n",
       "  'distribution',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variables'],\n",
       " ['If', '@', '@', 'independent'],\n",
       " ['Because',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variables',\n",
       "  '@',\n",
       "  'independent,',\n",
       "  '@',\n",
       "  'joined',\n",
       "  'distribution',\n",
       "  '@',\n",
       "  'equal',\n",
       "  '@',\n",
       "  '@',\n",
       "  'product',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'comparison',\n",
       "  '@',\n",
       "  'tell',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'variables',\n",
       "  '@',\n",
       "  '@',\n",
       "  'independent',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'independent,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'expect',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['But',\n",
       "  '@',\n",
       "  '@',\n",
       "  'numerator',\n",
       "  '@',\n",
       "  'different',\n",
       "  '@',\n",
       "  '@',\n",
       "  'denominator,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mean',\n",
       "  '@',\n",
       "  '@',\n",
       "  'variables',\n",
       "  '@',\n",
       "  '@',\n",
       "  'independent,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'helps',\n",
       "  'measure',\n",
       "  '@',\n",
       "  'association'],\n",
       " ['The',\n",
       "  'sum',\n",
       "  '@',\n",
       "  'simply',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'consideration',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'combinations',\n",
       "  '@',\n",
       "  '@',\n",
       "  'values',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variables'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'case,',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variable',\n",
       "  '@',\n",
       "  'choose',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'values',\n",
       "  '0',\n",
       "  '@',\n",
       "  '1,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'combinations',\n",
       "  '@'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'form',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  '@',\n",
       "  'shows',\n",
       "  '@',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  'measures',\n",
       "  '@',\n",
       "  'diversions',\n",
       "  '@',\n",
       "  '@',\n",
       "  'actual',\n",
       "  'joint',\n",
       "  'distribution',\n",
       "  '@',\n",
       "  '@',\n",
       "  'expected',\n",
       "  'distribution',\n",
       "  '@',\n",
       "  '@',\n",
       "  'independence',\n",
       "  'assumption'],\n",
       " ['The',\n",
       "  'larger',\n",
       "  '@',\n",
       "  'divergence',\n",
       "  'is,',\n",
       "  '@',\n",
       "  'higher',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['So',\n",
       "  '@',\n",
       "  \"let's\",\n",
       "  '@',\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'exactly',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  'involved',\n",
       "  '@',\n",
       "  '@',\n",
       "  'formula',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'I',\n",
       "  'listed',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  'involved',\n",
       "  '@',\n",
       "  \"it's\",\n",
       "  'easy',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'verify',\n",
       "  '@',\n",
       "  'basically',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '2',\n",
       "  'probabilities',\n",
       "  'corresponding',\n",
       "  '@',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  'absence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word'],\n",
       " ['So', '@', 'W1,', '@', '@', '@', 'probabilities', 'shown', '@'],\n",
       " ['They',\n",
       "  '@',\n",
       "  'sum',\n",
       "  '@',\n",
       "  '1',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'present',\n",
       "  '@',\n",
       "  'absent',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment'],\n",
       " ['And',\n",
       "  'similarly',\n",
       "  '@',\n",
       "  '@',\n",
       "  'second',\n",
       "  'word,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  'representing',\n",
       "  'presence',\n",
       "  '@',\n",
       "  'absence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sums',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'finally',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'lot',\n",
       "  '@',\n",
       "  'joint',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  'represented',\n",
       "  '@',\n",
       "  'scenarios',\n",
       "  '@',\n",
       "  'Co-occurrences',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words'],\n",
       " ['And', '@', '@', 'shown', '@'],\n",
       " ['Right,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'sums',\n",
       "  '@',\n",
       "  '1',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'possible',\n",
       "  'scenarios'],\n",
       " ['Either', '@', '@', 'occur'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  '@',\n",
       "  'variables',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'value',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'occurs'],\n",
       " ['There', '@', '@', 'scenarios'],\n",
       " ['In',\n",
       "  '@',\n",
       "  '@',\n",
       "  'cases,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'random',\n",
       "  'variables',\n",
       "  '@',\n",
       "  '@',\n",
       "  'equal',\n",
       "  '@',\n",
       "  '1',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '0'],\n",
       " ['And', 'finally', '@', '@', '@', 'scenario', '@', '@', '@', '@', 'occurs'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'variables',\n",
       "  'taking',\n",
       "  '@',\n",
       "  'value',\n",
       "  '@',\n",
       "  '0'],\n",
       " ['And',\n",
       "  \"they're\",\n",
       "  'summing',\n",
       "  '@',\n",
       "  '@',\n",
       "  '1,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  'involved',\n",
       "  '@',\n",
       "  '@',\n",
       "  'calculation',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information'],\n",
       " ['@'],\n",
       " ['Once',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  '@',\n",
       "  'calculate',\n",
       "  '@',\n",
       "  'probabilities,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'easily',\n",
       "  'calculate',\n",
       "  '@',\n",
       "  'mutual',\n",
       "  'information'],\n",
       " [\"It's\",\n",
       "  '@',\n",
       "  'interesting',\n",
       "  '@',\n",
       "  'note',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'relations',\n",
       "  '@',\n",
       "  'constraints',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'saw',\n",
       "  '@',\n",
       "  '@',\n",
       "  'them,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'previous',\n",
       "  'slide',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'seen',\n",
       "  '@',\n",
       "  '@',\n",
       "  'marginal',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  'sum',\n",
       "  '@',\n",
       "  'one,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'seen',\n",
       "  '@',\n",
       "  'constraint',\n",
       "  '@',\n",
       "  'says',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'different',\n",
       "  'scenarios',\n",
       "  '@',\n",
       "  'Co',\n",
       "  'occurrences,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'additional',\n",
       "  'constraints',\n",
       "  'listed',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['And',\n",
       "  'so,',\n",
       "  '@',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'means',\n",
       "  '@',\n",
       "  '@',\n",
       "  'add',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observe',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  'occur',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'occurs',\n",
       "  '@',\n",
       "  '@',\n",
       "  'second',\n",
       "  'word',\n",
       "  \"doesn't\",\n",
       "  'occur,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'exactly',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  'observed'],\n",
       " ['In',\n",
       "  '@',\n",
       "  'words,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  'observed',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  'observed',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'scenarios',\n",
       "  'depending',\n",
       "  '@',\n",
       "  'weather',\n",
       "  'second',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observed'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'probability',\n",
       "  'captures',\n",
       "  '@',\n",
       "  '@',\n",
       "  'scenario',\n",
       "  '@',\n",
       "  '@',\n",
       "  'signal',\n",
       "  'word',\n",
       "  'actually',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observed'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'captures',\n",
       "  '@',\n",
       "  'second',\n",
       "  'scenario',\n",
       "  '@',\n",
       "  '@',\n",
       "  'seond',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observed,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word'],\n",
       " ['And',\n",
       "  \"it's\",\n",
       "  'easy',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'equations',\n",
       "  '@',\n",
       "  'follow',\n",
       "  '@',\n",
       "  '@',\n",
       "  'reasoning'],\n",
       " ['Now',\n",
       "  '@',\n",
       "  'equations',\n",
       "  'allow',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  'based',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities'],\n",
       " ['And', '@', '@', 'simplify', '@', 'computation'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'specifically,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  'present,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'case',\n",
       "  'right?',\n",
       "  'So',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@'],\n",
       " ['And',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'second',\n",
       "  'word,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'easily',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'absence',\n",
       "  'probability,',\n",
       "  'right?',\n",
       "  \"It's\",\n",
       "  '@',\n",
       "  'easy',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'equation',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['An',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'care',\n",
       "  '@',\n",
       "  '@',\n",
       "  'computation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  'absence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word'],\n",
       " ['Now',\n",
       "  \"let's\",\n",
       "  'look',\n",
       "  '@',\n",
       "  '@',\n",
       "  'joint',\n",
       "  'distribution,',\n",
       "  'right?',\n",
       "  \"Let's\",\n",
       "  'assume',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'available',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  'occur',\n",
       "  '@'],\n",
       " ['Now',\n",
       "  \"it's\",\n",
       "  'easy',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'actually',\n",
       "  'compute',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'rest',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  'based',\n",
       "  '@',\n",
       "  '@'],\n",
       " ['Specifically,',\n",
       "  '@',\n",
       "  'example,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'equation,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'occurred',\n",
       "  '@',\n",
       "  '@',\n",
       "  'second',\n",
       "  'word',\n",
       "  '@',\n",
       "  'not,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  '@',\n",
       "  'boxes'],\n",
       " ['And',\n",
       "  'similarly,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'equation',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observe',\n",
       "  '@',\n",
       "  '@',\n",
       "  'second',\n",
       "  'word'],\n",
       " ['And', '@', 'finally', '@'],\n",
       " ['This',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  'calculated',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'equation,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'known',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'known',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'known',\n",
       "  'right?',\n",
       "  'So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'easier',\n",
       "  '@',\n",
       "  'calculate'],\n",
       " ['Right,', '@', '@', '@', '@', '@', 'calculated'],\n",
       " ['So',\n",
       "  '@',\n",
       "  'slide',\n",
       "  'shows',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'need',\n",
       "  '@',\n",
       "  'know',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  '@',\n",
       "  'shown',\n",
       "  '@',\n",
       "  '@',\n",
       "  'boxes,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'presence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '@',\n",
       "  '@',\n",
       "  'Co',\n",
       "  'occurrence',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segment'],\n",
       " [],\n",
       " ['In',\n",
       "  'general,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'use',\n",
       "  '@',\n",
       "  'empirical',\n",
       "  'counts',\n",
       "  '@',\n",
       "  'events',\n",
       "  '@',\n",
       "  '@',\n",
       "  'observed',\n",
       "  'data',\n",
       "  '@',\n",
       "  'estimate',\n",
       "  'probabilities'],\n",
       " ['@',\n",
       "  '@',\n",
       "  'commonly',\n",
       "  '@',\n",
       "  'technique',\n",
       "  '@',\n",
       "  'called',\n",
       "  '@',\n",
       "  'maximum',\n",
       "  'likelihood',\n",
       "  'estimate,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'simply',\n",
       "  'normalize',\n",
       "  '@',\n",
       "  'observed',\n",
       "  'accounts'],\n",
       " ['So',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'that,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'compute',\n",
       "  '@',\n",
       "  'probabilities',\n",
       "  '@',\n",
       "  'follows',\n",
       "  '@',\n",
       "  'estimating',\n",
       "  '@',\n",
       "  'probability',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'occurring',\n",
       "  '@',\n",
       "  'segment,',\n",
       "  '@',\n",
       "  'simply',\n",
       "  'normalize',\n",
       "  '@',\n",
       "  'counts',\n",
       "  '@',\n",
       "  'segments',\n",
       "  '@',\n",
       "  'contain',\n",
       "  '@',\n",
       "  'word'],\n",
       " ['So', \"let's\", '@', '@', '@', 'look', '@', '@', 'data', '@'],\n",
       " ['On',\n",
       "  '@',\n",
       "  'right',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'I',\n",
       "  'listed',\n",
       "  '@',\n",
       "  'hypothesizes',\n",
       "  '@',\n",
       "  'data',\n",
       "  '@',\n",
       "  '@',\n",
       "  'segments'],\n",
       " ['And', '@', '@', 'segments', '@', '@', '@', 'words', 'occur'],\n",
       " ['Their', 'indicator', '@', '@', '@', '@', 'columns'],\n",
       " ['In',\n",
       "  '@',\n",
       "  '@',\n",
       "  'cases,',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  'occurs,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'column',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'column',\n",
       "  '@',\n",
       "  'zero'],\n",
       " ['And',\n",
       "  '@',\n",
       "  'course',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'cases,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'words',\n",
       "  'occur,',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'zeros'],\n",
       " ['And',\n",
       "  'For',\n",
       "  'estimating',\n",
       "  '@',\n",
       "  'probabilities,',\n",
       "  '@',\n",
       "  'simply',\n",
       "  'need',\n",
       "  '@',\n",
       "  'collect',\n",
       "  '@',\n",
       "  '@',\n",
       "  'counts'],\n",
       " ['So', '@', '@', 'counts', '@', '1st,', '@', 'count', '@', 'W'],\n",
       " ['1',\n",
       "  '@',\n",
       "  \"that's\",\n",
       "  '@',\n",
       "  'total',\n",
       "  'number',\n",
       "  '@',\n",
       "  'segments',\n",
       "  '@',\n",
       "  'contain',\n",
       "  'world',\n",
       "  'W',\n",
       "  '@'],\n",
       " [\"It's\",\n",
       "  '@',\n",
       "  '@',\n",
       "  'ones',\n",
       "  '@',\n",
       "  '@',\n",
       "  'column',\n",
       "  '@',\n",
       "  'W',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'count',\n",
       "  '@',\n",
       "  '@',\n",
       "  'ones',\n",
       "  '@',\n",
       "  '@',\n",
       "  'seen',\n",
       "  '@'],\n",
       " ['The',\n",
       "  'second',\n",
       "  'counter',\n",
       "  '@',\n",
       "  '@',\n",
       "  'word',\n",
       "  '2',\n",
       "  '@',\n",
       "  '@',\n",
       "  '@',\n",
       "  'count',\n",
       "  '@',\n",
       "  'ones',\n",
       "  '@',\n",
       "  '@',\n",
       "  'second',\n",
       "  'column'],\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe79d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd4944de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "def tokenize(sentence):\n",
    "    \n",
    "    return [token if token not in STOP_WORDS else \"@\" for token in sentence.split() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb80948d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5113"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c320bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "    return re.sub(r'\\s{2,}', ' ', sentence)\n",
    "sentences = [clean_sentence(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b38e81f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [tokenize(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db3e2c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "def build_phrases(sentences):\n",
    "    phrases = Phrases(sentences,\n",
    "                      min_count=3,\n",
    "                      threshold=10,\n",
    "                      progress_per=1000)\n",
    "    return Phraser(phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f020b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_model = build_phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b09f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_bi_grams(phrases_model, sentence):\n",
    "    return ' '.join(phrases_model[sentence])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97ea86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrased = [sentence_to_bi_grams(phrase_model, s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb87e9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@ @ lecture @ @ @ overview @ text mining @ analytics',\n",
       " '@ lets define @ term text mining @ @ term text analytics',\n",
       " '@ title @ @ course @ called text mining @ analytics @ @ @ terms text mining @ text analytics @ actually roughly @ @',\n",
       " '@ @ @ @ going @ @ distinguish @ @ @ going @ use @ interchangeably',\n",
       " '@ @ reason @ @ @ chosen @ use @ terms @ @ title @ @ @ @ @ @ subtle difference @ @ look @ @ @ phrases literally',\n",
       " 'mining emphasizes @ @ @ process @ @ gives @ @ algorithmic view @ @ problem',\n",
       " 'analytics @ @ @ hand emphasizes @ @ @ result @ having @ problem @ mind',\n",
       " '@ going @ look @ @ text data @ help @ solve @ problem',\n",
       " '@ @ @ @ said @ @ treat @ @ terms roughly @ @ @ @ think @ @ literature @ probably @ find @ @',\n",
       " '@ @ @ going @ @ distinguish @ @ @ course',\n",
       " '@ text mining @ text analytics mean @ @ want @ turn text data @ high_quality information @ actionable_knowledge',\n",
       " '@ @ @ cases @ @ @ problem @ dealing @ @ lot @ text data @ @ hope @ turn @ text data @ @ @ useful @ @ @ @ raw text data',\n",
       " '@ @ @ distinguish @ different results @ @ high_quality information @ @ @ actionable_knowledge',\n",
       " '@ @ @ boundary @ @ @ @ @ @ clear @ @ @ want @ @ @ little_bit @ @ @ different angles @ @ result @ text mining',\n",
       " '@ @ case @ high_quality information @ refer @ @ concise information @ @ topic @ @ @ @ easier @ humans @ digest @ @ raw text data',\n",
       " '@ example @ @ face @ lot @ reviews @ @ product',\n",
       " '@ @ concise form @ @ information @ @ @ concise summary @ @ major opinions @ @ features @ @ product',\n",
       " 'positive @ lets @ battery_life @ @ laptop',\n",
       " '@ @ kind @ results @ @ useful @ help people digest text data @ @ @ @ @ minimize @ human_effort @ consuming text data @ @ sense',\n",
       " '@ @ kind @ output @ actionable_knowledge',\n",
       " '@ @ emphasize @ utility @ @ information @ knowledge @ discover @ text data',\n",
       " '@ actionable_knowledge @ @ decision problem @ @ actions @ @',\n",
       " '@ example @ @ @ able @ determine @ product @ @ appealing @ @ @ @ better choice @ @ shopping decision',\n",
       " '@ @ @ outcome @ @ called actionable_knowledge @ @ consumer @ @ @ knowledge @ @ @ decision @ act @ @',\n",
       " '@ @ @ case text mining supplies knowledge @ optimal decision_making',\n",
       " '@ @ @ @ @ @ @ clearly distinguished @ @ dont necessarily @ @ @ @ distinction',\n",
       " 'text mining @ @ related @ text retrieval @ @ @ essential component @ @ text mining systems',\n",
       " '@ text retrieval refers @ finding relevant information @ @ large @ @ text data',\n",
       " '@ ive taught @ separate mooc @ text retrieval @ search engines @ @ discussed @ techniques @ text retrieval',\n",
       " '@ @ @ taken @ mooc @ @ find @ overlap @ @ @ @ useful @ know @ background @ text retrieval @ understanding @ @ @ topics @ text mining',\n",
       " '@ @ @ @ @ taken @ mooc @ @ fine @ @ @ @ context mining @ analytics @ going @ repeat @ @ @ key concepts @ @ relevant @ text mining',\n",
       " '@ @ @ high_level let @ @ explain @ relation @ text retrieval @ text mining',\n",
       " 'text retrieval @ @ useful @ text mining @ @ ways @ text retrieval @ @ @ preprocessor @ text mining meaning @ @ @ help @ turn big text data @ @ relatively small @ @ @ relevant text data @ @ @ whats needed @ solving @ particular problem',\n",
       " '@ @ @ sense text retrieval @ helps minimize human_effort',\n",
       " 'text retrieval @ @ needed @ knowledge provenance @ @ roughly corresponds @ @ interpretation @ text mining @ turning text data @ actionable_knowledge',\n",
       " '@ @ find @ patterns @ text data @ actionable_knowledge @ generally @ @ @ verify @ knowledge @ looking @ @ original text data @ @ users @ @ @ @ @ text retrieval support @ @ @ @ @ original text data @ interpret @ pattern @ @ better understand @ knowledge @ @ verify @ @ pattern @ @ reliable',\n",
       " '@ @ @ @ high_level introduction @ @ concept @ text mining @ @ relation @ text mining @ retrieval',\n",
       " '@ lets talk @ text data @ @ special kind @ data',\n",
       " '@ @ interesting @ view text data @ data generated @ humans @ subjective_sensors',\n",
       " '@ @ slide_shows @ analogy @ text data @ non_text data @ @ humans @ subjective_sensors @ physical sensors @ @ network sensor @ thermometer',\n",
       " '@ @ general @ sensor @ monitor @ real_world @ @ way @ @ sense @ signal @ @ real_world @ @ @ report @ signal @ data @ @ forms @ example @ thermometer @ watch @ temperature @ real_world @ @ @ report @ temperature @ particular format',\n",
       " 'similarly @ geo sensor @ sense @ location @ @ report @ location specification @ example @ @ form @ longitude value @ lattitude value',\n",
       " 'network sensor @ monitor network traffic @ activities @ @ network @ report @ digital format @ data',\n",
       " 'similarly @ @ think @ humans @ subjective_sensors @ @ observe @ real_world @ @ perspective @ @ humans @ express @ @ @ observed @ @ form @ text data',\n",
       " '@ @ @ sense human @ actually @ subjective sensor @ @ @ sense whats happening @ @ world @ @ express whats observed @ @ form @ data @ @ case text data',\n",
       " '@ looking @ @ text data @ @ way @ @ advantage @ @ able @ integrate @ kinds @ data @ @ thats @ needed @ @ data mining problems',\n",
       " '@ @ @ @ looking @ @ general problem @ data mining @ @ general @ @ @ dealing @ @ lot @ data @ @ world @ @ related @ @ problem',\n",
       " '@ @ general @ @ dealing @ @ non_text data @ text data @ @ course @ non_text data @ usually produced @ physical sensors',\n",
       " '@ @ non_text data @ @ @ @ different formats numerical data @ categorical @ relational data @ multimedia data like @ video @ speech',\n",
       " '@ @ non_text data @ @ @ important @ @ problems',\n",
       " '@ text data @ @ @ important @ @ @ contain @ lot @ semantic content @ @ @ contain knowledge @ @ users especially preferences @ opinions @ users',\n",
       " '@ @ @ treating text data @ @ data observed @ human sensors @ @ treat @ @ data @ @ @ @ framework',\n",
       " '@ data mining problem @ basically @ turn @ data turn @ @ data @ actionable_knowledge @ @ @ @ @ advantage @ change @ real_world @ course @ better',\n",
       " '@ @ means @ data mining problem @ basically taking @ lot @ data @ input @ giving actionable_knowledge @ output',\n",
       " 'inside @ data mining module @ @ @ @ @ @ @ number @ different kinds @ mining_algorithms @ @ @ @ @ different kinds @ data @ generally need different algorithms @ mining @ data',\n",
       " '@ example video data @ require computer vision @ understand video content @ @ @ facilitate @ @ effective mining @ @ @ @ @ lot @ general algorithms @ @ applicable @ @ kinds @ data @ @ algorithms @ course @ @ useful @ @ @ particular kind @ data @ generally want @ @ develop special algorithms',\n",
       " '@ @ course @ cover specialized algorithms @ @ particularly_useful @ mining text data',\n",
       " '',\n",
       " '@ looking @ @ text mining problem @ closely @ @ @ @ problem @ similar @ general data mining @ @ @ @ focusing @ @ text data',\n",
       " '@ @ going @ @ text mining_algorithms @ help @ @ turn text data @ actionable_knowledge @ @ @ use @ @ real_world',\n",
       " 'especially @ decision_making @ @ completing @ tasks @ require text data @ support @ @ @ general @ @ real_world problems @ data mining @ @ tend @ @ @ kinds @ data @ @ non textual',\n",
       " '@ @ @ general picture @ @ @ include non_text data @ @',\n",
       " '@ @ @ reason @ @ @ concerned @ joint mining @ text @ non_text data @ @ @ @ course @ going @ focus @ @ text mining',\n",
       " '@ @ @ @ touch @ @ join @ analysis @ @ text data @ nontext_data',\n",
       " '@ @ problem definition @ @ @ look @ @ landscape @ @ topics @ text mining analytics',\n",
       " '@ @ slide_shows @ process @ generating text data @ @ detail',\n",
       " '@ specifically human sensor @ human observer @ look @ @ world @ @ perspective',\n",
       " 'different people @ @ looking @ @ world @ different angles @ @ @ pay_attention @ different things',\n",
       " '@ @ person @ @ different time @ @ pay_attention @ different_aspects @ @ observed world',\n",
       " '@ @ @ human sensor @ perceive @ world @ @ perspective',\n",
       " '@ @ human',\n",
       " '',\n",
       " '',\n",
       " '@ sensor @ @ form @ view @ @ world @ @ @ @ called @ observed world',\n",
       " '@ course @ @ @ different @ @ real_world @ @ @ perspective @ @ person @ taken',\n",
       " '@ @ @ @ biased @',\n",
       " '@ @ observable world @ @ represented @ @ example entity relation graphs @ @ @ @ @ general way @ knowledge representation language',\n",
       " '@ @ general @ @ basically @ @ person @ @ mind @ @ world @ @ dont @ know @ exactly @ looks_like @ course',\n",
       " '@ @ @ human @ express @ @ person @ observed @ @ natural_language @ @ english @ @ result @ text data',\n",
       " '@ course @ person @ @ @ @ different language @ express @ @ @ @ @ observed',\n",
       " '@ @ case @ @ @ text data @ mixed languages @ different languages',\n",
       " '@ @ main_goal @ text mining @ actually @ revert @ process @ generating test data',\n",
       " '@ @ hope @ @ able @ uncover @ aspect @ @ process',\n",
       " '@ @ specifically @ @ think @ @ mining @ example knowledge @ @ language',\n",
       " '@ @ means @ looking @ text data @ english @ @ @ able @ discover @ @ english',\n",
       " '',\n",
       " '',\n",
       " '@ usage @ english',\n",
       " '',\n",
       " '',\n",
       " '@ patterns @ english',\n",
       " '@ @ @ 1 type @ mining problems @ @ result @ @ knowledge @ language @ @ @ useful @ @ ways',\n",
       " '@ @ look @ @ picture @ @ @ @ @ knowledge @ @ observed world',\n",
       " '@ @ @ @ @ @ @ @ mining @ content @ text data',\n",
       " '@ going @ look @ @ @ @ text data @ @ @ @ try @ @ @ essence @ @',\n",
       " '@ extracting high_quality information @ @ particular aspect @ @ world @ @ interested @',\n",
       " '@ example @ @ @ @ said @ @ particular person @ particular entity @ @ @ @ regarded @ mining content @ describe @ observed world @ @ users mind @ @ persons mind',\n",
       " '@ @ look @ @ @ @ @ imagine @ @ @ knowledge @ @ observer @ @ @',\n",
       " '@ @ @ @ @ @ @ @ text data @ infer @ properties @ @ person',\n",
       " '@ @ properties @ include @ mood @ @ person @ sentiment @ @ person',\n",
       " '@ note @ @ distinguish @ observed @ world @ @ person @ text data @ describe @ @ person @ observed @ @ objective way @ @ description @ @ @ subject @ sentiment @ @ @ general @ @ imagine @ text data @ contain @ factual descriptions @ @ world plus @ subjective comments @ thats @ @ @ possible @ @ text mining @ @ knowledge @ @ observer',\n",
       " 'finally @ @ look @ @ picture @ @ left @ @ @ picture @ @ @ @ @ @ certainly @ @ @ @ @ real_world right @ @ @ @ @ text mining @ infer @ real_world variables @ @ @ @ called predictive analytics',\n",
       " '@ @ want @ predict @ value @ certain interesting variables',\n",
       " '@ @ picture basically covered multiple types @ knowledge @ @ @ @ @ text @ general',\n",
       " '@ @ infer @ real_world variables @ @ @ use @ @ @ results @ mining text data @ intermediate results @ help @ prediction',\n",
       " '@ example @ @ @ @ content @ text data @ @ generate @ summary @ content @ @ summary @ @ @ @ @ help @ predict @ variables @ @ real_world',\n",
       " '@ @ course @ @ @ generated @ @ original text data @ @ want @ emphasize @ @ @ @ processing @ text data @ generate @ features @ @ help @ @ prediction @ @ important',\n",
       " '@ thats @ @ @ @ @ @ results @ @ @ mining tasks including mining @ content @ text data @ mining knowledge @ @ observer @ @ @ @ helpful @ prediction',\n",
       " '@ fact @ @ @ @ nontext_data @ @ @ use @ nontext_data @ help prediction',\n",
       " '@ @ course @ depends @ @ problem',\n",
       " '@ general nontext_data @ @ @ important @ @ prediction tasks',\n",
       " '@ example @ @ want @ predict @ stocks',\n",
       " 'stock_prices @ changes @ stock_prices based @ discussion @ @ news_articles @ @ social_media @ @ @ @ example @ @ text data @ predict @ @ real_world variables',\n",
       " '@ @ @ case obviously @ historical stock price data @ @ @ important @ @ prediction @ @ thats example @ nontext_data @ @ @ @ useful @ @ prediction @ @ @ combine @ kinds @ data @ @ @ prediction',\n",
       " '@ nontext_data @ @ @ useful @ analyzing text @ supplying context',\n",
       " '@ @ look @ @ text data @ @ @ @ looking @ @ content @ opinions expressed @ text',\n",
       " '@ text data generally @ @ context associated',\n",
       " '@ example @ time @ location @ @ associated @ @ text data @ @ @ useful context information',\n",
       " '@ @ context @ provide interesting angles @ analyzing text data',\n",
       " '@ example @ @ partition text data @ different time periods @ @ @ availability @ time',\n",
       " '@ @ @ analyze text data @ @ time_period @ @ @ @ comparison',\n",
       " 'similarly @ @ partition text data based @ locations @ @ metadata thats associated @ form interesting comparison scenarios',\n",
       " '@ @ @ sense nontext_data @ actually provide interesting angles @ perspectives @ text analysis @ @ help @ @ context sensitive analysis @ content @ @ language usage @ @ opinions @ @ observer @ @ authors @ text data',\n",
       " '@ @ analyze @ sentiment @ different context @ @ @ fairly general landscape @ @ topics @ text mining @ analytics',\n",
       " '@ @ course @ going @ selectively cover @ @ @ topics',\n",
       " '@ actually hope @ cover @ @ @ general topics',\n",
       " '@ @ @ going @ cover natural_language processing @ briefly @ @ @ @ @ @ understanding text data @ @ determines @ @ @ represent text @ text mining',\n",
       " 'second @ going @ talk @ @ @ @ word_associations @ text data @ word_associations @ @ form @ useful lexical knowledge @ @ language',\n",
       " '@ @ going @ talk @ @ topic mining @ analysis @ @ @ @ @ way @ analyze content @ text @ @ @ @ useful way @ analyzing content',\n",
       " '@ @ @ @ @ @ useful techniques @ text mining',\n",
       " '@ @ @ going @ talk @ opinion mining @ sentiment_analysis',\n",
       " '@ @ @ @ regarded @ @ example @ mining knowledge @ @ observer',\n",
       " '@ finally @ @ going @ cover @ text based_prediction problems @ @ try @ predict @ real_world variable based @ text data',\n",
       " '@ @ slide @ serves @ @ road map @ @ course',\n",
       " '@ @ use @ @ outline @ @ topics @ @ cover @ @ rest @ @ course',\n",
       " '',\n",
       " '@ lecture @ @ @ naturallanguage content analysis',\n",
       " 'natural_language content analysis @ @ foundation @ text mining',\n",
       " '@ @ @ going @ @ talk @ @',\n",
       " '@ @ particular natural_language processing',\n",
       " '@ @ factor @ @ @ represent text data @ @ determines @ algorithms @ @ @ @ analyze @ @ text data',\n",
       " '@ going @ @ @ look @ @ basic concepts @ natural_language @',\n",
       " 'im_going @ explain @ concepts @ @ simple example @ @ @ seeing @',\n",
       " '@ dog @ chasing @ boy @ @ playground',\n",
       " '@ @ @ @ @ simple sentence',\n",
       " '@ @ read @ @ sentence @ dont @ @ think @ @ @ @ meaning @ @',\n",
       " '@ @ @ computer @ @ understand @ sentence @ computer @ @ @ @ @ steps',\n",
       " '@ @ computer needs @ know @ @ @ words @ @ segment @ words',\n",
       " '@ english @ @ @ easy @ @ @ @ look @ @ space @ @ @ computer @ need @ know @ categories @ @ words syntactical categories',\n",
       " '@ @ example dog @ @ noun chasing @ @ verb boy @ @ noun etc',\n",
       " '@ @ @ called @ lexical analysis',\n",
       " '@ particular tagging @ words @ @ syntactic categories @ called @ @ @ speech_tagging',\n",
       " '@ @ @ computer @ needs @ figure @ @ relation @ @ words',\n",
       " '@ @ @ @ dog @ form @ noun phrase',\n",
       " '@ @ playground @ @ @ prepositional phrase etc',\n",
       " '@ @ @ certain way @ @ @ @ connected @ @ order @ generate @ meaning',\n",
       " '@ @ combinations @ @ @ sense',\n",
       " '@ @',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '@ @ called syntactic parsing',\n",
       " '@ syntactical analysis @ parsing @ natural_language sentence',\n",
       " '@ outcome @ parse_tree @ youre_seeing @',\n",
       " '@ tells @ @ structure @ @ sentence @ @ @ know @ @ @ interpret @ sentence',\n",
       " '@ @ @ @ semantics @',\n",
       " '@ @ order @ @ @ meeting @ @ @ map @ phrases @ @ structures @ @ real_world entities @ @ @ @ @ mind',\n",
       " '@ dog @ @ concept @ @ know @ @ boy @ concept @ @ know',\n",
       " '@ connecting @ phrases @ @ @ know @ understanding',\n",
       " '@ computer @ @ @ formally represent @ entities @ @ symbols',\n",
       " '@ dog d1 means d1 @ @ dog boy b1 means b1 refers @ @ boy etc',\n",
       " '@ @ @ represented @ chasing action @ @ predicate',\n",
       " '@ chasing @ predic @ @ @ arguments d1 b1 @ p1 @ @ @ playground right @ @ @ @ formal representation @ @ semantics @ @ sentence',\n",
       " '@ @ reach @ level @ understanding @ @ @ @ inferences',\n",
       " '@ example @ @ assume theres @ rule @ says @ @ @ @ chased @ @ person @ @ scared @ @ @ infer @ boy @ @ scared',\n",
       " '@ @ @ inferred meaning based @ @ additional knowledge',\n",
       " '@ finally @ @ @ @ @ infer',\n",
       " '',\n",
       " '',\n",
       " '@ @ infer @ @ sentence @ requesting @ @ @ person @ said @ sentence @ saying @ sentence',\n",
       " '@ @ @ @ @ @ @ understanding @ purpose @ saying @ sentence @ @ @ called speech act analysis @ pragmatic analysis',\n",
       " '@ refers @ @ use @ language',\n",
       " '@ @ @ case person saying @ @ @ reminding @ person @ bring @ @ dog',\n",
       " '@ @ means @ saying @ sentence @ person actually takes @ action',\n",
       " '@ @ action @ @ @ @ @ request',\n",
       " '@ @ slide clearly shows @ @ order @ @ understand @ sentence @ @ @ lot @ things @ @ computer @ @ @ @',\n",
       " '@ general @ @ hard @ @ computer @ @ @ especially @ @ wanted @ @ @ correctly',\n",
       " '@ @ @ @ difficult',\n",
       " '@ @ main reason @ @ natural_language processing @ @ difficult @ @ designed @ @ human communications efficient',\n",
       " '@ @ result @ example @ omit @ lot @ common sense knowledge',\n",
       " '@ @ assume @ @',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '@ @ @ @ @ knowledge theres @ need @ encode @ knowledge',\n",
       " '@ @ makes communication efficient',\n",
       " '@ @ @ @ lot @ ambiguities like ambiguities @ words',\n",
       " '@ @ @ @ @ @ assume @ @ @ @ ability @ disambiguate @ word @ theres @ problem @ having @ @ word @ mean possibly different things @ different context',\n",
       " '@ @ @ computer @ @ @ @ difficult @ @ computer @ @ @ @ common sense knowledge @ @ @ @ @ computer @ @ confused @ @ @ makes @ hard @ natural_language processing',\n",
       " '@ @ makes @ @ hard @ @ step @ @ slide @ @ showed @ earlier',\n",
       " 'ambiguity @ @ main killer meaning @ @ @ step @ @ multiple choices @ @ computer @ @ @ decide whats @ right choice @ @ decision @ @ @ difficult @ @ @ @ @ @ @ moment',\n",
       " '@ @ general @ need common sense reasoning',\n",
       " '@ order @ fully understand @ natural_language @ computers today dont @ @ @ @ thats @ @ @ hard @ computers @ precisely understanding natural_language @ @ point',\n",
       " '@ @ @ @ specific_examples @ challenges',\n",
       " 'think @ @ word level ambiguity @ word like design @ @ @ noun @ @ verb @ weve_got ambiguous @ @ speech tag',\n",
       " 'root @ @ multiple meanings',\n",
       " '@ @ @ @ mathematical sense like @ square root',\n",
       " '@ @ @ @ @ root @ @ plant',\n",
       " 'syntactic ambiguity refers @ different interpretations @ @ sentence @ terms @ structures',\n",
       " '@ @ example natural_language processing @ actually @ interpreted @ @ ways',\n",
       " '@ @ @',\n",
       " '@ ordinary meaning @ @ @ @ getting',\n",
       " '@ @ talking @ @ topic @ @ processing @ natural_language',\n",
       " '@ @ @ @ @ possible interpretation @ @ @ @ language_processing @ natural',\n",
       " '@ @ dont generally @ @ problem @ imagine @ @ @ computer @ determine @ structure @ computer @ actually @ @ @ @ choice @ @ @',\n",
       " '@ classic example @ @ man saw @ boy @ @ telescope',\n",
       " '@ ambiguity lies @ @ question @ @ @ telescope',\n",
       " '@ @ called @ prepositional phrase attachment ambiguity meaning',\n",
       " '',\n",
       " '',\n",
       " '@ @ attach @ prepositional phrase @ @ telescope @ @ modify @ boy @ @ @ @ modifying saw @ verb @ problem anaphora resolution john persuaded bill @ buy @ tv @ @',\n",
       " '@ @ referred @ john @ bill pre supposition @ @ difficulty',\n",
       " '@ @ quit smoking implies @ @ smoked @ @ @ need @ @ @ knowledge @ order @ understand @ languages',\n",
       " '@ @ @ problems @ state @ @ art natural_language processing_techniques @ @ @ perfectly @ @ @ simplest @ @ speech_tagging @ @ @ solve @ @ problem',\n",
       " '@ accuracy @ @ listed @ @ @ 97 @ @ taken @ @ studies earlier @ @ studies obviously @ @ @ @ particular datasets @ @ numbers @ @ @ @ meaningful @ @ @ @ @ @ @ context @ @ data set @ @ @ @ evaluation @ @ @ @ numbers @ need @ @ @ @ sense @ @ accuracy @ @ @ @ @ @ things like @',\n",
       " '@ doesnt_mean @ @ data set @ @ accuracy @ @ precisely 97',\n",
       " '@ @ general @ @ @ @ @ speech_tagging fairly @ @ @ perfectly',\n",
       " 'parsing @ @ @ difficult @ @ partial parsing meaning @ @ @ @ phrases correct @ @ probably achieve 90 @ better accuracy',\n",
       " '@ @ @ @ complete parse_tree correctly @ @ @ @ difficult',\n",
       " '@ semantic_analysis @ @ @ @ @ aspects @ semantic_analysis particularly extraction @ entities @ relations',\n",
       " '@ example recognizing @ @ @ person thats @ location @ person @ @ person met @ @ place etc',\n",
       " '@ @ @ @ @ word sense disambiguation @ @ extent',\n",
       " '@ @ figure @ @ occurrence @ root @ @ sentence refers @ @ mathematical sense etc',\n",
       " 'sentiment_analysis @ @ aspect @ semantic_analysis @ @ @ @',\n",
       " '@ means @ @ tag @ sentences general positive @ @ talking @ product',\n",
       " '@ talking @ @ person',\n",
       " 'influence @ @ @ hard @ @ generally @ @ @ @ @ big domain @ @ @ feasible @ @ limited domain',\n",
       " '@ thats @ generally difficult problem @ artificial intelligence',\n",
       " 'speech act analysis @ @ @ difficult @ @ @ @ @ @ properly @ @ specialized cases @ @ lot @ help @ human',\n",
       " '@ annotate @ data @ @ computer @ learn @',\n",
       " '@ @ slides @ shows @ computers @ far @ @ able @ understand natural_language precisely @ @ @ explains @ @ text mining problem @ difficult @ @ @ rely @ mechanical approaces @ computational methods @ understand @ language precisely',\n",
       " '@ @ @ @ use @ @ @ today particular statistical machine_learning methods @',\n",
       " 'statistical analysis methods @ try @ @ @ @ meaning @ @ @ text @ possible',\n",
       " '@ later @ @ @ @ @ @ actually @ @ algorithms @ @ @ extract @ interesting knowledge @ text @ @ @ @ @ fully understand @ meaning @ @ @ natural_language sentences precisely',\n",
       " '',\n",
       " '@ @ @ @ specific_examples @ @ @ cant @ today @ @ @ speech_tagging @ @ easy @ @ @ @ percent correctly',\n",
       " '@ @ @ example @ turned @ @ highway versus @ turned @ @ fan @ @ @ offs actually @ somewhat different syntactic categories',\n",
       " '@ @ @ @ difficult @ @ complete parsing correct',\n",
       " '@ @ example @ man saw @ boy @ @ telescope @ actually @ @ difficult @ parse depending @ @ context',\n",
       " 'precise deep semantic_analysis @ @ @ hard',\n",
       " '@ example @ define @ meaning @ @ precisely @ @ difficult @ @ sentence like john owns @ restaurant',\n",
       " '@ @ state @ @ art @ @ summarized @ follows',\n",
       " 'robust @ general nlp tends @ @ shallow @ deep understanding @ @ scale @',\n",
       " '@ @ reason @ @ course @ techniques @ @ cover @ @ general shallow techniques @ analyzing text data @ @ text data',\n",
       " '@ @ @ generally based @ statistical analysis @ @ @ robust @ general @ @ @ @ @ @ category @ shallow analysis @ @ techniques @ @ advantage @ @ able @ @ applied @ @ text data @ @ natural_language @ @ topic',\n",
       " '@ @ downside @ @ @ dont @ @ @ deeper understanding @ text',\n",
       " '@ @ @ @ @ rely @ deeper natural_language analysis techniques',\n",
       " '@ typically @ require human_effort @ annotate @ lot @ examples @ analysis @ wed like @ @ @ @ computers @ use machine_learning techniques @ learn @ @ training_examples @ @ @ task',\n",
       " '@ @ practical applications @ generally combine @ @ kinds @ techniques @ @ general statistical @ methods @ backbone @ @ basis @ @ @ @ applied @ @ text data @ @ @ @ @ @ going @ use humans @ annotate @ data @ @ use supervised_machine learning @ @ @ tasks @ @ @ @ @ especially @ @ important tasks',\n",
       " '@ @ bring humans @ @ @ loop @ analyze fix @ analyze text data @ precisely',\n",
       " '@ @ course @ cover @ general statistical approaches @ generally dont require @ human_effort',\n",
       " '@ @ @ practically @ useful @ @ @ @ deeper analysis techniques @ require @ lot @ human_effort @ annotate text data',\n",
       " '@ @ summarize @ lecture @ main points @ @ away @ @ nlp @ @ foundation @ text mining',\n",
       " '@ obviously @ better @ @ understand @ text data @ better @ @ @ text mining',\n",
       " 'computers today @ far @ @ able @ understand @ natural_language',\n",
       " 'deeper nlp requires common sense knowledge @ inferences @ @ working @ @ limited domains',\n",
       " '@ feasible @ large scale text mining',\n",
       " 'shallow nlp based @ statistical methods @ @ @ @ large scale',\n",
       " '@ @ @ main topic @ @ course @ @ @ generally applicable @ @ lot @ applications',\n",
       " '@ @ @ @ sense @ @ useful techniques',\n",
       " '@ practice @ use statistical nlp @ @ basis',\n",
       " '@ @ @ humans @ help @ needed @ @ ways',\n",
       " '',\n",
       " '@ lecture @ @ @ text representation',\n",
       " '@ @ lecture @ going @ discuss text representation',\n",
       " '@ discuss @ natural_language processing @ allow @ @ represent text @ @ different_ways',\n",
       " 'lets @ @ look @ @ example sentence @',\n",
       " '@ @ represent @ sentence @ @ different_ways',\n",
       " '1st',\n",
       " '@ @ @ represent @ @ sentence @ @ string @ characters',\n",
       " '@ @ true @ @ @ languages @ @ store @ @ @ computer',\n",
       " '@ @ store @ natural_language sentence @ @ string @ characters @ @ @ @ @ general way @ representing text @ @ @ @ use @ approach @ represent @ text data',\n",
       " '@ unfortunately @ @ @ representation @ @ help @ @ semantic_analysis @ @ @ needed @ @ applications @ text mining',\n",
       " '@ reason @ @ @ @ @ recognizing words',\n",
       " '@ @ @ string @ going @ @ @ @ spaces @ @ ascii symbols',\n",
       " '@ @ @ count @',\n",
       " '',\n",
       " '',\n",
       " 'whats @ @ frequent character @ english text @ @ correlation @ @ characters @ @ cant @ analyze semantics',\n",
       " '@ @ @ @ @ general way @ representing text @ @ @ use @ @ represent @ natural_language text',\n",
       " '@ @ try @ @ @ little_bit @ natural_language processing @ @ word segmentation',\n",
       " '@ @ @ obtain @ representation @ @ @ text @ @ @ form @ @ sequence @ words',\n",
       " '@ @ @ @ @ @ @ identify words like @ dog @ chasing etc',\n",
       " '@ @ @ level @ representation @ certainly @ @ @ lot @ things @ @ @ mainly @ words @ @ basic units @ human communication @ natural_language @ @ @ @ powerful',\n",
       " '@ identifying words @ @ @ example easily count @ @ @ @ frequent words @ @ document @ @ @ @ collection etc',\n",
       " '@ @ words @ @ @ @ form topics',\n",
       " '@ @ combine related words @ @ @ words @ positive @ words @ negative @ @ @ @ @ sentiment_analysis',\n",
       " '@ representing text data @ @ sequence @ words opens @ @ lot @ interesting analysis possibilities',\n",
       " '@ @ level @ representation @ slightly @ general @ string @ characters @ @ @ languages @ @ chinese @ actually @ @ easy @ identify @ @ word boundaries @ @ @ @ language @ @ text @ @ sequence @ characters @ @ space @ @',\n",
       " '@ @ @ @ rely @ @ special techniques @ identify words',\n",
       " '@ @ @ language @ course @ @ @ @ mistakes @ segmenting words',\n",
       " '@ @ sequence @ words representation @ @ @ robust @ string @ characters',\n",
       " '@ @ english @ @ easy @ obtain @ level @ representation @ @ @ @ @ @ @ time',\n",
       " '@ @ @ @ @ @ @ natural_language processing @ @ add @ @ @ speech_tags',\n",
       " '@ @ @ @ @ @ @ count @ example @ @ frequent nouns @ @ kind @ nouns @ associated @ @ kind @ verbs etc',\n",
       " '@ @ opens @ @ little_bit @ interesting opportunities @ @ analysis',\n",
       " 'note @ @ use @ plus sign @ @ @ representing text @ @ sequence @ @ @ speech_tags',\n",
       " '@ dont necessarily replace @ original word sequence representation instead @ add @ @ @ additional way @ representing text data @ @ @ @ data @ represented @ @ @ sequence @ words @ @ sequence @ @ @ speech_tags',\n",
       " '@ enriches @ representation @ text data @ @ @ enables @ @ interesting analysis',\n",
       " '@ @ @ @ @ @ @ parsing @ sentence @ obtain @ syntactic structure',\n",
       " '@ @ @ course @ open @ @ interesting analysis @ @ example @ writing styles @ correcting grammar mistakes',\n",
       " '@ @ @ @ @ @ semantic_analysis @ @ @ @ able @ recognize dog @ animal @ @ @ @ recognize boy @ @ person @ playground @ @ location',\n",
       " '@ @ @ @ analyze @ relations @ example dog @ chasing @ boy @ @ boy @ @ @ playground',\n",
       " '@ @ @ @ add @ entities @ relations @ entityrelation recognition',\n",
       " '@ @ level @ @ @ @ @ @ interesting things',\n",
       " '@ example @ @ @ count easily @ @ frequent person thats mentioned @ @ @ collection @ news_articles @ @ @ mention @ person @ @ tend @ @ mention @ @ person etc',\n",
       " '@ @ @ @ useful representation @ @ @ related @ @ knowledge graph @ @ @ @ @ @ heard @',\n",
       " '@ google @ @ @ @ @ semantic way @ representing text data',\n",
       " '@ @ @ @ robust @ sequence @ words @ @ syntactic analysis @ @ @ @ easy @ identify @ @ entities @ @ right types @ @ @ @ mistakes @ relations @ @ harder @ find @ @ @ @ mistakes',\n",
       " '@ @ makes @ level @ representation @ robust @ @ @ useful',\n",
       " '@ @ @ @ @ @ logical representation @ @ @ @ predicates @ @ inference rules',\n",
       " '@ @ inference rules @ @ infer interesting derived facts @ @ text',\n",
       " '@ thats @ useful @ unfortunately @ @ level @ representation @ @ @ robust @ @ @ @ mistakes @ @ cant @ @ @ @ time @ @ kinds @ sentences',\n",
       " '@ finally speech acts @ added @ @ level @ representation @ @ intent @ saying @ sentence',\n",
       " '@ @ @ case @ @ @ @ request',\n",
       " '@ knowing @ @ allow @ @ analyze @ @ @ interesting things @ @ observer order',\n",
       " 'author @ @ sentence whats @ intention @ saying @ @ scenarios @ kind @ actions @ @ @ @ @ @',\n",
       " '',\n",
       " '',\n",
       " '@ level @ analysis @ @ @ @ interesting',\n",
       " '@ @ picture shows @ @ @ @ @ @ generally @ @ sophisticated natural_language processing_techniques @ @ @',\n",
       " '@ unfortunately @ techniques @ require @ human_effort',\n",
       " '@ @ @ @ accurate',\n",
       " '@ means @ @ mistakes',\n",
       " '@ @ @ analyze text data @ @ levels @ @ represented deeper analysis @ language @ @ @ @ tolerate @ errors',\n",
       " '@ @ @ means @ @ necessary @ combine @ deep analysis @ shallow analysis based @ @ example sequence @ words',\n",
       " '@ @ right @ @ @ @ arrow points @ @ indicate @ @ @ @ @ @ @ representation @ text @ closer @ knowledge representation @ @ mind @ need @ solving @ lot @ problems',\n",
       " '@ @ @ desirable @ @ @ @ represent text @ @ level @ knowledge @ @ easily extract @ knowledge',\n",
       " 'thats @ purpose @ text mining',\n",
       " '@ @ @ @ trade @ @ @ @ deeper analysis @ @ @ errors @ @ @ @ direct knowledge @ @ @ extracted @ text @ @ shallow analysis @ @ @ robust',\n",
       " '@ wouldnt actually @ @ @ necessary deeper representation @ knowledge',\n",
       " '@ @ @ @ @ text data @ generated @ humans @ @ meant @ @ consumed @ humans @ @ @ result @ @ text data analysis text mining humans play @ @ important_role',\n",
       " '@ @ @ @ @ loop',\n",
       " 'meaning @ @ @ optimize @ collaboration @ humans @ computers',\n",
       " '@ @ @ sense @ ok @ computers @ @ @ able @ @ completely accurate representation @ text data @ patterns @ @ extracted @ text data @ @ interpreted @ humans @ humans @ guide @ computers @ @ @ accurate analysis @ annotating @ data @ providing features @ guide @ machine_learning programs @ @ @ work @ effectively',\n",
       " '',\n",
       " '@ @ @ explained different textual representation tends @ enable different analysis',\n",
       " '@ particular @ @ gradually add @ @ @ deeper analysis results @ represent text data @ @ @ open @ @ interesting representation opportunities @ @ analysis capacities',\n",
       " '@ @ table summarizes @ @ @ @ seen',\n",
       " '@ @ @ column shows @ text recognition @ second visualizes @ generality @ @ representation meaning @ @ @ @ @ kind @ representation accurate @ @ @ text data @ @ @ @ @ @ @ column shows @ enabled analysis techniques',\n",
       " '@ @ final column shows @ examples @ application @ @ @ achieved @ @ level @ representation',\n",
       " '@ lets @ @ look @ @',\n",
       " '@ @ @ string text @ @ @ processed @ @ stream processing algorithms @ @ @ robust @ general',\n",
       " '@ @ @ @ @ interesting applications @ @ @ @ @ @ level',\n",
       " '@ example compression @ text doesnt necessarily need @ know @ word boundaries',\n",
       " '@ knowing word boundaries @ actually @ help',\n",
       " 'word based representation @ @ important level @ representation',\n",
       " '@ @ general @ relatively robust',\n",
       " '@ @ enable @ lot @ analysis techniques @ @ word relation analysis topic analysis @ sentiment_analysis @ @ @ @ applications @ @ @ enabled @ @ kind @ analysis',\n",
       " '@ example thesaurus discovery @ @ @ @ discovering related words @ topic @ opinion related applications @ abundant @ @ @ @ example @ people @ @ interested @ knowing @ major_topics covered @ @ collection @ text',\n",
       " '@ @ @ @ @ case',\n",
       " '@ research literature @ scientist want @ know @ @ @ @ important research topics today @ customer service people @ want @ know @ @ @ major complaints @ @ customers @ @ mining @ email_messages',\n",
       " '@ business intelligence people @ @ interested @ understanding consumers opinions @ @ products @ competitors products @ figure @ @ @ @ @ winning features @ @ products',\n",
       " '@ @ general @ @ @ applications @ @ @ enabled @ @ representation @ @ level',\n",
       " '@ moving @ @ @ @ @ gradually add additional representations',\n",
       " '@ adding syntactic_structures @ @ enable @ course syntactic graph analysis',\n",
       " '@ @ use graph mining_algorithms @ analyze syntactic graphs',\n",
       " '@ @ applications @ related @ @ kind @ representation',\n",
       " '@ example stylistic analysis generally requires syntactical representation',\n",
       " 'syntactical structure representation',\n",
       " '@ @ @ generate @ structure based feature features @ @ @ features @ @ help @ classify text objects @ different categories',\n",
       " '@ looking @ @ structures @ @ classification @ @ @ accurate',\n",
       " '@ example @ @ want @ classify articles @ different categories corresponding @ different authors want @ figure @ @ @ @ k authors @ actually written @ article',\n",
       " '@ @ generally need @ look @ @ syntactic_structures',\n",
       " '@ @ add entities @ relations @ @ @ enable lot @ techniques @ @ knowledge graph analysis @ information network analysis @ general @ @ analysis @ enable applications @ entities @ example discovery @ @ @ knowledge @ opinions @ @ real_world energy entity',\n",
       " '@ @ @ use @ level representation @ integrate @ @ entity @ scattered sources',\n",
       " 'finally @ @ add logic predicates @ @ @ enable logic inference ofcourse @ @ @ @ @ useful @ integrative analysis @ scattered knowledge',\n",
       " '@ example @ @ @ add ontology @ @ @ @ extracted information @ text @ @ inferences',\n",
       " '@ good example @ application @ @ enabled @ @ level @ representation @ @ intelligent knowledge assistant @ biologists',\n",
       " '@ @ @ intended program @ @ help biologists manage @ @ relevant knowledge @ literature @ @ research problem @ @ understanding functions @ genes',\n",
       " '@ @ computer @ @ inferences @ @ @ @ hypothesis @ biologists @ @ interesting @ example @ @ gene @ @ certain function @ @ @ intelligent program @ read @ literature @ extract @ relevant facts',\n",
       " '@ @ @ information extraction @ @ @ @ logical system @ actually track thats @ answers @ researchers questioning @ @ genes @ related @ @ functions',\n",
       " '@ @ order @ support @ level @ application @ need @ @ @ far @ logical representation',\n",
       " '@ @ course @ covering techniques mainly based @ word based representation',\n",
       " '@ techniques @ general @ robust @ @ @ @ widely @ @ @ applications',\n",
       " '@ fact @ virtually @ @ text mining applications @ need @ level @ representation @ @ techniques @ support analysis @ texting @ level',\n",
       " '@ obviously @ @ @ levels @ @ combined @ @ @ combined @ order @ support sophisticated applications',\n",
       " '@ @ summarize @ @ @ major takeaway points',\n",
       " 'text representation determines @ kind @ mining_algorithms @ @ applied',\n",
       " '@ @ @ multiple ways @ represent text strings words syntactic_structures @ @ relation graphs logical predicates etc',\n",
       " '@ @ different representations @ @ general @ combined @ real applications @ @ extent @ @',\n",
       " '@ example @ @ @ @ @ @ accurately @ application @ syntactic_structures @ @ stick @ partial structures extracted @ @ @ @ recognize @ entities @ @ @ @ great',\n",
       " '@ @ general @ want @ @ @ @ @ @ @',\n",
       " '@ @ different levels @ combined @ @ @ enable richer analysis',\n",
       " '@ powerful analysis',\n",
       " '@ course @ focuses @ word based representation',\n",
       " '@ techniques @ @ @ advantages',\n",
       " '@ @ @ general @ robust @ @ @ applicable @ @ natural_language',\n",
       " 'thats @ big advantage @ @ approaches @ rely @ @ fragile natural_language processing_techniques',\n",
       " 'secondly @ @ @ require @ manual effort @ @ @ @ @ require @ manual effort',\n",
       " '@ thats @ important benefit @ @ means @ @ apply directly @ @ application',\n",
       " '@ @ techniques @ actually surprisingly powerful @ effective @ @ applications',\n",
       " '@ @ @ @ course @ @ @ explained',\n",
       " '@ @ @ @ effective partly @ @ words @ invented @ humans @ basic units @ communications',\n",
       " '@ @ @ actually @ sufficient @ representing @ kinds @ semantics',\n",
       " '@ @ makes @ kind @ word based representation @ powerful',\n",
       " '@ finally @ @ word based representation @ @ techniques enabled @ @ @ representation @ @ combined @ @ @ sophisticated approaches',\n",
       " '@ theyre @ competing @ @ @',\n",
       " '',\n",
       " '@ lecture @ @ @ word_association mining @ analysis',\n",
       " '@ @ lecture @ going @ talk @ @ @ @ associations @ words @ text',\n",
       " '@ @ @ example @ knowledge @ natural_language @ @ @ @ @ text data',\n",
       " 'heres @ outline',\n",
       " '@ @ gooing @ @ talk @ @ @ word_association @ @ explain @ discovering @ relations @ useful @ finally @ @ going @ talk @ @ general_ideas @ @ @ @ word_associations',\n",
       " '@ general @ @ @ word relations @ @ @ @ basic',\n",
       " '@ @ called @ paradigmatic_relation @ @ @ syntagmatic_relations',\n",
       " 'aampb @ paradigmatic_relation @ @ @ @ substituted @ @ @',\n",
       " '@ means @ @ words @ @ paradigmatic_relation @ @ @ @ @ semantic class @ syntactic class @ @ @ @ general replace @ @ @ @ @ affecting @ understanding @ @ sentence',\n",
       " '@ means @ @ @ @ @ valid sentence',\n",
       " '@ example cat @ dog',\n",
       " '@ @ @ words @ paradigmatic_relation @ @ @ @ @ @ class @ animal',\n",
       " '@ @ general @ @ replace cat @ dog @ @ sentence @ sentence @ @ @ @ valid sentence @ @ @ @ sense @',\n",
       " 'similarly monday @ tuesday @ paradigmatic_relation',\n",
       " '@ second kind @ relation @ called syntagmatic_relation',\n",
       " '@ @ case @ @ words @ @ @ relation @ @ combined @ @ @',\n",
       " '@ aampb @ syntagmatic_relation @ @ @ @ combined @ @ @ @ @ sentence',\n",
       " '@ means @ @ words @ semantically related',\n",
       " '@ @ example cat @ sit @ related @ @ cat @ sit @',\n",
       " 'similarly car @ drive @ related semantically @ @ @ @ combined @ @ @ @ convey meaning',\n",
       " '@ @ general @ @ replace cat @ sit @ @ sentence @ car @ drive @ @ sentence @ @ @ @ valid sentence',\n",
       " 'meaning @ @ @ @ @ @ sentence @ @ somewhat meaningless',\n",
       " '@ @ @ different @ paradigmatic_relation @ @ @ relations @ @ fact @ fundamental @ @ @ @ generalized @ capture basic relations @ units @ arbitrary sequences',\n",
       " '@ definitely @ @ @ generalized @ describe relations @ @ items @ @ language',\n",
       " '@ aampb dont @ @ @ words @ @ @ @ phrases example',\n",
       " '@ @ @ @ @ @ complex phrases @ @ @ noun phrase',\n",
       " '@ @ think @ @ general problem @ @ sequence mining @ @ @ think @ @ units @ @ sequence data @ @ @ think @ paradigmatic_relation @ relations @ @ applied @ units @ tend @ occur @ similar locations @ @ sentence @ @ @ sequence @ data elements @ general',\n",
       " '@ @ occur @ similar locations relative @ @ neighbors @ @ sequence',\n",
       " 'syntagmatic_relation @ @ @ hand @ related @ cooccurring elements @ tend @ @ @ @ @ @ sequence',\n",
       " '@ @ @ @ complementary @ basically relations @ words @ @ interested @ discovering @ automatically @ text data',\n",
       " 'discovering @ world relations @ @ applications',\n",
       " '@ @ relations @ @ directly useful @ improving accuracy @ @ nlp tasks @ @ @ @ @ @ @ @ @ knowledge @ @ language',\n",
       " '@ @ @ know @ @ words @ synonyms @ example @ @ @ @ help @ lot @ tasks',\n",
       " '@ grammar learning @ @ @ @ @ @ @ techniques @ @ @ @ learn paradigmatic_relations @ @ form classes @ words',\n",
       " 'syntactic classes @ example',\n",
       " '@ @ @ learn syntagmatic_relations @ @ @ @ able @ know @ rules @ putting @ @ larger expression based @ component expressions',\n",
       " '@ @ learn @ structure @ @ @ @ @ @ @',\n",
       " 'word relations @ @ @ @ useful @ @ applications @ text retrieval @ mining',\n",
       " '@ example @ search @ text retrieval @ @ use word_associations @ modify @ query',\n",
       " '@ @ @ @ @ @ introduce additional related words @ @ query @ @ @ query @ effective',\n",
       " '@ @ called query expansion',\n",
       " '@ @ @ use related words @ suggest related queries @ @ user @ explore @ information space',\n",
       " '@ application @ @ use word_associations @ automatically construct @ topic map @ browsing @ @ @ @ words @ nodes @ associations @ edge',\n",
       " '@ user @ navigate @ @ word @ @ @ find information @ @ information space',\n",
       " 'finally @ word_associations @ @ @ @ @ compare @ summarize opinions',\n",
       " '@ example @ @ @ interested @ understanding positive @ negative opinions @ iphone_6',\n",
       " '@ order @ @ @ @ @ look @ @ words @ @ strongly associated @ @ feature word like @ battery @ positive versus negative reviews',\n",
       " '@ @ syntagmatic_relations @ help @ @ @ detailed opinions @ @ product',\n",
       " '@ @ @ @ discover @ associations automatically @ @ @ @ intuitions @ @ @ @ @',\n",
       " 'lets @ look @ @ paradigmatic_relation',\n",
       " '@ @ essentially @ @ advantage @ similar context',\n",
       " '@ @ @ @ @ simple sentences @ cat @ dog',\n",
       " '@ @ @ @ generally occur @ similar context',\n",
       " '@ @ @ @ @ @ definition @ paradigmatic_relation',\n",
       " '@ @ @ right @ @ @ @ @ extracted explicitly @ context @ cat @ dog @ @ small sample @ text data',\n",
       " '@ @ @ taken away cat @ dog @ @ corresponding sentences @ @ @ @ @ @ @ context',\n",
       " '@ @ course @ @ @ different perspectives @ look @ @ context',\n",
       " '@ example @ @ look @ @ @ words occur @ @ left @ @ @ context',\n",
       " '@ @ @ @ @ left context',\n",
       " '@ words occur @ @ @ cat cat @ dog',\n",
       " '@ @ @ @ @ @ case clearly dog @ cat @ similar left context',\n",
       " '@ generally @ @ cat @ @ cat @ @ @ @ @ dog @ @ dog',\n",
       " '@ @ makes @ similar @ @ left context',\n",
       " 'similarly @ @ look @ @ words @ occur @ cat @ dog @ @ @ @ right context @ @ @ @ similar @ @ case @ course @ extreme case @ @ @ @ eats @ @ general @ @ @ @ @ words',\n",
       " '@ course @ @ follow cat @ dog',\n",
       " '@ @ @ @ look @ @ general context',\n",
       " '@ @ @ improve @ @ words @ @ sentence @ @ sentences @ @ word',\n",
       " '@ @ @ @ general context @ @ @ @ similarity @ @ @ words',\n",
       " '@ @ @ @ suggesting @ @ @ discover paradigmatic_relation @ looking @ @ similarity @ context @ words',\n",
       " '@ @ example @ @ think @ @ following questions @ similar @ context @ cat @ context @ dog @ contrast @ similar @ context @ cat @ context @ computer @ intuitively @ imagine @ context @ cat @ context @ dog @ @ @ similar @ @ context @ cat @ context @ computer @ means @ @ @ @ @ case @ similarity value @ @ high',\n",
       " '@ @ context @ cat @ dog @ @ @ second @ similarity @ contexts @ cat @ computer @ @ low @ @ @ @ having paradigmatic relationship',\n",
       " '@ @ imagine @ words occur @ computer',\n",
       " '@ general @ @ @ @ different @ @ words occur @ cat',\n",
       " '@ @ @ @ basic_idea @ discovering_paradigmatic relation',\n",
       " '@ @ @ syntagmatic_relation @ @ @ @ going @ explore @ correlated occurrences @ based @ @ definition @ syntagmatic_relation',\n",
       " '@ @ @ @ @ sample @ text',\n",
       " '@ @ @ @ interested @ knowing @ @ words @ correlated @ @ verb eats',\n",
       " '@ @ words @ @ @ eat @ @ @ look @ @ right @ @ @ slide @ @ @ @ ive taken away @ @ words @ eats',\n",
       " 'ive taken away @ word @ @ left @ @ @ world @ @ right @ @ sentence',\n",
       " '@ @ @ @ ask @ question @ words tend @ occur @ @ left @ eat @ @ words tend @ occur @ @ right @ eat @ thinking @ @ question @ help @ discover_syntagmatic relations',\n",
       " '@ syntagmatic_relation essentially captures @ correlations',\n",
       " '@ @ important question @ ask @ syntagmatic_relation @ @ eats occurs @ @ words @ tend @ occur @ @ question @ @ @ @ @ @ @ @ @ @ words @ tend @ cooccur @ @ eats meaning @ @ @ @ eat @ tend @ @ @ @ words',\n",
       " '@ @ @ dont @ eat probably @ dont @ @ words @ @',\n",
       " '@ @ intuition @ help @ discover_syntagmatic relations',\n",
       " '@ @ consider example @ helpful @ @ occurrence @ eats @ predicting occurrence @ meat knowing @ eats occurs @ @ sentence @ generally help @ predict @ @ meat @ occurs @ @ @ @ @ @ eats occur @ @ sentence @ @ @ increase @ chance @ meat @ @ occur',\n",
       " '@ contrast @ @ look @ @ question @ @ @ @ helpful @ occurrence @ eats @ predicting @ occurrence @ text @ eats @ text @ @ @ related @ knowing @ eats occurred @ @ sentence doesnt @ help @ predict @ text @ occurs @ @ sentence',\n",
       " '@ @ @ @ contrast @ @ question @ eats @ meat',\n",
       " '@ @ helps explain @ intuition @ @ methods @ discovering_syntagmatic relation',\n",
       " 'mainly @ need @ capture @ correlation @ @ occurrences @ @ words',\n",
       " '@ @ summarize @ general_ideas @ discovering word_associations @ @ following',\n",
       " '@ paradigmatically relation @ represent @ word @ @ context @ @ compute @ context similarity',\n",
       " '@ @ gonna assume @ words @ @ high context similarity @ @ paradigmatic_relation',\n",
       " '@ syntagmatic_relation @ @ count @ @ times @ words occur @ @ @ context @ @ @ @ sentence paragraph @ @ document @',\n",
       " '@ @ going @ compare @ co_occurrences @ @ individual occurrences',\n",
       " '@ going @ assume words @ high cooccurrences @ relatively low individual occurrences @ @ syntagmatic_relations @ @ tend @ occur @ @ @ dont usually occur @',\n",
       " 'note @ @ paradigmatic_relation @ syntagmatic_relation @ actually closely related',\n",
       " '@ @ paradigmatically_related words tend @ @ syntagmatic_relation @ @ @ word @ @ tend @ @ associated @ @ @ word @ @ suggests @ @ @ @ @ join @ discovery @ @ @ relations',\n",
       " '@ @ general_ideas @ @ implemented @ @ different_ways @ @ course wont cover @ @ @ @ @ @ cover @ @ @ @ @ methods @ effective @ discovering @ relations',\n",
       " '',\n",
       " '@ lecture @ @ @ paradigmatic_relation discovery',\n",
       " '@ @ lecture @ going @ talk @ @ @ discover @ particular kind @ word_association called paradigmatic_relations',\n",
       " '@ definition 2 words @ paradigmatically_related @ @ share similar contexts',\n",
       " '@ @ occur @ similar positions @ text',\n",
       " '@ naturally @ idea @ discovering @ relation @ @ look @ @ context @ @ word @ @ try @ compute @ similarity @ @ contexts',\n",
       " '@ heres @ example @ context @ word cat',\n",
       " '@ @ @ taken @ word cat @ @ @ context',\n",
       " '@ @ @ @ @ @ seeing @ remaining words @ @ sentences @ contain cat',\n",
       " '@ @ @ @ @ @ thing @ @ word like @ dog',\n",
       " '@ @ general @ @ like @ capture @ @ context @ @ try @ assess @ similarity @ @ context @ cat @ @ context @ @ word like dog',\n",
       " '@ @ @ question @ @ @ @ formally represent @ context @ @ define @ similarity_function @ @ @ note @ @ context actually contains @ lot @ words',\n",
       " '@ @ @ @ regarded @ @ pseudo document',\n",
       " '@ imaginary document',\n",
       " '@ @ @ @ different_ways @ looking @ @ context',\n",
       " '@ example @ @ look @ @ word @ occurs @ @ word cat',\n",
       " '@ @ @',\n",
       " '@ @ @ @ context left1 context',\n",
       " '@ @ @ case @ @ @ words like @ @ @ big @ @ etc',\n",
       " '@ @ @ words @ @ occur @ @ left @ @ world cat',\n",
       " '@ @ @ @ cat @ cat big cat',\n",
       " '@ cat etc',\n",
       " 'similarly @ @ @ collect @ words @ occur right @ @ word cat',\n",
       " '@ @ @ @ context right1',\n",
       " '@ @ @ @ words eats ate @ @ etc',\n",
       " '@ @ generally @ @ look @ @ @ @ words @ @ window @ text @ @ word cat',\n",
       " '@ lets @ @ @ @ @ window @ @ words @ @ world cat',\n",
       " '@ @ @ context window8 @ @ course @ @ @ @ @ words @ left @ @ right @ @ @ @ @ bag @ words @ general @ represent @ context',\n",
       " '@ @ @ word based representation @ actually @ @ interesting way @ define @ perspective @ measuring @ similarity',\n",
       " '@ @ @ look @ @ @ similarity @ left1 @ @ @ words @ share @ @ words @ @ left context @ @ kind @ ignore @ @ words @ @ @ @ @ general context',\n",
       " '@ @ gives @ @ perspective @ measure @ similarity',\n",
       " '@ similarly @ @ @ use @ right1 context @ capture @ similarity @ @ perspective',\n",
       " '@ @ left1 @ right1 ofcourse @ allow @ @ capture @ similarity @ @ @ strict criteria',\n",
       " '@ @ general context @ contain adjacent words like eats @ @ @ @ @ @ @ nonadjacent words like saturday tuesday @ @ @ words @ @ context',\n",
       " '@ @ flexibility @ allows @ @ measure @ similarity similarity @ @ @ different_ways',\n",
       " '@ @ @ useful @ @ @ want @ capture similarity based @ general content @ @ @ @ loosely related paradigmatic_relations @ @ @ use @ @ words immediately @ @ left @ @ @ right @ @ world @ @ likely @ capture words @ @ @ @ related @ @ syntactical categories @ semantics',\n",
       " '@ @ general idea @ discovering_paradigmatic relations @ @ compute @ similarity @ context @ @ words',\n",
       " '@ @ @ example @ @ measure @ similarity @ cat @ dog based @ @ similarity @ @ contexts',\n",
       " '@ general @ @ combine @ kinds @ views @ @ context @ @ @ similarity_function @ @ general combination @ similarities @ different contexts',\n",
       " '@ @ course @ @ @ assign weights @ @ different similarities @ allow @ @ focus @ @ particular kind @ context @ @ @ @ naturally application specific @ @ @ @ main_idea @ discovering paradigmatically_related words @ @ compute @ similarity @ @ context',\n",
       " '@ @ lets @ @ @ exactly compute @ similarity functions',\n",
       " '@ @ answer @ question @ useful @ think @ bag @ words representation @ vectors @ @ vector_space model',\n",
       " '@ @ @ @ @ @ @ familiar @ information_retrieval @ text retrieval techniques @ realize @ vector_space model @ @ @ frequently @ modeling documents @ queries @ search',\n",
       " '@ @ @ @ find @ convenient @ model @ context @ @ word @ paradigmatically relation_discovery',\n",
       " '@ @ idea @ @ approach @ @ view @ word @ @ vocabulary @ defining @ dimension @ high_dimensional space @ @ @ n words @ total @ @ vocabulary',\n",
       " '@ @ @ n dimensions @ illustrated @',\n",
       " '@ @ @ @ @ @ @ frequency vector representing @ context',\n",
       " '@ @ @ @ @ eats occured @ times @ @ context ate occurred @ times etc',\n",
       " '@ @ vector @ @ @ placed @ @ vector_space model',\n",
       " '@ @ general @ @ represent @ pseudo document @ context @ cat @ @ vector',\n",
       " 'd1',\n",
       " '@ @ word dog @ @ @ @ different context @ d2',\n",
       " '@ @ @ @ measure @ similarity @ @ @ vectors',\n",
       " '@ @ viewing context @ @ vector_space model @ convert @ problem @ paradigmatic_relations discovery @ @ problem @ computing @ vectors @ @ similarity',\n",
       " '@ @ @ questions @ @ @ @ address @ @ @ @ compute @ vector @ @ @ @ compute @ xi @ yi @ @ @ question @ @ @ @ compute @ similarity @ @ general @ @ @ approaches @ @ @ @ @ solve @ problem @ @ @ @ @ developed @ information_retrieval',\n",
       " '@ @ @ @ shown @ work @ @ matching @ query vector @ @ document vector @ @ @ adapt @ @ @ @ ideas @ compute @ similarity @ context documents @ @ purpose @',\n",
       " '@ lets @ look @ @ @ possible approach @ @ try @ measure @ similarity @ context based @ @ expected_overlap @ words @ @ @ @ eowc',\n",
       " '@ @ idea @ @ represent @ context @ award vector @ @ word @ @ weight @ @ equal @ @ probability @ @ randomly picked word @ @ document vector @ @ word',\n",
       " '@ @ @ words',\n",
       " 'xi @ defined @ @ normalized count @ word wi @ @ context',\n",
       " '@ @ @ @ interpreted @ @ probability @ @ @ actually pick @ word @ d1 @ @ randomly pick @ word',\n",
       " '@ @ course @ xis @ sum @ 1 @ @ @ normalized frequencies',\n",
       " '@ @ means @ vector @ actually probability distribution @ words',\n",
       " '@ @ vector d2 @ @ @ computed @ @ @ way',\n",
       " '@ @ @ @ @ @ @ probability distributions representing @ contexts',\n",
       " '@ @ addresses @ problem @ @ compute @ vectors @ lets @ @ @ @ define similarity @ @ approach',\n",
       " '@ @ @ simply define @ similarity @ @ dot_product @ @ vectors @ @ @ defined @ @ sum @ @ products @ @ @ corresponding elements @ @ @ vectors',\n",
       " '@ @ interesting @ @ @ @ similarity_function actually @ @ nice interpretation',\n",
       " '@ @ @ @ dot_product infact gives @ @ probability @ @ randomly picked words @ @ @ contexts @ identical @ means @ @ try @ pick @ word @ @ context @ try @ pick @ word @ @ context @ @ @ ask @ question @ @ identical @ @ @ contexts @ @ similar @ @ @ expect @ @ frequently @ @ @ @ words picked @ @ @ contexts @ identical',\n",
       " '@ @ @ @ different @ @ chance @ seeing identical words @ picked @ @ @ contexts @ @ small',\n",
       " '@ @ intuitively_makes sense @ measuring similarity @ contexts',\n",
       " '@ @ @ want @ @ @ @ look @ @ exact formulas @ @ @ @ @ @ interpreted @ @ probability @ @ randomly picked words @ identical',\n",
       " '@ @ @ @ stay @ @ formula @ check whats inside @ sum @ @ @ @ basically @ @ case @ gives @ @ probability @ @ @ overlap @ @ particular word wi @ @ xi gives @ @ probability @ @ pick @ particular word @ d1 @ yi gives @ @ probability @ picking @ word @ d2 @ @ @ pick @ @ word @ @ @ contexts @ @ @ identical pick',\n",
       " 'alright @ thats @ possible approach',\n",
       " 'eowc expected_overlap @ words @ context',\n",
       " '@ @ @ @ @ like @ assess @ @ approach @ @ work @',\n",
       " '@ @ course ultimately @ @ @ test @ approach @ real data @ @ @ @ gives @ @ semantically related words @ @ @ @ paradigmatic_relations',\n",
       " '@ analytically @ @ @ analyze @ formula little_bit',\n",
       " '@ @ @ @ said @ @ @ sense right @ @ formula @ @ @ higher score @ @ @ @ overlap @ @ @ contexts',\n",
       " '@ thats exactly @ @ want',\n",
       " '@ @ @ analyze @ formula @ carefully @ @ @ @ @ @ @ @ potential problems',\n",
       " '@ specifically @ @ @ potential problems',\n",
       " '@ @ @ favor matching @ frequent_term @ @ @ matching @ distinct terms @ @ @ @ @ @ dot_product @ @ element @ @ high value @ @ element @ shared @ @ context @ @ contributes @ lot @ @ overall sum',\n",
       " '@ @ @ @ @ @ score higher @ @ @ case @ @ @ vectors actually @ @ lot @ overlap @ different terms @ @ term @ @ relatively low frequency',\n",
       " '@ @ @ @ @ desirable',\n",
       " '@ course @ @ @ desirable @ @ @ cases @ @ @ case @ @ intuitively prefer @ case @ @ match @ different terms @ @ context @ @ @ @ @ confidence @ saying @ @ @ words @ occur @ similar context',\n",
       " '@ @ @ rely @ @ term @ thats @ little_bit questionable',\n",
       " '@ @ @ @ robust',\n",
       " '@ second problem @ @ @ treats @ word equally @ @ @ match @ word like @ @ match @ @ @ @ @ @ @ matching @ @ word like eats',\n",
       " '@ intuitively @ know matching @ isnt @ surprising @ @ occurs @ @ matching @ @ @ @ @ @ strong evidence @ matching @ word like eats @ doesnt_occur frequently',\n",
       " '@ @ @ @ problem @ @ approach',\n",
       " '@ @ @ lecture @ going @ talk @ @ @ address @ problems',\n",
       " '',\n",
       " '@ @ lecture @ continue_discussing paradigmatic_relation discovery',\n",
       " 'earlier @ introduced @ method called expected_overlap @ words @ context',\n",
       " '@ @ method @ represent @ context @ @ word vector @ represents @ probability @ word @ @ context @ @ measure @ similarity @ @ @ dot_product',\n",
       " '@ @ @ interpreted @ @ probability @ @ randomly pick @ words @ @ @ contexts @ identical @ @ discuss @ @ problems @ @ method',\n",
       " '@ @ @ @ @ favors matching @ frequent_term @ @ @ matching @ distinct terms',\n",
       " '@ @ @ @ emphasis @ matching @ term @ @',\n",
       " '@ second @ @ @ treats @ word equally',\n",
       " '@ @ common word like @ @ contribute equally @ content word like eats',\n",
       " '@ @ @ @ going @ talk @ @ @ solve @ problems',\n",
       " '@ specifically @ going @ introduce @ retrieval heuristics @ @ text retrieval @ @ heuristics @ effectively solve @ problems @ @ problems @ occur @ text retrieval @ @ match @ query vector @ document vector',\n",
       " '@ @ address @ @ problem @ @ use @ sub linear_transformation @ term frequency',\n",
       " '@ @ @ dont @ @ use @ raw frequency count @ term @ represent @ context',\n",
       " '@ @ transform @ @ @ form @ wouldnt emphasize @ @ @ @ raw frequency',\n",
       " '@ address @ second problem @ @ @ @ weight @ rare terms',\n",
       " '@ @ @ @ reward matching @ rare word @ @ heuristic @ called idf term weighting @ text retrieval',\n",
       " 'idf stands @ inverse_document frequency',\n",
       " '@ @ @ going @ talk @ @ @ heuristics @ @ detail',\n",
       " '@ lets talk @ @ tf transformation',\n",
       " '@ @ @ convert @ raw_count @ word @ @ document @ @ weight @ reflects @ belief @ @ important @ word @ @ document',\n",
       " '@ @ @ @ @ denoted @ tf @ wampd @ shown @ @ y axis',\n",
       " '@ @ general @ @ @ ways @ map @ @ lets @ look @ @ simple way @ mapping',\n",
       " '@ @ case @ going @ @ @ non_zero counts @ @ mapped @ @',\n",
       " '@ @ zero count @ @ mapped @ 0',\n",
       " '@ @ @ mapping @ @ frequencies @ @ mapped @ @ @ values zero @ @ @ @ mapping function @ shown @ @ @ flat line @',\n",
       " '@ @ @ naive @ @ ignored @ frequency @ words',\n",
       " '@ @ actually @ @ advantage @ emphasizing matching @ @ words @ @ context @ @ @ @ allow @ frequent word @ dominate @ matching',\n",
       " '@ @ approach @ @ @ taken earlier @ @ expected_overlap account approach @ @ linear_transformation',\n",
       " '@ basically @ y @ @ @ @ x',\n",
       " '@ @ use @ raw_count @ representation',\n",
       " '@ @ created @ problem @ @ @ talked @',\n",
       " '@ @ answers @ @ @ @ matching @ frequent_term',\n",
       " 'matching @ frequent_term @ contribute @ lot',\n",
       " '@ @ @ @ @ lot @ @ interesting transformations @ @ @ @ extremes',\n",
       " '@ @ generally form @ sub linear_transformation',\n",
       " '@ @ example @ possibility @ @ @ logarithm @ @ raw_count @ @ @ @ @ curve @ looks_like @ right @ youre_seeing @',\n",
       " '@ @ case @ @ @ @ high_frequency counts @ high counts @ penalized @ little_bit right @ @ curve @ @ sub linear curve @ @ brings @ @ weight @ @ @ @ high counts',\n",
       " '@ @ @ @ @ want @ @ prevents @ kind @ terms @ dominating @ scoring_function',\n",
       " '@ @ @ @ @ interesting transformation called @ bm25 transformation @ @ @ shown @ @ @ effective @ retrieval @ @ @ transformation @ @ @ form @',\n",
       " 'looks_like @',\n",
       " '@ saw @ k_1 x x k @ k @ @ parameter',\n",
       " 'x @ @ count @ raw_count @ word',\n",
       " '@ @ transformation @ @ interesting @ @ @ @ actually kind @ @ @ @ extreme @ @ @ extreme @ varying k',\n",
       " '@ @ @ @ interesting @ @ @ upper bound k_1 @ @ case',\n",
       " '@ @ puts @ @ strict constraint @ high_frequency terms @ @ weight @ @ exceed k1',\n",
       " '@ @ vary k @ @ @ simulate @ @ extremes',\n",
       " '@ @ case @ set @ zero',\n",
       " '@ roughly @ @ 01 vector',\n",
       " '@ @ @ set @ key @ @ @ large value @ @ behave @ like @ linear_transformation',\n",
       " '@ @ transformation function @ @ far @ @ effective transformation function @ text retrieval @ @ @ makes_sense @ @ problem set @',\n",
       " '@ @ @ talk @ @ @ solve @ problem @ @ emphasizing @ frequently frequent_term',\n",
       " '@ lets_look @ @ second problem @ @ @ @ @ @ penalize popular terms',\n",
       " 'matching @ @ @ surprising @ @ occurs @ @ matching eats @ account @ lot',\n",
       " '@ @ @ @ address @ problem @ @ case @ @ use @ idf_weighting thats commonly @ @ retrieval',\n",
       " 'idf stands @ inverse_document frequency',\n",
       " 'document frequency means @ count @ @ total_number @ documents @ contain @ particular word',\n",
       " '@ @ @ @ @ @ idf measure @ defined @ @ logarithm function @ @ number @ documents @ match @ term @ document frequency',\n",
       " '@ k @ @ number @ documents containing word @ document frequency @ m @ @ @ total_number @ documents @ @ collection',\n",
       " '@ idf function @ giving @ higher value @ @ lower k meaning @ @ rewards @ rare term',\n",
       " '@ @ maximum value @ log @ m 1',\n",
       " 'thats @ @ word occurs @ @ @ @ context',\n",
       " '@ thats @ @ rare term @ rarest term @ @ @ collection',\n",
       " '@ lowest value @ @ @ @ @ @ k reaches @ maximum @ @ @ m',\n",
       " '@ @ @ @ @ low value close @ 0 @ fact',\n",
       " 'right @ @',\n",
       " '@ @ course measure @ @ @ search @ @ naturally @ @ collection',\n",
       " '@ @ case @ @ @ @ collection @ @ @ use @ context @ @ @ collect @ @ @ words @ @ collection @ @ @ @ @ @ word thats popular @ @ collection @ general @ @ @ @ low idf',\n",
       " '@ depending @ @ data set @ @ construct @ context vectors @ different_ways @ @ @ end @ @ term @ @ frequently @ @ original data set @ @ @ @ @ frequently @ @ collected context documents',\n",
       " '@ @ @ @ add @ heuristics @ improve @',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '@ similarity_function',\n",
       " 'heres @ way @ @ @ @ @ ways @ @ possible',\n",
       " '@ @ @ @ reasonable way @ @ @ adapt @ bm25 retrieval model @ paradigmatic_relation mining',\n",
       " '@ @ @ define @ @ case @ define @ document vector',\n",
       " '@ containing elements representing normalized bm_25 values',\n",
       " '@ @ @ normalization function @ @ @ @ sum @ @ @ @ @ words @ @ @ normalize @ weight @ @ word @ @ sum @ @ weights @ @ @ words',\n",
       " '@ @ @ @ ensure @ @ xi @ sum @ @ @ @ vector',\n",
       " '@ @ @ @ @ similar @ @ @ @ @ @ @ @ vector @ actually @ similar @ word distribution @ @ exercise @ sum @ @',\n",
       " '@ @ weight @ bm25 @ @ word @ defined @',\n",
       " '@ @ @ compare @ @ @ old definition @ @ @ @ @ normalized count',\n",
       " '@ @ @ right @ @ @ @ @ @ @ @ document length @ @ total count @ words @ @ context document',\n",
       " '@ thats @ @ @ @',\n",
       " '@ @ @ @ bm_25 transformation @ introduced @ @',\n",
       " '@ @ course @ extra occurrence @ @ count @ @ @ achieve @ sub linear normalization',\n",
       " '@ @ @ @ @ introduce @ parameter k @',\n",
       " '@ @ parameter @ generally non negetive number @ zero @ @ possible',\n",
       " '@ controls @ upper bound @ @ kinds controls @ choose @ @ extent @ simulates @ linear_transformation',\n",
       " '@ @ @ @ 1 parameter',\n",
       " '@ @ @ @ @ @ @ parameter @ b @ @ @ @ @ zero @ @',\n",
       " '@ @ @ @ parameter @ control length normalization',\n",
       " '@ @ @ @ case @ normalizing formula @ average document length @',\n",
       " '@ @ @ @ computed @ taking @ average @ @ lengths @ @ @ documents @ @ collection',\n",
       " '@ @ case @ @ lengths @ @ @ context documents @ @ @ considering',\n",
       " '@ @ average documents @ @ @ constant @ @ given collection @ @ actually @ @ affecting @ effect @ @ parameter b @',\n",
       " '@ @ @ @ constant',\n",
       " '@ @ kept @ @ @ @ constant thats useful @ retrieval @ @ @ @ @ @ stabilized interpretation @ parameter b',\n",
       " '@ @ @ purpose @ @ @ @ constant @ @ @ @ @ affecting @ length formalization @ @ parameter b',\n",
       " '@ @ @ definition @ @ @ @ new way @ define @ document vectors @ @ @ compute @ vector d2 @ @ @ way',\n",
       " '@ difference @ @ @ high_frequency terms @ @ @ @ somewhat lower weights @ @ @ help control @ influence @ @ high_frequency terms',\n",
       " '@ @ idf @ @ added @ @ @ scoring_function',\n",
       " '@ means @ introduce weight @ matching @ term',\n",
       " '@ @ @ recall @ sum indicates @ @ possible words @ @ @ @ overlap @ @ @ contexts',\n",
       " '@ @ xi @ yi probabilities @ picking @ word @ @ contexts @ @ indicates @ likely @ @ @ match @ @ word',\n",
       " '@ idf @ @ @ @ importance @ matching @ word',\n",
       " '@ common word @ @ worth @ @ rare word @ @ emphasize @ @ matching rare words @',\n",
       " '@ @ @ modification @ @ new function @ likely address @ @ problems',\n",
       " '@ interestingly @ @ @ use @ approach @ discover_syntagmatic relations',\n",
       " '@ general @ @ represent @ term vector @ represent @ sorry @ represent context @ @ term vector @ @ likely @ @ terms @ higher weights @ @ terms @ lower weights depending @ @ @ assign weights @ @ terms @ @ @ able @ use @ weights @ discover @ words @ @ strongly associated @ @ candidate word @ @ context',\n",
       " '@ lets @ @ look @ @ term vector @ @ detail @',\n",
       " '@ @ @ @ xi defined @ @ normalized weight @ bm_25',\n",
       " '@ @ weight @ @ reflects @ frequently @ word occurs @ @ context',\n",
       " '@ @ cant @ @ @ frequent_term @ @ context @ @ @ correlated @ @ candidate word',\n",
       " '@ @ common words like @ @ occur frequently @ @ @ context',\n",
       " '@ @ @ apply idf_weighting @ @ @ @ @ @ @ @ weight @ terms based @ idf @ means @ words @ @ common like @ @ @ penalized',\n",
       " '@ @ @ highest weighted terms @ @ @ @ common terms @ @ @ lower idfs',\n",
       " 'instead @ terms @ @ @ terms @ @ frequent @ @ context @ @ frequently @ @ collection',\n",
       " '@ @ @ clearly @ words @ tend @ occur @ @ context @ @ candidate word @ example cat',\n",
       " '@ @ @ reason @ highly weighted terms @ @ idf weighted vector @ @ @ assumed @ @ candidate @ syntagmatic_relations',\n",
       " '@ @ course @ @ @ @ biproduct @ @ approach @ discovering_paradigmatic relations',\n",
       " '@ @ @ @ lecture @ going @ talk @ @ @ @ discover_syntagmatic relations',\n",
       " '@ @ clearly shows @ relation @ discovering @ @ relations',\n",
       " '@ @ @ @ @ discussed discovered @ @ joint manner @ leveraging @ associations',\n",
       " '@ @ summarize @ main_idea @ discovering_paradigmatic relations @ @ collect @ context @ @ candidate word @ form @ pseudo document @ @ @ typically represented @ @ bag @ words',\n",
       " '@ @ compute @ similarity @ @ corresponding context documents @ @ candidate words',\n",
       " '@ @ @ @ @ @ highly similar word pairs @ treat @ @ having paradigmatic_relations',\n",
       " '@ @ @ words @ share similar context',\n",
       " '@ @ @ @ different_ways @ implement @ general idea @ @ @ talk @ @ @ @ approaches',\n",
       " '@ @ specifically @ talked @ @ text retrieval models @ help @ design effective similarity_function @ compute @ paradigmatic_relations',\n",
       " '@ specifically @ @ @ @ bm25 @ idf_weighting @ discover paradigmatic_relation @ @ approaches @ represent @ state @ @ art @ text retrieval techniques',\n",
       " 'finally syntagmatic_relations @ @ @ discovered @ @ biproduct @ @ discover paradigmatic_relations',\n",
       " '',\n",
       " '@ lecture @ @ @ syntagmatic_relation discovery',\n",
       " '@ entropy',\n",
       " '@ @ lecture @ going @ continue_talking @ word_association mining',\n",
       " '@ particular @ @ talk @ @ @ discover_syntagmatic relations',\n",
       " '@ @ going @ start @ @ introduction @ entropy @ @ @ basis @ designing @ measures @ discovering @ relations',\n",
       " '@ definition syntagmatic_relations hold @ words @ @ correlated co_occurrences',\n",
       " '@ means @ @ @ @ word occurs @ @ context @ tend @ @ @ occurrence @ @ @ word',\n",
       " '@ @ @ @ specific example @ @ @ ask @ question @ eats occurs @ @ words @ tend @ occur',\n",
       " '@ looking @ @ sentence @ @ @ left',\n",
       " '@ @ @ words @ @ occur @ @ eats like @ cat dog @ fish @ right',\n",
       " '@ @ @ @ @ @ @ @ @ look @ @ right @ @ @ @ @ eats @ @ @ words',\n",
       " '@ question @ @ @ @ predict @ @ words occur @ @ left @ @ @ right',\n",
       " 'right @ @ @ force @ @ think @ @ @ words @ associated @ eats',\n",
       " '@ @ @ associated @ eats @ tend @ occur @ @ context @ eats',\n",
       " '@ @ specifically @ prediction problem @ @ @ @ text segment @ @ @ @ sentence paragraph @ @ document @ @ @ asked @ question @ @ particular word present @ absent @ @ segment',\n",
       " 'right @ @ @ ask @ question @ @ world w @ present @ absent @ @ segment',\n",
       " '@ whats interesting @ @ @ words @ actually easier @ @ @ @ words',\n",
       " '@ @ @ @ look @ @ @ words shown @ meet @ @ unicorn',\n",
       " '@ @ @ @ think @ @ easier @ predict @ @ @ think @ @ @ @ moment @ @ conclude @',\n",
       " '@ @ easier @ predict @ @ tends @ occur @ @ @ @ @ @ @ @ @ @ semtence',\n",
       " 'unicorn @ @ relatively easy',\n",
       " '@ unicorn @ rare @ @ rare',\n",
       " '@ @ @ bet @ @ doesnt_occur @ @ sentence',\n",
       " '@ meat @ @ @ @ @ terms @ frequency @ @ makes @ hard @ predict @ @ possible @ @ occurs @ @ sentence @ @ segment @ accurately',\n",
       " '@ @ @ @ @ occur @ @ segment',\n",
       " '@ @ lets_start @ problem @ formally',\n",
       " 'alright @ @ problem @ @ formally defined @ predicting @ value @ @ binary random_variable',\n",
       " '@ @ denoted @ x sub w w denotes @ word',\n",
       " '@ @ random_variable @ associated @ precisely @ word',\n",
       " '@ @ value @ @ variable @ 1 @ means @ word @ present',\n",
       " '@ @ zero @ means @ word @ absent @ naturally @ probabilities @ @ @ zero @ sum @ 1',\n",
       " '@ @ word @ @ present @ absent @ @ segment',\n",
       " 'theres @ @ choice',\n",
       " '@ @ intuition @ discussed_earlier @ @ formally stated @ follows',\n",
       " '@ @ random @ random_variable @ @ @ difficult @ prediction @ @',\n",
       " '@ @ question @ @ @ @ quantitatively measure @ randomness @ @ random_variable like x sub w @ @ general @ @ quantify @ randomness @ @ variable @ thats @ @ need @ measure called entropy',\n",
       " '@ @ @ @ measure introduced @ information_theory @ measure @ randomness @ x',\n",
       " '@ @ @ @ connection @ @ information @ @ thats @ @ scope @ @ course',\n",
       " '@ @ @ purpose @ @ treat @ entropy function @ @ function defined @ @ random_variable',\n",
       " '@ @ case @ @ binary random_variable @ @ definition @ @ easily generalized @ @ random_variable @ multiple values',\n",
       " '@ @ function form looks_like @',\n",
       " 'theres @ sum @ @ @ possible values @ @ random_variable inside @ sum @ @ value @ @ @ product @ @ probability @ @ random_variable equals @ value @ log @ @ probability',\n",
       " '@ note @ @ @ @ @ negative sign @',\n",
       " '@ entropy @ general @ @ negative @ @ @ @ mathematically proved',\n",
       " '@ @ @ expand @ sum @ @ @ equation looks_like @ second @ @ explicitly plugged @ @ @ values zero @ @',\n",
       " '@ @ @ @ @ 0 log @ 0 @ @ generally find @ @ zero @ log @ 0 @ undefined',\n",
       " '@ @ @ @ entropy function @ @ function @ @ @ different value @ different distributions @ @ random_variable',\n",
       " '@ @ clear @ clearly depends @ @ probability @ @ random_variable taking @ value @ @ @ zero',\n",
       " '@ @ plotted @ function @ @ probability @ @ random_variable @ equal @ 1 @ @ @ function looks_like @',\n",
       " '@ @ @ ends @ means @ @ probability @ x 1 @ @ small @ @ large @ @ entropy function @ @ lower value @ @',\n",
       " '5 @ @ middle @ @ reaches @ maximum',\n",
       " '@ @ @ plot @ function @ @ probability @ @ x @ taking @ value @ 0 @ @ function @ @ exactly @ @ curve @',\n",
       " '@ @ @ imagine @ @ @ thats @ @ @ probabilities @ symmetric @ completely symmetric',\n",
       " '@ @ interesting question',\n",
       " '@ @ think @ @ general @ @ @ @ kind @ x @ @ entropy reached maximum @ minimum @ @ @ @ particular think @ @ special_cases',\n",
       " '@ example @ @ case @ @ @ @ random_variable @ @ takes @ value @ @ @ probability @ @ @ @ @ @ random_variable @ @ equally_likely taking @ value @ 1 @ 0',\n",
       " '@ @ case @ probability @ x 1 @',\n",
       " '5',\n",
       " '@ @ @ @ @ higher entropy @ easier @ look @ @ problem @ thinking @ simple example',\n",
       " '@ coin tossing @ @ @ think @ @ random experiment like @ tossing @ coin @ gives @ @ random_variable @ @ represent @ result',\n",
       " '@ @ @ head @ tail @ @ @ define @ random_variable x sub coin @ @ @ @ @ @ coin_shows @ @ head @ zero @ @ coin_shows @ @ tail',\n",
       " '@ @ @ @ compute @ entropy @ @ random_variable @ @ entropy indicates @ difficult @ @ @ predict @ outcome @ @ coin @ coin tossing',\n",
       " '@ @ @ think @ @ @ cases',\n",
       " '@ @ @ fair_coin @ completely fair',\n",
       " '@ coin_shows @ @ head hotel equally_likely @ @ @ probabilities @ @ 12 right @ @ @ @ @ equal @ 12',\n",
       " '@ extreme case @ completely_biased coin @ @ coin @ shows @ @ head @ @ @ completely_biased coin',\n",
       " '@ lets think @ @ entropies @ @ @ cases @ @ @ plug @ @ values @ @ @ @ entropies @ @ @ follows @ @ fair_coin @ @ @ entropy reaches @ maximum thats @',\n",
       " '@ @ completely_biased coin @ @ @ 0 @ @ intuitively_makes @ lot @ sense @ @ fair_coin @ @ difficult @ predict @ @ completely_biased coin @ @ easy @ predict @ @ @ @ @ @ @ head @ @ @ @ head @ @ time @ @ @ @ shown @ @ curve @ follows',\n",
       " '@ @ fair_coin corresponds @ @ middle point @ @ @ uncertain',\n",
       " '@ completely_biased coin corresponds @ @ end point',\n",
       " '@ @ @ probability @ 1',\n",
       " '0 @ @ entropy @ 0',\n",
       " '@ @ lets @ @ @ @ use entropy @ word prediction',\n",
       " '@ @ problem lets think @ @ problem right @ predicted @ w @ present @ absolutely @ @ segment',\n",
       " '@ think @ @ @ words',\n",
       " 'particularly think @ @ entropies',\n",
       " '@ @ @ assume high entropy words @ harder @ predict',\n",
       " '@ @ @ @ @ @ quantitative way @ tell @ @ word @ harder @ predict',\n",
       " '@ @ @ look @ @ @ words meat @ @ unicorn @',\n",
       " '@ @ clearly @ expect @ meat @ @ @ high entropy @ @ @ unicorn',\n",
       " '@ fact @ @ look @ @ entropy @ @ @ close @ 0 @ @ occurs @',\n",
       " '@ @ like @ completed biased_coin @ @ entropy @ 0',\n",
       " '',\n",
       " '@ lecture @ @ @ syntagmatic_relation discovery @ conditional_entropy',\n",
       " '@ @ lecture @ going @ continue @ discussion @ word_association mining @ analysis',\n",
       " '@ going @ talk @ @ conditional_entropy @ @ useful @ discovering_syntagmatic relations',\n",
       " 'earlier @ talked @ @ entropy @ capture @ easy @ @ @ predict @ presence @ absence @ @ word',\n",
       " '@ @ address @ different scenario @ @ assume @ @ know @ @ @ text segment',\n",
       " '@ @ @ question @ suppose @ know eats occured @ @ segment @ @ @ help @ predict @ presence @ absence @ @ word like meat @ @ particular @ want @ know @ @ presence @ eats @ helped @ predict @ presence @ meat',\n",
       " '@ @ @ frame @ @ entropy @ @ mean @ @ interested @ knowing @ knowing @ presence @ eats @ reduce uncertainty @ @ meat @ reduce @ entropy @ @ random_variable corresponding @ @ presence @ absence @ meat',\n",
       " '@ @ @ ask @ question @ @ @ know @ @ absence @ eats @ @ @ help @ predict @ presence @ absence @ meat',\n",
       " '@ @ questions @ @ addressed @ @',\n",
       " '@ concept called @ conditional_entropy',\n",
       " '@ @ explain @ concept lets @ look @ @ scenario @ @ @ @ @ know @ @ @ segment',\n",
       " '@ @ @ @ probabilities indicating @ @ word like meat_occurs @ @ @ occur @ @ segment @ @ @ @ entropy function @ looks_like @ @ @ @ @ slide',\n",
       " '@ suppose @ know eats @ present @ @ know @ value @ @ random_variable @ denotes eats',\n",
       " '@ @ @ change @ @ probabilities @ conditional probabilities @ @ look @ @ presence @ absence @ meat',\n",
       " 'given @ @ know eats occured @ @ context',\n",
       " '@ @ @ result @ @ replace @ probabilities @ @ corresponding conditional probabilities @ @ entropy function @ @ @ @ conditional_entropy',\n",
       " '@ @ equation @ @',\n",
       " '@ @',\n",
       " '@ conditional_entropy conditioned @ @ presence @ eats',\n",
       " 'right @ @ @ @ @ @ essentially @ @ entropy function @ @ @ seen @ @ @ @ @ @ probabilities @ @ @ condition',\n",
       " '@ @ @ tells @ @ entropy @ meat @ @ @ known eats occurring @ @ segment',\n",
       " '@ @ course @ @ @ define @ conditional_entropy @ @ scenario @ @ dont @ eats',\n",
       " '@ @ @ know eats @ @ occur @ @ segment @ @ conditional_entropy @ capture @ uncertainty @ meat @ @ content @ @ condition',\n",
       " '@ @ putting different scenarios @ @ @ @ complete definition @ conditional_entropy @ follows',\n",
       " 'basically',\n",
       " '@ going @ consider @ scenarios @ @ value @ eats zero @ @ @ @ gives @ @ probability @ eats @ equal @ 0 @ 1',\n",
       " 'basically @ eats @ present @ absent @ @ @ course @ @ entropy conditional_entropy @ meat @ @ particular scenario',\n",
       " '@ @ @ expand @ entropy @ @ @ @ following equation',\n",
       " '@ @ @ @ involvement @ @ conditional probabilities',\n",
       " '@ @ general @ @ discrete random_variables xampy @ @',\n",
       " '@ conditional_entropy @ @ larger @ @ entropy @ @ variable x @ basically @ @ upper bound @ @ conditional_entropy',\n",
       " '@ means @ knowing @ information @ @ segment @ wont @ able @ increase @ uncertainty',\n",
       " '@ @ @ reduce uncertainty @ @ intuitively_makes sense @ @ @ know @ information @ @ @ help @',\n",
       " '@ @ prediction @ @ @ hurt @ prediction @ @ case',\n",
       " '@ whats interesting @ @ @ @ think @ whats @ minimum possible value @ @ conditional_entropy',\n",
       " '@ @ know @ @ maximum value @ @ entropy @ x',\n",
       " '@ @ @ @ minimum @ @ @ @ think @ hope @ @ reach @ conclusion @ @ minimum possible value @ @ 0 @ @ @ @ interesting @ think @ @ @ @ situation @ achieve @',\n",
       " '@ lets @ @ @ @ use conditional_entropy @ capture syntagmatic_relations',\n",
       " '@ @ course @ conditional_entropy gives @ directly @ way @ measure @ association @ @ words',\n",
       " '@ @ tells @ @ @ extent @ @ predict @ @ word given @ @ know @ presence @ absence @ @ word',\n",
       " '@ @ @ look @ @ intuition @ conditional_entropy @ capturing syntagmatic_relations @ useful @ think @ @ @ special_case listed @ @ @ @ conditional_entropy @ @ word given @',\n",
       " '@ @ @ listed @ @ conditional_entropy @ @ middle',\n",
       " '@ @ @',\n",
       " '@ @ @ @ value @ @ @',\n",
       " '@ means @ know @ meat_occurs @ @ sentence @ @ hope @ predict @ @ meat_occurs @ @ sentence',\n",
       " '@ @ course @ @ zero @ theres @ uncertain @ anymore @ @ know @ @ word occurs @ @ segment @ @ @ know @ answer @ @ prediction',\n",
       " '@ @ @ 0',\n",
       " '@ thats @ @ @ conditional_entropy reaches @ minimum',\n",
       " '@ @ lets_look @ @ @ cases',\n",
       " '@ @ @ @ case @',\n",
       " 'knowing @ @ trying @ predict @ meat @ @ @ @ case @ knowing eats @ trying @ predict @ meat',\n",
       " '@ @ @ @ think @ smaller note @ @ smaller entropy means easier @ prediction',\n",
       " '@ @ @ @ think @ higher @ @ @ smaller @ @ look @ @ uncertainty @ @ @ @ case @ doesnt @ tell @ @ @ @ meat @ knowing @ occurrence @ @ doesnt @ help @ reduce @ entropy @ match @ @ stays @ fairly close @ @ original_entropy @ meat',\n",
       " '@ @ @ case @ eats eats @ related @ meet @ knowing presence @ eats @ absence @ eats @ help @ predict wether meat_occurs @ @ @ help @ reduce entropy @ meat @ @ @ expect @ second term @ @ @ @ @ @ smaller entropy',\n",
       " '@ @ means @ @ @ stronger association @ meat @ eats',\n",
       " '@ @ @ @ know @ @ w @ @ @ @ @ meat @ @ entropy conditional_entropy @ reach @ minimum @ @ 0 @ @ @ kind @ words @ @ reach @ maximum @ thats @ @ w @ @ @ related @ meat',\n",
       " 'like @ @ example @ @ @ @ close @ @ maximum @ @ @ entropy @ meat @',\n",
       " '@ @ suggests @ @ @ use conditional_entropy @ mining syntagmatic_relations',\n",
       " '@ algorithm @ look @ follows',\n",
       " '@ @ word w1 @ going @ enumerate @ overall @ words w2 @ @ @ @ compute @ conditional_entropy @ w1 given w2',\n",
       " '@ @ thought @ @ candidate words @ ascending order @ @ conditional_entropy @ @ want @ favor @ word @ @ @ small entropy meaning @ @ helps @ predict @ target word w1 @ @ @ @ @ @ @ ranked @ candidate words @ words @ @ potential syntagmatic_relations @ w1',\n",
       " 'note @ @ need @ use @ threshold @ find @ words',\n",
       " '@ threshold @ @ @ number @ @ candidates @ @ @ absolute value @ @ conditional_entropy',\n",
       " '@ @ @ allow @ @ @ @ @ strongly_correlated words @ @ particular word w1 @',\n",
       " '@ @ algorithm @ @ help @ @ @ strongest k syntagmatic_relations @ entire collection',\n",
       " '@ @ order @ @ @ @ @ @ ensure @ @ conditional_entropies @ comparable @ different words',\n",
       " '@ @ case @ discovering_syntagmatic relations @ @ target word like w1 @ @ need @ compare @ conditional_entropies @ w1 given different words',\n",
       " '@ @ @ case @ @ comparable right @ @ conditional_entropy @ w1 given w2 @ conditional_entropy @ w1 given w3 @ comparable',\n",
       " '@ @ measure @ hard @ @ @ predict w1',\n",
       " '@ @ @ think @ @ @ pairs @ @ share w2 @ @ @ condition @ @ try @ predict @ w1ampw3 @ @ conditional_entropies @ actually @ comperable',\n",
       " '@ @ @ think @ @ question @ @ @ @ @ @ comparable @ @ @ @ @ @ @ different upper bounds right @ @ upper bounds @ precisely @ entropy @ w1 @ @ entropy @ w3',\n",
       " '@ @ @ different upper bounds @ @ @ @ compare @ @ @ way',\n",
       " '@ @ @ @ address @ problem later @ discuss @ @ use mutual_information @ solve @ problem',\n",
       " '',\n",
       " '@ lecture @ @ @ syntagmatic_relation discovery @ mutual_information',\n",
       " '@ @ lecture @ going @ continue_discussing syntagmatic_relation discovery',\n",
       " '@ particular @ going @ talk @ @ concept @ information_theory called mutual_information',\n",
       " '@ @ @ @ @ @ @ discover_syntagmatic relations @ @ talked @ @ problem @ conditional_entropy @ @ @ @ conditional_entropy computed @ different pairs @ words @ @ @ comparable @ @ makes @ hard @ discover strong syntagmatic_relations globally @ corpus',\n",
       " '@ @ @ going @ introduce mutual_information @ @ @ concept @ information_theory @ allows @ @ @ @ sense normalize @ conditional_entropy @ @',\n",
       " '@ @ comparable @ different pairs',\n",
       " '@ particular mutual_information denoted @ ixy measures @ entropy reduction @ x obtained @ knowing y',\n",
       " '@ specifically @ question @ interested @ @ @ @ @ reduction @ @ entropy @ x @ @ obtain @ knowing y',\n",
       " '@ mathematically @ @ @ defined @ @ difference @ @ original_entropy @ x @ @ conditional_entropy @ x given y',\n",
       " '@ @ @ @ @ @ @ @ @',\n",
       " '@ @ @ @ defined @ @ reduction @ entropy @ y @ @ knowing x',\n",
       " 'normally @ @ conditional_entropies hxy @ hyx @ @ equal',\n",
       " '@ interestingly @ reduction @ entropy @ knowing @ @ @ @ actually equal @ @ quantity @ called mutual_information denoted @ @ @ @ @ function @ @ interesting properties',\n",
       " '@ @ @ non negative',\n",
       " '@ @ easy @ understand becausw @ original_entropy @ @ @ going @ @ lower @ @ possibly reduced conditional_entropy',\n",
       " '@ @ words @ conditional_entropy @ @ exceed @ original_entropy',\n",
       " 'knowing @ information @ @ help @ potentially @ wont hurt @ @ predicting x',\n",
       " '@ second property @ @ @ symmetric @ conditional_entropy @ @ symmetrical',\n",
       " 'mutual_information @',\n",
       " '@ @ property @ @ @ reaches @ minimum zero @ @ @ @ @ @ random_variables @ completely independent',\n",
       " '@ means knowing @ @ @ doesnt tell @ @ @ @ @',\n",
       " '@ @ @ property @ @ verified @ simply looking @ @ equation @',\n",
       " '@ @ reaches 0 @ @ @ @ @ conditional_entropy @ x given y @ exactly @ @ @ original_entropy @ x',\n",
       " '@ @ means knowing @ @ @ help @ @ @ thats @ xampy @ completely independent',\n",
       " '@ @ @ fix x @ rank different ys @ conditional_entropy @ @ @ @ order @ ranking based @ mutual_information @ @ @ function @ h @ x @ fixed @ x @ fixed',\n",
       " '@ ranking based @ mutual_information @ exactly @ @ @ ranking based @ @ conditional_entropy @ x given y',\n",
       " '@ @ mutual_information allows @ @ compare different pairs @ xampy @ thats @ mutual_information @ @ general @ @ general @ useful',\n",
       " '@ lets examine @ intuition @ @ mutual_information @ syntagmatic_relation mining',\n",
       " '@ @ question @ ask @ syntactic relation mining @ @ eats occurs @ @ words @ tend @ occur @ @ question @ @ framed @ @ mutual_information question @ @ @ @ @ higher mutual_information @ eats',\n",
       " '@ @ going @ compute @ mutual_information @ eats @ @ words',\n",
       " '@ @ @ @ @ @ @ basically @ based @ @ @ intuition @ @ conditional_entropy @ @ @ @ words @ @ strongly associated @ @ @ tend @ @ high mutual_information @ words @ @ @ related',\n",
       " '@ @ lower mutual_information @ @ @ @ @ example @',\n",
       " '@ mutual_information @ eats @ meats @ @ @ @ @ @ meats @ eats cause major information @ symmetric @ expected @ @ higher @ @ mutual_information @ eats @ @',\n",
       " '@ knowing @ doesnt @ help @ predict eats',\n",
       " 'similarly knowing eats doesnt help @ predicting @ @ @',\n",
       " '@ @ @ @ easily @ @ @ mutual_information @ @ word @ @ @ @ largest @ @ equal @ @ mutual info',\n",
       " '@ entropy @ @ word',\n",
       " '@ @ @ @ case @ reduction @ maximum @ knowing @ @ allow @ @ predict @ @ completely @ @ conditional_entropy @ zero',\n",
       " '@ @ mutual_information reaches @ maximum',\n",
       " '@ going @ @ larger @ @ equal @ @ mutual_information @ eats @ @ word',\n",
       " '@ @ words picking @ @ word @ computing mutual_information @ eats @ @ word @ wont @ @ mutual_information larger @ @ mutual_information @ eats @ @',\n",
       " '@ @ lets think @ @ @ compute @ mutual_information',\n",
       " '@ @ order @ @ @ @ @',\n",
       " 'use @ different form @ mutual_information @ @ @ mathematically write @ mutual_information @ @ form shown @ @ slide @ @ essentially @ @ formula @ computes whats called kldivergences @ callback labeler divergance',\n",
       " '@ @ @ term @ information_theory @ measures @ divergance @ @ distributions',\n",
       " '@ @ @ look @ @ formula @ @ sum @ @ combinations @ different values @ @ @ random_variables @ inside @ sum mainly @ @ @ comparison @ 2 joint distributions',\n",
       " '@ numerator @ @ joint actual observed',\n",
       " 'join @ distribution @ @ @ random_variables',\n",
       " '@ @ @ @ @ denominator @ @ interpreted @ @ expected joint distribution @ @ @ random_variables',\n",
       " '@ @ @ independent',\n",
       " '@ @ @ random_variables @ independent @ joined distribution @ equal @ @ product @ @ @ probabilities',\n",
       " '@ @ comparison @ tell @ @ @ @ variables @ @ independent @ @ @ independent @ @ @ expect @ @ @ @ @ @',\n",
       " '@ @ @ numerator @ different @ @ denominator @ @ mean @ @ variables @ @ independent @ @ helps measure @ association',\n",
       " '@ sum @ simply @ @ @ consideration @ @ @ combinations @ @ values @ @ @ random_variables',\n",
       " '@ @ case @ random_variable @ choose @ @ @ @ values 0 @ 1 @ @ @ @ combinations @',\n",
       " '@ @ @ look @ @ form @ mutual_information @ shows @ @ mutual_information measures @ diversions @ @ actual joint distribution @ @ expected distribution @ @ independence assumption',\n",
       " '@ larger @ divergence @ @ higher @ mutual_information @ @',\n",
       " '@ @ lets @ look @ @ @ @ exactly @ probabilities involved @ @ formula @ mutual_information',\n",
       " '@ @ @ listed @ @ probabilities involved @ @ easy @ @ @ verify @ basically @ @ @ 2 probabilities corresponding @ @ presence @ absence @ @ word',\n",
       " '@ @ w1 @ @ @ probabilities shown @',\n",
       " '@ @ sum @ 1 @ @ word @ @ @ present @ absent @ @ segment',\n",
       " '@ similarly @ @ second word @ @ @ @ probabilities representing presence @ absence @ @ word @ @ sums @ @ @ @',\n",
       " '@ @ finally @ @ @ lot @ joint probabilities @ represented @ scenarios @ cooccurrences @ @ @ words',\n",
       " '@ @ @ shown @',\n",
       " 'right @ @ sums @ 1 @ @ @ words @ @ @ @ @ possible scenarios',\n",
       " '@ @ @ occur',\n",
       " '@ @ @ case @ variables @ @ @ value @ @ @ @ @ @ occurs',\n",
       " '@ @ @ scenarios',\n",
       " '@ @ @ cases @ @ @ random_variables @ @ equal @ 1 @ @ @ @ @ 0',\n",
       " '@ finally @ @ @ scenario @ @ @ @ occurs',\n",
       " '@ @ @ @ @ @ variables taking @ value @ 0',\n",
       " '@ theyre summing @ @ 1 @ @ @ @ probabilities involved @ @ calculation @ mutual_information',\n",
       " '@',\n",
       " '@ @ know @ @ calculate @ probabilities @ @ easily calculate @ mutual_information',\n",
       " '@ @ interesting @ note @ @ @ @ relations @ constraints @ @ probabilities @ @ @ saw @ @ @ @ @ @ @ previous_slide @ @ @ seen @ @ marginal probabilities @ @ words sum @ @ @ @ @ @ seen @ constraint @ says @ @ words @ @ @ @ @ different scenarios @ co_occurrences @ @ @ @ @ additional constraints listed @ @ @',\n",
       " '@ @ @ example @ @ means @ @ add @ @ probabilities @ @ observe @ @ words occur @ @ @ probabilities @ @ word @ @ word occurs @ @ second word doesnt_occur @ @ exactly @ probability @ @ @ word @ observed',\n",
       " '@ @ words @ @ @ word @ observed @ @ @ word @ observed @ @ @ @ @ scenarios depending @ weather second word @ @ observed',\n",
       " '@ @ probability captures @ @ scenario @ @ signal word actually @ @ observed',\n",
       " '@ @ captures @ second scenario @ @ seond word @ @ observed @ @ @ @ @ @ word',\n",
       " '@ @ easy @ @ @ @ equations @ follow @ @ reasoning',\n",
       " '@ @ equations allow @ @ compute @ probabilities based @ @ probabilities',\n",
       " '@ @ @ simplify @ computation',\n",
       " '@ @ specifically @ @ @ know @ probability @ @ word @ present @ @ @ case right @ @ @ know @',\n",
       " '@ @ @ know @ presence @ @ probability @ presence @ @ second word @ @ @ easily compute @ absence probability right @ @ easy @ use @ equation @ @ @',\n",
       " '@ @ @ @ @ @ care @ @ computation @ @ probabilities @ presence @ absence @ @ word',\n",
       " '@ lets_look @ @ joint distribution right lets assume @ @ @ @ available probability @ @ occur @',\n",
       " '@ @ easy @ @ @ @ @ actually compute @ @ @ rest @ @ probabilities based @ @',\n",
       " 'specifically @ example @ @ equation @ @ compute @ probability @ @ @ word occurred @ @ second word @ @ @ @ know @ probabilities @ @ boxes',\n",
       " '@ similarly @ @ equation @ @ compute @ probability @ @ observe @ @ second word',\n",
       " '@ @ finally @',\n",
       " '@ probability @ @ calculated @ @ @ equation @ @ @ @ known @ @ @ @ known @ @ @ @ known right @ @ @ @ easier @ calculate',\n",
       " 'right @ @ @ @ @ calculated',\n",
       " '@ @ slide_shows @ @ @ need @ know @ @ compute @ @ probabilities @ @ shown @ @ boxes @ @ presence @ @ word @ @ co occurrence @ @ words @ @ segment',\n",
       " '',\n",
       " '@ general @ @ use @ empirical counts @ events @ @ observed data @ estimate probabilities',\n",
       " '@ @ commonly @ technique @ called @ maximum_likelihood estimate @ @ simply_normalize @ observed accounts',\n",
       " '@ @ @ @ @ @ @ @ @ @ compute @ probabilities @ follows @ estimating @ probability @ @ @ @ word occurring @ segment @ simply_normalize @ counts @ segments @ contain @ word',\n",
       " '@ lets @ @ @ look @ @ data @',\n",
       " '@ @ right @ @ @ @ listed @ hypothesizes @ data @ @ segments',\n",
       " '@ @ @ segments @ @ @ words occur',\n",
       " '@ indicator @ @ @ @ columns',\n",
       " '@ @ @ cases @ @ word occurs @ @ @ column @ @ @ @ @ column @ zero',\n",
       " '@ @ course @ @ @ cases @ @ @ words occur @ @ @ @ zeros',\n",
       " '@ @ estimating @ probabilities @ simply need @ collect @ @ counts',\n",
       " '@ @ @ counts @ 1st @ count @ w',\n",
       " '1 @ thats @ total_number @ segments @ contain world w @',\n",
       " '@ @ @ ones @ @ column @ w @ @ @ @ count @ @ ones @ @ seen @',\n",
       " '@ second counter @ @ word 2 @ @ @ count @ ones @ @ second column',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5ac61fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "        sentences= [p.split() for p in phrased],\n",
    "        min_count =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c445be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'bayes_classifier'),\n",
       " (1, 'biased_coin'),\n",
       " (1, 'language_processing'),\n",
       " (1, 'says_yes'),\n",
       " (2, 'sub_d'),\n",
       " (2, 'variable_z'),\n",
       " (3, 'given_x'),\n",
       " (3, 'precision_recall'),\n",
       " (4, 'best_guess'),\n",
       " (4, 'categorisation_task'),\n",
       " (4, 'causal_relation'),\n",
       " (4, 'cause_overfitting'),\n",
       " (4, 'cheaper_hotels'),\n",
       " (4, 'coin_shows'),\n",
       " (4, 'conditional_entropies'),\n",
       " (4, 'decision_making'),\n",
       " (4, 'detailed_understanding'),\n",
       " (4, 'discovering_paradigmatic'),\n",
       " (4, 'discovering_syntagmatic'),\n",
       " (4, 'doesnt_mean'),\n",
       " (4, 'effective_predictors'),\n",
       " (4, 'expected_overlap'),\n",
       " (4, 'expensive_hotels'),\n",
       " (4, 'feature_space'),\n",
       " (4, 'fewer_parameters'),\n",
       " (4, 'following_lectures'),\n",
       " (4, 'generated_independently'),\n",
       " (4, 'gold_standard'),\n",
       " (4, 'gradually_group'),\n",
       " (4, 'ground_truth'),\n",
       " (4, 'high_dimensional'),\n",
       " (4, 'important_role'),\n",
       " (4, 'inferred_weights'),\n",
       " (4, 'inverse_document'),\n",
       " (4, 'iphone_6'),\n",
       " (4, 'language_models'),\n",
       " (4, 'large_collection'),\n",
       " (4, 'latent_aspect'),\n",
       " (4, 'linear_separator'),\n",
       " (4, 'main_goal'),\n",
       " (4, 'meat_occurs'),\n",
       " (4, 'method_works'),\n",
       " (4, 'multiple_perspectives'),\n",
       " (4, 'new_york'),\n",
       " (4, 'observed_evidence'),\n",
       " (4, 'overall_precision'),\n",
       " (4, 'paradigmatically_related'),\n",
       " (4, 'pay_attention'),\n",
       " (4, 'probabilistic_models'),\n",
       " (4, 'r_sub'),\n",
       " (4, 'raw_count'),\n",
       " (4, 'relation_discovery'),\n",
       " (4, 'research_articles'),\n",
       " (4, 'specific_examples'),\n",
       " (4, 'strongly_correlated'),\n",
       " (4, 'sub_b'),\n",
       " (4, 'subjective_sensors'),\n",
       " (4, 'support_vector'),\n",
       " (4, 'support_vectors'),\n",
       " (4, 'tentative_clustering'),\n",
       " (4, 'true_positive'),\n",
       " (4, 'weighted_average'),\n",
       " (4, 'weve_got'),\n",
       " (5, 'assign_high'),\n",
       " (5, 'baseline_system'),\n",
       " (5, 'bm_25'),\n",
       " (5, 'co_occurrences'),\n",
       " (5, 'collaboration_network'),\n",
       " (5, 'completely_biased'),\n",
       " (5, 'continue_talking'),\n",
       " (5, 'deep_learning'),\n",
       " (5, 'dimensional_space'),\n",
       " (5, 'discriminative_approaches'),\n",
       " (5, 'discussed_earlier'),\n",
       " (5, 'dot_product'),\n",
       " (5, 'email_messages'),\n",
       " (5, 'external_time'),\n",
       " (5, 'high_quality'),\n",
       " (5, 'human_experts'),\n",
       " (5, 'idf_weighting'),\n",
       " (5, 'intuitively_makes'),\n",
       " (5, 'label_given'),\n",
       " (5, 'linear_combination'),\n",
       " (5, 'linear_transformation'),\n",
       " (5, 'main_difference'),\n",
       " (5, 'multiple_times'),\n",
       " (5, 'new_generation'),\n",
       " (5, 'new_orleans'),\n",
       " (5, 'ordinal_logistic'),\n",
       " (5, 'original_entropy'),\n",
       " (5, 'parse_tree'),\n",
       " (5, 'particularly_useful'),\n",
       " (5, 'peoples_opinions'),\n",
       " (5, 'predefined_categories'),\n",
       " (5, 'probabilistic_latent'),\n",
       " (5, 'processing_techniques'),\n",
       " (5, 'pseudo_counts'),\n",
       " (5, 'pseudo_segments'),\n",
       " (5, 'speech_tagging'),\n",
       " (5, 'suggested_readings'),\n",
       " (5, 'supervised_machine'),\n",
       " (5, 'syntactic_structures'),\n",
       " (6, 'battery_life'),\n",
       " (6, 'bayesian_estimation'),\n",
       " (6, 'binary_categorization'),\n",
       " (6, 'categorisation_results'),\n",
       " (6, 'continued_discussion'),\n",
       " (6, 'doesnt_occur'),\n",
       " (6, 'domain_knowledge'),\n",
       " (6, 'effective_features'),\n",
       " (6, 'fair_coin'),\n",
       " (6, 'feature_vector'),\n",
       " (6, 'general_ideas'),\n",
       " (6, 'government_response'),\n",
       " (6, 'hidden_variables'),\n",
       " (6, 'hurricane_katrina'),\n",
       " (6, 'information_theory'),\n",
       " (6, 'lets_start'),\n",
       " (6, 'local_maximum'),\n",
       " (6, 'main_idea'),\n",
       " (6, 'n_documents'),\n",
       " (6, 'probability_mass'),\n",
       " (6, 'seen_earlier'),\n",
       " (6, 'slide_shows'),\n",
       " (6, 'social_media'),\n",
       " (6, 'united_nations'),\n",
       " (7, 'arithmetic_mean'),\n",
       " (7, 'aspect_rating'),\n",
       " (7, 'clustering_result'),\n",
       " (7, 'different_locations'),\n",
       " (7, 'equally_likely'),\n",
       " (7, 'frequent_term'),\n",
       " (7, 'high_frequency'),\n",
       " (7, 'human_effort'),\n",
       " (7, 'im_going'),\n",
       " (7, 'ive_shown'),\n",
       " (7, 'k_nearest'),\n",
       " (7, 'major_topics'),\n",
       " (7, 'makes_sense'),\n",
       " (7, 'news_articles'),\n",
       " (7, 'opinion_target'),\n",
       " (7, 'presidential_election'),\n",
       " (7, 'product_reviews'),\n",
       " (7, 'special_cases'),\n",
       " (7, 'speech_tags'),\n",
       " (7, 'stock_prices'),\n",
       " (7, 'theta_subj'),\n",
       " (7, 'vector_space'),\n",
       " (7, 'word_association'),\n",
       " (7, 'z_values'),\n",
       " (8, 'based_approaches'),\n",
       " (8, 'causal_topics'),\n",
       " (8, 'classification_accuracy'),\n",
       " (8, 'dirichlet_distribution'),\n",
       " (8, 'discover_syntagmatic'),\n",
       " (8, 'discriminative_classifiers'),\n",
       " (8, 'generative_probabilistic'),\n",
       " (8, 'mining_algorithms'),\n",
       " (8, 'random_variables'),\n",
       " (8, 'search_engine'),\n",
       " (8, 'sentiment_weights'),\n",
       " (8, 'simply_normalize'),\n",
       " (8, 'social_network'),\n",
       " (8, 'theta_2'),\n",
       " (8, 'y_given'),\n",
       " (9, 'nontext_data'),\n",
       " (9, 'system_says'),\n",
       " (9, 'time_period'),\n",
       " (9, 'youre_seeing'),\n",
       " (10, 'bayesian_inference'),\n",
       " (10, 'beta_values'),\n",
       " (10, 'continue_discussing'),\n",
       " (10, 'high_level'),\n",
       " (10, 'high_probabilities'),\n",
       " (10, 'non_zero'),\n",
       " (10, 'paradigmatic_relations'),\n",
       " (10, 'previous_slide'),\n",
       " (10, 'sentiment_classification'),\n",
       " (10, 'word_associations'),\n",
       " (11, 'actionable_knowledge'),\n",
       " (11, 'based_prediction'),\n",
       " (11, 'basic_idea'),\n",
       " (11, 'generative_models'),\n",
       " (11, 'semantic_analysis'),\n",
       " (11, 'training_examples'),\n",
       " (12, 'clustering_bias'),\n",
       " (12, 'optimization_problem'),\n",
       " (12, 'posterior_probability'),\n",
       " (13, 'aspect_ratings'),\n",
       " (13, 'logistic_regression'),\n",
       " (13, 'overall_ratings'),\n",
       " (13, 'special_case'),\n",
       " (13, 'total_number'),\n",
       " (14, 'little_bit'),\n",
       " (14, 'logistical_regression'),\n",
       " (15, 'hidden_variable'),\n",
       " (15, 'information_retrieval'),\n",
       " (15, 'lower_bound'),\n",
       " (15, 'unigram_language'),\n",
       " (16, 'background_language'),\n",
       " (16, 'different_aspects'),\n",
       " (16, 'k_1'),\n",
       " (16, 'language_model'),\n",
       " (16, 'syntagmatic_relation'),\n",
       " (16, 'syntagmatic_relations'),\n",
       " (17, 'm_step'),\n",
       " (17, 'opinion_holder'),\n",
       " (17, 'overall_rating'),\n",
       " (17, 'paradigmatic_relation'),\n",
       " (18, 'contextual_text'),\n",
       " (18, 'naive_bayes'),\n",
       " (19, 'bayes_rule'),\n",
       " (20, 'parameter_values'),\n",
       " (21, 'e_step'),\n",
       " (21, 'looks_like'),\n",
       " (21, 'objective_function'),\n",
       " (21, 'random_variable'),\n",
       " (21, 'scoring_function'),\n",
       " (23, 'lets_look'),\n",
       " (23, 'sentiment_analysis'),\n",
       " (23, 'similarity_function'),\n",
       " (25, 'generative_model'),\n",
       " (25, 'real_world'),\n",
       " (31, 'different_ways'),\n",
       " (31, 'machine_learning'),\n",
       " (34, 'em_algorithm'),\n",
       " (34, 'topic_models'),\n",
       " (35, 'non_text'),\n",
       " (36, 'natural_language'),\n",
       " (39, 'conditional_entropy'),\n",
       " (39, 'time_series'),\n",
       " (40, 'training_data'),\n",
       " (42, 'maximum_likelihood'),\n",
       " (45, 'mutual_information'),\n",
       " (48, 'likelihood_function'),\n",
       " (49, 'theta_sub'),\n",
       " (58, 'mixture_model')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ph = [w for s in phrased for w in s.split() if \"_\" in w]\n",
    "phc = [(ph.count(x), x) for x in set(ph)]\n",
    "phc.sort()\n",
    "phc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84d5038a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Good day,  everyone',\n",
       " ' This is your lecturer, Monica wahi',\n",
       " \" And we're going to start now with section  1\",\n",
       " '1',\n",
       " \" What is statistics? So here's our learning objectives for this lecture\",\n",
       " ' At the end of  this lecture, the students should be able to state at least one definition of statistics',\n",
       " \"  Yes, there's more than one, give one example of a population parameter\",\n",
       " ' And one example  of a sample statistic',\n",
       " ' Also, the student should be able to classify a variable into quantitative  or qualitative and as nominal ordinal, interval, or ratio',\n",
       " \" So what we're going to cover in  this lecture is, first I'm going to go over some definitions of statistics\",\n",
       " \" Like I said,  there's more than one\",\n",
       " \" But they all sort of relate to the basic concept of why you're  doing statistics, and especially not math\",\n",
       " \" So what's the difference, right, then we're  gonna go over a population parameter and sample statistic\",\n",
       " \" And you'll know what those mean,  at the end of the lecture\",\n",
       " \" And finally, we're going to go over classifying levels of measurement\",\n",
       " \"  So let's start with the definition of statistics\",\n",
       " \" And so we're going to go over these concepts  like what it is\",\n",
       " \" And also I'm going to define for you the concept of individuals versus  variables\",\n",
       " \" You may know definitions for those words already, but I'm going to give you them  in statistics ease\",\n",
       " \" And then I'm going to give you examples of statistics, individuals  and variables in healthcare\",\n",
       " ' So here are the definitions',\n",
       " ' What is statistics? statistics  is the study, how to collect, organize, analyze, and interpret numerical information and data',\n",
       " \"  Well, that sounds pretty esoteric, right? But if you actually think about it, even if  he did a simple survey, like you just did  a wiki, you just  look on Yelp, right? You look on Yelp, and you see, you know, the restaurant, you want  to go to some people say five stars or four stars, but there's a few two stars one star  will do you go? I mean, there's a whole bunch of different answers\",\n",
       " ' So how do you do that,  you kind of have to analyze it, you kind of have to interpret it',\n",
       " \" So it's not that easy\",\n",
       " '  So statistics is both the science of uncertainty, and the technology of extracting information  from data',\n",
       " \" So in other words, if you've got a bunch of data about like a restaurant, um,  you don't know how it's gonna be if you actually go there, right? You don't know for sure\",\n",
       " \"  But, uh, so it's the science of uncertainty\",\n",
       " \" If you look on Yelp, and you're seeing almost  everybody's giving it a four or five star, maybe it's gonna be good for you, right? But  you don't know, maybe there's new management\",\n",
       " \" That's the uncertainty\",\n",
       " ' So statistics is used  to help us make decisions, not just whether to go to the restaurant or not, but important  statistics, such as in health care and public health',\n",
       " \" Well, I guess if it's an expensive  restaurant, maybe it's important\",\n",
       " ' But anyway, and health care and public health, you really  need these statistics, because they really guide you',\n",
       " \" Like, for example, let's think  of the Center for Disease Control and Prevention in the United States\",\n",
       " \" So what do they do?  They spend the whole year studying the different flu viruses that go round, because there's  more than one\",\n",
       " '  They spend  the whole year doing that they organize, analyze, and interpret numerical information and data  about these different viruses, the different influenza viruses that are going around',\n",
       " ' They  extract that information',\n",
       " \" And you know, what decisions I make the make the decisions about  what viruses to include, in the next year sexy? Are they always right? Sure enough,  they're not\",\n",
       " \" I mean, have you ever had a year where you're like, Oh, my gosh, everybody  I know, got vaccinated, and they're still getting sick? Well, you know, give him a break\",\n",
       " \"  It's this sign some uncertainty, they it just didn't work out that time\",\n",
       " ' However, this is  probably better than just randomly guessing',\n",
       " ' Right',\n",
       " \" So that's statistics for you\",\n",
       " \" Know,  I promised you I'd tell you the statistics ease version of individuals and variables\",\n",
       " \"  Now, if you're outside statistics, you know that individuals are people, right\",\n",
       " \" And you  know that a variable is a factor, like a factor that can vary, you know, like, the only variable  is I don't know what time  something's going to happen\",\n",
       " '  But when you  enter the land of statistics, there are specific meanings to these two words',\n",
       " ' Individuals are  people or objects included in a study',\n",
       " \" So if you're gonna do an animal study with some  mice in it, those would be the individuals\",\n",
       " \" If you do a randomized clinical trial, and  you include people who have Alzheimer's in it, then patients are your individuals\",\n",
       " ' But  we do a lot of different things in healthcare',\n",
       " \" We sometimes study hospitals, like the rate  of nosocomial infections, in which case if you're looking old bunch of stuff in hospitals,  those would be the individuals\",\n",
       " ' Sometimes we look at states rates of infant mortality,  for example, in different states, in that case, states would be individuals',\n",
       " ' So as you  can see at the bottom of the slide, a variable then is a characteristic of the individual  to be measured, or observed',\n",
       " ' I give some examples on the slide',\n",
       " ' But like I was saying, you know,  if you wanted to study a hospital, for example, I gave you the example of a variable of a  rate of nosocomial infections, you could also have other variables about that individual  or hospital,  like the rate of in hospital mortality',\n",
       " \" And so, as you can see, one of the things we do  in statistics is we sit down and we decide, well, who are going to be our individuals  that we're going to measure? And what variables are we going to measure\",\n",
       " ' So I just threw up  here a few examples of different kinds of individuals we have, that we use a lot in  health care and public health, and an example of just one variable, about those example  individuals',\n",
       " ' But there would theoretically be many variables about them',\n",
       " ' And I just want  you to notice, a lot of times, the individuals are geographic locations',\n",
       " ' Other times they  might be institutions, like I said, like hospitals, or clinics, or programs',\n",
       " \" There's other things  that they are, but these are just kind of the big ones\",\n",
       " ' So, um, as I was describing,  and just to review, what I went over, statistics is used in healthcare and other disciplines  to, to aid in decision making, like I gave the example the CDC and their vaccine for  influenza',\n",
       " \" And so therefore, it's really important to understand statistics, because you need  to understand these processes in healthcare, like how do we figure out  what to do?  Like not only what do we do, but how do we figure out what to do\",\n",
       " \" And that's really important  because we use statistics a lot in healthcare\",\n",
       " \" Now, we're going to move on to talk about  what a population parameter is, and what a sample statistic is\",\n",
       " \" So we're going to go  over first definition of a population and the definition of a sample\",\n",
       " \" So you're sure  about what those mean\",\n",
       " \" And we're going to talk about the data about a population and  the data about a sample and how those are different\",\n",
       " \" And then we're going to get into  what I was just describing parameters and statistics\",\n",
       " \" And I'll give you a few examples\",\n",
       " \"  So let's start with what is the population, again, another case where you just have a  normal word, but it has a special meaning and statistics? Well, it's a group of people  or objects with a common theme\",\n",
       " ' And when every member of that group is considered this population,  right',\n",
       " \" So here, here's just one example\",\n",
       " ' So the theme would be like nurses who work at  Massachusetts, Massachusetts General Hospital, so the population then if that was your theme,  will be the list from human resources of every nurse out currently employed at mgh',\n",
       " ' Now,  it really does depend on how you define that thing',\n",
       " \" Like I could have said, nurses who  belong to the American nursing Association, right? And then we'd be looking at a different  list\",\n",
       " \" I could say nurses who live in New Orleans, in the city limits of New Orleans who live  there, right, then we'll be looking at a different population\",\n",
       " ' So really has to do with the details  of how you describe the theme around that population',\n",
       " ' But the point is, once you describe  that theme, the population is every single individual in there',\n",
       " \" So then, what is the  sample? Well, it's a small portion of that population\",\n",
       " \" It can be a representative sample,  but it can also be a biased sample, and we're going to get into that\",\n",
       " \" So let's just go back  to mgh\",\n",
       " \" And think let's say we were going to survey a sample of the population of nurses  at mgh, let's say we only surveyed nurses in the intensive care unit\",\n",
       " ' That would be  a sample, but not a representative sample',\n",
       " ' So it would be a small portion of that population,  but not a representative one',\n",
       " ' Probably more representative would be if we asked at least  one nurse from each department',\n",
       " \" And so I just want to get in your head that the whole concept  of sample is, is that it's just a small portion of the population\",\n",
       " \" And it's not a portion  of some other population\",\n",
       " \" It's just that one\",\n",
       " ' But the problem is you can get a biased one  or representative one',\n",
       " \" So you have to think about So when you think about it, if you've  got a whole population, then you would get variables about each individual in that population\",\n",
       " '  And those variables would be your data',\n",
       " \" But if you chose samples, that you know, just  a portion will be a lot less work, right? You'd still have to get variables about those  individuals, but there's way fewer individuals, so it probably be easier\",\n",
       " ' So in population  data, data from every single individual in the population is available',\n",
       " \" And that's called  a census\",\n",
       " \" So I'm, I knew a person who decided to do a survey of every single professor at  a college\",\n",
       " \" She didn't take just some professors from each department, she sent the survey  to every single professor\",\n",
       " ' So she did not use a sample, she used a census',\n",
       " ' But in sample  data, the data are only available from some of the individuals in the population',\n",
       " ' So if  we go back to the researcher I described, if she had only taken some of the list, the  email list of the professors at that college, then she would have been serving a sample',\n",
       " \"  And that's actually very commonly used in research studies, especially if patients,  why would you need to go get every, for example, kidney dialysis patient and study every single  one, you only need a sample\",\n",
       " ' And why is that because we have statistics',\n",
       " \" So I'm going to  just give you a few examples of real population data in healthcare\",\n",
       " \" You're probably familiar  with Medicare, Medicare is the public insurance program in the United States, for elders\",\n",
       " '  So even my grandma was on Medicare when she was alive,  and she was not a US citizen, she was from India',\n",
       " ' So we really do a good job of covering  our elders in the US with Medicare',\n",
       " ' In fact, I even read a statistics that said, almost  100% of people aged 65 and over are in Medicare',\n",
       " ' And so therefore, if you download data from  Medicare, they make it confidential, you only just replace all the personal identifiers',\n",
       " \"  But there's this thing called the Medicare claims data set for every single transaction  that happens, like if you're in Medicare, and you go get some treatment that's in there\",\n",
       " '  So it has all the insurance claims filed by the Medicare population, because it has everybody,  everything than that is population data',\n",
       " ' Also, in the United States, every 10 years, the  government hires a bunch of people to go out and survey a bunch of people',\n",
       " ' And also, they  send out a bunch of surveys',\n",
       " ' And the idea is to try to get every single person in the  United States to fill out that survey',\n",
       " \" And that's called the United States Census\",\n",
       " \" So  now, I'm going to give you sort of a mirror image of the sample data\",\n",
       " ' Okay',\n",
       " ' Remember how  I was just talking to about Medicare? People who are enrolled in Medicare are called Medicare  beneficiaries, and Medicare cares what they think',\n",
       " ' So they do a survey of a sample of  individuals on Medicare',\n",
       " ' And they do this kind of often',\n",
       " ' I think they do it once a year',\n",
       " \"  I'm not sure it's a phone survey\",\n",
       " \" They only do a sample because they're going to use statistics  to try and extrapolate that knowledge back to the population of Medicare beneficiaries\",\n",
       " '  Also, in case you notice, the United States Census only takes place every 10 years',\n",
       " ' Do  you think changes happen in between? Yep, lots of changes',\n",
       " ' Like you just think about  Hurricane Katrina',\n",
       " \" That's very sad\",\n",
       " ' It changed the population distribution in Louisiana,  vary vary dramatically, and also other states around there',\n",
       " ' So how did they keep up? Well,  they used the American Community Survey, the government does this the United States Census  Bureau, and that, again, is done by phone',\n",
       " \" And that's conducted yearly\",\n",
       " \" And it's a sample  and so the US doesn't know exactly how many people would be in Louisiana or anywhere else\",\n",
       " '  But they can use statistics to extrapolate that from the sample of the American Community  Survey',\n",
       " ' I want to just do a shout out to statistical notation',\n",
       " \" So from now on, when we see a capital  N, like let's say you sack capital N equals 25, then you can assume that 25 means a population  that's just kind of a secret code we use in statistics\",\n",
       " ' However, if you saw a lowercase  n, n equals 25, and it was lowercase, then you could assume that this was a sample of  the population',\n",
       " \" And again, it's just kind of like a secret code, you have to pay attention\",\n",
       " \"  When I'm talking and I say n, and you can see uppercase and lowercase\",\n",
       " \" You don't know  if I'm talking about a population, or a sample\",\n",
       " \" Now I'm going to get into the concept of parameter  versus statistic, I want you to notice that the word parameter starts with P PA\",\n",
       " ' So parameter  is a measure that describes the entire population',\n",
       " ' So for instance, anything that would come  out of that whole Medicare claims data set, or that whole United States Census would be  a parameter',\n",
       " ' On the other hand, a statistic statistic starts with S, and statistic is  a measure that describes only a sample of a population',\n",
       " ' Here we have an, again, a situation  where the word statistic is used, like daily on the news',\n",
       " \" In fact, sometimes I hear on  the news, something like Oh, look at the rate of HIV in Africa, it's going up\",\n",
       " \" That's a  terrible statistic\",\n",
       " ' I agree',\n",
       " \" It's terrible\",\n",
       " \" But they mean parameter, because they're talking  about all of Africa, every single person in Africa, if the rate of HIV is going up in  Africa, they mean a parameter, they don't need a statistic\",\n",
       " \"  So here's an example of parameters and statistics that are based on the same population\",\n",
       " \" So  for example, the mean age of every American on Medicare is a parameter that's every single  person\",\n",
       " \" However, remember, the Medicare beneficiary survey, that's just a sample\",\n",
       " ' So if we took  the mean age of those people, we would just have a statistic',\n",
       " \" And again, you just have  to pay attention, because if you listen to the news, you'll hear them use the word statistic  to mean both parameter and statistic\",\n",
       " \" But in this situation with, when you're practicing  in the field of statistics, it's very important to point out when the number you're talking  about comes from a population versus comes from a sample\",\n",
       " ' So you should really use the  term',\n",
       " \" This is a parameter if it's from a population, or this is a statistic, if it's from a sample\",\n",
       " \"  And so again, don't get confused\",\n",
       " \" If you're listening to someone talk in a lecture or  in a video, you might want to look for clues that a number is a population parameter, or  as a sample statistic, if you hear that the data set that they use encompasses an entire  population\",\n",
       " \" And usually that's the kind of stuff done by governments, like remember when  I was talking about the rate of HIV in Africa, lead probably be done by governments of the  United Nations, or the World Health Organization\",\n",
       " \" So when you're talking about numbers that  might have come  out of an  entire population, usually done by the government, that's probably a population parameter\",\n",
       " \" clues  that someone's talking about a sample statistic is if you hear them talking about a study  that recruited volunteers,  well,  then, if it's volunteers, they didn't get everybody in the population\",\n",
       " \" So it's going  to be a sample\",\n",
       " \" Also, like surveys, for instance, surveys about who people are going to vote  for you public opinion surveys, they're never going to ask some every single person in the  state, who are you going to vote for build us ask a sample\",\n",
       " \" So if you hear about a survey,  you might even have them tell you say, n equals maybe a few 1000 people because that's all  they surveyed\",\n",
       " \" And so that's a clue that we're talking about a sample statistic rather than  a population parameter\",\n",
       " \" Now, I'm going to talk about the difference between descriptive  statistics and inferential statistics\",\n",
       " \" But first I'm going to remind you what the word  infer means\",\n",
       " ' So infer means to kind of get a hint from something indirectly',\n",
       " \" It's kind  of the complement to imply\",\n",
       " ' So if I said my friend implied that I should not call after  9pm and I figured that out',\n",
       " ' I would say I inferred that I should not call my friend  after 9pm',\n",
       " ' Okay',\n",
       " \" So in inferential is what I'm going to talk about next\",\n",
       " \" But first I'm  going to talk about descriptive descriptives is pretty easy, because you can do it to samples  and you can do it to populations will variables from samples and populations, right\",\n",
       " ' And so,  descriptive statistics involve methods of organizing picturing in some Rising information  from samples and populations',\n",
       " \" It's basically just making pictures of it right? Like look  at that bar chart\",\n",
       " \" And that's just a simple picture\",\n",
       " ' And that can be made with just about  any data',\n",
       " \" You get data from surveying people at work, you get data from surveying your  friends, what they're going to bring to the potluck\",\n",
       " ' If any of that can be used, you can  go download the census data, you can make descriptive statistics out of that',\n",
       " \" But there's  something very special about inferential statistics\",\n",
       " ' And that involves methods of using information  from a sample to draw conclusions regarding the population',\n",
       " ' Therefore, inferential statistics  can only be done on a sample',\n",
       " \" And therefore and that's why that's called inferential\",\n",
       " \"  Right? Because infer, because the sample is going to give a hint about what the population  is right? It's not going to say it directly, which is annoying, right? But that's that  uncertainty thing I was telling you about\",\n",
       " \" So the sample is going to imply something?  Well, we're gonna infer something from the sample about the population, right? So that's  what inferential statistics is, is where you take a sample, and you infer something about  the population\",\n",
       " ' Whereas descriptive statistics is more loosey goosey',\n",
       " ' You can just do that  to samples and populations, kind of like make pictures out of it, right',\n",
       " \" So in statistics,  it's really important to properly identify measures as either population parameters,  or sample statistics\",\n",
       " ' Because as you can see, you can only do inferential statistics on  samples',\n",
       " \" And so you have to really know what you're doing when you're doing statistics,  what you're talking about, because different types of data are used for parameters versus  statistics\",\n",
       " \" Alrighty, now we're going to get into classifying variables into different  levels of measurement\",\n",
       " ' So remember our variables, right, like we have individuals, and then  we have variables about them',\n",
       " ' And those variables actually can only fall into two groups, quantitative  versus qualitative',\n",
       " ' And then depending on which group they fall into, you can further  classify them as interval versus ratio, or nominal versus ordinal',\n",
       " \" And I'm going to give  you some examples of how to classify a few healthcare data, types of variables already,  so I like to draw this picture\",\n",
       " \" It's a four level data classification, I'll draw it solely  here for you\",\n",
       " \" So we start with human research data, that's what I like to start with\",\n",
       " \" Alright,  so we're going to split that into two\",\n",
       " \" Remember, I said that, we're going to start by talking  about quantitative\",\n",
       " \" Another word that's often used for that is continuous, but we're going  to use the word quantitative\",\n",
       " ' So what does that mean? That is a numerical measurement  of something',\n",
       " ' So like, this gives an example of temperature',\n",
       " \" So something  with a number in it, I always think if I can make a mean out of it, it must be a quantitative  variable, right? And so here's an example of quantitative variables\",\n",
       " ' So time of admin,  right? So imagine that you work a shift in the ER, right? And from maybe 8pm to 12',\n",
       " ' like  midnight, right? So you have this for hours',\n",
       " \" And you could say, what the average time of  admin would be for those who got admitted to the hospital, you know, somebody got admitted  at like, eight o'clock, and then somebody at 815, and whatever, you could put that together,  and you'd say what the average time was, also, like, if you were doing a study, and you as  you were saying, patients with a particular condition like Alzheimer's disease, you could  ask them their year of diagnosis, and then you could make an average of that\",\n",
       " ' And so  you know, that that is quantitative',\n",
       " ' systolic blood pressure is also numerical, and platelet  count',\n",
       " ' And these are variables we run into all the time in healthcare',\n",
       " \" So we're, you  said that this is quantitative\",\n",
       " \" Now, we'll get back to our picture\",\n",
       " \" So that's one side\",\n",
       " \"  So what if it's not quantitative? What else could it be? Well, the only other category,  it could be is categorical or qualitative\",\n",
       " \" I use the term qualitative, but some people  use the term categorical, but that's kind of what it is, is that it's a quality of something  or a characteristic of something like sex or race\",\n",
       " \" So here are some qualitative variables  in healthcare, like you can have type of health insurance, like whether you're on Medicare  or Medicaid or different types of private insurance\",\n",
       " \" Those are all just categorical,  right? You can't make a mean out of that\",\n",
       " ' Also country of origin',\n",
       " \" If you're in our group  of students and their international students in there\",\n",
       " \" Well, what countries are they from?  Right? Well, you can't make a mean out of that\",\n",
       " \" Also you have situations where you do  have numbers involved, like the stage of cancer, right? That's depressing\",\n",
       " \" Stage One, cancer,  stage two, cancer, stage three, well, you never can make a mean, out of the stage of  cancer, you wouldn't say, well, the mean stages is 1\",\n",
       " '4, or something like that',\n",
       " \" It's just  a category\",\n",
       " ' And of course, stage four is a lot worse than stage one',\n",
       " \" You know, they're  not just equal categories, but their categories\",\n",
       " \" Same with trauma center level level four Trauma  Center, where you wouldn't make a mean out of the number of after the term Trauma Center,  right, like what level it is\",\n",
       " ' But you could say, well, in the state, maybe',\n",
       " ' So many percent  of our trauma centers are level four trauma center',\n",
       " \" So it's really just a categorical  variable, even though there's a number involved\",\n",
       " \" Alright, so let's get back to our diagram,  we figured out how to take any variable, and first split it into one of two categories  is either quantitative, if it's numerical, or qualitative, if it's a characteristic\",\n",
       " \"  Now, we're going to just concentrate on quantitative because we're going to separate those variables  into two categories\",\n",
       " \" And the first one we're going to look at is interval\",\n",
       " \" And the second  one we're going to look at is ratio\",\n",
       " \" So if a if you happen to decide a variable as quantitative,  then it could be interval or ratio, but not if it's qualitative\",\n",
       " \" Okay, if it's qualitative,  it doesn't get to do that\",\n",
       " \" So let's look at interval versus ratio\",\n",
       " \" So on the left side  of the side, we have interval, which is where it's quantitative, and the differences between  data values are meaningful\",\n",
       " '  And  ratio has the same thing, the differences between the data values are meaningful',\n",
       " ' What  does that mean by that? Well, remember how I was talking before how level one trauma  center and level two trauma center that that those are really categories, and not quantitative  variables, because the difference actually between them is not equal',\n",
       " ' Especially if you  think of job classifications that might go in 1234, like nurse, one, nurse to nurse three,  nurse four, or I worked at a job where we had office specialist one, office specialist  to Office specialist three',\n",
       " '  And you know what the deal  for going from office specialists to to Office specialist three was really hard, you really  had to do a lot there',\n",
       " \" But to go from one to two wasn't that hard? So that was a categorical  variable, right? Because the differences between the values were meaningless\",\n",
       " ' Okay',\n",
       " \" Like the  difference between s one and s two versus Oh, s two, and s three, they weren't equal\",\n",
       " \"  Whereas when you're dealing with a quantitative variable, regardless of whether it's interval  or ratio, you're talking like years, or systolic blood pressure, one year for you is one year  for me\",\n",
       " \" So that's fine, right? But here's where the difference comes in between interval  and ratio\",\n",
       " ' So all quantitative variables have meaningful differences between their data  values, but this hairsplitting thing here is that an interval, there is no true zero',\n",
       " '  And in ratio, there is a true zero',\n",
       " ' And this is how I try to think about it',\n",
       " ' an interval  means kind of like, a space between two things',\n",
       " ' Like if you think of the word intermission  is kind of like an interval',\n",
       " \" It's like an interval of time during a show where you get  to get up and go the bathroom and get some coffee\",\n",
       " \" So that's interval\",\n",
       " \" And so if you  have something that's a space in between, that's not going to have a zero, it doesn't  really start anywhere, or end anywhere\",\n",
       " \" It's in between\",\n",
       " \" Whereas ratio, how are you number  that is, I don't know if you remember from like high school, but you can't have a zero  on the bottom of a ratio or a fraction\",\n",
       " \" So that's the way I use a pneumonic\",\n",
       " ' That ratio  means that you cannot have a true zero',\n",
       " \" But how does this work out literally? Well, I'll  show you\",\n",
       " \" So let's go back to those examples I showed you of quantitative variables, right?  Because those are the only ones we have to make this decision about whether they are  interval ratio\",\n",
       " ' So these are these examples',\n",
       " \" Now I'm going to remind you that ratio has  a true zero\",\n",
       " \" Remember that little pneumonic I said, like don't divide by zero\",\n",
       " ' And so  you know, like in a ratio, so they have a true zero',\n",
       " \" Well, let's think about it\",\n",
       " \" It's  not very pleasant to have a zero systolic blood pressure because you'd be dead\",\n",
       " \" Same  with the platelet count, but it is possible, right? But now when we go on to interval,  we can't have Like zero time, like time of admet, you know are your diagnosis, there's  no like, year zero\",\n",
       " \" So as you probably just guessed, ratio is where it's at\",\n",
       " ' In healthcare',\n",
       " \"  There's not a whole lot of times when we have interval data, but we do, you know, anytime  you have a time, so you got to keep that in mind that if you want to split your quantitative  variables into either interval or ratio, you got to keep this in mind the difference between  the true zero and the no true zero\",\n",
       " \" Okay, here's our handy dandy diagram\",\n",
       " \" We've just  gone through the tree classifying quantitative data into interval versus ratio\",\n",
       " \" Now let's  go pay attention to the other side of the tree qualitative\",\n",
       " ' So how do we split those?  Um, well, we can split those into nominal versus ordinal',\n",
       " ' All right',\n",
       " ' So nominal applies  to categories, labels, or names that cannot be ordered from smallest to largest',\n",
       " \" Okay,  like I kind of think of when they have an advertisement, they say, for a nominal fee,  you can do this, it means it's small, they're like, there's almost no difference\",\n",
       " \" And so  that's why I say, there's no difference, it's not smallest to largest is means they must  be equal\",\n",
       " \" That's how I remember it in my mind\",\n",
       " ' But then ordinal applies to data that can  be arranged in order in categories',\n",
       " \" But remember that thing I was saying about quantitative,  it's not quantitative, right? Because the difference between the data values either  cannot be determined or is meaningless, like I was talking about with cancer, especially,  you know, if you go from stage three to stage four, that's materially different than stage  one to stage two\",\n",
       " \" So you really can't determine those things\",\n",
       " \" So this is where we're gonna  get into that it's ordinal\",\n",
       " \" It's arranged in categories that can be ordered from smallest  to largest\",\n",
       " \" So remember, our old friends that I threw up there before of these examples  of qualitative variables and healthcare? Well, let's just reflect on this nominal cannot  be ordered, right\",\n",
       " ' So that would be more like type of health insurance and country of origin  because they could all be equal',\n",
       " ' Whereas ordinal is going to have a natural order, even though  the differences between the levels is meaningless, which is what makes it so different from a  quantitative variables',\n",
       " ' So which is why it stays on the qualitative side of the tree,  it just gets labeled ordinal',\n",
       " ' So what you want to do is if you think you have a qualitative  variable on your hands, look for a natural order',\n",
       " \" If there is one, it's ordinal\",\n",
       " \" And  if not, it's nominal\",\n",
       " ' So all data can be classified as quantitative or qualitative',\n",
       " \" So if you  have a variable, that's the first split you can make as the difference between quantitative  and qualitative, but once you do that, you can further classify it as interval ratio,  nominal, or ordinal\",\n",
       " \" And it's really important to know how to classify data in healthcare,  as you'll find out later\",\n",
       " ' Because depending on how you classify it, you might be able  to do different things with it in statistics already, so what we went over was the definition  of statistics',\n",
       " ' And we talked a little about why you use it and how you use it, especially  in healthcare',\n",
       " ' We went over what it means to talk about a population parameter and the  sample statistic, and we went over some examples about them',\n",
       " ' And then we talked about classifying  variables into the different levels of measurement, and even talked about a few examples there',\n",
       " '  So I hope you enjoyed my lecture',\n",
       " ' Greetings, this is Monica wahi lecturer at library college,  bringing you your lecture on section 1',\n",
       " '2 on the topic of sampling',\n",
       " '  So here  are your learning objectives for this particular lecture',\n",
       " ' At the end of this lecture, the students  should be able to define sampling frame and sampling error, the student should be also  able to give one example of how to do simple random sampling',\n",
       " ' And one example of how to  do systematic sampling',\n",
       " ' The students should be able to explain one reason to choose stratified  sampling over other approaches, state to differences between cluster sampling and convenience sampling,  and give an example of a national survey that uses multistage sampling',\n",
       " \" So let's jump right  into it here\",\n",
       " \" So we're going to go over in this lecture, sampling definitions, and then  those different types of sampling I mentioned in the learning objectives, simple random  sampling, stratified sampling, systematic sampling, and then convenience and multi state  sing\",\n",
       " \" So let's start with some sampling definitions\",\n",
       " \" What is a sample Okay, so we're going to revisit  that concept from the previous lecture, we're also going to talk about sampling frames,  and what errors mean and errors of sampling frames\",\n",
       " \" And then we're also going to just  go right back over that and make sure you understand before we go on, and talk about  the different types of sampling\",\n",
       " ' So we take a sample of a population, because we want  to do inferential statistics, remember that we want to infer from the sample to the population',\n",
       " \"  And it's just not necessary to measure the whole population, it would be impractical\",\n",
       " \"  And it's cost a lot\",\n",
       " \" And actually, what you'll find is, if you ever do an experiment, when  where you actually do measure the whole population, you'll find that if you get, you know, a pretty  good proportion of the population, and you just take that, you, that's all you really  needed to talk to\",\n",
       " ' So ultimately, we save resources, especially in health care, when  we do a good job of sampling, and use that to infer to the population rather than having  to take a census of the whole population all the top',\n",
       " ' So that brings us to the concept  of sampling frame',\n",
       " ' So the sampling frame is the list of individuals from which a sample  is actually selected',\n",
       " \" And the list may be this physical concrete list, like you could  have a list of students enrolled at a nursing college, or in my other lecture, I gave an  example of a list of nurses who work at Massachusetts General Hospital, that could be your list,  you'd go to human resources and get that\",\n",
       " ' Or it could be a theoretical list',\n",
       " \" It could  be like the list of patients who present to the emergency department today, obviously,  when you go into work, at the beginning of the shift, you're not going to know who's  on that list yet\",\n",
       " ' But it could be a theoretical list',\n",
       " ' But whatever that list is, that is your  sampling frame',\n",
       " ' So that those are the people who actually could be selected for your study',\n",
       " '  So the sampling frame is the part of the population from which you want to draw the sample',\n",
       " ' And  you want to work at such that everybody from your sampling frame has a chance of being  selected for your sample',\n",
       " \" In other words, you don't want to leave anyone that should  be in your sampling frame out in the cold\",\n",
       " ' That leads us to the concept of under coverage',\n",
       " \"  So what is it? It's omitting population members from the sampling frame? They're supposed  to be on the list, but they're not there\",\n",
       " \" So how can this happen? Well, let's say you  did what I was suggesting in the previous slide, you got a list of nursing students,  you know, from a college, let's say somebody signed up that day, or somebody was just admitted  that day, maybe they didn't make it into the database in time and you're missing them\",\n",
       " \"  Or even like that HR list I talked about, at mgh, well, you know, I know how nurses  are, sometimes they'll temp in different places, and maybe they're not on the payroll, maybe  they're through a temp agency\",\n",
       " ' And so then we would miss those nurses from the sampling  frame',\n",
       " ' And then, you know, people who present at the emergency department at night might  be different than those in the day',\n",
       " \" And so if you're really trying to sample from people  who present to the emergency department, you can't just look at like some small period  of time, you'd have to look at, you know, the whole 24 hour cycle\",\n",
       " \" So if you omit population  members from your sampling frame, they don't even get a chance to be in it\",\n",
       " \" And that's  called under coverage\",\n",
       " \" Now, I'm going to shift around, we're jumping around with a few different  definitions\",\n",
       " \"  And we're going to talk about errors\",\n",
       " \" Now, this is something that took me a while to  get used to in statistics, there's actually two kinds of errors in statistics\",\n",
       " ' The first  kind is I call it This is my own terminology, a fact of life error',\n",
       " \" It's just an error that  happens\",\n",
       " \" When you do statistics, it's not bad or good\",\n",
       " \" It's just what happens\",\n",
       " \" And in  this case, I'm going to describe one of those\",\n",
       " \" It's called a sampling error\",\n",
       " ' So the sampling  error just simply says the population mean will be different from your sample mean, and  the population percentage will be different from your sample percentage',\n",
       " ' So what does  that mean? That means that if I cut corners, like I said, I could write and just take a  sample to infer to the population',\n",
       " ' If I actually do one of those experiments I was telling  you about where I have the population data and I just take a sample and compare the means  they will be different',\n",
       " \" Okay, I mean, there might be this huge coincidence where they're  the same but they're typically different\",\n",
       " ' Same if you do percentages, and and we just  know this is going to happen',\n",
       " ' The statistics we account for it, we have ways of dealing  with it',\n",
       " \" But we know that there's always going to be sampling error whenever you take a sample  from a population To try to make a mean or percentage in the sample, it's just not going  to be exactly what's in the populations fine\",\n",
       " '  But then  there are other errors and statistics, which are actually bad',\n",
       " ' And your it means you made  a mistake',\n",
       " \" It's like mistakes, literally mistakes\",\n",
       " \" And so as you go through learning about statistics,  it's almost like you have to sit down and ask somebody, is this one of those fact of  life errors? Or is this one of those errors you want to avoid? Well, we just talked about  sampling error\",\n",
       " \" That's just a fact of life error\",\n",
       " ' But errors, you want to avoid non sampling  error',\n",
       " \" That's basically using a bad list\",\n",
       " ' I had an example in my life where I wanted  to study a whole bunch of providers, right',\n",
       " ' And my friend gave me this list of providers,  and and said, this is the entire list of all these providers in this particular professional  society',\n",
       " \" But when I sent the email to that list, I found there were not only duplicates  on this list, but a lot of people emailed me back and said, Why are you sending this  to me? I'm not a provider\",\n",
       " \" I'm not part of this professional society\",\n",
       " \" And also, some  people who were in that professional society, who had heard about the survey emailed me  and said, Why didn't I get the survey\",\n",
       " ' So this was a bad list',\n",
       " ' Some people had been  left out of the sampling frame',\n",
       " \" So people who were in the society somehow weren't on  my email list\",\n",
       " \" And that's a problem, right? So you have to pay careful attention\",\n",
       " ' This  was actually a mistake I made, you have to pay careful attention that everyone in the  population who was supposed to be represented in your sampling frame is actually there',\n",
       " '  So I should have really done a better job of calling the professional society and making  sure that this list was a good list',\n",
       " ' So sampling error was caused by the fact that regardless  of what you do, your sample will not perfectly resent represent the population',\n",
       " ' Whereas non  sampling error, yeah, I was sloppy',\n",
       " ' It was poor sample design, sloppy data collection,  and accurate measurement instruments, you can have bias and data collection, other problems  introduced by the researcher',\n",
       " \" So this is your fault if there's non sampling error, but sampling  error is just a  fact of life\",\n",
       " \"  Little whiplash here, we're gonna now move on to the concept of simulations\",\n",
       " ' So a simulation  is defined technically as a numerical facsimile, or representation of a real world phenomenon',\n",
       " \"  So it's like working through a pretend situation, to see how it would come out in the case that  was real\",\n",
       " ' And this, you know, when you study statistics, you end up doing a lot of simulations',\n",
       " \"  And remember how I've been talking about an experiment you could do if you somehow did  a census and had a whole bunch of data on a population, you could do an experiment where  you just took a sample from that population and looked at their mean to see the sampling  error\",\n",
       " \" That's an example of a simulation\",\n",
       " \" So to just conclude this little section, it's  really important to do your best to avoid non sampling error\",\n",
       " ' And this is achieved by  making sure you do not have under coverage when sampling from your sampling frame',\n",
       " ' So  this puts together some of our vocabulary',\n",
       " ' But just remember, sampling error is a fact  of life',\n",
       " \" Okay, now we're going to specifically talk about different types of sampling\",\n",
       " \" And  we're going to start with simple random sample\",\n",
       " \" Okay, so first, we're gonna start with just  explaining what is meant by simple random sampling, then we're going to talk about two  different methods of doing simple random sampling, they work the same way they achieve the same  thing\",\n",
       " \" It's just that depending on how you're doing your research, one might be more convenient  for you than the other\",\n",
       " ' Finally, we will go over the limits of simple random sampling,  because all these sampling methods seem perfect',\n",
       " ' But then you got to take a look at their limitations',\n",
       " \"  So let's first define simple random sampling\",\n",
       " \" So here's a definition\",\n",
       " ' A simple random sample  of n measurements from a population is a subset of the population selected in such a manner  that every sample of size n from the population has an equal chance of being selected',\n",
       " \" Well,  it's kind of complicated, but what it means is, is that if you use the proper approach  for simple random sampling, whatever sample you get, you could have had just as easily  a chance of getting another batch, another group of people from that sample\",\n",
       " \" In other  words, like, let's say you have a list of the population of students in the class\",\n",
       " \" So  I'm going to define a class as a population\",\n",
       " ' And you want to take a sample of five students  from this bigger class',\n",
       " ' If you take a simple random sample, it means that all the different  groups of five students you could pick from the list has an equal chance of being the  sample group you actually pick',\n",
       " \" Now, you can just imagine that if you race into the class  right at the beginning, and you take your sample of five and not everybody's in the  class, what does that sound like, right, a sampling frame problem, maybe an under coverage  problem, maybe biases creeping in there, right\",\n",
       " \" And so you just got to be careful, if you're  going to do simple random sampling, that you start with a list with everybody in your sample  frame, because every single sample that you could possibly take should have equal chance  of ending up being your sample\",\n",
       " \" And I'll kind of explain it by explaining the two different  methods that can be used of obtaining that  sample\",\n",
       " '  So one of the best things that you can do is just start with a really good list of all  the people in your population',\n",
       " ' So maybe, you know, if I was going to study, I used to work  at the army',\n",
       " \" So let's say I was going to study all the people who are active duty in the  US Army, I would like to get a list of all of those people from an accurate place at  the army\",\n",
       " ' And I would like to have them have a unique ID',\n",
       " ' Okay',\n",
       " \" And that's true in the  army, everybody in the army has a unique numerical ID\",\n",
       " \" So what I would do, like in here, if you  were looking at students, you'd take maybe take a student ID, so then you take the IDS  from everybody on the list, and you cut them up, like you print them out, and you cut them  up, and you put them in a hat, right, or a bag where you can't see in it\",\n",
       " \" And they mix  them all up where you can't see it\",\n",
       " \" And you draw five of them up, or like in the picture,  you know, what they did was mix up all those papers, and now they're not looking\",\n",
       " \" And they're  drawing a few out\",\n",
       " ' Okay, so what did you just do, you just made sure, first of all, that  everybody in the population had an ID number',\n",
       " \" And that when you printed it out and cut it  up, all, you didn't lose any of them, if you drop them on the floor, or something that's  not simple random sample, you got to make sure you keep all of them, and that you put  them all in the hat, and that you didn't look and you draw five or whatever, because then  any five of those slips of paper could have been drawn in there for your meeting with  simple random sampling\",\n",
       " \" Okay, that method will work, right? Another method that works,  that might work better if you can't do this ID thing where you cut a paper is where you  simply just make your own list of unique random numbers, right, you just make your own list\",\n",
       " '  And then you assign those to the population',\n",
       " \" A great example is if you're, you know, kind  of teaching kids and you want to put them in a random order, maybe you're gonna do a  game or something\",\n",
       " \" Well, all you do is you you get, like, let's say you have 10, kids,  you number one to 10, you put it in the hat, and then you pull out the first number, let's  say it's five, you give it to the first kid, right? And then you just keep pulling out  numbers and giving them to the kids and then tell them to stand in order, right? So you  generate a list of random numbers as long as the list of the population\",\n",
       " \" So I said,  What if you have 10 kids? Well, if you have, you know, 500 names, then you get 500 numbers,  and they don't have to be one through 500\",\n",
       " ' They just have to be unique',\n",
       " ' Okay, I like  smaller numbers',\n",
       " \" So I'd say keep them small, but you can do what you want\",\n",
       " \" And then, in  any case, you randomly assign these numbers, you can use the hat, I'm big on hats to this  population\",\n",
       " \" And then, you know, you ask them to stand in order, or somehow you figure out  it's kind of like a raffle you call out who's got number one, you know, and whoever says  yes, you're like, you're lucky you get to be in my study, you know, so you can take  the first five numbers in the order, right\",\n",
       " \" And that's, that'll achieve the same thing  as the last method, you'll get a simple random sample, it's just two different ways of doing  it\",\n",
       " ' So ultimately, being in a simple random sample means that the sample has an equal  chase chance of being selected out of the hat that this group of people or a group of  whatever has an equal chance of being selected',\n",
       " \" And you'll see this picture on the left here  is bingo, as some of you may play bingo\",\n",
       " ' You know, they pull balls out of there and they  call off the names of the balls',\n",
       " ' Well, each ball has a unique actually a letter and a  number unique on there',\n",
       " \" And that's how they make them random\",\n",
       " \" That's they take a simple  random sample of these bingo balls each time that they do a bingo game\",\n",
       " ' So I described  to you the first method of doing that using an old fashioned hat',\n",
       " \" The second method, you  know, where you generate your own numbers, and you just make sure they're unique\",\n",
       " ' And  then you assign them to things and put them in order',\n",
       " \" Well, that's my electronic hat\",\n",
       " \"  That's how I handle it\",\n",
       " ' If I have, for example, somebody sends me an Excel sheet with a list  of hospitals on it',\n",
       " \" I'll just assign each hospital random number and sort them in order\",\n",
       " \"  And I'll sample the top few hospitals\",\n",
       " \" That'll be how I get a simple random sample of possibles\",\n",
       " \"  That way, I'm not biased, picking out my favorite hospitals where all my friends work, right?  If I do it that way, the first method or the second method, all members of the population  have the equal probability of being selected in the sample\",\n",
       " ' And more importantly, all possible  samples, all possible groups had an equal chance of being selected',\n",
       " ' Of course, I only  did it once',\n",
       " ' So I only got one of them',\n",
       " \" But the other ones that weren't selected had an  equal chance of being selected\",\n",
       " \"  All right, you probably saw the limits, is this whole list? Even if I'm sampling hospitals,  right? I still need a list of hospitals to sample from\",\n",
       " \" So you may not know who's gonna  show up in the emergency department that day, if you do, while you're psychic, because most  people are not\",\n",
       " \" So how would you sample from them using simple random sampling? So simple  random sampling is okay, when you got a list like hospitals, but it's not so good when  you don't know who's going to show up that day\",\n",
       " ' And even if you do a simple random sampling,  you need a good list',\n",
       " ' I made a mistake once, where I did a survey with a bunch of professionals  using a professional society list',\n",
       " ' And when I sent out the survey, I learned that there  were people on the list who were no longer part of the society that it was an old list',\n",
       " '  And more importantly, there were people who had joined the society that had not made it  onto that list',\n",
       " ' So I was getting under coverage',\n",
       " \" So like, if you were doing a study with students,  you know, what if they just left off the part time students, then you'd be missing them\",\n",
       " '  So this is a great example of non sampling error',\n",
       " \" And so if you're going to do simple  random sampling, you do need a list and you really want to research it and make sure it's  the best list possible\",\n",
       " ' So I just went over the characteristics of simple random sampling,  and two different methods you can use from to sample from a list',\n",
       " ' And I also mentioned  the limits of it',\n",
       " \" Now we'll talk about a different kind of sampling, stratified sampling\",\n",
       " \" So  we're gonna go over what it is\",\n",
       " \" And then I'm just like, simple random sampling had all  these steps to it, there are different steps in stratified sampling\",\n",
       " \" And I'll give you  some examples\",\n",
       " \" And then of course, just like simple random sampling, this stratified sampling  has limitations, and I'll talk about those\",\n",
       " ' So I first wanted to just remind you what  the word stratified means, or what strata are, the single word is stratum, and more  than one a strata',\n",
       " \" Now you see that rock on the slide, you see that big, horizontal line  across it, that those that's a stratum, there are strata, right? Those are strata of rock,  if you stay geology, that'll the geologists will explain that where those breaks are,  it means something happened often in the weather or the environment or whatever\",\n",
       " ' But the reason  why I put this picture up there is I want you to sort of imagine those layers',\n",
       " \" Because  that's what we do in stratified sampling is first, we divide our list, of course, you  know, a list, we divide our list into layers\",\n",
       " ' Okay, so remember how I was just talking about  simple random sampling? Like, what if I sample from hospitals? Well, I could take this hospital  list and divide it until layers by for example, how close they are to the city, I could say,  urban, suburban, and rural, I could first put them into those strata',\n",
       " ' Okay',\n",
       " \" And if I  was doing that, I'd be doing stratified sampling\",\n",
       " \" Same with students, like I could put them  in, you know, first year nursing students, second year students, you know, and I'd have  this them divided into strata first\",\n",
       " \" Um, so this is what so why would you do that? Why  not just do simple random sampling? Well, if you think about it, let's say that you've  got a class like statistics, maybe a lot of you know, they're not that many first year  students in it\",\n",
       " \" So let's say the very small proportion is that way\",\n",
       " ' If you do simple random  sampling, you might just by lock miss all of them',\n",
       " ' Right',\n",
       " \" And so, if you're really concerned  about what a minority thinks, then you can make sure to get representative from that  stratum\",\n",
       " ' By doing stratified sampling, because the first thing you do is you put those that  list into groups',\n",
       " ' And then you take a simple random sample from each of the strata',\n",
       " \" So  here's the steps\",\n",
       " ' So step one, divide the entire population, the whole list you have  into distinct subgroups called strata',\n",
       " ' And remember, each individual has to fit into  one of those categories',\n",
       " \" So if you have somebody who's sort of halfway halfway between first  year and second year, or you've got a hospital that's kind of on the border, it you got to  choose, you got to put it in one of those groups\",\n",
       " \" Step two, um, well, it's not really  step two, but you've got to think about the strata like what is it based on, it's got  to be based on one specific characteristics, such as age income, education level, you know,  a great example is you could take people of all different incomes, right, that's a quantitative  variable, but you can put them in strata by you know, less than a certain amount\",\n",
       " ' And  then that to that, that to that you can make,  you know, four or five strata',\n",
       " ' And then, um, you know, you just want to make sure that  all members of the stratum, each stratum, share the same characteristic',\n",
       " ' And then you  could do step four, which is draw a simple random sample from each stratum',\n",
       " \" So like,  in the case where I was describing, like, maybe you have a class with very few first  year students, if you take a random sample of five from each strata, you know, each stratum,  then you might be, you know, you're kind of getting almost like, extra votes from a small  minority, right? Like, you're kind of treating them fairly, even though there's a way bigger  group of the other people you're taking exactly five from\",\n",
       " \" And, but you just that, that's  the risk you take, because you want to make sure you hear from that small group\",\n",
       " ' Because  if you just do sample random sampling with groups, so small, you might just accidentally  miss it',\n",
       " ' So here are some examples of stratified sampling',\n",
       " \" And you'll see this in the youth  Behavioral Risk Factor Surveillance surveys that they do in high schools, that they'll  stratify by grade, right, because if they did a simple random sample, you know, a lot  of students drop out of junior and senior year, they get probably too many freshmen  and sophomores\",\n",
       " \" And so they're gonna want to look at getting a certain amount of freshman  classes, certain amount of sophomore classes, certain amount of junior classes, student  run the senior classes, so they can have enough of each to make good estimates, right\",\n",
       " \" And  in hospitals, they often sample providers from each department, right? Like, they don't  just do a simple random sample of providers, if they're asking about like provider satisfaction,  or if you know about a policy, they won't just do that, because they might, for example,  Miss everybody in the ICU\",\n",
       " \" Or if you're studying, you know, ICU is you have multiple ICU is  there,  then  you would want to maybe stratify by ICU, just to make sure even if one of them's smaller,  just to make sure you have  a good,  good solid representation from each ICU\",\n",
       " ' So those are the reasons that push you to do  stratified sampling',\n",
       " \" It's not always necessary\",\n",
       " ' But when you have these situations where you  have these distinct groups, especially the little one involved, and you want to hear  from everybody, you really want to consider the stratified sampling',\n",
       " \" So of course, there's  limitations\",\n",
       " \" And I've been sort of leading up to this, what you end up doing is over  sampling, one of the groups usually, you know, like the smallest group, if you make the same  amount of people you take from that stratum, the same amount as you take from the big stratum\",\n",
       " \"  It's like the smallest group is having all these powerful votes and the biggest group  has is weaker, you know, they're both equal when they're not technically equal in the  population\",\n",
       " \" But that's the way it goes, right? And I do higher level statistics, there's  ways to adjust back for that, to just sort of say, take a penalty for that and go back  and say, Well, what if the real pot you know, we can extrapolate this back to the population  proportions? It's possible, but it's it takes some post processing is just the issue\",\n",
       " \" And  it's also like simple random sampling not really possible to do without a list beforehand\",\n",
       " \"  And it's also hard to do, because you actually have to split the list into groups into these  strata\",\n",
       " \" So let's say I had these hospitals and I didn't know where they were, I didn't  know exactly if they were urban or rural or suburban\",\n",
       " ' Well, that adds another level of  complexity to this whole stratified sampling',\n",
       " ' So, in summary, I just went over what stratified  means, and it means you know, putting things in groups and then taking from that, and I  describe the steps involved',\n",
       " \" And it's a stratified sample\",\n",
       " ' It goes a lot easily',\n",
       " \" A lot more easily  if the strategist happened to be equal to begin with, you know, I gave the example of  high schools, usually there's maybe slightly fewer people in junior and senior year, but  it's kind of close\",\n",
       " \" And it's always nice\",\n",
       " \" Like if you're comparing ice use, for example,  if the ice use are roughly the same size, because then you don't have to worry about  this whole, one of them is smaller, but it's getting an equal vote\",\n",
       " ' Already, now we are  going to move on to talk about systematic sampling',\n",
       " ' Okay, well, systematic sampling  actually can be done with or without a list',\n",
       " \" So it's a little more flexible than the kind  of sampling we've been talking about\",\n",
       " \" systematic sampling, it's easier for me to like, define  it by describing the steps you go through to do it\",\n",
       " \" So I'm just gonna explain how to  do it\",\n",
       " \" And then you'll understand, in fact, you'll understand why it's called systematic\",\n",
       " '  So whether you have a list or not, what you have to do for step one is arrange all the  individuals of the population in a particular order',\n",
       " \" Now, if it's a list, you just make  it in whatever order you want to make it in\",\n",
       " \" But if we're talking about, for example, patients  coming into the ER, well, they come in, in the order that they want  to\",\n",
       " \"  So they already are arranged in the list, right? You just don't know what that list  is\",\n",
       " ' Okay, then step two is pick a random individual as a start',\n",
       " \" So let's say I had a list of hospitals,  and let's say it was just sorted by state, right? I, let's say I picked a random individual,  maybe I went down, you know, seven on the list, and I picked that hospital\",\n",
       " ' Or maybe  you could be at the ER, you start your shift',\n",
       " ' And the seventh patient who is admitted to  the ER, you pick that person, just I picked seven, I mean, you could have picked five,  you could have picked 20, you know, just you pick a random person',\n",
       " ' Then the next step,  step three is take every case member of the population in the sample',\n",
       " \" Now, don't try this  in Scrabble case is not a word in Scrabble, okay? It's just a word and statistics ease,  in what case means spelled k th, it means every so many\",\n",
       " \" So let's pick a number and  fill it in for K\",\n",
       " \" So let's pick the number three\",\n",
       " \" So let's say after you pick your first  hospital from the list, or the first patient from the ER, it doesn't matter what number  you chose for that, then you take every third after that\",\n",
       " ' So every third patient that comes  in after that, you ask them if they want to be in a study, or every third hospital after  that original random one, I pick and I say, Okay, this is going to be part of my systematic  sample',\n",
       " \" So as you can see, it's like pretty simple to do, it's easy to do, if you have  a list, it's easy to if you don't have a list, it's just the deal is you have to pick K,  well, first you pick a random place to start, then you pick K, and then you just keep going  every so many\",\n",
       " \" So you could do this with classes, you could take out a list of classes available  at your college next semester, she pick a random number like three, you know, and it's  sorted some way\",\n",
       " ' So you go to the third class and you circle that, then you pick another  random number like five and then after that you pick every fifth class',\n",
       " ' So after the third  one, you go 45678, and then 910 11 1213',\n",
       " ' And you keep picking classes',\n",
       " ' Okay, this is not  career advice',\n",
       " ' Okay? Do not pick your classes that way',\n",
       " ' This was just an example',\n",
       " \" Alright,  so as you probably guessed, I'm going to be negative Nelly, again, there are problems  with systematic sampling\",\n",
       " ' If already things are set up, boy, girl, boy, girl, for example',\n",
       " \"  If you pick like an even number, you're going to get all boys are all girls, right? And  I noticed this actually, when I was doing a study in the lab, we wanted to study like  whenever they put the assay through the machines, we thought some of the assays weren't running,  right\",\n",
       " ' And so we wanted to take a sample',\n",
       " ' And I wanted to take a systematic sample',\n",
       " \"  But I wanted to take a systematic sample, like every seven days, and that's a week\",\n",
       " '  And so I asked my colleague, does the lab vary day by day in what assez it runs because  of it always runs the sexually transmitted disease assays, it saves them up and runs  them all on Friday',\n",
       " \" And I'm sampling from every Friday, that's all I'm gonna get, right?  That's actually called periodicity\",\n",
       " \" You don't have to remember that I don't think I've ever  even seen that written\",\n",
       " \" It's just I remember my lecture in my class telling us that that's  what you have to worry about with systematic sampling\",\n",
       " \" It's not real common problem, though\",\n",
       " \"  But what's awesome about it is you can do it in a clinical setting\",\n",
       " ' So you You can sample  patients that way, coming into a clinic or coming to a central lab or like in the emergency  room',\n",
       " \" And that's why this is a particular power, particularly powerful way to sample  is that if you have an ongoing sort of patient influx, when you design your research, you  could simply say, once you decide how many people you need to recruit for your sample,  that you would use systematic sampling, and just have somebody in the clinic inviting  every case person who qualifies every case patient who qualifies into your study\",\n",
       " \" So  it's easy to do systematic sampling, it's easy to do with or without a list\",\n",
       " ' And you  just pick a random starting point, and then you pick every case individual',\n",
       " \" Next, we're  gonna move on to cluster sampling\",\n",
       " ' So what is up with cluster sampling? Why do we need  even other kinds of sampling? I just went over so many kinds',\n",
       " ' I mean, you could use  stratified systematic or simple random sampling, why would you even need another kind? Well,  cluster is very special',\n",
       " \" It's special, because it's the kind of sampling you use when you  think there's a problem at a particular geographic location\",\n",
       " \" Typically, that's how cluster sampling  is used\",\n",
       " \" And, and I'll explain it further\",\n",
       " \"  Imagine, for example, there's a particular factory that's is believed to admit fumes  that cause problems with people's health\",\n",
       " \" Well, you can't do simple random sampling  all over the nation, right, or you won't even get people by that factory, can't really do  easily do stratified or systematic sampling their cluster sampling is what's designed  when you want to study something that's coming from a geographic location\",\n",
       " ' So when you do  cluster sampling, you start by dividing a map into geographic areas',\n",
       " \" So I'm from Minnesota,  and I know that there was a mine there with vermiculite in it\",\n",
       " ' And it was it was contaminated,  a lot of people got sick from it',\n",
       " \" But they didn't know that's what was going on\",\n",
       " ' So they  first I think divided Minnesota into different geographic areas, areas',\n",
       " ' after dividing the  area into these different geographic areas, some with the, with the bad thing in it, and  some without the bad thing in it, you randomly pick these clusters or areas from the map',\n",
       " \"  So the app, like if you'll see there on the screen, there's a map of the state of Virginia,  and it's all been divided into different groups\",\n",
       " \" And then this, this cluster is is highlighted,  you usually probably pick more than one cluster, sometimes it's only four or five\",\n",
       " \" But the  idea is you try to enroll all of the individuals in the cluster, it's usually people, although  you can do it with animals, if there's a disease going around among animals, you know, you  would have these, the divide the area up into clusters, and then you try to measure all  the animals in the cluster\",\n",
       " \" So as you can imagine, not only is this sort of practically  difficult, but there's reasons why people live together, right? People live in communities\",\n",
       " \"  I mean, people don't just randomly scattered themselves, you know, cultural communities  grow\",\n",
       " ' companies grow around art, you know, affluent communities have different people  in them, then communities that have less money',\n",
       " ' So sometimes the people located in the cluster  are all similar in a way that makes the problem hard to study',\n",
       " \" And this is, especially if  you're studying some geographic thing, like maybe a factory or a sewage plant, that you  think might be causing cancer, if you're in an area where there's a lot of pollution anyway,  from other things, and a lot of low income people live there\",\n",
       " \" Because if you're high  income you can afford not to, well, they're already being exposed to higher rates of carcinogens  and probably have a higher cancer rate\",\n",
       " \" It's hard to tell what the independent effect might  be of that thing in that geographic location because of the other similarities of the people  around\",\n",
       " ' And so this is cancer ends up being a really difficult, tough nut to crack',\n",
       " \" Because  where we see high rates, there are often a lot of different geographic issues going on  there in cluster sampling doesn't really help tease  that out\",\n",
       " '  So to wrap this up, cluster sampling is used when geography is important',\n",
       " \" So if there is  something geographically located in a certain spot and you can't move it, then you kind  of are stuck doing cluster sampling\",\n",
       " ' So briefly, the map around that areas divided into different  sub areas, right',\n",
       " ' And those are Not all the areas are picked, just a few are randomly  picked',\n",
       " ' And then all of the people in that particular area are sampled',\n",
       " \" And of course,  it's biased towards the people living in the area\",\n",
       " \" If you you know, in the area you pick  with a bunch of affluent people, you'll get affluent people pick an area with a bunch  of immigrants, he'll get immigrants\",\n",
       " \" And so a cluster sampling is not perfect, but you're  kind of stuck with it\",\n",
       " \" When there's a situation with geography, how long it was, remember  it is, when I used to live in Florida, we'd like to drive up to Georgia because they had  the best pecan clusters\",\n",
       " \" That's like a type of dessert with pecans and Carmel and stuff\",\n",
       " \"  So when I think of cluster sampling, I think of those pecan clusters that they're only  really good in Georgia\",\n",
       " \" So that's my way of remembering that cluster sampling has to do  with geography\",\n",
       " \" Now I'm finally going to talk about the last two types of sampling that  I'm going to cover in this lecture, convenience sampling and multistage sampling\",\n",
       " \" They're  both a little quick, so I'm going to just cover them quickly\",\n",
       " \" First, we're going to  start by talking about convenient sampling\",\n",
       " \" And we like that name, right? It's convenient\",\n",
       " \"  Convenient sampling can be used under low risk circumstances, like if the findings of  what you're doing aren't really that important\",\n",
       " \" Like, for instance, let's say that you wanted  to know what ice cream is the best from the restaurant next to the hospital, let's say  a new restaurant opens up, and you're gonna go off your diet, you're gonna go get some  ice cream, but you don't want to waste it right\",\n",
       " \" So you want to ask people, what's the  best one, you might ask your coworkers, you might ask, you know, the people at the restaurant,  hey, what's the best ice cream, but the results are not so reliable, because you might end  up on Yelp and see that other people disagree\",\n",
       " ' So a convenient sampling is basically using  results or data that are conveniently or readily obtained',\n",
       " \" And my master's degree, one of the  things I did was I surveyed people anonymously who were coming to a health fair, I sat at  a booth, and I gave them the survey, to view questions in it\",\n",
       " ' That was definitely a convenient  sample, you know, just people showing up for the health fair',\n",
       " \" And this can be useful when  there's not a lot of resources allocated to the study, like, I was a starving master's  student, right, like, I didn't have any money\",\n",
       " ' So that that was perfect for me convenience  sampling',\n",
       " ' And also, you know, the questions I was asking them about were just characteristics  of whether or not they had risk for diabetes',\n",
       " \" Well, I'm not a doctor, and I wasn't going  to do anything about it\",\n",
       " ' But it was interesting',\n",
       " \" So it wasn't a very high risk survey to fill  up\",\n",
       " ' It and convenience sampling is convenient, because it uses an already assembled group  for surveys like I was doing at the health fair',\n",
       " \" An example might be to ask patients  in the waiting room to fill out a survey or ask students in a class, you know, sometimes  I do when I'm teaching, I'll do a convenient sample of whoever sitting there\",\n",
       " \" I'll say,  Hey, is the homework that I signed you this week too hard?  Well, it's always too hard\",\n",
       " \" I  don't even know why I do the survey\",\n",
       " \" But anyway, um, sometimes as a teacher, you'll just want  to do a convenient sample just to get the gauge on where the classes but there are problems  with it, right? You can't just use it for everything, even though it's nice and convenient\",\n",
       " \"  There's bias in every group, right? So if I let everybody go on break, and then whoever's  still sitting there, I asked them a thong works too hard, I might get a totally different  answer than if I waited for everybody come back\",\n",
       " ' Right',\n",
       " \" And, you know, just about any  time you just waltz into a room, like when I went to the health fair, who do you think,  is there a bunch of sick people? No, there's a bunch of health minded people there\",\n",
       " \" And  so I'm gonna get a bunch of bias, right\",\n",
       " ' And also, more importantly, when you do convenient  sampling, you often miss important subpopulations',\n",
       " \" So remember, stratified sampling, how sometimes  people don't group evenly into the different strata? Maybe they do kind of in high schools,  but especially when it comes to job classifications, they usually have fewer bigwigs than they  do\",\n",
       " ' lackeys, right',\n",
       " ' And if they just have a few bigwigs, if you do a simple random sample,  you you might miss all of them',\n",
       " ' So maybe you try a stratified sample',\n",
       " ' On the other hand,  if you walk into the break room that is used by the lackeys and you say, hey, I want to  fill out my, you know, work satisfaction survey',\n",
       " \" All of the ones you're going to get are going  to be from the lackeys, you're not going to get any representation from the upper job  classes because they don't go in that lounge, so you'd be missing them\",\n",
       " \" So that's the main  problem with convenience sample is the results can be so severely biased because you're only  asking the small, biased group of people that probably are all alike in some way\",\n",
       " \" It's not  very representative sample\",\n",
       " \"  Next,  I'm going to talk about multi stage sampling\",\n",
       " \" So, you know, if you have a kid and the kids  crying somebody like What's up, you say, well, the kids going through stage as well\",\n",
       " \" That's  exactly what you're doing when you're doing multi stage sampling, as you're going through  stages\",\n",
       " \" It's basically like mixing and matching, the different sampling I just talked about,  only you do one stage, and then two stages, and then three stages, and then four stages,  or maybe even more\",\n",
       " \" And that's how you get your sample\",\n",
       " \" So if you're imagining why I  got to start with a lot of people, you're probably right, I just gave an example I made  up of a way that you could do multistage sampling is you could start one with stage one as a  cluster sample, right? Remember, where you take out a map, and then you divide into areas?  Well, let's divide into states and take two census regions of states like about 10 states  from those clumps\",\n",
       " ' Okay, now, we limited it to that',\n",
       " \" Now let's go to stage two of our  multistage sampling\",\n",
       " ' Now, from each of those, we could take a random sample of counties,  right',\n",
       " ' So we go and look at all the counties and then take that random sample',\n",
       " ' Then after  we get those counties, stage three, we could take a stratified sample of schools from each  county',\n",
       " ' So some of the counties will be totally rural, some will be totally urban, but most  will have some mix',\n",
       " \" So we'll take a look at a few schools from the urban a few schools  from the rural in stage three from the stratified will tell you a stratified sample schools  from the simple random sample of counties from this cluster sample of states\",\n",
       " ' Okay,  now we got our schools, stage four could be a stratified sample of classrooms',\n",
       " ' So once  we figured out our urban schools or rural schools, we could go in there and look at  all the classrooms, freshman, sophomore, junior senior and take a stratified sample of those',\n",
       " \"  So it's basically mixing and matching\",\n",
       " \" But you're right, you got to start with a lot  to begin with, if you're gonna whittle it down, and a whole bunch of stages, doesn't  have to be four I just gave you for\",\n",
       " \" Now I'm going to give you a real life example\",\n",
       " ' This  is the National Health and Nutrition Examination Survey',\n",
       " \" And Haynes definitely not a Master's  project\",\n",
       " ' This is done by the Centers for Disease Control and Prevention at the United States,  right',\n",
       " \" So what I'm kind of hinting towards is the kinds of places doing multistage sampling  our governments, not only do you have to start with a whole bunch of people and things and  individuals, states and schools, and what have you, right, is that it's a lot of work  to do all the sampling, and it better be for good reason\",\n",
       " ' And the National Health and Nutrition  Examination Survey is a good reason',\n",
       " \" That's, that's a survey that's done by the CDC to  try and measure America's Health\",\n",
       " \" Of course, it's doing inferential statistics, right,  it's taking sample and trying to extrapolate that information back to the population\",\n",
       " \" And  so it's got to be really careful about how it does a sampler you can't just waltz in  and do a bunch of convenient sampling\",\n",
       " ' So this is how it does it, just briefly, they  start by in stage one, sampling counties',\n",
       " \" Then from those counties, they sample something  called segments, which is defined in the census, it's their different areas, from those segments,  those areas, they sample households\",\n",
       " \" And that's what they mean, like, wherever you live as  a household\",\n",
       " \" Even if you live in a dorm, that's a household or you live in assisted living,  that's a household\",\n",
       " \" I'm an apartment building house\",\n",
       " ' So they sample those and once they  knock on your door of your household, they sample individuals from the house',\n",
       " ' So they  use four stages of sampling',\n",
       " \" And that's a real life example of multi stage sampling\",\n",
       " \"  So in summary, convenience and multi stage sampling, with respect to convenience sampling,  you want to avoid it unless it's really a low risk question you're asking about\",\n",
       " \" And  you also want to avoid it unless it's really the only type of sampling possible under the  circumstances\",\n",
       " ' When you have situations where you have patients with very rare disease,  probably convenience sampling from your Rare Disease clinic is reasonable',\n",
       " \" There, it's  also used when resources are low\",\n",
       " ' And so those are a few good reasons to try to use convenient  sampling',\n",
       " \" It's really something that you want to use only if it's the thing  you're stuck with\",\n",
       " \" It's much better to look towards these other sampling approaches I  described\",\n",
       " ' And then finally, multistage sampling is usually used in large governmental studies',\n",
       " \"  So don't expect to actually design anything alone with multistage sampling\",\n",
       " ' When that  happens, I showed you those four things for that survey that the CDC does hundreds of  people work on that even just a sampling tons of people work to try and set that up',\n",
       " \" It's  very difficult\",\n",
       " \" But I wanted you to know about that kind of sampling, because it's important  in healthcare, and it happens a lot\",\n",
       " \" So in conclusion, we made it through the sampling  lecture didn't wait\",\n",
       " ' I first started by describing some definitions, you needed to be able to  understand all these different types of sampling',\n",
       " ' Then I went into simple random sampling, and  showed you how to do it two different ways and what it achieves and also its limitations',\n",
       " '  We next talked about stratified sampling, why you do that and how you do that, and the  limitations of that one, too',\n",
       " ' Then we got into systematic sampling, which is a little  more flexible, and pretty easy to explain',\n",
       " ' Next, we talked about cluster sampling, and  why you might need to pull that tool out of your sampling toolbox',\n",
       " ' And then finally, we  covered convenient sampling and multistage sampling',\n",
       " ' Already',\n",
       " ' Well, I hope you better  understand sampling now and can keep all of these different types of sampling straight  in your mind',\n",
       " \" Hello, everybody, it's Monica wahi labarre\",\n",
       " ' College lecture for statistics  are on to Section 1',\n",
       " '3',\n",
       " ' Introduction to experimental design',\n",
       " ' And here are your learning objectives',\n",
       " '  So at the end of this lecture, you should be able to first state the steps of conducting  a statistical study, and then select one step of developing a statistical study and state  the reason for the step, you should be able to name one common mistake that can introduce  bias into a survey and give an example should be able to explain what a lurking variable  is, and give an example of that',\n",
       " ' And you should be able to define what a completely randomized  experiment  is',\n",
       " \"  So let's get started\",\n",
       " ' This lecture is in a cover four basic topics',\n",
       " \" First, we're going  to look at the steps to conducting a statistical study, you may think there's a lot of steps  to conducting a study, this is from the point of view of the statistician\",\n",
       " \" Okay? Then we're  gonna go over basic terms and definitions\",\n",
       " \" And by now, you're probably used to the fact  that in statistics, certain words are reappropriated\",\n",
       " ' And they mean something specific in statistics',\n",
       " \"  So we'll talk about that\",\n",
       " \" Then we'll talk about bias and what that is and how to avoid  it in when designing your studies\",\n",
       " \" Finally, we'll talk about randomization in particular  topics you need to think about when thinking about randomization\",\n",
       " \" So let's get started\",\n",
       " \"  We're going to start with, of course, basic terms and definitions\",\n",
       " \" And so first, we're  going to review these steps that I keep talking about to conducting a statistical study\",\n",
       " \" But  there's some vocabulary, vocabulary that comes up\",\n",
       " \" And so we're going to talk about those  vocabulary terms that come up\",\n",
       " \" And then also, I'm going to give you a few examples from  healthcare\",\n",
       " ' So here are the steps I keep talking about',\n",
       " ' So these are the basic guidelines for  planning a statistical study',\n",
       " ' So the first thing you want to do is state your hypothesis',\n",
       " \"  And you know, I'm in a scientist a while now\",\n",
       " \" And I can't tell you how many times I get  in a group of us, and people are all curious, and they start thinking about let's do a study\",\n",
       " \"  And it's only halfway through our conversation that I suddenly say, Hey, wait a second, we  don't have a hypothesis, what's our apotheosis? So it's easy, even for scientists to forget  that that's really step one, is you have to have a hypothesis\",\n",
       " ' And so whatever hypothesis  you pick, the hypothesis is about some individuals, if I have a hypothesis about hospitals, those  are the individuals I have a hypothesis about patients',\n",
       " ' Those are the individuals',\n",
       " \" But it's  important actually, to nail that down\",\n",
       " \" Because am I talking about patients in the hospitals?  Or am I talking about the hospitals, so make sure that you understand after you, you know,  percolate and decide on your hypothesis, who the actual individuals of interest are? And  that's because you're going to have to marry measure variables about these individuals\",\n",
       " \"  So step three is to specify all the variables you're going to need to measure about these  individuals\",\n",
       " ' You know, and of course, they relate to the  hypothesis',\n",
       " \"  So it's good thing is that was step one, right? Step four is to determine whether you want  to use the entire population in your study or a sample\",\n",
       " ' If you already have a bunch of  data like you have the census data you You might as well use the entire population',\n",
       " \" But  typically, if you don't have the data, you're going to want to sit down and think about  using a sample\",\n",
       " \" And if you do that, while you're sitting down, you should probably also  choose the sampling method on the basis of what I talked about in the sampling lecture\",\n",
       " \"  Now that you've figured out your hypothesis, you got your individuals, you figured out  your variables, and you figured out whether you're going to do a census or a sample, if  you're going to do a sample what type of sample Step five is you think about the ethical concerns  before data collection\",\n",
       " \" If you're going to be asking some sensitive questions, you think  about privacy, if you're going to be doing some invasive procedures, you think about  how painful that would be, and how hard that would be on somebody, especially if they're  not even, you know, it's they're just healthy\",\n",
       " \" And you're just doing an experiment of unhealthy  people just to better understand biology\",\n",
       " ' So you have to really sit down and think about  these ethical concerns',\n",
       " ' And they may change slightly your study design',\n",
       " \" Finally, after  you get steps one through five, are taken care of, that's when you actually jump in  and collect the data\",\n",
       " ' And like I was saying, you know, when I meet with my scientist, friends,  we get all excited about an idea',\n",
       " \" We're often talking about Step six, we're like, oh, we  should do a survey, we should this we should that\",\n",
       " \" And I realized I ended up saying, Hey,  we actually have to go back to step one and start talking about a hypothesis, because  I suddenly realized, I don't even know what data to collect, right? If you don't go through  the steps in order, you really aren't doing it right\",\n",
       " ' Step seven, is after you get the  data, you finally use either descriptive or inferential statistics to answer your hypothesis',\n",
       " \"  And that's what statistics is about\",\n",
       " \" It's here for that\",\n",
       " \" And then finally, after you  use the statistics, you have to write up what you find, even if you're at a workplace\",\n",
       " ' And  they asked you to do a little survey that happened once when I was working somewhere',\n",
       " '  And they wanted us to do a survey',\n",
       " \" Their hypothesis was that they didn't have enough leadership  programs, and they weren't building good leaders they could promote\",\n",
       " \" And so I was on a team  that did the survey, we didn't, you know, really publish it, like, everywhere\",\n",
       " ' But we  made an internal report, right',\n",
       " ' And in that internal report, we had to do step eight,  which we had to note any concerns about data collection or analysis, you know, that happened  when we were doing a report',\n",
       " ' And we also had to make recommendations for future studies,  or if you wanted to study this in future groups of employees',\n",
       " ' So in science, what it usually  ends up being is a peer reviewed literature report, right? is you do a scientific study,  maybe you get a grant',\n",
       " ' And then you do all these steps',\n",
       " ' And then step eight is where  you actually prepare a journal publication',\n",
       " ' And in that, you have to note any concerns  about your data collection or analysis, anything that might have gone wrong, or not gone exactly  the way you planned, or something you need to take into account to really properly interpret  what the study found',\n",
       " ' You also want to make recommendations for future studies, especially  if you screwed something up, or especially if you answered a really good question',\n",
       " \" No  reason to per separate on that question, why don't we move forward and ask the next one\",\n",
       " '  Now, these are a lot of steps to remember',\n",
       " \" So I'm going to help you try to remember them  in sort of clumps\",\n",
       " \" So let's look at the first clump, which are steps one through three,  which is data hypothesis, identify the individuals of interest, and specify the variables to  measure\",\n",
       " \" So let's give an example of that\",\n",
       " \" So let's say our hypothesis was air pollution  causes asthma, and children who live in urban settings\",\n",
       " \" You know, that's how we'd stated  or we could say that as a research question, like does air pollution cause asthma in children  who live in urban settings\",\n",
       " \" And so in that case, the individuals would be children in  urban settings, and the variables we'd have to measure our air pollution at least, and  asthma at least\",\n",
       " \" And of course, we'd want to know more things about these individuals,  these children, we probably measure their income and where exactly they were living,  and how old they were, and if they're male or female, and these kinds of things, but  that just kind of helps you think about the first three steps together\",\n",
       " \" Now let's think  about the second three steps together four, five, and six, which is determine if you're  going to use a population or sample If it's sample, pick the sampling method, look at  the ethical concerns and then actually collect the data\",\n",
       " \" So, when you do that, you can either  quote unquote, collect data, you know, like, by using existing data by downloading data  from the census, or like Medicare, they have data sets available that are, are de identified,  so you don't know who exactly is in there\",\n",
       " ' Or you can collect data yourself, like do  a survey or, you know, get a bunch of patients that will allow you to measurement',\n",
       " ' When you  use it, a government data set, often you can make population measures out of it',\n",
       " \" And so  you don't really have to go through a lot of sampling, or ethics, because they've already  provided it for you\",\n",
       " \" And it's confidential\",\n",
       " \" And that's kind of your data collection\",\n",
       " \" But  most of the time, what you'll see, especially for studying patients, and treatments, and  cures, and things like that, those are on a smaller scale\",\n",
       " ' So you end up collecting  data from a sample for those estimates',\n",
       " ' And again, you need to choose a sampling approach',\n",
       " '  And then you need consent, if legally found to be human research',\n",
       " \" So I just want to share  with you in case you didn't know, if you want to go do research on humans, you're a nursing  student, or your medical students or a dental student, any any students or or your dentist,  your physician, whatever, a nurse, you can't just make up a survey, or study design and  go out and do it, you have to get approval from an ethical board\",\n",
       " \" And that ethical board  will talk to you if what you're doing is considered li li human research, that you need to get  consent from the patients or the participants in your study if they're humans\",\n",
       " \" And if you're  collecting data about children, for example, you have to get the consent of their parents  and the assent of the children\",\n",
       " \" And in the United States, that way, we have a setup,  it's called an institutional review board for the protection of human subjects and research  or the short answer is IRB\",\n",
       " \" And so I just want to make sure that if you ever do design  a study that you know about this IRB thing, and you realize you have to go through this  ethical board and make sure that they're cool with it\",\n",
       " ' Before you can move on to the next  step of designing a statistical study',\n",
       " \" All right, finally, we're on to the last clump  of steps, which is seven, and eight, right? So that's using descriptive or inferential  statistics to answer your hypothesis you in six, you collected the data\",\n",
       " \" Now we're going  to do the statistics\",\n",
       " ' And then step eight is noting any concerns about your data collection  or analysis and making recommendations for future studies',\n",
       " \"  So you can kind of imagine this is where we're sitting in our offices, and writing up our  research, whether we're writing an internal report to our bosses, over writing for the  scientific literature to publish for everybody\",\n",
       " ' So at this point, I just want to remind you  that it matters whether you picked a census or a sample, for your study design',\n",
       " \" Because  if you pick the census, you're going to do a certain kind of analysis\",\n",
       " \" And if you pick  the sample, you're going to do a different kind of analysis and statistics\",\n",
       " \" So again,  that's all kind of cycles back to your study design\",\n",
       " \" And what's important here is I want  to talk to you about the two different main types of studies\",\n",
       " ' Now within these two categories,  you have different subtypes',\n",
       " ' But these are the two main types that you can have',\n",
       " ' The  first is called an experiment',\n",
       " ' experiment is where a treatment or intervention is deliberately  assigned to the individuals',\n",
       " \" So you can kind of imagine that if you enter a study, and  they assign you to take a drug in the study that you weren't taking before, that would  be an experiment\",\n",
       " ' But another thing could happen',\n",
       " ' I mean, you could do this to individuals,  you could do it to animals, but you could do it, I keep getting the example of hospitals,  we could choose some hospitals and say, Hey, you need to try a new policy as the intervention  and and that was assigned by the researcher',\n",
       " ' So that makes this an experiment',\n",
       " ' And the  reason why we have experiments is sometimes you need them',\n",
       " ' The purpose is to study the  possible effect of the treatment or the intervention on the variables measured',\n",
       " \" And so that's one  option you can do is have an experimental study where the researcher assigns the individuals  to do certain things in the study\",\n",
       " \" There's another kind of study The other kind, which  is called observational, and the way you can think about it is in experiments, the researcher  does something, they intervene, they give a treatment, right? But an observational,  the researcher doesn't do that the researchers just observes\",\n",
       " \" So, if you enroll in the study,  and you say, Do I have to take a drug? Am I supposed to eat something? What am I supposed  to do? And the researcher just says, No, we're just going to measure you, we're just going  to ask you questions, and we're going to measure things about you, we're not going to tell  you to do anything different, then you're in an observational study\",\n",
       " ' So no treatment  or intervention is assigned by the researcher in an observational study',\n",
       " \" Now, let's say  you're taking a drug, you know, just because maybe you have migraines, you're taking a  migraine drug, well, you just keep taking it, or you can stop taking it, you know, they  don't care, they might ask you about taking the drug, but they're not going to assign  you to take it\",\n",
       " \" It's an observational study\",\n",
       " ' I wanted to give you a couple of real life  examples',\n",
       " \" So Women's Health Initiative up on the slide was mainly an experiment, okay\",\n",
       " '  This is was run by the United States government, but of course, had the cooperation of many,  many universities and, and health care centers, and most importantly, women',\n",
       " ' So women in America,  women who were postmenopausal, volunteered to be in the study',\n",
       " ' And the study actually  had two separate sections, the experiment section, and the observational study section',\n",
       " \"  They really wanted women to qualify for the experiment, and that the purpose of the experiment  was to study whether hormone replacement therapy, which is a therapy for symptoms that women  can get if they're postmenopausal, that are unpleasant\",\n",
       " ' What whether that therapy is good  for women, or bad for women, because they thought maybe it helps them the post menopause  system symptoms',\n",
       " ' But they thought maybe it causes cancer, right? So they know',\n",
       " ' So what  they had to do was assign, get a bunch of women who were agreeing, you know that they  would take whatever was assigned to them',\n",
       " ' And they had to assign the drug to some of  these women',\n",
       " \" So that's what made an experiment\",\n",
       " ' The problem is not all the women qualified  for the study',\n",
       " ' So they had a separate observational study, if if the woman did not qualify to  get the experimental drug assigned to her, then she could be in the observational study',\n",
       " '  And because this is these big government studies, why not, you know, somebody wants to be in  a study, why not study them, just put them in the observational section',\n",
       " '  A very huge, popular long, ongoing study',\n",
       " \" That's an observational study, again, run  by Well, this one actually started out of Harvard\",\n",
       " \" And that's called the nurses Health  Study\",\n",
       " \" Some really smart person figured out a long time ago, that nurses are, are smart  people, they understand their own health, they understand other people's health\",\n",
       " \" And  they're good at filling out surveys about health\",\n",
       " \" So they started studying nurses and  regularly sending them surveys, of course, they didn't tell the nurses what to do\",\n",
       " \" They  didn't assign the nurses any sort of drug to take or any diet or intervention or anything\",\n",
       " '  They just observe the nurses, they send the nurses a survey, and about the nurses health,  and then the nurse vault fills out that information',\n",
       " \" I think it's every two years that they do  that,  they're still doing it\",\n",
       " '  Also, at this point, I do want to point out the concept of replication',\n",
       " ' So just the word  replication, right, regular speaking means to copy, right? Like, if you ever, you know,  have a new roommate, you might need to replicate your key',\n",
       " ' So you have a copy of the key for  the new roommate? Well, part of the whole science thing is that studies must be done  rigorously enough to be replicated',\n",
       " ' So those are little keywords in there',\n",
       " \" A rigorous study  means one that's done really carefully, like thinking about sampling very carefully\",\n",
       " ' You  know, like avoiding, for example, non sampling error not being sloppy, not getting a lot  of under coverage, using a good sampling frame',\n",
       " \" You know, I'm just giving you examples that  you might know about\",\n",
       " \" But there's a lot of things that have to be done in research to  do it properly\",\n",
       " \" It's just like driving or anything else\",\n",
       " ' You really have to keep your  eye on a lot of different things and you want to try to do them perfectly',\n",
       " ' And the main  reason why you want to do that is so if somebody tries to do this same experiment you did or  roughly the same experiment you did',\n",
       " \" Because you can't do exactly the same, right? If I  study this hospital over here, and somebody wants to study that hospital over there, well,  they're going to get different people in there, right? But even so if that person decides  that they want to study that hospital over there, if I did my study rigorously, then  it won't be so hard for that person to replicate how I did the study\",\n",
       " \" And then we can see if  that person and my study if we get the same thing, or if there's something slightly off  or what's going on\",\n",
       " ' And so replicating the results of both observational studies and  experiments, is necessary for science to progress',\n",
       " \" So you'll know that a lot of experiments are  done on drugs, before they can be approved to be given to everybody, because they can't  just do one study, they have to replicate it, to make sure that the findings are all  sort of coming in about the same and that we can deduce some information about it, you  really just don't want to rely on one study for your findings\",\n",
       " \" So I just went over several  steps that we need to follow when we're doing a statistical study, and we actually have  to follow them in order\",\n",
       " \" And you also have to determine the type of study you're doing,  you know, is an experiment, or observational study\",\n",
       " \" And there's a ton of study decisions  you have to make\",\n",
       " ' So you got to keep that in mind',\n",
       " \" Now, we're going to talk about avoiding  bias in specifically survey design\",\n",
       " ' Now, you can do a lot of different kinds of studies',\n",
       " \"  But let's just talk about surveys, because that happens a lot in nursing\",\n",
       " ' Nurses interact  with patients a lot, and with the community with each other',\n",
       " ' And often they gather information  about those interactions or attitudes or, or how the healthcare system functions by  using a survey',\n",
       " ' So surveys can provide a lot of information and useful information',\n",
       " \" But  it's important that all aspects of survey design and administration when you're giving  it, you got to think about minimizing bias and try you know, try to get a representative  sample trying to get accurate measurements\",\n",
       " ' And so several considerations should be made',\n",
       " '  When you want to think about non response and also voluntary response, okay, so I talked  a lot about sampling in the previous lecture',\n",
       " \" But just because you invite someone to participate  in your study, like maybe you're doing systematic sampling, and every third patient, you asked,  Would you like to fill out a survey? That doesn't mean they're going to, right? And  so if that person says no, thank you, even though there were a sample, that's called  non response\",\n",
       " \" So if I was helping you with a survey, and you said, Hey, I was getting  a lot of non response, I would look at the proportion if you approach 200 people, and  80 said, No, you know, that's only a 20% response rate and an 80% non response rate\",\n",
       " ' if many  people are refusing your survey, the few who actually completed are likely to have a biased  opinion',\n",
       " \"  I've noticed this at in in situations where things are really bad, okay\",\n",
       " ' Like, I remember  going to a subway station and it was flooded, and it was really in a bad situation',\n",
       " ' And  there was a man handing out surveys from the Transportation Authority',\n",
       " ' And he was like,  please take my survey, please take my survey',\n",
       " ' And everybody was waving past him',\n",
       " \" They didn't  want to grab a survey\",\n",
       " ' While you know me, I got a bleeding heart for surveys',\n",
       " ' So I took  his survey, and I filled it out',\n",
       " ' You know, I think the transportation authorities not  so bad',\n",
       " \" Right? I lived in Florida, there's no transportation there, right? So and here  in Massachusetts, we got a great transportation system, even if it's flooded or doesn't work  half the time, right\",\n",
       " \" It's way better than not having one\",\n",
       " \" Well, I'm not the only one  who grabbed a survey a bunch of nice Pollyannas, like me grabbed a survey\",\n",
       " ' So probably the  Trent Transit Authority thinks that everybody loves the subway when everybody was waving  past this poor guy because they were so disgusted, because the station was flooded',\n",
       " '  Right? So if so many people are refusing your survey, a high proportion, the feebly will  actually fill it out are going to be kind of weird, probably like me',\n",
       " \" You know, you're  gonna get a bunch of happy people when most of the people who said no might be sad people\",\n",
       " '  And so, the reason they may not be completing your survey has may have to do with how they  feel about your topic',\n",
       " ' This is not just in terms of satisfaction',\n",
       " \" Let's say you want  to talk about how many drinks per night somebody has\",\n",
       " \" Okay? Do you think a lot of people who  are struggling with alcoholism are gonna want to fill out that survey? You know, how about  illegal drugs or other illegal activity, people who are into that they don't always feel so  good about talking about it\",\n",
       " ' And so, you know, you might get a few people to fill out your  survey, but those are not necessarily the people who are engaging in the behaviors',\n",
       " '  So the fact that we have the freedom to choose whether or not we want to be in a survey is  great',\n",
       " ' But from a researcher standpoint, is you have to be careful',\n",
       " ' If you get low response  rates, you need to ask yourself who was not responding? And, you know, am I missing a  good share of opinion there? And then, when you get people who do respond, you got to  be careful with that two, respondents may lie on purpose',\n",
       " \" If you've got a pretty cool  survey, but you suddenly ask a question, that's too personal\",\n",
       " ' People might just lie',\n",
       " \" If you  ask, maybe a students you're doing a sin, you know, maybe satisfaction survey with how  the front desk runs at a dorm or something\",\n",
       " \" If you, you know, ask a question, have you  ever cheated on a test? You know, my, everybody's probably gonna say no\",\n",
       " \" Also, if you ask a  question where people don't really know the answer, offhand, they're not gonna put it\",\n",
       " \"  Like if you ask somebody, you know, when you're, you know, you asked a kid who's been living  in the house forever, when your parents bought the house? How much did it cost? I mean, they're  not gonna know\",\n",
       " \" Maybe they'll know, but probably not\",\n",
       " \" And so you want to be careful when you  design your questions that you're not asking anything that's so personal, everybody's in  lie about it? Or that you're not asking a question, then you would have Trump people  try to be accurate, they're probably not even give you the right answer, because it's just  too hard to think about\",\n",
       " ' Um, respondents also to, you know, to surveys may lie without meaning  to, like, inadvertently',\n",
       " \" Again, if you ask a question about something that happened really  a long time ago, they're not probably going to get it right\",\n",
       " \" This is called recall bias,  like you can have you can you know how, like, you can look back at a time in your life,  like, especially if you went through something really harsh, like if you were a part of a  sports team, and you went to state and it was really tough that you don't remember the  tough part, right? You sit around singing, you know, your sports songs, and you say,  Hey, that was awesome\",\n",
       " \" Well, that's recall bias, right? Because after winning state,  everything looks rosy\",\n",
       " \" But, you know, on the bus, there really wasn't that easy\",\n",
       " \" So people  tend to have recall bias, it's influenced by events that have happened since the original  event\",\n",
       " \" So if you're giving people a survey, and you're saying, Well, before you applied  for nursing school, you know, what did you think this? Or did you think that, you know,  they might just tell you and think they're telling you the truth, but they're actually  lying\",\n",
       " ' If you actually managed to go back in time and ask them, then they tell you something  different',\n",
       " ' So again, you can kind of screw up your own data by screwing up your own questions',\n",
       " '  So you want to think about how you word your questions',\n",
       " ' You can also screw up your questions  by introducing a hidden bias',\n",
       " ' Something happened to me recently, where a company sent me a  free app',\n",
       " ' And they said, try our free app, and I downloaded it, and it was awful',\n",
       " ' Okay',\n",
       " '  And then about a month later, they sent me a survey',\n",
       " ' And these were the questions I said',\n",
       " \"  When do you use the app? You know, what time of day? Do you use it? Right? Like how how,  how do you use it? Do you read scientific literature? Do you read news? And the problem  was, I couldn't really answer any of this\",\n",
       " ' Because from the day I downloaded it, I never  used it',\n",
       " ' It was so bad',\n",
       " ' Right? So question wording may induce a certain response',\n",
       " \" They  were asking me how do you use this, but they didn't give me a choice of I don't\",\n",
       " ' So I had  to say something',\n",
       " \" I don't even know what I said\",\n",
       " ' I mean, there was nothing I could say  To be honest, because of that bias',\n",
       " \" So you have to be careful that you aren't too rosy  about whatever your topic is, and and assume everybody loves everything\",\n",
       " \" I mean, you've  got to put out questions like are you even using the software? Did you have any problems  with the software? Right? I'm just assuming they're using it and liking it and using it\",\n",
       " \"  You know, like it's supposed to be used is a big assumption\",\n",
       " \" Order of questions and other  wording may induce a certain response and you'll see this a lot if you take a public  opinion poll\",\n",
       " \" I used to do a lot of polling We'd ask questions like, how likely are you  to vote for candidate x? You know, very likely someone likely? Somewhat unlikely and not  at all likely? And people say, I don't know, no, no likely\",\n",
       " \" And then you'd say, Well, what  if you knew that candidate x supported this new proposition? proposition? 69\",\n",
       " \" Right, then  would you be more likely to vote for candidate x? And so that's why order of questions other  wording and stuff\",\n",
       " \" They're trying to see if I add this fact that that fact is that going  to make the person like the candidate better\",\n",
       " ' And so you do have to think about the order  you put the questions',\n",
       " ' And if you want to ask about two different subjects, kind of  think about which subject should come first, because it might color the respondents answering  of the subsequent subject',\n",
       " ' And also on the slide, I wanted to point out that the scales  of questions may not accurately measure responses',\n",
       " ' Do your feelings always fit on a scale from  one to five? Well, you know, yelps kind of figured it out',\n",
       " \" If people's feelings about  restaurants tend to fit on a scale of one to five, I'd have a lot of trouble filling  that out if they gave me a scale of one to 17\",\n",
       " ' Right',\n",
       " ' But sometimes people have more  granular feelings about things, maybe they need a longer scale one to seven',\n",
       " \" Um, you'll  see a lot of pain scales, where they offer more than just five choices, because probably  pain can maybe go from one to seven or one to 10\",\n",
       " \" So think about your scales when you're  creating these questions, because that's your choice if you're designing the study\",\n",
       " ' Another  point to be made is the influence of the interviewer',\n",
       " \" Now, we don't have as much interviewing going  on these days, because we have the internet where we can do anonymous surveys, and people  just fill them out self report, we have Robo phones that you can call robo call\",\n",
       " \" And using  an automated voice, that's obviously not a person, you can get survey data\",\n",
       " \" But there's  always situations where you actually have to interview people, especially if somebody  is really sick in bed, and you have to show up there, you have to talk to them\",\n",
       " ' And so  even on the phone, you have to interview people, and they can hear your voice, right',\n",
       " \" So you  got to think about when you're pairing up whoever's being interviewed with whoever's  interviewing, um, I've found that it's best to have the interviewer come from the same  population as the research participant, in general, the only time that can be a problem  is a thirst from the same community, and there's a privacy issue\",\n",
       " ' But it can be very helpful,  for the most part, not always, to have your interviewers be actually from the population  that you would be studying, you know, from the individuals that you would be studying',\n",
       " \"  So for instance, if you need to interview a bunch of young African American, you know,  like some African American teenage men, like I recently saw a study on how health care  in the United States really isn't suited for them\",\n",
       " ' And it needs to improve and needs to  better cater to this population',\n",
       " \" Well, let's say you wanted to better understand that,  the best thing would be is to hire a young African American male and train him on how  to be good interviewer and do be good data collector, because you probably get the best  data that way\",\n",
       " \"  On the other hand, let's think of different ways that that could go, you could take a  person who was older, who is maybe of a different race, and maybe that would change how this  young African American male would respond to this interviewer\",\n",
       " \" I mean, the interviewer  could be like, in many ways, like the respondent, but the respondents perception might change,  then how they answer all verbal and nonverbal influences matter, you know, clothing, the  setting that the person's being interviewed in\",\n",
       " \" And so I'm not saying there's really a  solution to all this\",\n",
       " \" I'm just saying, make some good decisions\",\n",
       " ' Like I remember working  on a data set where there were some questions that had been asked about some older men about  their sexual function',\n",
       " ' And I, it looks the data look funny to me in the statistician  who was there during data collection told me that they had chosen young, female nursing  students to interview these elders',\n",
       " ' Men about their sexual habits',\n",
       " ' And I just said, you  know, that might be subject to interviewer influence',\n",
       " ' And then you of course have to  worry about vague wording',\n",
       " \" Just because it looks clear to you doesn't mean it looks clear  to everyone\",\n",
       " ' There are simple ways of avoiding vague terms in the survey, when you can just  put a number on it',\n",
       " \" So instead of asking a person, if they've waited a long time in the  waiting room, you can say, more than 10 minutes\",\n",
       " ' You can say exactly like within the last month,  have you done certain a certain activity or within the next year? Do you expect to change  schools or whatever',\n",
       " ' And so try to wherever you can use numbers or something very specific,  you know, instead of go to the clinic, go to the public health clinic at this particular  corner, or whatever',\n",
       " \" And then you're going to get some pretty accurate information\",\n",
       " \"  But  sometimes you're stuck using vague terms, because you're studying vague terms, right?  I was doing a study of controllable lifestyle attitudes towards controllable lifestyle in  medical students\",\n",
       " \" So we asked this question, how important is having a controllable lifestyle  to you in your future career? Well, what does that mean? That's pretty vague\",\n",
       " ' So what we  did is we use this grounding this anchoring language,  we added the sentence, a controllable lifestyle is defined as one that allows the physician  to control the number of hours devoted to practicing his or her specialty',\n",
       " \" So even though  we're talking about something kind of wofully, and watery, loosey goosey like control of  a lifestyle, who knows what that means? And that's not to say that that sentence could  be interpreted differently by people it certainly is\",\n",
       " \" But if you're stuck with vague wording,  try to put some grounding language in it\",\n",
       " \" So everybody's at least sort of led in the  same direction with their thought before they answer the question\",\n",
       " \" Now, I want to also point  out, you probably have noticed, there's all these issues, you have to think about when  doing surveys, there's this other issue called the lurking variable, well, you know, lurk  means to sneak around behind the scenes, right? Behind the scenes, a lurking variable is a  variable that's associated with a condition, but it may not actually cause it\",\n",
       " ' I remember  when I was studying epidemiology, they talked about how a lot of people with motorcycle  accidents, you unfortunately got in motorcycle accidents that they had tattoos',\n",
       " \" So therefore,  they said, Everybody shouldn't get a tattoo, you might get it in a motorcycle accident?  Well, that's a great example of a lurking variable\",\n",
       " \" Yeah, a lot of people who do get  into motorcycle accidents, have tattoos, but that the tattoos don't cause that\",\n",
       " \" Um, we  also know that having more education increases income, but people have the same education  level do not all make the same income, there's this thing, you know, called, it's sexism\",\n",
       " \"  And it's called racism\",\n",
       " \" So it matters whether you're a woman or a man, it matters, the color  of your skin\",\n",
       " \" If the you know, if you've got a darker skin, doesn't matter, that you have  the same education as somebody with lighter skin, you're still gonna make less money\",\n",
       " '  And so you have these lurking variables behind the scenes',\n",
       " \" So when people are looking at  Well, why are people you know, making less income, because they're less educated, whatever?  Well, you got to look for also the lurking variables\",\n",
       " \" So current studies show that why  women and African Americans make less money on the whole, it's not explained by fewer  of them working or fewer of them getting degrees\",\n",
       " \" It's really these lurking variables\",\n",
       " ' And so  you got to think critically',\n",
       " \" And I guess what I would say is, whenever you do a survey,  if you're studying something that has a lot of lurking variables associated with it, make  sure you measure those variables\",\n",
       " ' Like early studies where they were looking to see if  drinking a lot of alcohol causes lung cancer',\n",
       " ' Some of them forgot to really study how much  these people would smoke',\n",
       " ' Because we know smoking causes lung cancer',\n",
       " \" And we know if  you're hanging out in a place with a lot of drinking and they allow smoking, you'll see  a lot of people smoking too\",\n",
       " ' They seem to go hand in hand',\n",
       " \" So you don't want to miss  measuring variables that you think might be lurking variables\",\n",
       " \" It's no problem to measure  them and not use them later, but just make sure they're included\",\n",
       " ' So, as a final note  on bias, I just want to point out that survey results are so important',\n",
       " \" for healthcare,  and for the progression of science, that you really owe it to even a simplest survey, to  think about all of these things, these possible things that could go wrong, just with the  wording of questions or with how you're approaching things, and just really consider how you can  improve it\",\n",
       " \" It's really important to pay attention to avoiding bias when you're designing and  conducting your survey\",\n",
       " ' So think about all these things at the design phase',\n",
       " \" Finally,  I'll get into the last section of this lecture, which is about randomization, which I think  a lot of us have heard about\",\n",
       " \" So I'm going to explain the steps to a completely randomized  experiment\",\n",
       " \" And after I go through all that, I'm going to also talk about the concept of  a placebo and the placebo effect\",\n",
       " \" Then we're going to briefly touch on blocked randomization,  and also define for you what is meant by blinding\",\n",
       " ' So why ever randomize, right? So what randomizing  is, is when you take a bunch of respondents or participants in your study, and you randomly  choose what group they go in',\n",
       " \" And if you remember, like I was talking about experiment versus  observational study, we can't do that in observational study\",\n",
       " \" This is definitely an experiment because  you're telling them what group to go,  right\",\n",
       " ' So randomization is used to assign individuals to treatment groups',\n",
       " \" And when  you do that, when you randomly assign them, not only you're assigning them, but you're  randomly assigning them, you're not picking, you know, you're using like dice or some sort  of random method, and helps prevent bias and selecting members for each group\",\n",
       " \" It distributes  the lurking variables evenly, even if you don't know about the lurking variables, even  if you aren't measuring them\",\n",
       " ' By using this randomization method, they get equally allocated  in each group',\n",
       " ' So just to remind you, how you actually do that is, first I remember  the steps to that statistical study, you have to follow those',\n",
       " \" And after you get to the  point where you have ethical approval, that's when you start doing the data collection step\",\n",
       " \"  And that's where you start recruiting sample or, you know, hanging up signs and saying,  Be in my study, and people come in, and you see if they qualify, and if they qualify,  you've got this group of sample, right\",\n",
       " ' And what you do with those people is you say thank  you for being in my study',\n",
       " ' And you measure the confounders, which is another word for  lurking variables',\n",
       " \" You also measure the outcome, whatever you're trying to study, if you're  doing a randomized experiment, I know I've been involved in a lot of these where they're  studying drugs for lowering blood pressure\",\n",
       " \" So they'll often have maybe two groups or  three groups, where they're randomizing people into, but they don't do that first, the first  thing to do is get everybody in there and measure their blood pressure, right? The outcome,  you know, because they want to know that before, they are going to take a picture of that before\",\n",
       " '  And they also measure confounders, like smoking, remember, smoking is not good for your blood  pressure, you know, other things are not good for your blood pressure, like not exercising,  well measure all of those things',\n",
       " \" Okay, now, here's where we get into things\",\n",
       " \" That's when  the whole randomization happens\",\n",
       " ' So I showed this picture of a dye, but we usually use  a computer for it',\n",
       " ' So we got all these people together',\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bk = \"\"\n",
    "with open(\"bkgd.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        bk = \" \".join([bk, line])\n",
    "bk = str(bk)\n",
    "bk = bk.split(\".\")\n",
    "bk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c37cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bk = [clean_sentence(s) for s in bk]\n",
    "bk = [tokenize(s) for s in bk]\n",
    "\n",
    "bm = build_phrases(bk)\n",
    "bphrased = [sentence_to_bi_grams(bm, x) for x in bk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c346339",
   "metadata": {},
   "outputs": [],
   "source": [
    "bph = [w for s in bphrased for w in s.split() if \"_\" in w]\n",
    "bphc = [(bph.count(x), x) for x in set(bph)]\n",
    "bphc.sort()\n",
    "bphc = [(b,a) for a,b in bphc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d39bfa4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.051470588235294115, 'im_going'),\n",
       " (0.26666666666666666, 'weighted_average'),\n",
       " (0.7, 'little_bit'),\n",
       " (0.8, 'pay_attention'),\n",
       " (0.875, 'makes_sense'),\n",
       " (1.0, 'bayes_classifier'),\n",
       " (1.0, 'biased_coin'),\n",
       " (1.0, 'language_processing'),\n",
       " (1.0, 'says_yes'),\n",
       " (1.625, 'time_series'),\n",
       " (2.0, 'sub_d'),\n",
       " (2.0, 'variable_z'),\n",
       " (3.0, 'given_x'),\n",
       " (3.0, 'precision_recall'),\n",
       " (3.4444444444444446, 'different_ways'),\n",
       " (4.0, 'best_guess'),\n",
       " (4.0, 'categorisation_task'),\n",
       " (4.0, 'causal_relation'),\n",
       " (4.0, 'cause_overfitting'),\n",
       " (4.0, 'cheaper_hotels'),\n",
       " (4.0, 'coin_shows'),\n",
       " (4.0, 'conditional_entropies'),\n",
       " (4.0, 'decision_making'),\n",
       " (4.0, 'detailed_understanding'),\n",
       " (4.0, 'discovering_paradigmatic'),\n",
       " (4.0, 'discovering_syntagmatic'),\n",
       " (4.0, 'doesnt_mean'),\n",
       " (4.0, 'effective_predictors'),\n",
       " (4.0, 'expected_overlap'),\n",
       " (4.0, 'expensive_hotels'),\n",
       " (4.0, 'feature_space'),\n",
       " (4.0, 'fewer_parameters'),\n",
       " (4.0, 'following_lectures'),\n",
       " (4.0, 'generated_independently'),\n",
       " (4.0, 'gold_standard'),\n",
       " (4.0, 'gradually_group'),\n",
       " (4.0, 'ground_truth'),\n",
       " (4.0, 'high_dimensional'),\n",
       " (4.0, 'important_role'),\n",
       " (4.0, 'inferred_weights'),\n",
       " (4.0, 'inverse_document'),\n",
       " (4.0, 'iphone_6'),\n",
       " (4.0, 'language_models'),\n",
       " (4.0, 'large_collection'),\n",
       " (4.0, 'latent_aspect'),\n",
       " (4.0, 'linear_separator'),\n",
       " (4.0, 'main_goal'),\n",
       " (4.0, 'meat_occurs'),\n",
       " (4.0, 'method_works'),\n",
       " (4.0, 'multiple_perspectives'),\n",
       " (4.0, 'new_york'),\n",
       " (4.0, 'observed_evidence'),\n",
       " (4.0, 'overall_precision'),\n",
       " (4.0, 'paradigmatically_related'),\n",
       " (4.0, 'probabilistic_models'),\n",
       " (4.0, 'r_sub'),\n",
       " (4.0, 'raw_count'),\n",
       " (4.0, 'relation_discovery'),\n",
       " (4.0, 'research_articles'),\n",
       " (4.0, 'specific_examples'),\n",
       " (4.0, 'strongly_correlated'),\n",
       " (4.0, 'sub_b'),\n",
       " (4.0, 'subjective_sensors'),\n",
       " (4.0, 'support_vector'),\n",
       " (4.0, 'support_vectors'),\n",
       " (4.0, 'tentative_clustering'),\n",
       " (4.0, 'true_positive'),\n",
       " (4.0, 'weve_got'),\n",
       " (5.0, 'assign_high'),\n",
       " (5.0, 'baseline_system'),\n",
       " (5.0, 'bm_25'),\n",
       " (5.0, 'co_occurrences'),\n",
       " (5.0, 'collaboration_network'),\n",
       " (5.0, 'completely_biased'),\n",
       " (5.0, 'continue_talking'),\n",
       " (5.0, 'deep_learning'),\n",
       " (5.0, 'dimensional_space'),\n",
       " (5.0, 'discriminative_approaches'),\n",
       " (5.0, 'discussed_earlier'),\n",
       " (5.0, 'dot_product'),\n",
       " (5.0, 'email_messages'),\n",
       " (5.0, 'external_time'),\n",
       " (5.0, 'high_quality'),\n",
       " (5.0, 'human_experts'),\n",
       " (5.0, 'idf_weighting'),\n",
       " (5.0, 'intuitively_makes'),\n",
       " (5.0, 'label_given'),\n",
       " (5.0, 'linear_combination'),\n",
       " (5.0, 'linear_transformation'),\n",
       " (5.0, 'main_difference'),\n",
       " (5.0, 'multiple_times'),\n",
       " (5.0, 'new_generation'),\n",
       " (5.0, 'new_orleans'),\n",
       " (5.0, 'ordinal_logistic'),\n",
       " (5.0, 'original_entropy'),\n",
       " (5.0, 'parse_tree'),\n",
       " (5.0, 'particularly_useful'),\n",
       " (5.0, 'peoples_opinions'),\n",
       " (5.0, 'predefined_categories'),\n",
       " (5.0, 'probabilistic_latent'),\n",
       " (5.0, 'processing_techniques'),\n",
       " (5.0, 'pseudo_counts'),\n",
       " (5.0, 'pseudo_segments'),\n",
       " (5.0, 'speech_tagging'),\n",
       " (5.0, 'suggested_readings'),\n",
       " (5.0, 'supervised_machine'),\n",
       " (5.0, 'syntactic_structures'),\n",
       " (6.0, 'battery_life'),\n",
       " (6.0, 'bayesian_estimation'),\n",
       " (6.0, 'binary_categorization'),\n",
       " (6.0, 'categorisation_results'),\n",
       " (6.0, 'continued_discussion'),\n",
       " (6.0, 'doesnt_occur'),\n",
       " (6.0, 'domain_knowledge'),\n",
       " (6.0, 'effective_features'),\n",
       " (6.0, 'fair_coin'),\n",
       " (6.0, 'feature_vector'),\n",
       " (6.0, 'general_ideas'),\n",
       " (6.0, 'government_response'),\n",
       " (6.0, 'hidden_variables'),\n",
       " (6.0, 'hurricane_katrina'),\n",
       " (6.0, 'information_theory'),\n",
       " (6.0, 'lets_start'),\n",
       " (6.0, 'local_maximum'),\n",
       " (6.0, 'main_idea'),\n",
       " (6.0, 'n_documents'),\n",
       " (6.0, 'probability_mass'),\n",
       " (6.0, 'seen_earlier'),\n",
       " (6.0, 'slide_shows'),\n",
       " (6.0, 'social_media'),\n",
       " (6.0, 'united_nations'),\n",
       " (7.0, 'arithmetic_mean'),\n",
       " (7.0, 'aspect_rating'),\n",
       " (7.0, 'clustering_result'),\n",
       " (7.0, 'different_locations'),\n",
       " (7.0, 'equally_likely'),\n",
       " (7.0, 'frequent_term'),\n",
       " (7.0, 'high_frequency'),\n",
       " (7.0, 'human_effort'),\n",
       " (7.0, 'ive_shown'),\n",
       " (7.0, 'k_nearest'),\n",
       " (7.0, 'major_topics'),\n",
       " (7.0, 'news_articles'),\n",
       " (7.0, 'opinion_target'),\n",
       " (7.0, 'presidential_election'),\n",
       " (7.0, 'product_reviews'),\n",
       " (7.0, 'special_cases'),\n",
       " (7.0, 'speech_tags'),\n",
       " (7.0, 'stock_prices'),\n",
       " (7.0, 'theta_subj'),\n",
       " (7.0, 'vector_space'),\n",
       " (7.0, 'word_association'),\n",
       " (7.0, 'z_values'),\n",
       " (8.0, 'based_approaches'),\n",
       " (8.0, 'causal_topics'),\n",
       " (8.0, 'classification_accuracy'),\n",
       " (8.0, 'dirichlet_distribution'),\n",
       " (8.0, 'discover_syntagmatic'),\n",
       " (8.0, 'discriminative_classifiers'),\n",
       " (8.0, 'generative_probabilistic'),\n",
       " (8.0, 'mining_algorithms'),\n",
       " (8.0, 'random_variables'),\n",
       " (8.0, 'search_engine'),\n",
       " (8.0, 'sentiment_weights'),\n",
       " (8.0, 'simply_normalize'),\n",
       " (8.0, 'social_network'),\n",
       " (8.0, 'theta_2'),\n",
       " (8.0, 'y_given'),\n",
       " (9.0, 'nontext_data'),\n",
       " (9.0, 'system_says'),\n",
       " (9.0, 'time_period'),\n",
       " (9.0, 'youre_seeing'),\n",
       " (10.0, 'bayesian_inference'),\n",
       " (10.0, 'beta_values'),\n",
       " (10.0, 'continue_discussing'),\n",
       " (10.0, 'high_level'),\n",
       " (10.0, 'high_probabilities'),\n",
       " (10.0, 'non_zero'),\n",
       " (10.0, 'paradigmatic_relations'),\n",
       " (10.0, 'previous_slide'),\n",
       " (10.0, 'sentiment_classification'),\n",
       " (10.0, 'word_associations'),\n",
       " (11.0, 'actionable_knowledge'),\n",
       " (11.0, 'based_prediction'),\n",
       " (11.0, 'basic_idea'),\n",
       " (11.0, 'generative_models'),\n",
       " (11.0, 'semantic_analysis'),\n",
       " (11.0, 'training_examples'),\n",
       " (12.0, 'clustering_bias'),\n",
       " (12.0, 'optimization_problem'),\n",
       " (12.0, 'posterior_probability'),\n",
       " (13.0, 'aspect_ratings'),\n",
       " (13.0, 'logistic_regression'),\n",
       " (13.0, 'overall_ratings'),\n",
       " (13.0, 'special_case'),\n",
       " (13.0, 'total_number'),\n",
       " (14.0, 'logistical_regression'),\n",
       " (15.0, 'hidden_variable'),\n",
       " (15.0, 'information_retrieval'),\n",
       " (15.0, 'lower_bound'),\n",
       " (15.0, 'unigram_language'),\n",
       " (16.0, 'background_language'),\n",
       " (16.0, 'different_aspects'),\n",
       " (16.0, 'k_1'),\n",
       " (16.0, 'language_model'),\n",
       " (16.0, 'syntagmatic_relation'),\n",
       " (16.0, 'syntagmatic_relations'),\n",
       " (17.0, 'm_step'),\n",
       " (17.0, 'opinion_holder'),\n",
       " (17.0, 'overall_rating'),\n",
       " (17.0, 'paradigmatic_relation'),\n",
       " (18.0, 'contextual_text'),\n",
       " (18.0, 'naive_bayes'),\n",
       " (19.0, 'bayes_rule'),\n",
       " (20.0, 'parameter_values'),\n",
       " (21.0, 'e_step'),\n",
       " (21.0, 'looks_like'),\n",
       " (21.0, 'objective_function'),\n",
       " (21.0, 'random_variable'),\n",
       " (21.0, 'scoring_function'),\n",
       " (23.0, 'lets_look'),\n",
       " (23.0, 'sentiment_analysis'),\n",
       " (23.0, 'similarity_function'),\n",
       " (25.0, 'generative_model'),\n",
       " (25.0, 'real_world'),\n",
       " (31.0, 'machine_learning'),\n",
       " (34.0, 'em_algorithm'),\n",
       " (34.0, 'topic_models'),\n",
       " (35.0, 'non_text'),\n",
       " (36.0, 'natural_language'),\n",
       " (39.0, 'conditional_entropy'),\n",
       " (40.0, 'training_data'),\n",
       " (42.0, 'maximum_likelihood'),\n",
       " (45.0, 'mutual_information'),\n",
       " (48.0, 'likelihood_function'),\n",
       " (49.0, 'theta_sub'),\n",
       " (58.0, 'mixture_model')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bphc = dict(bphc)\n",
    "nphc = [(a / bphc.get(b, 1), b) for a,b in phc]\n",
    "nphc.sort()\n",
    "nphc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
