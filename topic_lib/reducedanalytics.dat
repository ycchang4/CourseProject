 First, let's define the term text mining and the term text analytics. Text mining is also related to text retrieval, which is an essential component in any text mining systems. So this is a high level introduction to the concept of text mining and the relation between text mining and retrieval. The title of this course is called Text Mining and Analytics, but the two terms text mining and text analytics are actually roughly the same. Both text mining and text analytics mean that we want to turn text data into high quality information or actionable knowledge. So I've taught another separate MOOC on text retrieval and search engines, where we discussed various techniques for text retrieval. Now text retrieval refers to finding relevant information from a large amount of text data. But at the high level, let me also explain the relation between text retrieval and text mining. Text retrieval is also needed for knowledge provenance and this roughly corresponds to the interpretation of text mining as turning text data into actionable knowledge. Here we emphasize the utility of the information or knowledge we discover from text data. And in this sense, text retrieval also helps minimize human effort.In this lecture we give an overview of text mining and analytics. Now it's interesting to view text data as data generated by humans as subjective sensors. And in general would be dealing with both non text data and text data and of course the non text data are usually produced by physical sensors. So this slide shows an analogy between text data and non text data and between humans as subjective sensors and physical sensors such as network sensor or thermometer. So in this case, text mining supplies knowledge for optimal decision making. And here we distinguish two different results one is high quality information, the other is actionable knowledge. But text data is also very important, mostly because they contain a lot of semantic content and they often contain knowledge about the users, especially preferences and opinions of users. Text retrieval is very useful for text mining in two ways: First, text retrieval can be a pre-processor for text mining, meaning that it can help us turn big text data into a relatively small amount of most relevant text data, which is often what's needed for solving a particular problem. And those non text data can be also of different formats - numerical data or categorical or relational data or multimedia data like a video or speech. So this course will cover specialized algorithms that are particularly useful for mining text data. Once we find the patterns in text data or actionable knowledge, we generally would have to verify the knowledge by looking at the original text data so the users would have to have some text retrieval support to go back to the original text data to interpret the pattern, or to better understand the knowledge or to verify whether the pattern is really reliable. The more concise form of the information would be very concise summary of the major opinions about the features of the product. In the case of high quality information we refer to more concise information about the topic, which might be much easier for humans to digest than the raw text data. So this means that data mining problem is basically taking a lot of data as input and giving actionable knowledge as output. Similarly, we can think of humans as subjective sensors that would observe the real world from some perspective, and then humans would express what they have observed in the form of text data. If you have taken that MOOC, you will find some overlap and it would be useful to know the background of text retrieval for understanding some of the topics in text mining. Now, such an outcome could be called actionable knowledge because a consumer can take the knowledge and make a decision and act on it
 But text data generally have also context associated. And the context can provide interesting angles for analyzing text data. Now non-text data can be also useful for analyzing text by supplying context. In general, non-text data can be very important for such prediction tasks. Similarly, we can partition text data based on locations or any metadata that's associated to form interesting comparison scenarios. We could analyze the sentiment in different context, so this is fairly general landscape of the topics in text mining and analytics. So in this sense, non-text data can actually provide interesting angles or perspectives for text analysis, and can help us make context sensitive analysis of content or the language usage or the opinions about the observer or the authors of text data. But we can also touch how to join the analysis of both text data and non-text data. Now the observable world can be represented as for example entity relation graphs or more in a more general way, using knowledge representation language. First, we are going to cover natural language processing very briefly because this has to do with understanding text data, and this determines how we can represent text for text mining. In fact, when we have a non-text data, we could also use the non-text data to help prediction. And then we're going to talk about opinion mining and sentiment analysis. And finally, we are going to cover a text based prediction problems where we try to predict some real world variable based on text data. And for this reason, we might be concerned with joint mining of text and non text data and so in this course we're going to focus more on text mining. And that human. When we infer other real world variables, we could also use some of the results from mining text data as intermediate results to help the prediction. But then the human would express what the person has observed using a natural language such as English, and the result is text data. We actually hope to cover most of these general topics. It's also one of the most useful techniques in text mining. And that's why here we show that the results of some other mining tasks, including mining the content of text data and mining knowledge above the observer can all be very helpful for prediction. Most specifically, human sensor or human observer would look at the world from some perspective. Now of course, this is still generated from the original text data, but I want to emphasize here that often the processing of text data to generate some features that can help with the prediction, is very important. So a more general picture would be to include non text data as well. Second, we're going to talk about how to mine word associations from text data and word associations is a form of useful lexical knowledge about a language. And so specifically we can think about the mining, for example, knowledge about the language. So this picture basically covered multiple types of knowledge that we can mine from text in general. For example, the time, the location, of that associated with the text data and these are useful context information. Third, we're going to talk about the topic mining and analysis, and this is only one way to analyze content of text, but it's a very useful way of analyzing content. So this can be regarded as one example of mining knowledge about the observer.So looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data. This can often be biased also. When we look at the text data alone will be mostly looking at the content and opinions expressed in text. As so, this has much to do with mining the content of text data. And note that we distinguish the observed the world from the person because text data can describe what the person has observed in an objective way, but the description can be also subject with sentiment, and so in general you can imagine the text data would contain some factual descriptions of the world plus some subjective comments, so that's why it's also possible to do text mining to mine knowledge about the observer. So the main goal of text mining is actually to revert this process of generating test data. Or extracting high quality information about a particular aspect of the world that we're interested in
 And this is called a lexical analysis. Natural language content analysis is the foundation of text mining. And in particular, natural language processing. This is the inferred meaning based on our additional knowledge.This lecture is about the natural language content analysis. with a factor how we can represent text data? And this determines what algorithms can be used to analyze and mine text data. Or syntactical analysis or parsing of natural language sentence. And this . But this is not semantics yet. For semantic analysis, we can also do some aspects of semantic analysis, particularly extraction of entities and relations. And in general we need common sense reasoning. Syntactic ambiguity refers to different interpretations of the sentence in terms of structures. And we also represented the chasing action as a predicate. Sentiment analysis is another aspect of semantic analysis that we can do. Statistical analysis methods to try to get as much meaning out from the text as possible. So A and the dog will form a noun phrase. Because we assume all the. So the slides also shows that computers are far from being able to understand natural language precisely, and that also explains why the text mining problem is difficult, because we cannot rely on mechanical approaces or computational methods to understand the language precisely. And that makes communication efficient. But there is also another possible interpretation, which is to say language processing is natural. And this is again because we assume that we have the ability to disambiguate a word, so there's no problem with having the same word to mean, possibly different things in different context. And that's a generally difficult problem in artificial intelligence. And finally, we might even further to infer. As a result, for example, we omit a lot of common sense knowledge. Parsing would be more difficult, but for partial parsing, meaning to get that some phrases correct, we can probably achieve 90% or better accuracy. Think about the word level ambiguity a word like design can be a noun or a verb, so we've got ambiguous part of speech tag. it can also do a word sense disambiguation for some extent. And later you will see that there are actually many such algorithms that can indeed extract the interesting knowledge from text, even though we cannot really fully understand the meaning of all the natural language sentences precisely. For computer, would have to formally represent these entities by using symbols. That means we can tag the sentences general positive when it's talking about product. In particular tagging these words with these syntactic categories is called a part of speech tagging. It doesn't mean on any data set to the accuracy will be precisely 97%. As well talking about this topic so it's processing of natural language. This  is called a prepositional phrase attachment ambiguity meaning. And so this has to do with understanding the purpose of saying this sentence, and this is called SPEECH Act analysis or pragmatic analysis. This is called syntactic parsing. might further infer what This sentence is requesting or why the person who said the sentence is saying this sentence. Now the main reason why a natural language processing is very difficult because it's designed to make human communications efficient. So for example, natural language processing can actually be interpreted in two ways. But in general we can do part of speech tagging fairly well, although not perfectly. First, the computer needs to know what are the words, how to segment the words. So for example Dog is a noun, chasing is the verb, boy is another noun, etc. Influence, however, is very hard and we generally cannot do that for any big domain, and it's only feasible for very limited domain. To annotate enough data for the computer to learn from. Now this is a very simple sentence. Root has also multiple meanings. all of us have this knowledge, there's no need to encode this knowledge. And there are certain way for them to be connected together in order to generate the meaning. After that, the computer also needs to figure out the relation between these words
 Not feasible for large scale text mining. And is the main topic of this course and they are generally applicable to a lot of applications. Robust and general NLP tends to be shallow, while deep understanding does not scale up. So obviously the better we can understand the text data, the better we can do text mining. But this course will cover the general statistical approaches that generally don't require much human effort. And they are generally based on statistical analysis, so they are robust, and general, and, And they are in the category of shallow analysis, so such techniques have the advantage of being able to be applied to any text data in any natural language about any topic. For that we have to rely on deeper natural language analysis techniques. For this reason, in this course, the techniques that we cover are, in general, shallow techniques for analyzing text data, to mine text data. So to summarize this lecture, the main points to take away are, first NLP is the foundation for text mining. Deeper NLP requires common sense knowledge and inferences, thus only working for very limited domains. In practice, we use statistical NLP as the basis. And we have humans to help as needed in various ways. So to bring humans in, into the loop to analyze, fix, to analyze text data more precisely. Precise deep semantic analysis is also very hard
 And these words can be used to form topics. 1st. And with inference rules we can infer interesting, derived facts from the text.This lecture is about the text representation. That's the purpose of text mining. Then we can obtain a representation of the same text, but in the form of a sequence of words. Yet this is the most general way of representing text, because we can use this to represent any natural language text. I should also say that text data are generated by humans and are meant to be consumed by humans, so as a result in a text data analysis text mining, humans play a very important role. And discuss how natural language processing can allow us to represent text in many different ways. When we combine related words together and some words are positive, some words are negative, so we can also do sentiment analysis. By identifying words we can, for example, easily count what are the most frequent words in this document or in the whole collection, etc. Now, this is desirable because as we can represent text at the level of knowledge, we can easily extract the knowledge. This enriches the representation of text data and thus, also, enables a more interesting analysis. So representing text data as a sequence of words opens up a lot of interesting analysis possibilities. But unfortunately, using such a representation would not help us do semantic analysis, which is often needed for many applications of text mining. And unfortunately, such techniques would require more human effort. In this lecture we're going to discuss text representation. And they are less accurate. We don't necessarily replace the original word sequence representation Instead, we add this as an additional way of representing text data, so that now the data is represented as both a sequence of words, and a sequence of part of speech tags. Now this is to add more entities and relations through entity-relation recognition. We can represent this sentence in many different ways. So you have to rely on some special techniques to identify words. So the sequence of words representation is not as robust as string of characters. So this picture shows that if we move down, we generally see more sophisticated natural language processing techniques to be used. When we store a natural language sentence as a string of characters, we have perhaps the most general way of representing text, since we can always use this approach to represent any text data. what's the most frequent character in English text, or the correlation between those characters, but we can't really analyze semantics. Meaning that we should optimize the collaboration of humans and computers. So there is a trade off here between doing deeper analysis that might have errors, but would give us direct knowledge that can be extracted from text and doing shallow analysis, which is more robust. But wouldn't actually give us the necessary deeper representation of knowledge. So in that sense, it's OK that computers may not be able to have completely accurate representation of text data and patterns that are extracted from text data can be interpreted by humans, and humans can guide the computers to do more accurate analysis by annotating more data by providing features to guide the machine learning programs to make them work more effectively. That Google is doing as a more semantic way of representing text data. So that also means it's still necessary to combine such deep analysis with shallow analysis based on, for example sequence of words. Note that I use the plus sign here, because by representing text as a sequence of part of speech tags. So if we analyze text data at the levels that are represented, deeper analysis of language, then we have to tolerate the errors
 This course, however, focuses on word based representation.So, as we explained, different textual representation tends to enable different analysis. For example, stylistic analysis generally requires syntactical representation. Syntactical structure representation. Now this course is covering techniques mainly based on word based representation. Word based representation is very important level of representation. These techniques are general and robust and thus are more widely used in various applications. And finally such a word based representation and the techniques enabled by such a representation can be combined with many other sophisticated approaches. And there are multiple ways to represent text - strings, words, syntactic structures and the relation graphs, logical predicates, etc. It's quite general and relatively robust. It can enable a lot of analysis techniques such as word relation analysis, topic analysis and sentiment analysis, and there are many applications that can be enabled by this kind of analysis. We can use graph mining algorithms to analyze Syntactic graphs. Text representation determines what kind of mining algorithms can be applied. For example, Thesaurus discovery has to do with discovering related words and topic and opinion related applications are abundant, and there are for example, and people might be interested in knowing the major topics covered in the collection of text. Although knowing word boundaries might actually also help. And some applications are related to this kind of representation. Such techniques have also several advantages. We can also generate the structure based feature features and those are features that might help us classify text objects into different categories. First, they are general and robust, so they are applicable to any natural language. Third, these techniques are actually surprisingly powerful and effective for many applications. When we add entities and relations, then we can enable lot of techniques such as knowledge graph analysis or information network analysis in general and this analysis would enable applications about entities, for example, discovery of all the knowledge and opinions about the real world energy entity. By adding syntactic structures we can enable, Of course, syntactic graph analysis. Now they are very effective, partly because the words are invented by humans as basic units for communications. And these different representations should in general be combined in real applications to the extent we can. In particular, we can gradually add more and more deeper analysis results to represent text data, and that would open up more interesting representation opportunities and also analysis capacities. Secondly, it does not require much manual effort or sometimes it does not require any manual effort. And in general there are many applications that can be enabled by the representation at this level. So as a string text can only be processed by using stream processing algorithms, but it's very robust, it's general. And this can be the case. For example, compression of text doesn't necessarily need to know the word boundaries. And business intelligence people might be interested in understanding consumers opinions about their products and competitors products to figure out the what are the winning features of their products
 Note that the paradigmatic relation and syntagmatic relation, are actually closely related. For paradigmatically relation we represent each word by its context, and then compute the context similarity. Because syntagmatic relation essentially captures such correlations. Word relations can be also very useful for many applications in text retrieval and mining.This lecture is about the Word Association mining and analysis. And this can be used to introduce additional related words to a query to make the query more effective. We are gonna assume the words that have high context similarity to have paradigmatic relation. Or you can use related words to suggest related queries to the user to explore the information space. Finally, such word associations can also be used to compare and summarize opinions. That means these two words are semantically related. So this intuition can help us discover syntagmatic relations. You can see they generally occur in similar context. For example, in search in text retrieval we can use word associations to modify a query. Such a syntagmatic relations would help us show the detailed opinions about the product. But here we are interested in knowing what other words are correlated with the verb eats. We're going to assume words with high co-occurrences, but relatively low individual occurrences to have syntagmatic relations because they tend to occur together, and they don't usually occur alone. One is called a paradigmatic relation, the other is syntagmatic relations. So to summarize, the general ideas for discovering word associations are the following. In general there are two word relations, and these are quite basic. Discovering such world relations has many applications. What about the syntagmatic relation? Here we we are going to explore the correlated occurrences again based on the definition of syntagmatic relation. It's often called query expansion. And that might improve the all words in the sentence or in sentences around this word. So this is just suggesting that we can discover paradigmatic relation by looking at the similarity of context of words. Similarly, Monday and Tuesday have paradigmatic relation. And even in the general context you also see some similarity between the two words. Similarly, car and drive are related semantically, and they can be combined with each other to convey meaning. Another application is to use word associations to automatically construct the topic map for browsing where we can have words as nodes and associations as edge. Syntagmatic relation on the other hand, is related to co-occurring elements that tend to show up in the same sequence. The second kind of relation is called syntagmatic relation. For syntagmatic relation, we will count how many times two words occur together in a context which can be a sentence, paragraph or a document even. So these two are complementary and basically relations of words, and we're interested in discovering them automatically from text data. In that paradigmatically related words tend to have syntagmatic relation with the same word that they tend to be associated with the same word, and that suggests that we can also do join the discovery of the two relations. And then imagine what words occur after computer. Now again, consider example- How helpful is the occurrence of eats for predicting occurrence of meat? knowing whether eats occurs in a sentence would generally help us predict the Whether meat also occurs indeed as if we will see eats occur in a sentence, and that should increase the chance that meat will also occur. First, such relations can be directly useful for improving accuracy of many NLP tasks, and this is because this is part of our knowledge about the language. And we're going to compare their Co occurrences with their individual occurrences. Here's the outline. And grammar learning can be also done by using such techniques because If we can learn paradigmatic relations, then we form classes of words. Here we essentially can take advantage of similar context. In contrast, if you look at the question in the bottom, how helpful is occurrence of eats for predicting the occurrence of text? Because eats and text are not really related, so knowing whether eats occurred in a sentence doesn't really help us predict whether text also occurs in the sentence. For example, we might be interested in understanding positive and negative opinions about iPhone 6. This also helps explain the intuition behind the methods for discovering syntagmatic relation. So this is different from paradigmatic relation and these two relations are in fact so fundamental, that they can be generalized to capture basic relations between units in arbitrary sequences. This is an example of knowledge about natural language that we can mine from text data. And if we learn syntagmatic relations, then we would be able to know the rules for putting together a larger expression based on component expressions. So the important question to ask for syntagmatic relation is whenever eats occurs, what other words also tend to occur? So the question here has to do with whether there are some other words that tend to co-occur together with eats, meaning that whenever you see eat, you tend to see the other words. If you think about the general problem of the sequence mining, then we can think about the units in the sequence data, and then we think of paradigmatic relation as relations that are applied to units that tend to occur in similar locations in a sentence or in a sequence of data elements in general. And then we can ask the question what words tend to occur to the left of eat and what words tend to occur to the right of eat? Now thinking about this question would help us discover Syntagmatic relations
 An imaginary document. And this means the vector is actually probability distribution over words.This lecture is about the paradigmatic relation discovery. So they can be regarded as a pseudo document. By definition, 2 words are paradigmatically related if they share similar contexts. And they have been shown to work well for matching a query vector and a document vector, but we can adapt the many of the ideas to compute the similarity of context documents for our purpose here. But here we also find it convenient to model the context of a word for paradigmatically relation discovery. So in general, we can represent a pseudo document or context of cat as one vector. Now those of you who have been familiar with information retrieval or text retrieval techniques would realize that vector space model has been used frequently for modeling documents and queries for search. So the idea here is represent a context by award vector where each word has a weight that is equal to the probability that a randomly picked word from this document vector is this word. So the general idea of discovering paradigmatic relations is to compute the similarity of context of two words. EOWC expected overlap of words in context. But intuitively we know matching the isn't really surprising because the occurs everywhere, so matching the is not as such a strong evidence as matching a word like eats which doesn't occur frequently. And of course we can also assign weights to these different similarities to allow us to focus more on particular kind of context, and this would be naturally application specific, but again here that main idea for discovering paradigmatically related words is to compute the similarity of their context. And similarly, if we only use the right1 context will capture the similarity from another perspective. xi is defined as the normalized count of word wi in the context. And on the bottom you can see frequency vector representing a context. Now, such a word based representation would actually give us interesting way to define the perspective of measuring the similarity. We can call this context left1 context. And here we see words eats, ate, is, has, etc. So this intuitively makes sense for measuring similarity of contexts. Sometimes this is useful as we might want to capture similarity based on general content that would give us loosely related paradigmatic relations, whereas if you use only the words immediately to the left and to the right of the world, then you likely will capture words that are very much related by their syntactical categories and semantics. So by viewing context in the vector space model, we convert the problem of paradigmatic relations discovery into the problem of computing the vectors and their similarity. In general, we can combine all kinds of views of the context and so the similarity function is in general combination of similarities on different contexts. And this flexibility also allows us to measure the similarity similarity in some other different ways. So in general we would like to capture such a context and then try to assess the similarity of the context of cat and the context of a word like dog. Similarly, we can also collect the words that occur right after the word cat. So naturally, our idea for discovering such relation is to look at the context of each word and then try to compute the similarity of those contexts. And then we can measure the similarity of these two vectors. So here for example, we can measure the similarity of cat and dog based on the similarity of their contexts. Namely, they occur in similar positions in text. And this would give us then two probability distributions representing two contexts. So now the question is, how can we formally represent the context and then define the similarity function? So first we note that the context actually contains a lot of words. First it might favor matching one frequent term very well over matching more distinct terms, and that is because in the dot product, if one element has a high value and this element is shared by both context and it contributes a lot to the overall sum. For example, we can look at the word that occurs before the word cat. And this can be interpreted as a probability that you would actually pick this word from d1 if you randomly pick the word. So this vector can then be placed in this vector space model. Well, here we simply define the similarity as a dot product of two vectors and this is defined as the sum of the products of all the corresponding elements of the two vectors. Now to answer this question it's useful to think of bag of words representation as vectors in the vector space model. It may not be robust. And specifically there are two potential problems. So in general, context may contain adjacent words like eats and my that you see here or non-adjacent words like Saturday, Tuesday or some other words in the context. An another word dog might give us a different context, so d2. And there is this dot product infact gives us the probability that two randomly picked words from the two contexts are identical that means if we try to pick a word from one context and try to pick another word from another context, we can then ask the question, are they identical? If the two contexts are very similar, then we should expect that we frequently will see the two words picked from the two contexts are identical. So that gives us one perspective to measure the similarity
 And they generally form a sub linear transformation. That is, we can reward matching a rare word and this heuristic is called IDF term weighting in text retrieval. And more specifically, we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations. And then compute the similarity of the corresponding context documents of two candidate words. Most specifically, we're going to introduce some retrieval heuristics used in text retrieval, and these heuristics can effectively solve these problems, as these problems also occur in text retrieval when we match a query vector with document vector. Finally, Syntagmatic relations can also be discovered as a bi-product when we discover paradigmatic relations. More specifically, we have used the BM25 and IDF weighting to discover paradigmatic relation and these approaches also represent the state of the art in text retrieval techniques. Matching 'the' is not surprising because 'the' occurs everywhere, but matching 'eats' will account a lot. Matching one frequent term can contribute a lot. IDF stands for inverse document frequency. IDF stands for inverse document frequency. But this is a reasonable way where we can adapt the BM25 retrieval model for paradigmatic relation mining. These are the words that share similar context. In general, when we represent a term vector to represent the sorry to represent context with the term vector, we would likely see some terms have higher weights and other terms have lower weights depending on how we assign weights to these terms, we might be able to use these weights to discover the words that are strongly associated with the candidate word in the context. So to summarize, the main idea for discovering paradigmatic relations is to collect the context of a candidate word to form a pseudo document, and this is typically represented as a bag of words. Even a common word like 'the' would contribute equally as content word like 'eats'. And the Xi and Yi probabilities of picking the word from both contexts, therefore it indicates how likely will see a match on this word. In this method, we represent each context by a word vector that represents the probability of word in the context and we measure the similarity by using the DOT product. Now interestingly we can also use this approach to discover syntagmatic relations. So for this reason, the highly weighted terms in this IDF weighted vector can also be assumed to be candidate for Syntagmatic relations. So here we define in this case we define the document vector. Our similarity function. That means we'll introduce weight for matching each term. And this parameter is generally non negetive number, although zero is also possible. A common word will be worth less than rare word, so we emphasize more on matching rare words now.In this lecture, we continue discussing paradigmatic relation discovery. And we have each Xi, defined as a normalized weight of BM 25. But it clearly shows the relation between discovering the two relations. The difference is that the high frequency terms will now have a somewhat lower weights and this would help control the influence of these high frequency terms. However, this actually has the advantage of emphasizing matching all the words in the context, so it does not allow a frequent word to dominate the matching. We basically take Y as the same as X. So we use the raw count as representation. Now this weight alone only reflects how frequently the word occurs in the context. And, and in this case the normalizing formula has average document length here. And this is a parameter to control  length normalization. Instead, those terms would be the terms that are frequent in the context, but not frequently in the collection. So those are clearly the words that tend to occur in the context of the candidate word, for example, cat. But if we apply IDF weighting as you see here, we can then we weight these terms based on IDF That means the words that are common, like 'the' will get penalized. An then we can take the highly similar word pairs and treat them as having paradigmatic relations. Document frequency means the count of the total number of documents that contain a particular word. So K is the number of documents containing word or document frequency and M here is the total number of documents in the collection. Now IDF would give us the importance of matching this word. The IDF function is giving a higher value for a lower K, meaning that it rewards a rare term. And indeed they can be discussed, discovered in a joint manner by leveraging such associations. We roughly have the 01 vector. X is the count, the raw count of word. Because many common words like 'the' will occur frequently in all the context. But we can't just say any frequent term in the context that would be correlated with the candidate word. So here we show that the IDF measure is defined as a logarithm function of the number of documents that match the term, or document frequency. So that's a very rare term, the rarest term in the whole collection. So this transformation function is by far the most effective transformation function for text retrieval, and it also makes sense for our problem set up. That is, to convert the raw count of word in the document into some weight that reflects our belief about how important this word in the document. So this puts a very strict constraint on high frequency terms, because their weight would never exceed K+1. In our case, what will be our collection? We can also use the context that we can collect for all the words as our collection and that is to say, a word that's popular in the collection in general would also have a low IDF. As containing elements representing normalized BM 25 values. But we also see we introduce the parameter K here. So you may recall this sum indicates all the possible words that can be a overlap between the two contexts. Now there is also another interesting transformation called a BM25 transformation which has been shown to be very effective for retrieval and in this transformation we have a form that. So now the highest weighted terms will not be those common terms because they have lower IDFs. Now the approach that we have taken earlier in the expected overlap account approach is a linear transformation. And so that will be denoted by TF of W&amp;D as shown in the Y axis. As we vary K, if we can simulate the two extremes. So how can we address that problem? In this case we can use the IDF weighting that's commonly used in retrieval. First, of course, this extra occurrence of this count is just to achieve the sub linear normalization
 An entropy. Now we can assume high entropy words are harder to predict. Here we denoted by X sub w, w denotes a word.0 and the entropy is 0.This lecture is about the syntagmatic relation discovery. By definition, Syntagmatic relations hold between words that have correlated Co occurrences. So the intuition we discussed earlier can be formally stated as follows. So take a more specific example, here we can ask the question whenever eats occurs, but other words also tend to occur. That means when we see one word occurs in the context, we tend to see the occurrence of the other word. In particular, we can talk about how to discover syntagmatic relations. So now let's see how we can use entropy for word prediction. If they are associated with eats, they tend to occur in the context of eats. Now, entropy in general is not negative and that can be mathematically proved. And you can imagine why and so that's because the two probabilities are symmetric and completely symmetric. When it's zero, it means the word is absent, and naturally the probabilities for one and zero should sum to 1. So more specifically, our prediction problem is to take any text segment, which can be a sentence, paragraph or a document, and then I asked the question is a particular word present or absent in this segment. And this is a measure introduced in information theory to measure the randomness of X. So now we can compute the entropy of this random variable, and this entropy indicates how difficult it is to predict the outcome of a coin for coin tossing. In this case, the probability that X = 1 is . An we clearly would expect the meat to have a high entropy, then the OR Unicorn. Now the question is, how does one quantitatively measure the randomness of a random variable like X sub w, how in general, can we quantify the randomness of a variable? And that's why we need a measure called entropy. In this lecture, we're going to continue talking about word Association mining. So, it's like a completed biased coin, therefore the entropy is 0. So this random variable is associated with precisely one word. We have a probability of 1. But meat is somewhere in between in terms of frequency, and it makes it hard to predict because it's possible that it occurs in the sentence or the segment more accurately. The question that is, can you predict what other words occur? To the left or to the right. For the completely biased coin we see is 0 and that intuitively makes a lot of sense because a fair coin is most difficult to predict whereas a completely biased coin is very easy to predict that we can always say it's a head because it is a head all the time so they can be shown on the curve as follows. But it may also not occur in the segment. And we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations. The completely biased coin corresponds to the end point. And I can bet that it doesn't occur in this sentence. Because Unicorn is rare, is very rare. The is easier to predict because it tends to occur everywhere, so I can just say with the in the semtence. Now, what's interesting is that some words are actually easier for it, in other words. Another extreme case is completely biased coin, where the coin always shows up as head, so it's a completely biased coin. There is also some connection with the information here, but that's beyond the scope of this course. In this case it's a binary random variable, although the definition can be easily generalized for a random variable with multiple values. So this is the entropy function and this function will give a different value for different distributions of this random variable. Because a word is either present or absent in the segment
 Basically.This lecture is about the syntagmatic relation discovery and conditional entropy. So this suggests that we can use conditional entropy for mining syntagmatic relations. So let's see how we can use conditional entropy to capture syntagmatic relations. For each word W1, we're going to enumerate the overall other words W2, and then we can compute the conditional entropy of W1 given W2. The conditional entropy conditioned on the presence of eats. So as a result, if we replace these probabilities with their corresponding conditional probabilities in the entropy function, we will get the conditional entropy. Another concept, called the conditional entropy. Basically, whether eats is present or absent, and this of course is the entropy conditional entropy of meat in that particular scenario. And of course, we can also define this conditional entropy for the scenario where we don't see eats. In this case of discovering Syntagmatic relations for a target word like W1, we only need to compare the conditional entropies For W1 given different words. Knowing the and trying to predict the meat and this is the case of knowing eats and trying to predict the meat. Earlier we talked about using entropy to capture how easy it is to predict the presence or absence of a word. Now this would allow us to mine the most strongly correlated words with a particular word W1 here. Given that we know eats occured in the context. And we thought all the candidate words in ascending order of the conditional entropy, because we want to favor a word that has a small entropy, meaning that it helps us predict the target word W1, and then we can take the top ranked the candidate words as words that have potential syntagmatic relations with W1. And that means there is a stronger association between meat and eats. The conditional entropy is no larger than the entropy of the variable X, so basically this is upper bound for the conditional entropy. In this lecture, we're going to continue the discussion of word association mining an analysis. But this algorithm does not help us mine the strongest K syntagmatic relations from entire collection. And in this case they all comparable right? So the conditional entropy of W1 given W2 and conditional entropy of W1 given  W3 are comparable. Because it tells us to what extent we can predict the one word given that we know the presence or absence of another word. We're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations. And if we frame this using entropy, that would mean we are interested in knowing whether knowing the presence of eats could reduce uncertainty about the meat or reduce the entropy of the random variable corresponding to the presence or absence of meat. Now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case listed here, that is, the conditional entropy of the word given itself. This means we know whether meat occurs in the sentence and we hope to predict whether the meat occurs in the sentence. And that's also when this conditional entropy reaches the minimum. Now that would change all these probabilities to conditional probabilities where we look at the presence or absence of meat. Make the prediction and it cannot hurt the prediction in any case. Note that we need to use a threshold to find these words. We can only reduce uncertainty, and that intuitively makes sense because as we know more information, it should always help us. Whereas in the case of eats, eats is related to meet, so knowing presence of eats or absence of eats would help us predict wether meat occurs so it can help us reduce entropy of meat, so we should expect the second term, namely, this one to have a smaller entropy. Because in order to do that, we have to ensure that these conditional entropies are  comparable across different words. So we have these probabilities indicating whether a word like meat occurs or does not occur in the segment, and we have the entropy function that looks like what you see on the slide. So, here we listed the this conditional entropy in the middle. Now, of course this conditional entropy gives us directly one way to measure the association of two words
This lecture is about the syntagmatic relation discovery and mutual information. So ranking based on mutual information is exactly the same as ranking based on the conditional entropy of X given Y. The numerator has the joint actual observed. In particular, mutual information, denoted by I(X;Y), measures the entropy reduction of X obtained from knowing Y. Now these equations allow us to compute some probabilities based on other probabilities. The entropy of this word. And this can simplify the computation. So let's examine them intuition of using mutual information for syntagmatic relation mining. In other words, the conditional entropy would never exceed the original entropy. And if we do that, and it's basically a based on the same intuition as in conditional entropy, we will see that words that are strongly associated with each will tend to have high mutual information, whereas words that are not related. And they are shown here. And similarly, using this equation we can compute the probability that we observe only the second word. So mathematically, it can be defined as the difference between the original entropy of X and the conditional entropy of X given Y. And here I listed all the probabilities involved and it's easy for you to verify that basically we have first 2 probabilities corresponding to the presence or absence of each word. Mutual information is. In other words, and when the word is observed when the first word is observed and there are only two scenarios depending on weather second word is also observed. Similarly knowing eats doesn't help us predicting the as well. This is easy to understand becausw the original entropy is always not going to be lower than the possibly reduced conditional entropy. So this probability captures the first scenario when the signal word actually is also observed. And so, for example, this one means if we add up the probabilities that we observe the two words occur together and the probabilities when the word the first word occurs and the second word doesn't occur, we get exactly the probability that the first word is observed. In other words, picking any other word, and computing mutual information between eats and that word, you won't get any mutual information larger than the mutual information between eats and itself. And similarly for the second word, we also have two probabilities representing presence or absence of this word, and this sums to one as well. The mutual information between eats and meats, which is the same as between meats and eats cause major information is symmetric is expected to be higher than The mutual information between eats and the. So we're going to compute the mutual information between eats and other words. Now the question we ask for syntactic relation mining is whenever eats occurs, what other words also tend to occur? So this question can be framed as a mutual information question, that is, which was have higher mutual information with eats. And this captures the second scenario when the seond word is not observed, so we only see the first word. It's going to be larger than or equal to the mutual information between eats and another word. And how it can be used to discover syntagmatic relations? Before we talked about a problem of conditional entropy, and that is the conditional entropy computed on different pairs of words is not really comparable, so that makes it hard to discover strong syntagmatic relations globally from corpus. Specifically, for example, using this equation, we can compute the probability that the first word occurred and the second word did not, because we know these probabilities in the boxes. And it reaches 0 if and only if the conditional entropy of X given Y is exactly the same as original entropy of X. And then finally we have a lot of joint probabilities that represented the scenarios of Co-occurrences of the two words. The larger this divergence is, the higher the mutual information would be. In this lecture, we're going to continue discussing syntagmatic relation discovery. But the mutual information allows us to compare different pairs of X&amp;Y, so that's why mutual information is more general and in general more useful. Now when we fix X to rank different Ys using conditional entropy would give the same order as ranking based on mutual information, because in the function here H of X is fixed because X is fixed. And it's easy to see the other equations also follow the same reasoning. use a different form of mutual information, and we can mathematically write the mutual information into the form shown on this slide, where we essentially see a formula that computes what's called KL-divergences or callback labeler divergance. Normally the two conditional entropies H(X|Y) and H(Y|X) are not equal. And you also can easily see that the mutual information between  a word and itself is the largest which is equal to the mutual info. So if we look at this form of mutual information it shows that the mutual information measures the diversions of the actual joint distribution from the expected distribution under the independence assumption. Once we know how to calculate these probabilities, we can easily calculate the mutual information. Because knowing the doesn't really help us predict eats. So for W1, we have two probabilities shown here. There are two scenarios. But if the numerator is different from the denominator, that would mean the two variables are not independent, and that helps measure the association. More specifically the question we're interested in here, is how much reduction in the entropy of X can we obtain by knowing Y
 A relation discovery an. Any other applications in both information retrieval and text data mining. and a commonly used technique is called a maximum likelihood estimate, where we simply normalize the observed accounts. And similarly this . And mostly becausw. Each is weighted 1/4. But here, once we use mutual information to discover Syntagmatic relations, we can also represent the context with this mutual information as weights. These are fairly general. And For estimating these probabilities, we simply need to collect the three counts. Now note that there is some relation between syntactic medical relation discovery and paradigmatically relation discovery. So to summarize, this whole part about word Association mining, we introduce the two basic associations, called Paradigmatic and Syntagmatic relations. So, to summarize, select the cinematic relation can generally be discovered by measuring correlations between occurrences of two words. Of a word like a cat, and if we do the same for all the words, then we can cluster these words or computer similarity between these words based on their context similarity.In general, we can use the empirical counts of events in the observed data to estimate probabilities. Yet they can actually discover interesting relations of words. From these pseudo segment. So if we do that, we can see we can compute these probabilities as follows for estimating the probability that we see a word occurring in segment, we simply normalize the counts of segments that contain this word. The second is the article about the using various statistical measures to discover lexical atoms. We can also use different ways to define context and segment and this would lead to some interesting variations of applications. And mutual information of X&amp;Y which matches the entropy reduction of X. So this basically concludes the discussion of how to compute the mutual information, how to use this for syntagmatic relation discovery. And these represent the four different combinations of occurrences of these words. For example, the context can be very narrow, like a few words around a word or sentence or maybe paragraphs and using different contexts, which allows you to discover different flavors of paradigmatic relations. And in some segments you see both words occur. We introduce the three concepts from information theory, entropy, which meshes uncertainly over random variable X conditional entropy, which measures the entropy of X. And similarly, counting Co occurrences using, let's say mutual information to discover syntagmatic relations, we also have to define the segment and the segment can be defined as an arrow, text window or longer text article and this would give us different kinds of associations. In particular, mutual information is a principled way for discovering such a relation. These approaches can be applied to any text with no helmet human effort. Once we have these counts, we can just normalize. Join the analysis as well. It allows us to have values computer on different pairs of words that are comfortable, and so we can rank these pairs and discover the strongest cinematical relationship from collection of documents. These counts by n, which is the total number of segments and this will give us the probabilities that we need to compute mutual information. They are based on counting of words. So we already discussed the possibility of using BM 25 to achieve waiting for terms in the context to potentially also suggest the candidates that have seen like medical relations with the candidate word. The third one is new paper on unified way to discover both paradigmatic a relation and select medical relations using random walks on world graphs. Their indicator as once for both columns.05 comes from one single pseudo segment that indicates the two words occur together. Observed more data than we actually have
This lecture is  about topic mining and analysis. The main topics. And we call that topic mining and analysis. Also we want to generate the coverage of topics in each document d sub i and this is denoted by π sub i j and π sub i j is the probability of document d sub i covering topic theta sub j. as C. The first is to discover K topics from a collection of text data. To what extent did the document covers each topic. And other topics perhaps are not covered. Here we can denote that text connection. What are these K topics? OK, major topics in the text data. Document one we might see that topic 1 is covered a lot, topic 2 and topic k are covered with a small portion. And denote a text article as di and we generally also need to have as input the number of topics K. So different granularities of topics obviously have different applications. In general, we can view topic as some knowledge about the world. First, we have as input a collection of N text documents. As you see on this roadmap, we have just covered mining knowledge we have just covered mining knowledge about the language namely discovery of word associations such as paradigmatic relations relations and syntagmatic relations. In this case K topics. Document 2, on the other hand, covered topic 2 very well, but it did not cover topic 1 at all and also covers topic K to some extent. An we can assume that these probabilities sum to one, because a document won't be able to cover other topics outside the topics that we discussed we discovered. In the text. Or we are interested in knowing about the research topics. Now let's look at the tasks of topic mining and analysis. And then we can use these context variables to help us analyze patterns of topics. The second task is to figure out which documents cover which topics to what extent. So that's why mining topics is very important. Now when we have some non-text data then we can have more context for analyzing the topics. All such meta data or context variables can be associated with the topics that we discover. But there may be techniques that can automatically suggest a number of topics, but in the techniques that we will discuss which are also the most useful techniques, we often need to specify a number of topics. Indeed, there are many applications that require discovery of topics in text and then analyze them. So more formally, we can define the problem as follows
 similar.This lecture is about the topic mining and analysis. So a particular approach could be based on, TF-IDF weighting from retrieval. word sports. these counts as our estimate of the coverage probability for each topic. A term can be a word or a phrase. This forms a distribution over the topics for the document to characterize coverage of different topics in the document. Our idea here is to define a topic simply as a term. They are semantically similar or closely related or even synonyms. Finally, there's this problem of word sense ambiguation, a topical term or related term can be ambiguous. Actually, in general, we might want to favor title words becauses the authors tend to use the title to describe the topic of an article. Here, candidate terms can be words or phrases. In the task definition for topic mining and analysis, we have two tasks, one is to discover the topics and the 2nd is to analyze the coverage. These words then become candidate topics. Intuitively, we would like to favor representative terms, meaning terms that can represent a lot of content in the collection. So we need to count related words. For example, in news we might favor title words. First, when we count what words belong to the topic, we also need to consider related words. It can be applied to any language and any text. But it cannot represent the complicated topics that might require more words to describe. We can't simply just count the topic. We are going to talk about using a term as topic. Now if we define a topic in this way, we can analyze the coverage of such topics in each document. And in general, we can use these terms to describe topics, so our first thought is just to define a topic as one term. So that means we need to mine K topical terms from a collection. First, it lacks expressive power, meaning that it can only represent the symbol general topics. OK so after this then we will get K topical terms and those can be regarded as the topics that we discovered from the collection. So let's first think about how we can discover topics if we represent each topic by a term. It does not suggest what other terms are related to the topic, even if we're talking about the sports, there are many terms that are related, so it does not allow us to easily count related terms toward contributing to coverage of this topic. What's worse, term travel actually occurred in the document, so when we estimate the coverage of the topic travel, we have gotten a non-zero count, so it's estimated coverage would be non zero. But when we apply such an approach to a particular problem, we might also be able to leverage some domain specific heuristics. The term science also did not occur in the document, and it's estimated also zero. So in general the formula would be to collect the counts of all the terms that represented the topics and then simply normalize them so that. Second, it's incomplete in vocabulary coverage, meaning that the topic itself is only represented as one term. But if we simply count these words that represent our topics, and we will find that the word sports actually did not occur in the article, even though the content is about the sports. Next let's think about how we can compute the topic coverage pi sub i j So looking at this picture, we have sports, travel and science and these topics and now suppose you are given a document How should we figure out the coverage of each topic in the document? One approach can be to simply count occurrences of these terms. So naturally hashtags can be good candidates for representing topics. However, if we simply use the frequency to design the scoring function, then the highest scored terms would be general terms or functional terms, like "the", "a" etc. Here, for example, we might want to discover to what extent document 1 covers sports and we found that 30% of the content of document 1 is about sports. Basically, the idea is to go down the list based on our scoring function an gradually take terms to collect the K topical terms. So we also want to avoid having such words on the top, so we want to penalize such words, but in general would like the favor terms that are fairly frequently but not so frequent. And TF stands for term frequency IDF stands for inverse document frequency and we talked about some of these ideas in the lectures about the discovery of word associations. For example a very specialized topic would be hard to describe by using just a word or one phrase, we need to use more words, so this example illustrates some general problems with this approach of treating a term as topic. Anyway, after we have designed the scoring function, then we can discover the K topical terms by simply picking K terms with the highest scores. So first we're going to parse the text data in the collection to obtain candidate terms. So this estimate has problem
This lecture is about the probabilistic topic models for topic mining and analysis. Each is word distribution. But as a distribution, this topic representation can in general involve many words to describe the topic and can model subtle differences in semantics of the topic. By varying the model, of course we can discover different knowledge. So this is a general idea of using a generative model for text mining. We're going to introduce probabilistic topic models. So we design a probabilistic model to model how the data are generated. We talked about the task of topic mining and analysis when we define a topic as a distribution, so the input is a collection of text articles. So now each topic is word distribution. So the basic idea here is improved representation of topic as a word distribution. But in general not so much related to the topic. And then we can infer the most likely parameter values lambda star given a particular data set, and we can then take the Lambda star as knowledge discovered from the text for our problem, and we can adjust the design of the model and parameters with this discover various kinds of knowledge from text. Similarly, we can model travel and science with their respective distributions. The second constraint is on the topic coverage in each document. So to summarize, we introduced a new way of representing a topic, namely represented as word distribution, and this has advantage of using multiple words to describe a complicated topic. It has the highest probability for the travel topic 0. We simply assume that they are generated this way and inside the model, we embed some parameters that were interested in denoted by Lambda. We also introduce the general idea of using a generative model for text mining and the idea here is to first design a model to model the generation of data. This would allow you to distinguish subtle differences in topics and to introduce semantically related words in the fuzzy manner. Second, it assigns weights to terms, so now we can model several differences of semantics and you can bring in related words together to model topic. For analysis, and that means each word is a unit. And also the coverage of all the topics in each document and these are formally represented by theta_i's and pi_i's and we have two constraints here for these parameters. Third, because we have probabilities for the same word in different topics. In general, we can imagine a non zero probability for all the words and some words that are not relevant would have very very small probabilities and these probabilities will sum to one. It also allows us to assign weights on words so we can model subtle variations of semantics. The number of topics and vocabulary set and the output is a set of topics. But now we're going to use a word distribution to describe the topic. In most cases, we use words as the basis. The first is the constraint on the word distributions. These are sports-related terms and of course it would also give a non zero probability to some other words like "travel" which might be related to sports. Now, once we set up with a model, then we can fit the model to our data, meaning that we can estimate the parameters or infer the parameters based on the data. As a set of words that determines what units would be treated as the basic units for analysis. Now intuitively, this distribution represents a topic in that if we sample words from the distribution, we tend to see words that already do sports. And similarly you can see "star" also occurred in sports and science with reasonably high probabilities, because they might be actually related to the two topics. First, we design a model with some parameters that we are interested in, and then we model the data. Now the output would consist of as first a set of topics represented by Theta i's Each theta_i is a word distribution. Of course this is based on our assumption. Now obviously we can already see N * K parameters for pi's. Now in this case, of course, for our tax mining problem, or more precisely topic mining problem, we have the following parameters. It's oversimplification obviously, but it suffices is to show the idea and the Y axis shows the probability of the data observe. So this would be then our estimate of the parameters and these parameters note that are precisely what we hope to discover from text data, so would treat these parameters as actually the outcome or the output of the data mining algorithm. Finally, to solve the problem of word ambiguity, we need to split an ambiguous word so that we can disambiguate its topic. We have non zero probabilities, it's just that for a particular topic of some words we have very very small probabilities. A document is not allowed to cover a topic outside the set of topics that we are discovering. And these parameters in general, will control the behavior of the probabilistic model, meaning that if you set these parameters for different values, it will give some data points higher probabilities than others. First, we have theta_i's Each is a word distribution and then we have a set of pi's for each document. We adjust the parameters to fit the data as well as we can. So how do we model the data in this way? And we assume that data are actually samples drawn from such a model that depends on these parameters. When we have more words that we can use to describe the topic, we can describe complicated topics, to address the second problem, we need to introduce weights of words. The input, of course is our text data C is the collection, but we also generally assume we know the number of topics K or we hypothesize a number and then try to mine K topics, even though we don't know the exact topics that exist in the collection and these vocabulary set
 Such a model can also be regarded as a probabilistic mechanism for generating text. These models are general models that cover probabilistic topic models as special cases. In such a case, we simply assume that text is generated by generating each word independently. Now this signals a topic about text mining, because when we sample words from such a distribution we tend to see words that often occur in text mining context. The first one has higher probabilities for words,  text, mining, association, etc. And this is called a maximum likelihood estimate. And that just means we can view text data as data observed from such a model. And so this suggests that such a distribution can actually characterize topic. The probability of each word. But the same sequence of words might have a different probability in a different context. And query as a relatively small probability, just observd once. One is given a model.This lecture is about an overview of statistical language models which cover probabilistic topic models as special cases. So first, what is the statistical language model? A statistical language model is basically the probability distribution over word sequences. Some word sequences might have higher probabilities than others. In this case, let's assume we have a text mining paper. So the simplest language model is called a unigram language model. The text that we generated by drawing words from this distribution is unlikely coherent, although the probability of generating a text mining paper publishing in the top conference is non zero. So that just means given a particular distribution, different text will have different probabilities. Assuming that no word has a zero probability in the distribution and that just means we can essentially generate all kinds of text documents, including very meaningful text documents. Now, in general, the words may not be generated independently, but after we make this assumption, we can significantly simplify the language model. For this reason, we also call such a model generative model. It depends on the context of discussion.00001 So as you can see, such a distribution clearly is context dependent. So now we can assume our text is a sample drawn according to this word distribution. So if we sample words from such distribution, then the probability of observing a text mining paper would be very very small. We can try to sample words according to a distribution. Because I've observed text 10 times in the text that has a total of 100 words. And similarly another sentence, "The eigenvalue is positive", might get a probability of 0. If we ask the question, what is the most likely language model that has been used to generate this text data, assuming that the text is observed from some language model, what's our best guess of this language model? OK, so the problem now is just the estimated probabilities of these words as I've shown here. So in this case, if we ask the question about what is the probability of generating a particular document, then we likely will see text that looks like a text mining paper of course. Now, with this we actually can also compute the probability of every sequence, even though our model only specifies the probabilities of words.001 It might give "Today Wednesday is" which is a non grammatical sentence very, very small probability as shown here. So now Given a model, we can then sample sequences of words. So as you can see, with N probabilities, one for each word, we actually can characterize the probability distribution over all kinds of sequences of words, and so this is a very simple model. So in general, in order to characterize such a distribution, we must specify probability values for all these different sequences of words. The second distribution show on the bottom has different words that with higher probability. So here we assume we have N words, so we have N probabilities, one for each word, and they sum to one
 It's basically a word distribution. inference. And this estimate is a more general estimate than the maximum likelihood estimate. The simplest language model is unigram language model. And this estimator is called the Maximum a Posteriori or MAP estimate. So this is a general illustration of Bayesian estimation and Bayesian inference. So to summarize, we introduced the language model which is basically probability distribution over text. The other is Bayesian estimation. It's also called a generative model for text data. And in such a case, we can maximize this posterior probability to find a theta that would maximize this posterior probability. We also talked about the Bayesian estimation or influence. Because giving a non zero probability would take away probability mass from some observed words which obviously is not optimal in terms of maximizing the likelihood of the observed data. Otherwise your estimate may not necessarily be more accurate than maximum likelihood estimate. But this zero probability for all the unseen words may not be reasonable sometimes, especially if we want the distribution to characterize the topic of text mining. So in the case of text data, let's say our observed 100 words did not contain another word related to text mining, then our maximum likelihood estimator would give that word zero probability. As a special case, we can assume F of Theta is just equal to Theta. That includes our prior knowledge about the parameters. This point is the maximum likelihood estimate that represents the theta that gives the data the maximum probability. Where we define the estimate as arg max of the probability of X given Theta. We assume that we have some prior belief about the parameters. In this case we must define a prior on the parameters P of Theta, and then we're interested in computing the posterior distribution of the parameters which is proportional to the prior and the likelihood. And later you will see this can be useful for topic mining where we want to inject some prior knowledge about the topics. It's the most likely value of theta given by the posterior distribution, and it represents a good compromise of the prior mode and the maximum likehood estimate. So the problem of Bayesian inference is to infer this posterior distribution and also to infer other interesting quantities that might depend on Theta. It's the Theta that makes the probability of X given Theta reach its maximum, so this estimate intuitively also makes sense, and it's often very useful, and it seeks the parameters that best explain the data. So in Maximum likelihood estimation, we define best as meaning the data likelihood has reached the maximum, so formally it's given by this expression here. Given a data point, sorry, given a data sample X, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum likelihood estimate. We introduced the concept of likelihood function which is the probability of data given some model. And this kind of distribution would allow us, then, to infer any derived values from Theta. And allows us to write down one conditional probability of X given Y in terms of the conditional probability of Y given X. OK, but if we have some informative prior, some bias towards certain values, then MAP estimate can allow us to incorporate that, but the problem here of course is how to define the prior. The prior tells us some theta values are more likely than others. That means the most likely parameter value according to our prior before we observe any data. And this probability of X given Y is a conditional probability, and this is our posterior belief about X, because this is our belief about X values after we have observed Y. In Bayesian inference, we treat data as uncertain variable. Given a particular set of parameter values, this function can tell us which X, which data point has a higher likelihood, higher probability. And then we have our data likelihood. And by using Bayes rule that I have shown here, we can then combine the likelihood function with the prior to give us this posterior probability of the parameter. Therefore we can estimate the value of this function F as the expected value of F according to the posterior distribution of data given the observed evidence X. Because once if we define our prior as a noninformative prior meaning that it's uniform over all the theta values, no preference, then, we basically would go back to the maximum likelihood estimator because in such a case it's mainly going to be determined by this likelihood value here. And we have some prior belief about which hypothesis to choose and after we have observed Y, we will update our belief and this updating formula is based on the combination of our prior here and the likelihood of observing this Y if X is indeed true. And it's sometimes the same as posterior mode, but it's not always the same, so it gives us another way to estimate the parameters
This lecture is a continued discussion of probabilistic topic models. Each word here is denoted by X sub I. And we introduce Lagrange multiplier here, Lambda. Our model is a unigram language model, a word distribution that we hope to denote a topic and that's our goal. Because these words might have repeated occurrences. Which means we'll take the estimated parameters as a knowledge that we discover from the text. And this Lambda is simply taken from here. So this is the simplest case of topic modeling. And for convenience we're going to use theta sub I to denote the probability of word W sub I. So the main goal is just to discover the word probabilities for this single topic, as shown here. And this is multiplied by the logarithm of the probability. And the collection has only one document also. Eventually to find the optimal setting for Theta Sub I. Because we assume the independence in generating each word, so the probability of the word the document would be just a product of the probability of each word. As a topic representation, you will see this is not ideal, right? The because of the high probability words are functional words they are not really characterizing the topic. And the likelihood function will have some parameters in the function and then we are usually interested in estimating those parameters, for example by maximizing the likelihood which would lead to maximum likelihood estimator and these estimated parameters would then become the output of the mining algorithm. So if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document D here, let's imagine this document is a text mining paper. And since some word might have repeated occurrences, so we can also rewrite this product in a different form. On the top you will see the high probability words tend to be those very common words, often functional words in English, and this will be followed by some content words that really characterized the topic well like text, mining etc and then in the end you also see various more probabilities of words that are not really related to the topic, but they might be externally mentioned in the document. And when we set it to zero, we can easily see theta sub i is related to Lambda in this way. Now, what does the likelihood function look like? This is just the probability of generating this whole document given such a model. And we also have constraints over these probabilities. In the output we also no longer have coverage because we assumed that the document covers this topic 100%. And you might also notice that this is the general result of maximum likelihood estimator. And similarly, this is the count of words of M in the document. The objective function is the likelihood function, and the constraint is that all these probabilities must sum to one. Next, we're going to find the theta values, or probabilities of these words that would maximize this likelihood function. So our goal is to maximize this likelihood function. And this function would combine our objective function with another term that encodes our constraints. You can also see if a word did not occur in the document, it would have a zero count and therefore that corresponding term will disappear. In this lecture, we're going to continue discussing probabilistic models, we are going to talk about a very simple case where we are interested in just mining one topic from one document. So this is basically an analytical solution to our optimization problem. As you may recall from calculus, an optimal point would be achieved when the derivative is set to 0. Now this is different from the previous line where the product is over different positions of words in the document. In general, the estimate would be to normalize count and it's just sometimes the counts have to be done in a particular way, as you will also see later. So in this simple setup we are interested in analyzing one document and trying to discover just one topic. This denotes the count of word one in document. And obviously these thetas of i's would sum to one. So our data in this case is just the document which is a sequence of words. So it's a sum over all the words in the vocabulary and inside the sum there is a count of words in the document. So we will have as many parameters as many words in our vocabulary, in this case M
 and probability of Theta sub B. This is the probability of selecting the background word distribution denoted by Theta sub B. Specifically, Theta sub D which is intended to denote the topic of document D and Theta sub B which is representing a background topic that we can set to attract the common words. So mathematically, this model. Second, the probability of actually observing the word from this component model. So this is the probability of selecting the topic word distribution. And the two terms are ,first, the probability of selecting a component like Theta sub D. The coverage of each topic and this is determined by probability of Theta sub D. Because common words would be assigned high probabilities in this model. And we're going to use the background word distribution to generate the word. The first case is to use the topic word distribution to generate the word, and in such a case, then the probability would be the probability of Theta sub D, which is the probability of choosing the model multiplied by the probability of actually observing the word from that model. In this lecture we will continue discussing probabilistic topic models. But we can still think of this as a model for generating text data and such a model is called a mixture model. And this is basically a more complicated mixture model. And in each case it's a product of the probability of selecting that component model. Well, this is obviously because these words are very frequent in our data and we are using a maximum likelihood estimate and then the estimator obviously would have to assign high probabilities for these words in order to maximize the likelihood. This way our target is the topic theta here would be only generating the content words that characterize the content of the document. Now obviously the probability of text the same is all similar, right? So we also consider two ways of generating text, and each case is a product of the probability of choosing a particular word distribution multiplied by the probability of observing the word from that distribution. This way the two distributions can be mixed together to generate the text data and will let the other model which we called background topic model to generate the common words. And the model is a mixture model with two components: two unigram language models. When we generate the word, however, we're going to 1st decide which of the two distributions to use, and this is controlled by another probability: probability of theta sub D and probability of theta sub B here. And the maximum likelihood estimator is, as usual, just to find the parameters that would maximize this likelihood function and the constraints here, of course, are two kinds. This is nothing but to just define the following generative model where the probability of word is assumed to be a sum over 2 cases of generating the word. Then we're going to use this word distribution to generate a word. So to summarize, what we observe here, the probability of a word from a mixture model is in general a sum over all different ways of generating the word. multiplied by the probability of actually observing the data point from that component model, and this is something quite general and you will see this occurring often later. And the data we consider here is just still 1 document. An eventually we generated a lot of words. I just used a simple w to denote any word, but you can still see. But now we had this sum because of the mixture model and because of the mixed model We also have to introduce the probability of choosing that particular component distribution. Those are two topics and the other is the coverage of each topic in each. Like And this sum is due to the fact that the word can be generating multiple ways. So now once we set up the model and we can write down the likelihood function as we see here, the next question is how can we estimate the parameter or what to do with the parameters given the data? Well, in general we can use some observed text data to estimate the model parameters and this mission would allow us to discover the interesting knowledge about the text, so in this case, what do we discover? Well, these are represented by our parameters, and we have two kinds of parameters. This is basically the first sum. At inside sum, each term is a product again of two terms. So in such a case we have a model that has some uncertainty associated with the use of a word distribution.This lecture is about a mixture of unigram language models. So to summarize, and we talked about the mixture of two unigram language models.0 and the other is 0. We first must have chosen the topic of and then we also have to actually have sampled the word "the" from the distribution and similarly the second part accounts for a different way of generating the word from the background. In this case, what's the probability of observing the word w? Now here I showed some words like "the" and "text", so as in all cases, once we set up the model, we're interested in computing the likelihood function. So now the process of generating a word would be the first to flip a coin based on these probabilities of choosing each model and if. So in this sense, the mixture model is more general than the previous model where we have just one distribution and it can cover that as a special case. The basic question is, so what's the probability of observing a specific word here? Now we know that the word can be observed from each of the two distributions, so we have to consider 2 cases
5 multiplied by the probability of observing text from that model. They are precisely the two probabilities of the two words text and the given by theta sub D.This lecture is about mixture model estimation. Furthermore, we are going to assume there are precisely two words: the and text. The document has just the two words text and the. And this also means that by using a background model that is fixed to assign high probabilities to background words, we can indeed encourage the unknown topic world distribution to assign smaller probabilities for such common words, instead put more probability mass on the content words that cannot be explained well by the background model. Meaning that they have a very small probability from the background model, like a text here. And the problem here is how can we adjust theta sub D in order to maximize the probability of the observed document here and we assume all the other parameters are known. So we assume the background model is already fixed. Now, although we designed the model heuristically to try to factor out this background words. In this lecture, we're going to continue discussing probabilistic topic models. So let's first look at our motivation for using a mixture model and we hope to factor out the background words from the topic word distribution. And, this is obvious from examining this equation because the background part is weak for text it's small. Similarly, the would have a probability of the same form, just with different exact probabilities. So we further assume that the background model gives probability of point nine to the word the and text point one. So the probability of text is basically the sum over 2 cases, where each case corresponds to each of the word distribution. Now you can imagine we could have assumed that we also don't know the background word distribution, but in this case our goal is factor out precisely those high probability background words. And they will tend to bet high probabilities on different words to avoid this competition in some sense. And as you can see, indeed the probability of text is now much larger than probability of the. Obviously this is a very naive oversimplification of the actual text, but again it is useful to examine the behavior in such a special case. This is not the case when we have just one distribution and this is clearly because of the use of the background model which assigns a very high probability to the and low probability to text. And this is because we have assumed all the other parameters are known. In particular, you will see in order to make them equal and then the probability assigned by theta sub D must be higher for a word that has a smaller probability given by the background. Once you understand what's the probability of each word, which is also why it's so important to understand what exactly the probability of observing each word from such a mixture model. So the idea is to assume that the text data actually contain two kinds of words. But we can't do that because text and the must sum to one. On the two probabilities. So in order to compensate for that we must make the probability of text given by theta sub D somewhat larger so that the two sides can be balanced. An inside each case we have the probability of choosing the model which is . And it accounts for the two ways of generating text. We can't give both a probability of 1. But we are going to simplify other things. So the is away etc and the other kind is from our topic word distribution that we're interested in. We are going to assume we have knowledge about others. So this is in fact a very general behavior of this mixture model, and that is if one distribution assigns a high probability to one word than another, then the other distribution. In particular, we're going to talk about how to estimate the parameters of a mixture model
 It also encourages the unknown distribution theta sub d to assign somewhat higher probability to this word. And this is to collaboratively maximize likelihood. 1st Every component component model attempts to assign high probabilities to high frequency words in the data.9 and the probability of 0. And that would make the discovered topic more discriminative. This means the behavior here, which is high frequency words tend to get higher probabilities are affected or regularised somewhat by the probability of choosing each component. Second, different component models tend to bet high probabilities on different words, and this is to avoid competition or waste of probability, and this would allow them to collaborate more efficiently to maximize the likelihood. So to summarize, we have just discussed the mixture model and we discussed the estimation problem of mixture model and in particular we discussed some general behavior of the estimate an that means we can expect the our estimator to capture these intuitions. This is also an example of imposing a prior on the model parameters and the prior here basically means one model must be exactly the same as the background language model, and if you recall what we talked about in Bayesian estimation and this prior would allow us to favor a model that's consistent with our prior. We also talk about the special case of fixing one component to a background word distribution, and this distribution can be estimated by using a collection of documents. 3rd, the probability of choosing each component regulates the collaboration and competition between the component models. An added the probability mass to the other word. The question is which word to have a reduced the probability and which word to have a larger probability? And in particular, let's think about the probability of the. The more likely a component that is being chosen, it's more important than to have higher values for these frequent words. So all the more word occurs, then it's it makes more sense to give such a word a high probability because the impact would be more on the likelihood function. This is in fact a very general phenomenon of all the maximum likelihood estimator, but in this case we can see as we see more occurrences of term. Now, we've being so far, assuming that each model is equally likely and that gives us 0. When this is larger the overall result would be larger and that also makes them less important for theta sub D to increase the probability for the because it's already very large so the impact here of increasing the probability of the is somewhat regulated by this coefficient 0. Right, so this means there is another behavior that we observe here that is high frequency words generally will have high probabilities from all the distributions. As we add more words, we know that, we have to just multiply the likelihood function by additional terms to account for the additional occurrences of the. The probability of choosing one of the two component models. Now you will see these terms for the will have a different form where the probability of 'the' would be even larger because the background that has a high probability for the word and the coefficient in front of 0. A large collection of English documents, by using just one distribution and then we'll just have normalized frequencies of terms to give us the probabilities of all these words. Now it's also interesting to think about the impact of probability of theta sub B. So now, as you can imagine, it would make sense to actually assign a smaller probability for text and to make room for a larger probability for the. Now when we use such a specialized mixture model, we show that we can effectively get rid of background words in the other component
 distribution. Text has a much higher probability here. The probability of generating text is another product of similar form. On the other hand, the other words, text, mining, clustering, etc are known to be from the topic word distribution. Then we simply we can simply normalize them to have estimate of the probability that the word text is from theta sub d or from theta sub b. These blue words are then assumed to be from the topic word distribution. And now let's consider word like text. By the theta sub d, than by the background model, which has a very small probability. So the only thing unknown is this word probabilities are given by theta sub d update. Now that we are interested in the word text, so text can be regarded as evidence. The prior is very high. And by this we are going to say text is more likely from theta sub d. Of the word given by each distributions. We are going do tend to guess the distribution that gives the word higher probability and this is likely to maximize the likelihood right so. And equivalently, the probability that Z is equal to 0 given that the observed evidence is text. Specifically, given all the parameters can we infer the distribution the word is from So let's assume that we actually know tentative probabilities for these words in theta sub D. They are equally likely. We also introduced a latent variable Z here to denote whether the word is from the background or the topic. In this case the prior is Saying that each model is equally likely, but we can imagine perhaps a different prior possible. So the question is, do you think text is more likely have been having been generated from theta sub d or from theta sub B? So in other words, we want to infer which distribution has been used to generate this text. If we had known which words are from which distribution precisely. So this is now familiar scenario of using a two component mixture model to try to factor out the background words from one topic word distribution here. So now we have the probability that text is generated from each. So more specifically, let's think about the probability that this word text has been generated. We're going to choose word that has a higher likelihood. In this lecture, we're going to continue the discussion of probabilistic topic models. When Z is zero, it means it's from the topic theta sub d when it's one, it means it's from the background theta sub b. But suppose we actually know which word is from which distribution, so that would mean, for example these words: the is and we are known to be from this background word distribution. So when we multiply the two together, we get the probability that text has in fact has been generated from theta sub d Similarly, for the background model an. And that's fairly straightforward, and it's just dictated by the maximum likelihood estimate now. The theta sub d must have been selected, so we have the selection probability here, and Secondly, we also have to actually have observed text from the distribution. So what is evidence here? While the evidence here is the word text
 For example, is made to what extent this document has covered background words. We can then normalize the count to estimate the probabilities or to revise our estimate of the parameters. re estimate our parameters. So these words are believed to be more likely from the topic. In our case the parameters are mainly the probability of a word given by status update. So assume we have just 4 words and their counts are like this and this is our background model that assigns high probabilities to common words like the. Of the distribution that has been used to generate each word, we can see we have different probabilities for different words. By the probability that we believe this would has been generated by using the theta sub d. Why that's be cause these words have different probabilities in the background. For example for the it's from Background, Z value is 1 and text on the other hand is from the topic. We have high probability like this one text. So in this setting we have assumed that the two models have equal probabilities and the background model is known. To help us re estimate the parameters. So what are the relevant statistics? Well, these are the word counts. So let me also illustrate we can group the words that are believed to have come from Cedar sub D and as text mining algorithm for example and clustering. That means we have improved parameters from here to. This is given by this formula. For the word distribution that we're interested in. And So. What's also interesting is do not last column, and these are the inferred word split, and these are the probabilities that a word is believed to have come from one distribution. These, on the other hand, are less likely probably from background. And this is done by this multiplication. And this when we add this up or take the average will kind of know to what extent it has covered background versus content words that are not explained well by the background. OK, so as I said, the bridge between the two is really variable Z hidden variable, which indicates how likely this world is from the topic word distributions theta sub d. So even though the two distributions are equally likely, and then our initialization says uniform distribution because of the difference in the background world distribution, we have different guest probabilities. In this case the topic distribution, and you might wonder whether this would be also useful because our main goal is to estimate these word distribution right? So this is our primary goal.0, so we're going to just get some percentage of the counts toward this topic, and then we simply normalize these counts. Now the idea that we talked about before for predicting the word distribution that has been used with the general the world is it'll predict this. Not only that, we also see some words that are believed to have come from the topic. And the M step is to take advantage of the additional information to separate the data to split the data accounts and then collect the right data counts. But in general, as I said, it's not going to be 1. And this binary variable would indicate whether the world has been generated from theta on Sunday or theater Super B. So here this probability that we're interested in is normalized into an uniform distribution over all the words. And you can see this. So this is a hill climbing algorithm that would gradually improve the estimate of parameters and as I will explain later, there's some guarantee for reaching a local maximum of the likelihood function. And note that these log likelihood is negative becausw the probability is between zero and one when you take logarithm, it becomes a negative value. So compare this with this one and will see at the probability is different. But note that before we just set these parameter values randomly, But with this guess we will have a somewhat improved estimate of this. But the last column is also by product and this actually can also be very useful and you can think about that. And of course, this new generation of parameters would allow us to further adjust the infer the latent variable or hidden variable values. In this step we simply take advantage of the inferred values and then just group words that are in the same distribution like this from background, including this as well. That were interested in so these will help us re estimate these parameters. And one use is to. So the EM algorithm with iteratively improve our initial estimate of parameters by using E-step first and then M step. the E step is to augment the data with additional information like Z.So this is indeed a general idea of the expectation maximization, or EM algorithm
 We see the likelihood function. Here improve is guaranteed in terms of the likelihood function. And we don't direct it, just computed this likely or function, but we computed the latent variable values and. In the end step, then would exploit such augmented data, which would make it easier to estimate the distribution to improve the estimate of parameters. Be cause it's a lower bound, we are guaranteed to improve this gas. But in the case of mixture model, we cannot easily find the analytical solution to the problem. And this actually in general is a difficult problem in numerical optimization. So the E step is basically to compute this lower bound. This is a general algorithm for computing. The general idea is that we will have two steps to improve the estimate of parameters in the E step. To summarize, in this lecture we introduce the EM algorithm. In our case, this is the distribution that has been used to generate the world. These are basically part of this lower bound. We roughly all augmenting our data by predicting values of useful hidden variables that we would use to simplify the estimation. This helps determine the lower bound the M step on the other hand, is to maximize the lower bound. And that we can then map to the original like role function. Right, because we improve our lower bound and then the original lighter Holder curve which is above this lower bound will definitely be improved as well. Knowledge about some of the inequalities that we haven't really covered yet. Note that it's not necessary that we will have a stable converged parameter values, even though the likelihood function is insured to increase. An once we fit the lower bound we can then maximise the lower bound and of course the reason why this works is because the lower bound is much easier to optimize so we know our current gas is here an by maximizing the lower bound will move this point to the top two here. And this is the one that we hope to maximize an we hope to find a set of value at this point to maximize this. Maximum regular is made of all kinds of mixture models. So in our example, the current gas is parameter value given by the current generation and then the next guest is the RE estimated parameter values.So I just showed you that empirically the likelihood will converge, but theoretically it can also be proved that EM algorithm with converge to a local maximum. So not just for our simple mixture model and so here climbing algorithm so can only converge it or local maximum, and it would depend on initial points. From this illustration you can see the next gas is always better than the current gas unless it has reached the maximum where it would be stuck there. So we have to resolve a numerical algorithm. First will fix a lower bound of likelihood function, so this is the lower bound you can see here
 A topical theta sub k. Maximum likelihood estimator. Lambda sub B versus non background. And background help. Specifically we have K topics. K among these K topics. The other is the topic coverage distribution. One is the topic category characterization Seedies HCI is a water distribution and 2nd it's the topic coverage for each document. Say this is the most basic topic model. And the other is the word distributions that characterize all the topics.This lecture is about the probabilistic latent semantic analysis or P LSA. So here I illustrate how we can generate the text that I was multiple topics. second, we see the background language model and typically we also assume this is known. In this lecture we're going to introduce probabilistic latent semantic analysis, often called the PLSA. The probability that the world is generated from the background model is Lambda multiplied by the probability of the world from the background model, right? Two things must happen. Also, one of the most useful topic models. All the words must have probabilities that sum to 141 distribution. We also see background words or mixed with them, so the goal of topic analysis here is try to decode these topics behind the text. So these are the tasks of topical model. We can use a large collection of text or use all the tests that we have available to estimate the water distribution. Now, this kind of models can in general be used to mine multiple topics from text documents, and PLSA is one of the most basic topic models for doing this, so let's first examine this problem in a little more detail. This represents the percentage of background words. Anna Document will have to cover precisely these K topics, so the probability of covering each topical would have to sum to one. If we can discover these topics can color this words as you see here to separate the different topics, then you can do a lot of things such as summarization or segmentation of the topics, clustering of sentences, etc. These are pie some ideas and they tell us which document covers which topic to what extent. How many unknown parameters are there? Now trying to figure out that question would help you understand the model in more detail, and it would also allow you to understand what would be the output that we generate when we use PLSA to analyze text data, and these are precisely the unknown parameters. So the formal definition of the problem of mining multiple topics from text is shown here, and this is actually a slide that you have seen in the earlier lecture, so the input is the collection, the number of topics and vocabulary set. And that's probability of Lambda sub B and then the second we must have actually obtained the world W from the background, and that's probability of W given sit out submit. And naturally, in all cases of probabilistic modeling, would want to figure out the likelihood function. First, we have to have chosen the background model. OK, so similarly we can figure out the probability of observing the world from another topic. You can see in the article we use words from all these distributions. Now the first line shows the probability of a word as illustrated on the previous slide and this is an important formula as I said. So next, once we have the likelihood function, we would be interested in knowing the parameters right? So to estimate the parameters. And similarly, the probability of generating the water from the second topic and his first topic popular like what you're seeing here and then. So again, it's a constrained optimization problem like what we have seen before, only that we have a collection of text and we have more parameters to estimate and we still have two constraints, different constraint, two kinds of constraints. So first we see Lambda sub b here. So let's first look at the how the world can be generated from the background model. So after we have obtained the likelihood function shown here, the next is to worry about parameter estimation. Those are the most important parameters that we are asked, so one is pies and these are the coverage of topic in the document. And of course, the text data right? And then the output is of two kinds. But now we have multiple is. That would believe exist in the text data and this can be unknown value that we set empirically. So the idea of PLSA is actually very similar to the two component mixture model that we have already introduced. So this one minus Lambda B gives us the probability of actually choosing a topic. So we hope to generate these as output because there are many useful applications if we can do that. So before we have just one topic besides the background topical, but now we have more topics. Just accounting for more documents in the collection
 Represented by the topic. Allocated counts for each topic. And, We show that with maximum likelihood estimator we can discover topical knowledge from text data. And we also added a predetermined background language model to help discover discriminating topics. So to summarize, we introduced the PLSA model, which is a mixture model with K unigram language models representing K topics. We can assign a document to the topic cluster that's covered most in the document. And we split words among the topics. Why? Because Whether a word is from a particular topic, actually depends on the document. Because this background language model can help attract the common terms. This equation allows us to predict the probability that the word W in Document D is generated from topic theta sub j And the bottom one is the predicted probability that this word has been generated from the background. At all, sorry, these are probabilities of observing the word from each distribution, so you can see basically the prediction of word from topic theta sub-j is based on the probability of selecting that theta sub-j as a word distribution to begin to generate the world multiplied by the probability of observing the word from that distribution. We can assign the document to the topic cluster that has the highest pi. And in addition to this, we can also cluster terms and cluster documents. This gives us different distributions and these tells us how to improve the parameters? And as I just explained in both E step formulas, we have a maximum likelihood estimator based on the allocated word counts to topic theta sub-j. And the higher probability words can be regarded as in belonging to one cluster. But the normalization is different because in this case we are interested in the word distribution. In both cases we are using the bayes rule as I explained, basically assessing the likelihood of generating word in from each distribution and is normalized. We can also aggregate topics covered in documents associated with a particular author, and then we can characterize the topics written by this author, etc. So remember pis indicate to what extent each topic is covered in the document. And such detailed characterization of coverage of topics in documents can enable a lot of further analysis. And that might give a different guess of word for word in different documents, and that's desirable. You can see this is the same discounted count, it tells us to what extent we should allocate this word to topic theta sub-j. One is k-word distributions, each representing a topic and the other is the proportion of each topic in each document. and re-estimate with normalizing. In our case we are interested in all those coverage parameters-- pis--and word distributions, thetas. And similarly, the bottom one is to re-estimate the probability of word for topic. This tells us how likely this word is actually from theta sub-j, and when we multiply them together we get the discounted count that's allocated for topic theta sub-j and we normalize this over all the topics we get the distribution over all the topics to indicate the coverage. The pis are tied to each document. In fact, each topic can be regarded as a cluster, so we already have term clusters. For all the words and that would give us a word distribution. Note that we use Document D here to index the word. Or we can renormalize based on the. Each document can have a potentially different pis, right? The pis will then affect our prediction, so the pis are here, and this depends on the document. So in this case we can re estimate our coverage probability and this is re estimated based on collecting all the words in the document. In contrast, here we normalized among all the topics. And in general, there are many useful applications of this technique. And then we're going to look at the to what extent this word belongs to the topic's theta sub-j, and this part is our guess from E-step. And that's why we have the count of the word in document and sum over all the words
 Or we can prevent a topic from being used in generated document.This lecture is about the latent Dirichlet allocation or LDA. For example, we might expect to see retrieval models as a topic in information retrieval. The second is to extend the PLSA as a generative model fully generated model. Basically is to maximize the posterior distribution probability and this is a combination of the likelihood of data and the prior. We can also use the prior to favor set of parameters with topics that assign high probabilities to some particular words. Now, a user may also have knowledge about the topic coverage. All kinds of preferences and constraints. The document must be generated by using the topics corresponding to the assigned tags. We can use this prior, which is denoted as P of Lambda to encode. And you may recall in Bayesian inference we use prior together with data to estimate parameters, and this is precisely what will happen. This has led to the development of Latent Dirichlet Allocation or LDA. Document D will set to the Pi value to 0 for that topic. So in this case we can use maximum a posteriori estimate, also called map estimate, and the formula is given here. So battery obviously will have a high pseudocounts similar life would have also high pseudocounts or the other words. And we may know which topic is definitely not covered in which document or is covered in the document. The map can be computed using a similar EM algorithm as we have used for that maximum likelihood estimator with just some modification to smallest parameters reflect the prior preferences. And in such a estimate, if we use a special form of the prior called conjugate prior, then the functional form of the prior will be similar to the data. Additional counts to reflect our prior right? So here you can see the pseudocounts are defined the based on the probability of words in our prior. And this is only reasonable, of course, when we have prior knowledge that strongly suggests this. So for example, we can use this to encode the need of having precisely 1 background the topic. In practice, when we apply PLSA to analyze text data, we might have additional knowledge that we want to inject to guide the analysis. This is also very useful, but sometimes a user might have some expectations about which topics to analyze. Now this can be encoded as a prior because we can say the prior for the parameters is only a non zero if the plan does contain one topic that's equivalent to the background language model. The standard PLSA is going to blindly listen to the data by using maximum likelihood estimator. And those tag could be treated as topics if we do that, then a document that can only be generated using topics corresponding to the tags already assigned to the document. For example, we can force the document D to choose topic one with probability of 1/2. It doesn't mean conjugate prior is the best way to define the prior. We have 0 pseudocounts because their probability is zero in the prior and when you see this is also controlled by a parameter mu and We're going to add mu multiplied by the probability of W given our prior distribution to the connected counts. We can also impose some other constraints. Now if we use map estimator with the conjugated prior, which is Dirichlet prior Dirichlet distribution based on this preference, then the only difference in the EM algorithm is in the M step. So we can say the third topic should not be user generated. To give more probabilities to these words by adding them to the pseudocounts so to artificially in effect, we artificially inflated their probabilities and to make this distribution we also need to add this many pseudocounts to the denominator. And this would mean we don't allow that topic to participate in generating that document. For example, we might have seen those tags topic tags assigned to documents. When we re estimate word distributions, we are going to add. We also may be interested in certain aspects such as battery and memory when looking at the opinions about the laptop, because the user is particularly interested in these aspects. For example, we can set any parameters for constraints, including zero as needed. As a result, we can combine the two and the consequences that you can basically convert the influence of the prior into the influence of having additional pseudo data because the two functional forms are the same and they can be combined. So the prior says that the distribution should contain one distribution that would assign high probabilities to battery, and life
 Influence. And PLSA is the basic topic model, and in fact the most basic topic Model. It means it can compute the topic coverage and topic word distributions as in PLSA. So LDA is proposed to improve that and it basically to make PLSA a generative model by imposing a Dirichlet prior on the model parameters. And you can achieve the same goal as PLSA for text mining. So to summarize, our discussion of probabilistic topic models and these models provide a general principal way of mining and analyzing topics in texts with many applications. And similar here. The 2nd paper has a discussion about how to automatically label a topic model. So these word distributions. In this case we have to use Bayesian inference or posterior inference to compute them based on the parameters Alpha and beta. By using the parameters of alphas and beta. The topic word distributions are drawn from another Dirichlet distribution with beta parameters and note that here Alpha has K parameters corresponding to our inference on the k values of pis for a document, whereas here beta has N values corresponding to controlling the N words in our vocabulary. And that's shown here. Each topic is characterized by a word distribution, and we're going to also output proportions of these topics covered in each document. And this gives us the probability of getting a word from a mixture model. First is a nice review of probabilistic topic models. Now LDA improves over PLSA by imposing priors. These parameters that we are interested in, namely the topics and the coverage, are no longer parameters in LDA. Another integral here. The Dirichlet distribution is a distribution over vectors, so it gives us a probability for a particular choice of a vector. So this is a likelihood function for LDA. So we can use the maximum likelihood estimated to compute that. And this formula is a sum of all the possibilities of generating the word inside the sum is a product of the probability of choosing a topic multiplied by the probability of observing the world from that topic. First, it's not really generating model because we cannot compute the probability of a new document. And this is also often adequate for most applications. Right, so basically in the LDA we just added these integrals to account for the uncertainties and we added of course the Dirichlet distributions to govern the choice of these parameters, pi's and theta's. Now next in the probability of a document we see there is a PLSA component in the LDA formula. And this is for convenience to introduce LDA and we have one vector for each document. The likelihood function now is more complicated for LDA, but there's a close connection between the likelihood function of LDA and PLSA, so I'm going to illustrate the difference here. Dirichlet is just a special distribution that we can use to specify prior. For example, you might favor a choice of relatively uniform distribution of all the topics, or you might favor generating skewed coverage of topics, and this is controlled by Alpha. Now I've shown some distributions and they intuitively suggest the topic, but what exactly is the topic? Can we use phrases to label the topic to make it more easy to understand? And this paper is about the techniques for doing that. However, in practice, LDA and PLSA intended to give similar performance, so in practice, PLSA, an LDA, would work equally well for most tasks. First you see the first equation is essentially the same and this is the probability of generating a word from multiple word distributions. And in this case in theta we have one vector for each topic. And there was some heuristic. Now in this model, all these parameters are free to change and we do not impose any prior, so these word distributions are now represented as theta i vectors. The third one is empirical comparison of LDA and PLSA for various tasks. And then we're going to use the pi to further choose which topic to use, and this is of course very similar to the PLSA model. So we cannot compute the pis for future document. So this is a very important formula as I have stressed but multiple times and this is actually the core assumption in all the topic models and you might see other topic models that are extensions of LDA or PLSA and they all rely on this. So here and the other set of parameters are pis and we present as a vector also. But the LDA formula would add some integral here, and that's to explain to account for the fact that the pis are not fixed, so they are drawn from Dirichlet distribution. This, however, is not necessary problem for text mining because here we are often only interested in fitting the training documents that we have. Take for example pis, right? So this Dirichlet distribution tell us which vector of pis is more likely, and this distribution itself is controlled by another vector of parameters of alpha's. Depending on alpha's, we can characterize the distribution in different ways and with force certain choices of pi's. Now I remove the background model just for simplicity. That's why we have to take the integral to consider all the possible pi's that we could possibly draw from this Dirichlet distribution
 Becausw clusters likely represent different senses of ambiguous word. For assessing similarity. So what is text clustering ? Clustering actually is a very general technique for data mining. So more generally, why is text clustering interesting? Well, it's brcause it's a very useful technique for text mining, particularly exploratory text analysis. Results returned for a query. And we call this perspective the clustering bias. And so, for example, we can cluster documents in the whole text collection. A sense about the major topics or what are some typical or representative document in the collection? And clustering help us achieve this goal. And Cluster of terms can be used to define the concept or theme or topic. Terms are objects. So there are in general many applications of text clustering any. And when the query is ambiguous, this is particularly useful. Similarity is not well defined. The goal of doing more sophisticated mining and analysis of text data. If you take the terms with high probabilities from world distribution. We sometimes also want to link similar text objects together and these. This is very important that technique for doing topic mining an analysis. Furthermore, text clusters can also be further clustered. And the problem lies in how to define similarity. These objects might be duplicated content for example, and in that case such a technique can help us remove redundancy, removing duplicated documents. For example, we might extract all the text segments about the topic, let's say by using a topic model. In particular, in this lecture organ to start with some basic questions about the clustering: What is text clustering and why we are interested in text clustering? In the following lectures, we're going to talk about how to do text clustering and how to evaluate the clustering results. For example, they can be documents, turns, passages, sentences or websites. So there are many examples of text clustering.This lecture is the first one about the text clustering. And when you define a clustering problem, it's important to specify your perspective for similarity or for defining the similarity that would be used to group similar objects 'cause otherwise. We may also use text the clustering to create a structure on the text data, and sometimes we can create a hierarchy of structures and this is very useful for browsing. So this is a case of combining text clustering with some other techniques, and in general you will see a lot of text mining algorithms can be actually combined in a flexible way to achieve. From there. One is to cluster search results for example and You can imagine a search engine can cluster the search results so that user can see overall structure of those. Regenerate the hierarchy that that's 'cause we can in general, cluster any text object at different levels. Similarly, we can also cluster articles written by the same author, for example. We may also use text clustering to induce additional features to represent text data when we cluster documents together, we can treat each cluster as a feature and then we can say when a document is in this cluster and then the feature value would be one and if a document is not in this cluster, then the future value is zero and this helps provide additional discrimination that might be used for texture classification as we will discuss later. Without perspective, it's very hard to define what is the best clustering result. Another application is to understand the major complaints from customers based on their emails, right? So in this case we can cluster email messages and then find the major clusters. We can also cluster fairly large text law gets, and by that I just mean text objects may contain a lot of documents. So as you can see clearly here, depending on the perspective will get different clustering results, so that also clearly tells us that in order to evaluate the clustering result we must use perspective. In fact, the topic models that you have seen some previous lectures. Another example is to just a cluster any texts segments, for example passages, sentences or any segments that you can extract the from a large text objects. We may be able to cluster terms in this case. Sometimes they are about the same topic and by linking them together we can have more complete coverage of the topic. In this way, we might group authors together based on whether they are published papers or similar. The idea is to discover natural structures in the data. And so a typical scenario is that you are getting a lot of text data. Has to be clearly defined in order to have well defined clustering problem
 So that's N documents and topics. We generated each word independently.This lecture is about the generative probabilistic models for text clustering. And here we hope to estimate a topic model or word distribution based on precisely one document, and that's when we assume that this document covers precisely one topic. The topics and each document that covers one topic, and we hope to embed such such preferences in a generative model. For document clustering where we hope this document will be generated from precisely one topic. As shown here. For example, we can consider there are N documents, each covers different topic. An CI and CI is decision for the document i. In text clustering, however, we only allow a document to cover one topic. But we can further allow these documents share topics and then. Each is a word distribution and the other is a pi ij's and these are the probabilities that each document covers each topic. It has allowed multiple topics to contribute the words to the document. One is generating probabilistic models, which is the topic of this lecture, and later will also discuss similarity based approaches. So to talk about generative models for text clustering, it would be useful to revisit the topic mining problem using topic models. And of course, the main problem in document clustering is to infer. Now a main difference between this and text clustering problem is that here a document is assumed to possibly cover multiple topics, and indeed in general document will be covering more than one topic with non zero probabilities. One is a set of topics denoted by Theta i's. In the case of document clustering. In this lecture we can do continue discussing text clustering, and we're going to introduce generative probabilistic models as a way to do text clustering So this is the overall plan for covering text clustering in the previous lecture we have talked about what is text clustering and why text clustering is interesting. As illustrated here, we no longer have multiple topics covered in each document is precisely one topic, although which topic is still uncertain. Is theta one or theta two right? And then it's this probability is multiplied by the probability of observing this document from this particular distribution. Multiple distribution could have been used to generate the words in the document. And if we have N documents for share k topics, then will again have precisely the document clustering problem. Model that will give us a probability of a document. Using a particular distribution is made just once for document clustering model, whereas in the topic model it's made multiple times. Four different words. One topic that we discussed earlier. If we assume one topic is a cluster. And basically tells us document Di is in which cluster. That means the words in the document could have been generated in general from multiple distributions. topic model with two components here. So this document must share some topics. Now in the topic model, we see that the sum is actually inside the product and that's be cause we generated each word independently. Here we made the assumption that each word is generated independently, so the probability of the whole document is just a product of the probability of each word in the document. We can also assume that we are going to assume there are fewer topics. The second is that word distribution here is going to be used to generate all the words for a document. But there's also some difference. But in the case of topic model we have to make as many decisions as the number of words in the document because for each word we can make a potential different decision and that's the key difference between the two models. A product of two probabilities and one is the probability of choosing a distribution. Notice that in such a generative model. But when we generate each each word, we have to make a decision regarding which distribution we use. And if you further expand this probability of observing the whole document, we see that it's product of observing each word X sub i. Basically we make the decision once for this document and stay with this to generate all the words. But in the case of topic modeling, one distribution doesn't have to generate with all the words in a document. But if you think about the main difference between this problem and the topic model that we talked about earlier and then you will see a main requirement is how can we force every document to be generated from precisely one topic instead of K topics? As in the topic model. Of course, in this case these documents are independent and these topics also independent. Theta two, whereas text is more likely generated from the first one on the top. So because of these connections, naturally we can think about how to use a probabilistic generating model to solve the problem of text clustering. That means if we change the topic definition just slightly by assuming that each document can only be generated by using precisely one topic. The sum corresponds to the choice. Problem of mining. If we really have one topic to correspond to one cluster of documents, then we would have a document to be generated from precisely one topic. But we can also consider some variations of the problem. Choosing the world distribution. So here we see that when we generate a document. The other is the probability of observing a particular data point from that distribution. Which distribution has been used to generator a document and that would allow us to recover the cluster identity over document So it would be useful to think about the difference from the topic model, as I have also mentioned multiple times. And in particular, the differences on the top you see for the mixture model, document clustering, we first take a product and then take a sum. Here you will see the probability of observing a document D is basically a sum, in this case over two different distributions. But in general, ideas are all mixture models that we can estimate these models by using the EM algorithm as we will discuss more later. So the question now is what generating model can be used to do clustering
This lecture is a continued discussion of generative probabilistic models for text clustering. This is general presentation of the mixture model for document clustering. But we have similar parameters. We have a set of theta i's denote the word distributions corresponding to the K unigram language models. That means a large cluster and also a high probability of generating D. With two distributions in two component mixture model for document clustering. That most likely has been used to generate D. But of course, given a particular document that we still have to infer which topic is more likely. We first choose a theta I according to probability of theta I and then generate all the words in the document using this distribution. The more likely a cluster is used to generate the document, we can assume the larger the cluster size is. An P of theta i can be interpreted as. That means each document can have a potentially different choice of topics, but here we have a generic choice probability for all the documents. And that intuitively makes sense because the chance of a document being a large cluster is generally higher than in a small cluster. We have P of each theta I as the probability of selecting each of the K distributions to generate the document. Cluster that has the highest probability of being able to generate the document. You may recall that the topic choice in each document actually depends on D. For this mixture model. Intuitively, that makes sense. All the words in the document. In this lecture, we're going to continue talking about the tax capture text clustering, particularly generative probabilistic models. It's a little bit different from the topic model. To generate the document so in that sense, we can still have a document dependent probability of clusters. So the model basically would make the following assumption about the generation of the document. So this means once we can estimate the parameters of the model, then we can easily solve the problem of document clustering. Note that unlike in PLSA and this probability of theta I is not dependent on D. Indicating the size of cluster because it tells us how likely cluster would be used to generate the document. In this case the prior is P of Theta I. And if we choose theta based on this posterior probability and we would have the following formula that you see here. Think about the model. Let's first you might think about a way to use likelihood only, and that is to assign D to the cluster corresponding to the topic Theta I. And together, that is, we're going to use the base formula to compute the posterior probability of Theta given D. So as more cases we follow these steps using a generated model. And this, as you see later, would allow us to assign a document to the. So we're going to favor a cluster that's large and also consistent with the document. So next we have to discuss how to actually compute the estimate of the model. And so a better way is to use the likelihood together with the prior
 Given the document.This lecture is a continued discussion of generative probabilistic models for text clustering.6 for Theta 1 so Theta 1 is more likely than Theta 2. That's most likely to have generated the document. Discount them by the probabilities that each document is likely be generated from Theta 1. well, whereas the probability of a word given Theta is a probability distribution over all the words. So to summarize, our discussion of generating models for clustering. So basically this normalizes the probability of generating this document by using this average word distribution. Indeed in this case the document is more likely to be generated from Theta 1,  much more likely than from than Theta 2. So in this case, each class is represented by unigram language model or word distribution, and that's similar to topic model. That's why you see the numerator has the product of the probability of selecting Theta one and the probability of generating the document from Theta 1. A document that is generated by first choosing a unigram language model and then generating all the words in the document that using this single language model and this is very different from again topic model where we can generate the words in the document by using multiple unigram language models. So in our case the output from a mixture model or the observation from mixture model is a document not a word. And let's assume we randomly initialized to probabilities of selecting each cluster as . And then the estimated model pamateter will give both a topic capitalization of each cluster and the probabilistic assignment of each document into a cluster. So equally likely. And this probabilistic assignment that sometimes is useful for some applications. And more specifically, basically we're going to apply Bayes rule to infer, or which distribution is more likely to have generated this document or computing the posterior probability of the distribution. In this lecture we're going to finish the discussion of generative probabilistic models for text clustering. We showed that a slight variation of Top Model can be used for clustering documents and this also shows the power of generating models in general by changing the generation assumption and changing the model slightly we can achieve different goals and we can capture different patterns in text data. One is P of Theta and this is the probability of selecting a particular distribution. We can use a normalized. we're going to infer which distribution has been used to generate each document. And here we just pulled the counts of words that are in documents that are inverted to have been generated from a particular topic Theta I here and this would allow us to then estimate how many words have actually been generated from Theta I. And then we normalize again. An we know it's proportional to the probability of selecting this distribution P of Theta I and the probability of generating this whole document from that distribution, which is a product of all the probabilities of words for this document, as you see here. So here you can see the word distribution actually generates a term cluster as a byproduct. So from the E step we can see our estimate of which distribution is more likely to have generated a document, and you can see D1 is more likely from the first topic. I said I to generate a document and we put them together and again we normalize them into probabilities. There are two words, sorry, two occurrences of text and two occurrences of mining. Because that's the output from what mixture model. Use all the counts of text in these documents to estimate the probability of tax given still awhile, but we're not to use their raw counts or total account. So this is a slide that you have seen before and here we show how we define the mixture model for text clustering an what the likelihood function looks like and we can also compute the maximum liklihood estimate to estimate the parameters. Basically we need to re estimate all the parameters. But if we want to achieve a harder clusters mainly to partition documents into disjoint clusters. It must have also been generating the four words in the document, namely two occurrences of text and two occurrences of mining. And if this constraint then you can easy to compute this distribution as long as. To infer which cluster is more likely, and so this is proportional to the sum of the probability of Z sub DJ is equal to I. Now the other kind of parameters are the probabilities of words in each distribution, each cluster, and this is very similar to the case of PLSA. In this case we need to use a special normalization technique to avoid underflow. Note that it's very important to understand these constraints as they are precisely the normalizers in all these formulas, and it's also important to know that distribution is over what? For example, the probability of Theta is overall the key topics and that's why these K probabilities sum to 1. So there estimation involves two kinds of parameters. So we can then divide the numerator and the denominator both by this normalizer. And this does the average distribution will be comperable to each of these distributions. So you can see the probability of Theta 2 would be naturally . These counts into probabilities so that the probabilities on all the words some to one. And since we have used exact the same normalizer for the numerator and denominator, the whole value of this expression is not changed. Before we observe anything, we don't have any knowledge about which cluster is more likely, but after we have observed these documents, then we can collect the evidence. Imagine if a document has many words and it's going to be a very small value here, as it can cause the problem of underflow. Then we can just force the document into the cluster corresponding to the water distribution. And then so this is for P of Theta sub I. What about these world probabilities? What we do the same? And intuition is the same, so we're going to see in order to estimate the probabilities of words in Theta one, we're going to look at which documents have been generated from Scylla and we're going to pull together the words in those documents and normalize them
 Given K clusters. Both approaches can and generate the term clusters and document clusters. The model defines clustering bias. And that is similarity based approaches. Based on probabilistic allocations.This lecture is about the similarity based approaches to text for clustering. An term clusters can be in general generated by representing each term with some text content. Directly specify similarity functions. Complex generative models can be used to discover complex clustering structures. Anne, try to maximize the intragroup similarity and minimize the intergroup similarity. To define the clustering bias, there's no explicit definer similarity function. So to summarize our discussion of clustering methods, we first discussed the model based approaches, mainly the mixture model. So the general idea of similarity based clustering is to explicitly specify a similarity function to measure the similarity between 2:00 text objects. Of course, term clusters can be generated by using generative models as well as we have seen. We also talked about the similarity based approaches. And the clustering structure is built into a generated model. Which is called K means clustering will represent each text object as a term vector and then assuming similarity function defined onto objects. And here we use is implicitly similarity function. Now this is in contrast with a generative model where we implicitly define the clustering bias. The K means algorithm has a clearly defined the objective function, but it's also very similar to a model based approach. But rather we make a choice. We did not talk about it, but we can easily design generated model to generate a hierarchical clusters. Now, there are many different methods for doing similarity based clustering. So here we have a better clustering result by adjusting the centroid. By using a particular objective function like a likelihood function. For example, take the context of each term as a representation of each term as we have done in paradigmatic relation learning. We can also use prior to further customize clustering algorithm to for example, control the topic of 1 cluster or multiple clusters. In general, I think we can distinguish two strategies at high level. And then we can certainly cluster terms based on actually their tax representations. And this is. Can be expected to generate the loose clusters. The other general strategy is to start with the initial tentative clustering and then iteratively improve it and this often leads to a flat clustering. To adjust the centroid and then we had repeated this process until the similarity based on objective function. That's why we can use potentially a different model to recover different instruction. There are different variations here and there mainly differ in the ways to computer group similarity based on the individual object similarity. Now, these different ways of computing group similarities will need to different clustering algorithms, and they will generally give different results. In that when we allocate vector into one of the clusters based on our tentative clustering, it's very similar to inferring the distribution that has been used with generally the document in the mixture model. And there's often a tradeoff between achieving both goals. So this is a general idea. So as I just said, there are many different clustering methods available and. In particular, we're going to cover a different kind of approaches than generative models. We are only given the similarity function or two objects, but as we group groups together we also need to assess the similarity between two groups. However, one disadvantage of this approach is that there is no easy way to direct or control the similarity measure. Now we're going to start with some tentative clustering result by just selecting Kate randomly selected vectors as centroids of K clusters and treat them as sentence as they represent each cluster. The reason is becausw. These approaches are more flexible. In this lecture, we're going to continue the discussion of how to do a text clustering. Sometimes we want to do that, but it's very hard to inject the such a explicit definition of similarity into such a model. But in nature they are very similar and that's why it's also maximizing where defined objective function and it's guaranteed to convert converted local minimum. Close so it's optimistic view of similarity. And then we're going to recovery, compute the centroid based on the allocated objects in each cluster. The average link, of course is in between and group decision, so it's going to be insensitive to outliers. Now the next we're going to adjust the centroid, and this is very similar to M step where we re estimate the parameters. One is to progressively construct the hierarchy of clusters. That is, to ensure the objects that are put in the same group to be similar, but the objects that are put into different groups to be not similar, and these are the general goals of clustering. But the question now is, how can we compute the similarity between the two groups? And then we can in general basis on the similarities of the objects in the two groups. And note that the central is adjusted based on the average of the vectors in the. So let's illustrate how can induce a structure based on just similarity. Of course based on the provider similarity function and then we can see which pair has the highest similarity and then just group them together. Average link defines the similarity as average of similarity of all the pairs of the two groups. And there are also different ways to do that, and there's the three popular methods are single link complete link an average link? So given two groups and singling algorithm is going to define the group similarity as the similarity of the closest repair of the two groups. The whole process is driven by optimizing the likeable, but here we explicitly provide a review of what we think are similar, and this is often very useful because then it allows us to inject any particular view of similarity into the clustering program. Indeed, this algorithm is very similar to the EM algorithm for the mixture model for clustering. In this case, we are giving a similarity function calls to measure similarity between two objects and then we can gradually group similar objects together in a bottom up profession to form larger and larger groups, and they also form a hierarchy and then we can stop when some stopping criterions that. And we're not going to say exactly which distribution has been used to generate the data point. And so this often leads to hierarchical clustering an we can further distinguishes two ways to construct the hierarchy depending on whether we started with the collection to divide the collection or start with individual objects and gradually group them together. And once we have these Central Eastside, we're going to assign a vector to the cluster hosts entry that is closest to the current vector. The hierarchical clustering algorithm, on the other hand, is. But here we can talk about the two representative methods and. So start with all the text objects and we can then measure the similarity between them
 There are many approaches that can be used for text clustering and we discussed them: Model based approaches and similarity based approaches. So to summarize text clustering, it's a very useful unsupervised general text mining technique as particularly useful for obtaining an overall picture of the text content. The 2nd way to evaluate text clusters is to do indirect evaluation. And that's precisely why the perspective or clustering bias is crucial for evaluation. One is direct evaluation and the other is indirect evaluation. And finally, evaluation of clustering results and can be done both directly and indirectly.This lecture is about evaluation of text cluster. Basically, humans would bring the needed or desired clustering bias. Mutual information captures the correlation between these cluster labels and normalized mutual information is often used for quantifying the similarity for this evaluation purpose. In general, we can evaluate text clusters in two ways. And this is sometimes desirable. In this case, the clustering bias is imposed by the intended application as well. So in this case we call it indirect evaluation of clusters because there's no explicit assessment of the quality of clusters, but rather its to assess the contribution of clusters to a particular application. So in directl valuation, we want to answer the following question: How close are the system generated clusters to the ideal clusters that are generated by humans? So the closeness here can be assessed assessed from multiple perspectives and that would help us characterize the quality of clustering results in multiple angles. F measure is another possible measure. And they will use their judgments based on the need of a particular application to generate what they think are the best clustering results. And ideally we want the system results to be the same as human generated results, but in general they are not going to be the same, so we would like to then qualify the similarity between the system generated clusters and the gold standard clusters, and this similarity can be also measured from multiple perspectives and this will give us various measures to quantitatively evaluate a cluster clustering result and some of the commonly used measures include purity, which measures whether a cluster has similar objects from the same cluster in the gold standard and normalized mutual information is a commonly used measure which basically measures based on the identity of or the cluster of object in the system-generated results. This is often needed to explore text data. Also the effectiveness of a method highly depends on whether the desired clustering bias is captured appropriately, and this can be done either through using the right generative model, the model design, appropriate for clustering, or the right similarity function to explicitly define bias. So what counts as the best clustering result would be dependent on the application. And this would be then used to compare with the system generated clusters from the same test set. So here's some suggested reading, and this is particularly useful to better understand the how the measures are calculated and clustering in general. But in general, that has some implied application of clustering bias there, and that's just not specified. The second application or second kind of application is to discover interesting clustering structures in text data, and these structures can be very meaningful. So in this case the question to answer is how useful are the clustering results for the intended applications? Now this of course is application specific question, so usefulness is is going to depend on specific applications. And finally, you can see in this case we essentially inject the clustering bias by using humans. We also want to quantify the closeness because this would allow us to easily compare different methods based on their performance figures. So this perspective is also very important for evaluation. And then we're going to compare the performance of your clustering system and the baseline system in terms of the performance measure for that particular application
 to text content. Distinguishing spams from non spam. Distinguish it relevant documents from non relevant documents for a particular query. And. First is related to topic mining analysis. Applications of text categorisation. Finally, it's also related to text based prediction. And the categorisation results. Often together with non text data specifically to text. For example, topic categories. We can categorize restaurants or categorize products based on their corresponding reviews.This lecture is about the text categorization. And finally, author Attribution. Sorry, text categorization to discover knowledge about the world. We can in general categorize the observer based on the content. One is text Categorization helps us enrich text representation, and that's to achieve more understanding of text data that's always useful for text analysis. This is a very important technique for a text, data mining and analytics. And this is ontology of terms characterize content of literature articles in detail. These are categories that characterize content of text object. And the system will in general assign categories to these documents as shown on the right. And that's because it has to do with analyzing text data based on some predefined topics. It is relevant to discovery of various different kinds of knowledge as shown here. And text categorisation allows us to infer the properties of such entities that are associated with text data. Secondly, categories can also vary, and we can generally distinguish two kinds of categories. Yet another variation is joint categorization. And so this is a very important technique for text data mining. So this means we can use text categorization to discover knowledge about the world in general, as long as we can associate the entity with text data, we can always use the text data to help categorize the corresponding entities. Because we can often use text categorization techniques to predict some variables in the real world that are only remotely related to text data. Semantic categories assigned can also be directly or indirectly useful for application. Analytics, where we are often interested in using text data as extra sensor data collected from humans to infer certain desicion factors. Secondly, it's also related to opinion mining and sentiment analysis, which has to do with discovering knowledge about the observer that the human sensor. So this is a general way to allow us to use text mining tool. Spam filter is interesting. So first text objects can vary, so we can categorize a document. There are, there's a meaningful connection between the entity and text data. For example, K category categorisation task can be actually performed by using binary categorization. This is the overall plan for covering the topic. And sometimes you miss seeing some applications, text or categorization is called a text coding encoding with some controller vocabulary. A more general case would be K-category categorization and there are also many applications like that. News categorization is very common, has been stuided. So now with categorisation, text can be represented in multiple levels, meaning keyword bag of words representation as often used for a lot of text processing tasks. Sentiment categorization of product reviews or tweets is yet another kind of applications where we can categorize content into positive or negative or positive and negative or neutral. Try to leverage the dependents of these tasks to improve accuracy for each individual task. And then these text data can help us infer properties of product or a restaurant. And another variation to have hierarchical categorization, where categories form hierarchy, again, topical hierarchy is very common. But you can imagine you can build automatic text categorization system to help routing a request. Or we can have any other meaningful categories associated with text data, as long as. Another example of application spam, email detection or filtering right? So we often have a spam filter to help us distinguish spam from legitimate emails, and this is clearly a binary classification problem. For example, authors or entities associated with the content that they produce. We can further categorize into sub categories etc. The obvious entities that can be directly connected are authors, but you can also imagine the authors affiliations or the authors ages and other things can be actually connected to text data indirectly. When we do text categorization, we have a lot of text objects to be processed by a categorisation system. That's when you have multiple categorization tasks that are related. Because we can categorize the authors, for example, based on the content of the articles that they have written. And often also a set of training examples or training set of labeled text objects. But we can also add categories and they provide 2 levels of representation. Now among all these, binary categorization is most fundamental and partly also because it's simple and partly it's cause it can actually be used to perform all the other categorization tasks. The second kind of reasons is to use text categorization to infer properties of entities
 Conditional. 1st. And once we can estimate these models, then we can compute this conditional probability of label given data based on. One example is naive Bayes classifier. The probability of data given label. You can easily identify text data.This lecture is about the methods for text categorization. The main topic could be another topic, different topic then sports. So we first model distribution of labels and then we model how the data is generated given a particular label here. But if we can model the data in each category accurately, then we can also classify accurately. But they're not as powerful as non linear combination, but nonlinear models might be more complex for training. And in the case of text, natural choice would be the words. So the objective function tends to directly measure the errors of categorisation on the training data. And then the train classifier can be applied to a new text object to predict the most likely category, and that's to simulate the prediction of what a human would assign to this text object. Some examples include the logistical regression support vector machines and the K nearest neighbors. And this is called a training data. Probability of the label given the data point directly. But one can also imagine some text articles that mention these keywords. Often the rules aren't 100% reliable take for example, and looking at the occurrences of words in text and try to decide the topic. So these discriminative classifiers attempted to model the. The distribution of labels and join the probability of sorry the conditional probability of X given Y so it's Y. And they will also combine multiple features in a weighted matter with weights to be optimized to minimize the errors on the training data. So in such approaches, the objective function is actually likelihood, so we model how the data are generated, so only thus it only indirectly captures the training errors. There are many methods for text categorization In such a method, the idea is to determine the category based on some rules that we design carefully to reflect the domain knowledge about the categorization problem. Secondly, the categories have to be easy to distinguish based on surface features in text, so that means superficial features like keywords or punctuations or whatever. Now this is the most important thing 'cause this conditional probability of the label can then be used directly to decide which label is most likely. And then Secondly the human experts also need to provide a set of features to represent each text object that can potentially provide a clue about the category. For example, if there is some special vocabulary that is known to only occur in a particular category, and that would be most effective because we can easily use such a vocabulary or pattern of such a vocabulary to recognize this category. So in general, we can distinguish the two kinds of classifiers at a high level one is going to generative classifiers. First, the human experts must annotate datasets with category labels, will tell the computer which documents should not receive which categories. The other is called discriminative classifiers. It's the basis for learning. And the label distribution here by using the base rule. So it attempts to model the join the distribution of the data and the label X&amp;Y. They also tend to vary in their ways of combining the features, so linear combination for example is simple is often used. So in general, all methods would rely on discriminating features of text objects to distinguish different categories, so that's why these features are very important and they have to be provided by humans. Different methods tend to vary in their ways of measuring the errors on the training data. And it's going to also figure out how to optimally combine features to minimize errors of categorisation on the training data, so the training data as you can see very important. So once human experts can provide this, then we can use machine learning to learn soft rules for categorization from the training data. So using each word as a feature is a very common choice to start with. So ultimately, the learning processes optimization problem and the objective function is often tide to the errors on the training data. So in this lecture were going to discuss how to do text categorization
 Suppose D has L words represent represent as Xi here. Each topic is 1 cluster.This lecture is about how to use generative probabilistic models for text categorization. T sub I and that's denoted by C of w and T sub I. And this is also called IDF weighting inverse document frequency weighting that you have seen in mining word relations. And this is generally not accurate. And Bayes rule allows us to update this probability based on the prior and I shown the details. So which word has higher probability? Well, we simply count the word occurrences in the documents that are known to be generated from theta i. One is generative probabilistic models, the other is discriminative approaches. But because of the similarity of the two problems, we can actually adapt document clustering models for text categorization. To estimate all these parameters. But this conditional probability here Is the posterior probability of the topic after we have observed the document d. And this count of the word serves as a feature and to represent the document. Or we can understand how we can use generative models to do text categorization from the perspective of clustering. Perhaps the simplest case. And this is what we can collect from document. Now, how can you compute the probability that particular topic word distributions theta i has been used to generate this document? In general, we use bayes rule to make this inference. That means the more we observe such a word, the more likely the document is actually from theta 2. So introduce the beta zero to denote the bias and Fi to denote each feature, and then beta sub i, to denote the weight on which feature. But this assumption allows us to simplify the problem, and it's actually quite effective for many text categorization tasks. Once you see some word and other words will more likely occur. So each word has the same probability. when a word doesn't occur in the document. And also to understand why adding a background language model will actually achieve the effect of idea of IDF weighting and to penalize common words. The probability of theta i and this indicates how popular each category is or how likely we would have observed the document in that category. When we go from the posterior probability of the topic to a product of the likelihood and the prior. Theta sub b Now that background model in general can be estimated by using a large collection of text, or in this case we can use the whole set of all the training data to estimate this background language model. Here it's the weight on each word and this weight. So to estimate the probability of each category. Namely, we are going to assign document D to the category that has the highest probability of generating this document. Actually, this general form is very close to a classifier called logistical regression, and this is actually one of those conditional approaches or discriminative approaches to classification. What would happen? Or in this case, the estimate will approach to the background language model will tend to the background language model, so we would bring every word distribution to the same background language model. Because every category has some help from their background for words, like the, a which have high probabilities. Indeed, in text categorization, we typically assume that there are training data available and those are the documents that are known to have been generated from which category. And we simply just normalize this count to make this a probability. And this is related to how well this word distribution explains the document here, and the two are related in this way. Obvious Delta is a smoothing parameter here, meaning that the larger delta is and the more we will do smoothing and that means we'll more rely on pseudo counts and we might indeed ignore the actual counts if delta is set to Infinity. Now, this addition of background model would cause nonuniform smoothing of this word distributions we are going to bring the probability of those common words, or to a higher level because of the background model. We have observed some data from some model and we want to guess the parameters of this model. To what extent observing this word helps contributing to our decision to put this document in Category One. Before we observe any document. The other kind is word distributions and we want to know what words have high probabilities for each category. And this is related to the prior and the likelihood an as you have seen on the previous slide. And this has to do with whether the topic word distribution can explain the content of this document well. Because they get higher probability by the background language model so the pseudocounts added for such words would be higher, rare words on the other hand will have smaller pseudocounts. Therefore it's no longer so important that each category has documents that contain such a lot of occurrences of such word, or the estimate is more influenced by the background model and the consequences that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities. And so this is a slide that we've talked about before about text clustering, where we assume there are multiple topics represented by word distributions. So basically with naive Bayes classifier, we're going to score each category for a document by this function. So in general, in naiyes bayes categorization we have to do such smoothing and once we have these probabilities, then we can compute the score for each category for a document and then choose the category with the highest score as we discussed earlier. But in the case of categorization, we are given the categories. And you can see this Prior information here. That we need to consider if a topic or cluster has a higher prior then it's more likely that the document has been from this cluster, so we should favor such a cluster. From the background language model, those words don't get some help from the background language model, so the difference would be primarily because of the differences of the occurrences in the training documents in different categories. We know there is one word distribution that has been used to generate documents. Becausw we have some constraints on these distributions and so the normalizer is dictated by the constraint. then based on these categories and training data, we would like to allocate a document to one of these categories, or sometimes multiple categories. Essentially we are comparing the probability of the word from the two distributions and if it's higher according to theta one, then according to theta 2 then this weight would be positive and therefore it means when we observe such a word. So the problem of text categorization is actually very similar to document clustering in that we assume that each document belongs to one category or one cluster. And we're going to score based on this probability ratio. Just like in the case of Naive Bayes we can clearly see naive Bayes classifier is a special case of this general classifier. So if HF sub I is a feature value then we multiply value by the corresponding weight beta sub i and we just take sum and this is to aggregate. And to answer the question which category is most popular, then we can simply normalize the count of documents in each category. These documents have been all generated from category one, namely have been all generated using this same word distribution. And of course there are parameters here. Right, so these are the words that are observed in the document, but in general we can consider all the words in the vocabulary. An we now can see clearly how we can assign a documentary to a category based on the information about word distributions for these categories and the prior on these categories. What if Delta is zero? Well we just go back to the original estimate based on the observed training data to estimate the probability of each category. For example, if you have seen a word like a text, and then it makes categorization or clustering more likely to appear And if you have not seen text. And what about the basis for estimating the probability of word in each category? Well, the same and you'll be just assuming that words that are observed frequently in the documents that are known to be generated from a category. will likely have higher probability, and that's just the maximum likelihood estimator indeed, and that's what we could do. Now we can do the same for the word distribution, but in this case we sometimes we find it useful to use a non-uniform pseudo counts for the words. There are in general are two kinds of approaches to text categorization by using machine learning. So here you see we'll add pseudocounts to each word and that's mu multiplied by the probability of the world given by a background language model. Therefore in the sum we have to also add K multiplied by Delta as a total pseudo counts that we add to the estimate. Now it's also called a Naive because We've made an assumption that every word in the document is generated independently, and this is indeed a naive assumption, because in reality they are not generated independently. So this idea can be directly adapted to do categorization and This is precisely what Naive Bayes classifier is doing, so here it's mostly the same information, except that we're looking at the categorization problem now, so we assume that if Theta i represents category I accurately that means the word distribution characterizes the content of documents in category i accurately. When the data set is small, we tend to rely on some prior knowledge to to solve the problem. And here you see that T1 represents the set of documents that are known to have been generated from category one, and T2 represents the documents that are known to have been generated from category two, etc. So let's say we are considering category I or Theta I. And in the denominator we also add K multiplied by Delta because we want the probability to sum to one. So what are those words? Well those are the common words. In other words, we're going to maximize this posterior probability as well. When we use maximum likelihood estimator we often face the problem of zero probability. So mu is also non-negative constant and it's empirically set to control smoothing. If, on the other hand, the probability of the word from theta one is smaller than the probability of the word from theta 2, then you can see this weight is negative. And we generated each word in the document independently and we know that we have observed the set of N sub one documents in the set of T1. And so naturally, we can then decompose this likelihood into a product. There is another issue in Naive Bayes which is a smoothing. Now if we use such a background language model to add pseudocounts, we find that some words will receive more pseudocounts. And of course you can even use a mixture model to model what the document looks like in each category. We'll say that it's more likely to be from category One, and the more we observe such a word, the more likely the document will be classified as theta one. Now the keyword Bayes is understandable because we are applying a Bayes rule here. Why? Because already know which distribution has been used to generate which documents. Here in naive Bayes. So the idea then is to just use the observed training data to estimate these two probabilities. Of course in this case FI is the count of a word, but in general we can put any features that we think are relevant for categorization. We pretend that every category has actually some extra number of documents represented by Delta. So to find the topic that has the highest posterior probability here, it's equivalent to maximize this product as we have seen also multiple times in this course. So now the question is, how can we make sure each theta i actually represents category i accurate? Now, in clustering we learned this category i or the word distributions for category i from the data. We could, for example, assume the words may be dependent on each other, so that would make it a bigram language model or trigram language model. So more rigorously, this is what we would be doing, so we're going to choose the topic that will maximize this posterior probability of the topic given the document. And we want to pick a topic that's high by both values. In fact, that's the goal of text clustering. Get posterior becausw this one P of Theta i is the prior, that's our belief about which topic is more likely. So what are the parameters? Well These betas are the weights, and with appropriate settings of weights then we can expect the such a scoring function to work well to classify documents. In other words, these are the documents with known categories assigned, and of course human experts must do that. So in this case our prior knowledge says that no words should have zero probability, so smoothing allows us to inject this prior to make sure that no word has a zero probability. So this is called a Naiyes Bayes classifier. In other words, we make this probability proportional to the size of training dataset in each category. That we collect from the document that would help us make the decision and that each feature has a weight that tells us how does this feature support category one or support that support the category two, and this is estimated as the log of probability ratio
 And we mentioned that this is precisely similar to logistic regression. And that's basically the logistical regression function. The condition likelihood here is basically to model y given the observed X. And with M features. That are denoted as X.This lecture is about the discriminative classifiers for text categorization. But mathematically, this can also be regarded as a way to directly estimate the conditional probability of label given data that is P of Y given X. They try to model the conditional distribution of labels given the data directly rather than using Bayes rule to compute that indirectly. Note that this is a conditional probability of Y given X. And so in logistic regression, we basically assume that the probability of y = 1  given X is dependent on this linear combination of all these features. So the key assumption that we made in this approach is that the distribution of the label given the document or probability of a category given document. Basically the results might depend on the K and indeed K is an important parameter to optimize. So in our case we are interested in maximizing this conditional likelihood. And in each case, this is the modeled probability of observing this particular training case. But here we actually would assume explicitly that we would model our Probability of Y given X. But here we explicitly requires a similarity function. Obviously the assumption would be a problem, and then the classifier would not be accurate. So, as in other cases, when compute the maximum likelihood estimator Basically lets go find a beta value, a set of beta values that will maximize this conditional likelihood. So you can see basically the if we maximize the conditional likelihood we're going to basically try to make the prediction on the training data as accurate as possible. As directly as a function of these features. And this is also precisely what we want for classification. So this is the basic idea of logistic regression, and it's a very useful classifier that can be used to do a lot of classification tasks, including text categorization. And when Y is 1 it means the category of the documents first class theta 1 Now the goal here is to model the conditional probability of Y given X directly as opposed to model the generation of X&amp;Y as in the case of Naive Bayes. Now this kind of scoring function can be actually a general scoring function where we can in general represent text data as a feature vector. Basically now we can use the known categories of all the documents in this region to estimate this probability. So as I mentioned that KNN can be actually regarded as estimate of conditional probability of Y given X, and that's why we put this in the category of discriminative approaches. Then what we are saying is that in order to estimate the probability of a category given a document, we can try to estimate the probability of the category given that entire region. The probability that the document is in category one. This is a slide that you have seen from the discussion of Naive Bayes classifier, where we have shown that although naive Bayes classifier tries to model the generation of text data from each categories, we can actually use bayes rule and to eventually rewrite the scoring function as you see on this slide and this scoring function is basically a weighted combination of a lot of word features where the feature values are word count and the feature weights are the log of probability ratios of the word given by two distributions here. So the general idea of logistical regression is to model the dependency of the binary response variable Y here, On some predictors. That means the document is in topic one. So the numerator that you see here c of Theta and R is a count of the documents in region R with category Theta I. The training data here, X i and Y i and each pair is basically feature vector of X and a known label for that X Y, either one or zero. If D is not different, very different than we're going to assume that the probability of theta given D would be also similar, and so that's a very key assumption, and that that's. So the design of this similarity function is closely tied to the design of the features in logistic regression. Basically, effective features are those that would make the objects that are in the same category look more similar, but distinguishing objects in different categories. So the parameter as there has to be set empirically and typically you can optimize such a parameter by using cross validation. As we have seen in naive bayes. In this approach, we're going to also estimate the conditional probability of label. For example, probability of theta I given document D is locally smoothed and that just means we're going to assume that this probability is the same for all the documents in this region. It's not like a modeling X, but rather we're going to model this. To model the classifier, then the next step is to compute the parameter values. Now this similarity function. Given data, but in a very different way. So most specifically, we assume that log of the ratio of probability of y = 1 and the probability of y = 0. But the general idea is to look at the neighborhood and then try to assess the category based on the categories of the neighbors. And each feature has a value X sub I here and our goal is model the dependency of this binary response variable on all these features. The key point here is that the function form here depends on the observed. But if our similarity function could not capture that. So in our categorization problem we have two categories, lets say theta 1 and theta 2, and we can use the Y value to denote the two categories. You can see, and that's precisely what we want. And the function form looks like this. OK, so much for logistical regression. In this lecture, we're going to continue talking about how to do text categorization and cover discriminative approaches. And if you think about when we want to maximize this probability we will basically going to want this probability to be as high as possible when the label is one. In general, we're going to adjust these parameter values, optimize the performance of classifier on the training data. Here, just want to include the basic introduction to some of the most commonly used classifiers, since you might use them often for text categorization. Since we have a probability here. And another advantage of this kind of approach is that it would allow many other features than words to be used in this vector. And so it's a function of X, and it's a linear combination of these feature values, controlled by beta values. So most specifically, in logistic regression the assumed functional form of y depending on X is the following, and this is very closed, closely related to the log or log odds that I introduced in the naive bayes or log of probability ratio of the two categories that you have seen on the previous slide. But if the document is not we are going to maximize this value, and what's going to happen is actually to make this value as small as possible. Since these are training documents, we know they're categories. Since we're not modeling the generation of this vector and we can plug in any signals that we want, so this is potentially advantages for doing text categorization. An in logistical regression, we did not talk about the similarity function either. So given a particular XI, how likely we are going to observe the corresponding Y i of course, Y I could be one or zero and in fact the function form here would vary depending on whether Y sub I is one or zero. And I have even given a formula here where you can see we just count the topics in this region and then normalize that by the total number of documents in the region. Basically this is to find the neighbors of this text object in the training data set. Intuitively, this makes a lot of sense. And suppose we draw a neighborhood and we're going to assume in this neighborhood, since the data instances are very similar, we're going to assume that the conditional distribution of the label, given the data, would be roughly the same. Of course the features don't have to be all the words and their features can be other signals that we want to use. I note that in naive base classifier we did not need a similarity function. And then denominator is just a total number of documents training documents in this region, so this gives us a rough estimate of which category is most popular in this neighborhood, and we're going to assign the popular category to our data objective since it falls into this region. If our similarity function captures objects that do follow similar distributions, then this assumption is OK
 Discriminative classifiers for text categorization. And. As features. The words. Imagine the words are original feature representation, but the representation can be mapped to the topic space representation. So we see transpose of W vector multiplied by the feature vector. For example word count. Basically, we can use, let's say a naive Bayes classifier to classify all the unlabeled text documents. But We can. Although in Texas domain cause words are excellent representation of text content because these are. To maximize the margin. And I also use the vector form of multiplication here. Meaning to recognize the weights. A line defined by just three parameters here beta0 beta 1 beta 2. These are linear constraints and the objective function is a quadratic function of the weights. And then finally we can leverage some machine learning techniques. Although the label is not completely reliable. And then we're going to assume the high confidence classification results, or actually reliable. An we want to classify documents into these two categories and we're going to represent again a document by a feature vector X here. LDA can actually help us reduce the dimension of features. And this is. Also a linear separator. An so the original word features can be also combined with such such latent dimension features or low dimensional space features to provide a multiresolution representation, which is often very useful. Let's say we have K topics, so a document cannot be represented as a vector of justice K values corresponding to the topics. So P is a biased constant and W is a set of weights and with one wait for each feature we have M features and so have aim weights and are represented as a vector. Typical biclustering of features in various ways, so metrics factorization has been used to do such a job, and this and some of the techniques are after very similar to the topic models that we discussed, so topic models. And symbol approaches that would combine different methods and tend to be more robust and can be useful in practice. It has shown some promise and one important advantage of this approach in relationship with the feature design is that they can learn intermediate representations or compound features automatically, and this is very valuable for learning effective representation for text localization. Now we also have the objective that's Tide to maximization of margin and this is simply to maximize sorry to minimize W transpose multiplied by W and we often denote this by file W. Humans invention for communication and they are generous sufficient for representing content for many tasks. So to train a sentiment categorizer meaning we want to distinguish positive from negative opinions and categorize reviews into these two categories then. Didn't know the parameters and, but instead I'm going to use W, although W was used to denote the words before. So as a general rule, and if you can improve the feature representation an and then provide more training data, then you can generate do better. Allow some errors. So in this lecture will introduce yet another discriminative classifier called a support vector machine or VM, which is a very popular classification method, and there has been also shown to be effective for text categorization. But the optimization problem will be very similar. And there are also other ways to ensure the sparsity of the model. To improve categorization method and so this idea is very powerful and when the enable data and training data are very different and we might need to use other advanced machine learning techniques called domain adaptation or transfer learning, this is when we can borrow some training examples from a related problem that may be different or from a categorisation task that. Here you need to optimize the C and this is the general also achievable by doing cross validation. So again we let's recall that our classifier is such a linear separator where we have weights for all the features and the main goal is to learn these weights W&amp;B. Although there are new machine learning methods like representation learning that can help with learning features. So I'm also using locates be to denote beta zero, the bias constant. So feature design tends to be more important than the choice of specific classifier. So that means these methods can be actually applied to any text categorization problem as long as we have humans to help annotate some training data set and design features, then supervised machine learning an all these classifiers can be easily applied to those. Now the margin can be shown to be related to the magnitude of the weights. So for example, training categorisation on news might not give you an immediately effective classifier for classifying topics in tweets, but you can still learn something from news to help categorizing tweets, so there are machine learning techniques that can help you. The feature representation is very critical an so that these methods all require effective feature representation and to design effective feature set that we need domain knowledge and humans definitely play important role here. And we don't really optimize the training errors and then see I can be set to a very large value to make the constraints easy to satisfy. So now you can see this is basically optimization problem, right? We have some variables to optimize and these are the weights and B and we have some constraints. So performance is often much more affected by the effectiveness of features and then by the choice of specific classifiers. To allow us to characterize content of text concisely with categories or the predictor, some properties of real world variables that are associated with text data. But basically when the two domains are very different than we need to be careful not to overfit the training domain, but yet we can still want to use some signals from the related training data. And do some analysis of the categorization problem and try to understand the what kind of features might help us distinguish categories, and in general we can use a lot of domain knowledge to help us design features. But deep learning is still very promising for learning effective features, especially for complicated tasks like a sentiment analysis, and has been shown to be effective because it can provide replenishing that goes beyond bag of words. This can allow you to obtain. Most techniques that we introduce the use supervised machine learning and which is a very general method. And this line, of course, is determined by the vector beta, the coefficients, different coefficient will give us a different line. An another way to figure out effective features is to do error analysis on the categorisation results. It's another kind of classifier where you can have intermediate features embedded in the model so that it's highly non linear classifier. Problems to solve the categorization problem. Ann is the technique has been shown to be quite effective for speech recognition, computer vision and recently it has been applied through text as well. We could assume five star reviews are all positive training examples. So in our case actually it's easy to see the mixture model can be used for both text clustering and categorisation, so even imagine if you have a lot of unable text data for categorization then you can actually do clustering on these text data to learn categories. It's particularly useful for learning representations, so different learning refers to deep neural network. So the logistical regression classifier that you have seen earlier actually uses some criteria to determine where this line should be, and it's a linear separate as well and uses a conditional likelihood on the training data to determine which line is the best. And there are instances do represented as X. So to summarize, the text categorisation methods we have introduced many methods and some are generative models, some more discriminative methods, and these tend to perform similarly when optimized, so there's still no clear winner, although each one has its pros and cons, and the performance might also very different data sets for different problems. The cause from the unlabeled data we some are labeled as category ones and more labeled as category two. For example, if you take a reviews from the Internet, they might have overall ratings. We're going to choose a linear separator to maximize the margin. But there are also some ways to help with this, so one is to assume some low quality training examples can also be used so those can be called a pseudo training examples. So know that beta1 and beta2 have different signs or one is negative and there is positive. The principle can stay right, so we want to minimize the training error, but try to also maximize the margin. At this, porters define the margin basically. But in order to achieve good performance, they all require effective features and also plenty of training data. There are also techniques for dimension reduction, and that's to reduce the high dimensional feature space into a lower dimensional space. And once we solve, the problem, will obtain the weights W&amp;B and then this would give us a well defined the classifier, so we can then use this classifier to classify any new texture objects. I so let's just assume that beta one is negative and beta two is positive. Sorry it's mainly affected by the support vectors and that's why it is called a support vector machine. Now the previous formulation did not allow any error in the classification, but sometimes the data may not be linearly separable. So in the simplest case, the linear osfm is just a simple optimization problem. So for example, the SVM actually tries to minimize the weights on features, but you can further for some features to falsely use only a small number of features. The computers of course here are trying to optimize the combinations of the features provided by human an.This lecture is a continued discussion of. We want every instance we classified accurately, but if we allow this to be. Here that you see and it's very similar to what you have seen or just for logistic regression. And so as a result in the objective function we also add more to the original 1, which is only an by basically ensuring that we're going to not only minimize the weights, but also minimize the errors as you see here, we simply take a sum over all the instances. Here the text object is represented by also a feature vector of the same number of elements. The training laid out would be basically like a in other classifiers we have a set of training points where we know the X vector and then we also the corresponding label, why I? An here we define why I as two values, but these two values are not 01 as you have seen before, but rather negative one and positive one and their corresponding to these two categories as I've shown here. You could, for example, look at the which category tends to be confused with each other categories and you can use a confusion matrix to examine the errors systematically across categories, and then you can look into specific instances to see why the mistake has been made and what features can prevent the. So because of this reason, the value of deep learning for text processing tends to be lower than for computer vision and speech recognition, where there aren't corresponding wedding design. Another idea is really exploit unable data and there are techniques called a semi supervised machine learning techniques that can allow you to combine label data with unlabeled data. OnStar negative but of course sometimes in five star reviews. So let's assume they are actually training label examples and then we combine them with the true training examples. Be performing similarly on the data set but with different mistakes and so their performance might be similar, but then the mistakes that make might be different, so that means it's useful to compare different methods for particular problem and then maybe combine multiple methods 'cause this can improve the robustness and they want to make the same mistakes so. Insights for design new features. So for example, feature selection is a technique that we haven't really talked about, but it's very important and it has to do with trying to select the most useful features before you actually trainer for classifier, and sometimes training a classifier would also help you identify which features have high values. Why I multiplied by the classifier value must be larger than or equal to 1? An obviously when? Why is just one you see. See here and that's a constant to control the tradeoff between minimizing the errors and maximizing the region of the margin if C is set to zero, you can see we go back to the original object function where we only maximize margin. So we can let each topic define one dimension. Often another way to learn factor features, especially, we could also use the categories to supervise learning of such low dimensional structures. An similarly the data instance. As I say that there are many different ways of combining them and they also optimize different objects and functions. So we've just assume that we have a constraint for the getting the data on the training set to be classified correctly. And then try to somehow align these categories with the categories defined by the training data where we already know which documents are in which category. The separate is only determined by a few data points, and these are the data points that we call support vectors. The decision boundary between two categories. So this is our assumption or setup. Regarding the training examples, it's generally hard to get a lot of training examples because it involves human labor. So we have K dimensional space instead of the original high dimensional space corresponding to words. So in general in high dimensional space such a zero point corresponds to a hyperplane. And you can also define the margin on the other side. Some error to the constraint so that now we allow. So in the linear is UVM, we're going to then seek these parameter values to optimize the margins and then the training error. And then that inducing intuitively makes a lot of sense. Each one has a CI to model the error allowed for that instance an when we combine them together, we basically want to minimize the errors on. That involves data that follow very different distributions from what we are working on. And the classifier will say X is in category one if it's positive. Maximizing margin, right? So we want to ensure the separate can do well on the training data, but then, among all the cases where we can separate the data, we also would like to choose the separate that has the largest margin
 For short documents. And classification error classification accuracy does not address this issue. So for example, we can look at the perspective from each document perspective based on each document. Or equivalently, to measure the difference between the system output and desired ideal output generated by the humans? So obviously the higher similarity is, the better the results are. And similarly we can look at the humans decision. And as in the previous case, we can define precision and recall and it will just basically answer the questions from a different perspective. The strengths and weaknesses of different methods. Categories have been assigned to those documents by humans and we want to quantify the similarity of these decisions. Basically has assigned this category to this document or no, so this is denoted by Y or N. Basically it provides.This lecture is about the evaluation of taxable categorization. In multiplied by K. That's the system to decision. Similarly, we can look at the popular category valuation. This gives us a detailed view of the decision on each document. The other recall the other meshes called Recall an this measures. In APA category or per document basis? One example that shows clearly the desicion errors are having different causes, spam filtering that could be retrieved as a two category categorization problem. And that would lead to different measures, and sometimes it's desirable also to measure the similarity from different perspectives just to have a better understanding of the results in detail. And that's the cause. The basic idea is to help humans to create test collection. And so, in general, when we use classification accuracy as a measure, we want to ensure that the classes are balanced. Is sometimes also useful to combine precision and recall as one measure, and this is often done by using if mesh. How to we have to know how to evaluate categorisation results? So first some general thoughts about the evaluation in general for evaluation of this kind of empirical tasks such as categorisation, we use methodology that was developed in 1960s by information retrieval researchers called Cranfield Evaluation Methodology. And even tried to compute the precision and recall in that case and see what would happen. So ideally we would like to model such differences. And these are also proposed by information retrieval researchers in 19, six days for evaluating searching results. Ann It's also controlled by a parameter beta two to indicate the weather precision is more important, or recall is more important when beta is set to one, we have a measure called F1, and in this case we just take a equal weight on both precision and recall. Now with this ground truth test collection, we can then reduce the collection to test many different systems and compare different systems. I basically this kind of measure will not the arithmetic mean is not going to be as reasonable FF1, which tends to prefer a tradeoff between precision and recall. The similarity can be measured in different ways. So the measure of classification accuracy is similar to count how many of these decisions are correct and normalize that by the total number of decisions we have made. And this gives you some insight for improving the better. And this is just the harmonic mean of precision and recall defined on this slide. So the first measure that we will introduce is called classification accuracy, and this is basically to measure the percentage of corrective decisions. It's often OK not to consider such a cost variation when we compare different methods. If one is very often used as a measure for categorisation. And the number of characters decisions obviously are basically of two kinds. For example, it might be also interested in knowing which category performs better, which category is easy to categorize, etc. An we when we are interested in knowing the relative difference of these methods. So this methodology has been virtually used for all the tasks that involve empirically defined problems. A way to do controlled experiments to compare different methods. An recall would tell us has the category being actually assigned to all the documents that should have this category. For example, and this allows us to analyze errors in more detail as well. In general, different categorization mistakes, however, have different costs for a specific application, so some errors might be more serious than others. We can separate the documents of certain characteristic from others and then look at the errors. As I said, it's beneficial to look at the actual must multiple perspectives. So it's OK to introduce some bias as long as the bias is not correlated with a particular method. That would give us 98% accuracy. For example, in each class the minority categories or classes tend to be overlooked in the evaluation of classification accuracy. Alright, so then we can have some meshes to just better characterize the performance by using these phone numbers and so 2 popular measures of precision and recall. But obviously there's a reason why we didn't do that and why. Where we already every document is tagged with the desired categories, or in the case of search for which query, which documents should have been retrieved and this is called ground truth. And we're going to compare our systems decisions on which documents should get which category with what. Then we can aggregate them later. Also, if the human has assigned a category to the document, there will be a plus sign here. And to understand the performance of these methods in detail. So recall tells us whether the system has actually indeed assigned all the categories that it should have to this document. When the system says yes, but human says no, that's incorrect
 Recall and F. An then compute the precision and recall. Also, sometimes categorisation results might actually be evaluated from ranking perspective. That's be cause people tend to examine the results sequentially, so ranking evaluation more reflects the utility from users perspective. Sometimes categorisation, task and maybe better frame as a ranking task and there are machine learning methods for optimizing ranking measures as well. Earlier we have introduced measures that can be used to compute the precision and recall for each category and each document.This lecture is continued discussion of evaluation of textual categorisation. So to reflect the utility for humans in such a task, it's better to evaluate the ranking accuracy, and this is basically similar to search again. And similar we can do that for recall and F score, so that's how we can then generate the overall precision, recall and F score. But typically we frame this as a ranking problem and we evaluated as a ranked list. So for example, ranking documents in the search engine can also be framed as a binary categorization problem, distinguishing relevant documents that are useful to users from those that are not useful. The second is a paper about the comparison of different approaches to text categorization and it also has excellent discussion of how to evaluate the text categorisation. And then after we have completed the computations for all these documents we were going to aggregate them to generate the overall precision, overall recall and overall F score. And if the system can give a score to the categorisation decision or confidence, then we can use the scores to rank these decisions and then evaluate the results as a ranked list, just as in search engine evaluation, where you rank the documents in response to the query. We generally need to consider how will the results be further processed by a user and then think from a user's perspective what quality is important. Then we can actually compute, for example, weighted classification accuracy where you associate the different cost or utility for each specific decision. Some commonly used measures for relative comparison of different methods or the following classification accuracy is very commonly used for especially balanced tester set. For example, in spam filtering and news categorization results are used in maybe different ways. Sometimes there are tradeoffs between multiple aspects, like precision and recall, and then, so we need to know for this application is high recall more important or high precision is more important. Precision, recall, and F scores are commonly reported to characterize the performances in different angles, and there are some also variations like per document based evaluation, per category evaluation and then take average of all of them in different ways. Again, for each category, we can compute the precision recall and F1 so for example, for category C one. So then we would need to consider the difference and design measures appropriately. We have precision P1 recall R1 and F value F1 and similarly we can do that for Category 2 and all the other categories. Measures must also reflect the intended use of the results for particular application. So for example, we can aggregate all the precision values for all the categories to compute the overall precision and this is often very useful. Or we do that for each document and then aggregate over all the documents. What aspect of quality is important. Finally, sometimes ranking may be more appropriate, so be careful. And in such a case, often the problem can be better formulated as a ranking problem instead of categorization problem. Categorisation results are sometimes or often indeed passed to human for various purposes. And, this may not be desirable. These are again examining the results from different angles and which one is more useful would depend on your application. Basically computing the values to fill in this contingency table and then we can compute precision recall just once. For example, news articles can be tentatively categorized by using the system and then human editors would then correct them. In general, you want to look at the results from multiple perspectives and for particular application in some perspectives would be more important than others, but for diagnosis, analysis of categorization methods and it's generally useful to look at as many perspectives as possible to see subtle differences between methods or to see where a method might be weak, from which you can obtain insights for improving a method
 Ideally we can also infer opinion sentiment from the content and context to better understand the opinion. As we discussed earlier, text data can be regarded as the data generated from humans as subjective sensors.This lecture is about opinion mining and sentiment analysis covering its motivation. In each representation we should identify opinion Holder, target content and context. Finally, the opinion context can also vary. And in general, we can gain a lot of advantage in any prediction task because we can leverage the text data as extra data about any problem and so we can use text based prediction techniques to help you make prediction or improve the accuracy of prediction. Now this is directly related to humans as sensors, and we can usually aggregate opinions from a lot of humans to kind of assess the general opinion. So the task of opinion mining can be defined as taking text data as input to generate a set of opinion representations. We can have simple context, like different time or different locations, but there could be also complex text such as some background topic being discussed. So you can see product reviews are fairly easy to analyze in terms of obtaining a basic opinion representation. Now the main difference between text data and other data like video data is that it has rich and rich opinions and the content tends to be subjective because it's generated from humans. And so such understanding obviously goes beyond just extracting the opinion content and needs some analysis. Opinion Target can also vary a lot. And we can also then identify the context. I can help us optimize our decisions. It's more difficult than the analysis of opinions in product reviews. Now the content of course is the review text that's in general also easy to obtain. In particular, we're going to talk about the opinion mining and sentiment analysis. From computational perspective, we're most interested in what opinions can be extracted from text data, so it turns out that we can also differentiate distinguish different kinds of opinions in text data from computation perspective. And that means we also want to understand, for example, the context of the opinion and what situation was opinion expressed. So from practical viewpoint, sometimes we don't necessarily extract the subjective sentences. In this lecture we are going to start talking about mining a different kind of knowledge, namely knowledge about the observer or humans that have generated text data. The opinion Holder and opinion target that have already been identified. So when opinion expressed in the particular discourse context, it has to be interpreted in different ways than when it's expressed in another context, so the context can be very rich to improve the entire discourse context of opinion. For example, it can help understand peoples preferences. So the task is much harder and we need a deeper natural language processing. You can identify one sentence opinion or one phrase opinion, but you can also have longer text to express the opinion like a whole article. For example, we can optimize the product search engine, optimize recommender system if we know what people are interested in, what people think about products. What are the winning features of their product or winning features of competitive products? Market research has to do with understanding consumers opinions and this is clearly very useful, directed for that. In this case, this actually explicit opinion Holder and explicit target, so it's It's obviously what's opinion Holder, and that's just a reviewer, and it's also often very clear what's the opinion target, and that's the product being reviewed. So clearly the two kinds of opinions need to be analyzed in different ways and sometimes in product reviews you can see, although mostly the opinions are from this reviewer. The opinions is expressed on this something. Sometimes if you want to understand further, we want to enrich the opinion representation. And now, of course, believes or thinks implies that the opinion would depend on the culture or background and context in general, because of person might think differently in the different context. It can be about 1 entity, a particular person, a particular product, a particular policy, etc. Also, for this reason about the indirect opinions. What's this opinion about? And 3rd, of course we want opinion content and So what exactly is the opinion? If you can identify this, we get a basic understanding of an opinion and can already be useful. Data Driven social science research can benefit from this because they can do text mining to understand the people's opinions. And this is a key differentiating factor from opinion, which tends to be not easy to prove wrong or right because it reflects what a person thinks about something. Now opinion content, of course, can also vary a lot on the surface. The first is it can help decision support. Now unlike in the product review, all these elements must be extracted by using natural language processing techniques. I just gave a good example, in the case of product reviews where the opinion Holder and opinion target are often explicitly identified, and that's not why this turns out to be one of the simplest opinion mining tasks. So this lecture and the following lectures will be mainly about how we can mine and analyze opinions buried in a lot of text data. Now it's interesting to think about other tasks that might be also simple, because those are the cases where you can easily build applications by using opinion mining techniques. So this analysis shows that there are multiple elements that we need to include in order to characterize an opinion. Now I highlighted a quite a few words here, and that's because it was thinking a little more about these words and that would help us better understand what's in the opinion and this further helps us to define opinion more formally, which is always needed to computationally solve the problem of opinion mining. So opinion is subjective statement. And it's very broad, of course. People from different background may also think in different ways. Or we want to know that the sentiment of this review is positive, and so this additional understanding of course adds value to mining the opinions. In contrast, the text might also report opinions about others so the person could also make observation about another person's opinion and report this opinion. We can mine the text data to understand the opinions, understand the people's preferences, how people think about something. And the task is in general harder so we can identify opinion holder here and that's governor of Connecticut. And Furthermore, we can identify the variation in the sentiment or emotion dimension. Analyzing sentiment in news is still quite difficult. Could be about the product from a company in general. Sometimes a reviewer might mention opinions of his friend or her friend, right? And another complication is that there may be indirect opinions or inferred opinions that can be obtained by making inferences on what's expressed in the text that might not necessarily look like opinion. So what's the opinion? Well, this negative sentiment here that's indicated by words like a bad and worst. It's often also very useful to extract it or whatever the person had said about the product, and sometimes factual sentences like this are also very useful. First, the Observer might make a comment about the opinion target in the observed world. When the review was posted, usually you can extract such information easily. In fact, here we're going to examine the variations of opinions more systematically. And these examples also, suggest that a lot of work can be easily done for product reviews, and that's indeed what has happened
 So NLP enriches text representation. And these clusters can be features to supplement the word based representation. They can enrich text representation. In our case, for text categorization, or more specifically, sentiment classification. But of course such a representation would not be as discriminative as words. We can also learn word clusters empirically, for example we talked about mining associations of words and so we can have cluster of paradigmatically related words or sementically related words. And this is a very general way, and a very robust way to represent the text data. So a main challenge in designing features, a common challenge is to optimize the tradeoff between exhaustivity and specificity. So as you can see, the task is essentially a classification task or categorisation task. And machine learning can be applied to select the most effective features or construct the new features that feature learning.This lecture is about the sentiment classification. This also means any text categorization method can be used to do sentiment classification. But it may cause overfitting because with such very unique features the machine learning program can easily pick up such features from the training set and to rely on such unique features to distinguish categories. But we also have locations where the words might occur more closely together. Sentiment classification can be defined more specifically as follows: The input is opinionated text object. So in general, natural language processing is very important to derive complex features. So suppose we know who is the opinion holder and what's the opinion target and also know the content and context of the opinion. And these features can then be further analyzed by humans through error analysis. So in general, feature design actually affects categorization accuracy significantly, and it's a very important part of any machine learning application. So now let's talk about some features that often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis. Now, of course, if you just do that, the accuracy may not be good because sentiment classification does require some improvement over regular text categorization technique or simple text categorization technique. And this is also robust to spelling errors or recognition errors, right? So if you misspelled the word by 1 character and this representation actually would allow you to match this word when it occurs in the text correctly. And you can look at the categorization errors and then further analyze what features can help you recover from those errors or what features cause overfitting and cause those errors, and so this can lead to feature validation that would revise the feature set and then you can iterate and we might consider using a different feature space. One is polarity analysis where we have categories such as positive, negative or neutral. Or could be semantic and they might represent concepts in the thesaurus or ontology like word net. Specificity requires the feature to be discriminative, so naturally infrequent features tend to be more discriminating, so this really caused tradeoff between frequent versus infrequent features, and that's why feature design is generally an art. An obviously that kind of classifier won't generalize well to future data when such discriminating features will not necessarily occur. So this is a case of just using sentiment classification for understanding opinion. In the case of polarity analysis, we sometimes also have numerical ratings, as you often see in some reviews on the web. Or they can be recognized the named entities like people or place and these categories can be used to enrich the representation as additional features. In general, I think it would be most effective if you can combine machine learning, error analysis and domain knowledge in designing features. Uni Grams are actually often very effective for a lot of text processing tasks and that's mostly because words are well designed features by humans for communication, and so they often good enough for many tasks, but it's not good or not sufficient for sentiment analysis clearly. Now, exhaustivity means we want the features to actually have high coverage of a lot of documents. That could be useful for sentiment analysis. For example, the word great might be followed by a noun and this could become a feature, a hybrid feature. As we've seen before, so it's a special case of text categorization. Furthermore, we can also have frequent pattern syntax and these could be frequent word set. In general you have just discrete categories to characterize the sentiment. It's unlikely, very ambiguous. It allows much larger search space of features. If we assume that most of the elements in the opinion representation are already known, then our only task maybe just the sentiment classification as shown in this case. And in General, Patton discovery algorithms are very useful for feature construction, because they allow us to search in a larger space of possible features that are more complex than words that are sometimes useful
 For example, positive words generally suggest a higher rating. We can apply logistical regression. And if its probability according to this logistical regression classifier is larger than . Intuitively, the features that can distinguish Category 2 from 1, or rather rating 2 from 1, may be similar to those that can distinguish K from K - 1.This lecture is about the ordinal logistic regression for sentiment analysis. So this is intuitively appealing assumption. Now the idea of ordinal logistic regression is precisely that A key idea is just the improvement over the K -1 independent logistical regression classifiers, and that idea is to tie these beta parameters and that means we are going to assume the Beta parameters these are the parameters that indicate the influence of those weights. So now the logistic regression classifiers indexed by J, which corresponds to a reading level. So the predictors are represented as X and these are the features and there are M features altogether, which feature value is a real number, and this can be representation of a text document. By using the features and the parameter values, beta values. And the idea is we can introduce multiple binary classifiers and each case we ask the classifier to predict whether the rating is J or above all the ratings lower than J. You may recall that in logistic regression we assume the log of probability that Y is equal to 1 is assumed to be a linear function of these features as shown here. So this is just a direct application of logistical regression for binary categorization. Different J has a different alpha, but the rest of the parameters of beta are the same. What if we have multiple categories, multiple levels? We actually use such a binary logistic regression program to solve this multi level rating prediction. So that's just the basically that's basically the main idea of ordinal logistic regression. There are many parameters. So these training data for different classifiers. However, each classifiers there has a distinct Alpha value, the Alpha parameter, the except it's different and this is of course needed to predict the different levels of ratings. Now we can use a regular text for categorization technique to solve this problem, but such a solution would not consider the order and dependency of the categories. In general, words that are positive would make the rating higher and for any of these classifiers, for all these classifiers. So anyway, so here we now have basically K - 1 regular logistic regression classifiers. And we're going to assume these better values are the same for all the K - 1 premise, and this just encodes our intuition that positive words in general would make a higher rating more likely. So this is our problem set up for a typical sentiment classification problem, or more specifically, rating prediction. And of course, B_i is our parameters here. It's at least K - 1 and if the probability is larger than . Larger than or equal to negative of alpha_j as shown here. And the other is to allow us to share the training data, because all these parameters are assumed to be equal. And I have also used offer subject to replace beta 0. So now with this approach we can now do rating prediction as follows. It depends on J. We have an opinionated text document D as input an we want to generate as output already in the range of one through K, so it's discrete rating and thus this is a categorization problem. Now let's first think about how we use logistic regression for binary setting categorization problem. When it's zero, that means the rating is lower than J. And then of course, this is a standard two category categorization problem. And this is to make the notation more consistent with what we can show in the ordinal logistic regression. So when the classifier has a lot of parameters would in general need a lot of data to actually help us training data to help us decide the optimal parameters of the this such a complex model? So that's not the idea. Now you can see the General Decision rule is thus when the score is in the particular range of our values, then we will assign the corresponding rating to that text object. Can then be shared to help us set the optimal value for beta. 1 means X is positive, 0 means X is negative. And that's our classifier one, and then we're going to have another classifier to distinguish K - 1 from the rest. So suppose we just want to distinguish it positive from negative and then it's just a two category categorization problem. Specifically, we have N + K - 1 because we have M beta values and plus K minus one alpha values. So what's the solution? In general, we can add order to classify and there are many different approaches, and here we are going to talk about one of them is called the ordinal logistic regression
 Infer this aspect ratings. And to generate the overall rating. We can generate a aspect level opinion summary.This lecture is about the latent aspect rating analysis or opinion mining and sentiment analysis. As shown here, and that's denoted by beta sub I and W. And we can assume the overall rating is simply a weighted average of this aspect ratings. And As for aspect. As I said, the overall rating is assumed to be a weighted average of aspect ratings. And this is a problem called latent aspect rating analysis. Observer ratings condition on their respective reviews. In this case, the aspect ratings and aspect of weights. And for example, we can do opinion based and the ranking. So the task here is to get these reviews and their overall ratings as input and then generate the both the aspect ratings, decomposed aspect ratings and the aspect of weights as output. So the aspect rating is assumed to be a weighted combination of these word frequencies where the weights are the sentiment weights on the words. Basically we're going to maximize the product of the prior of our according to our assumed market valued Gaussian distribution and the likelihood in this case likely is the probability of generating this observed overall rating given this particular Alpha value and some other parameters. Here is a set of review documents with overall ratings. Note that here the sentiment weights are specifically to aspects, so beta is indexed by I. But here we predicting the rating and the parameters of course are also very different. We can also analyze reviewers preferences, compare them or compare their preferences on different hotels. In order to maximize the probability of the data in this case, the conditional probability of the observed rating given the document. So this setup allows us to predict the overall rating based on the observed word frequencies. Now the model is going to predict the rating based on the. So the task in general is given a set of review articles about the topic with overall ratings. So we are interested in the conditional probability of R sub T given D. And then we're going to set up a generation probability for the overall rating given the observed words. As a bi product that will also get the beta vector and these are the aspects of specifica sentiment, weights of words, so more formally. And each review documents denoted by AT and overall rating is denoted by R sub D and these pre segmented into K as their segments and we're going to use C sub W and D and to denote the count of world W in aspect segment I. And this would also allow us to rank hotels along different dimensions, such as valuable rooms, but in general such detailed understanding would reveal more information about the users, preferences, reviews, preferences and also we can understand better how reviewers view this hotel from different perspectives. And this model is set up as follows. Now this alpha Values of a alpha sub of D together by our vector that depends on D is the document specific weights and we can assume this factor itself is drawn from another multivariate Gaussian distribution with mean denoted by a mule vector and covariance matrix Sigma, yeah. And 3rd is the relative weights placed on different aspects by the reviewers, and this task has a lot of applications. In this lecture, we're going to continue discussing opinion mining and sentiment analysis. Now of course these sentiment weights might be different for different aspects. Late in the aspect of rating analysis, which allows us to perform detailed analysis of reviews with overall ratings. Then it would increase the aspect rating for location. In the segmentation stage, which is called latent rating regression, we're going to use these words and their frequencies in different aspects to predict the overall rating, and this prediction happens in two stages. So in order to interpret the ratings on different aspects accurately, we also need to know these aspect weights. Then we can obtain more detailed understanding of the reviewers opinions about the hotel. Or price to retrieve the relevant the segments and then from those segments we can further mine correlated words. Into ratings on different aspects such as value, rooms, location and service. And of course there are also reviews that are in text. And we can do personalized recommendation of products. The beta values that Delta and then the mu and Sigma. So now more specifically, we can now once we estimate the parameters, we can easily compute the abstract rating for aspect I or sub I of D and that's simply to take all the words that occurred in the segment I and then take their accounts and then multiply that by the sentiment weight of each word and take a sum. And this would be used to take a weighted average of the aspect ratings, which are denoted by our supply of the. Now this is a typical case of generating model where we would embed the interesting variables in the generating model. Now the aspect rating as I just said is the sum of the sentiment weights of words in their spectrum. So next question is, how can we estimate these parameters and so we collectively denote all the parameters by Lambda here. In the first stage, we're going to use the sentiment weights of these words in each aspect to predict the aspect rating. But another word, like a far, which is a negative weight if it's mentioned many times and it will decrease the rating. What premise we have here, but I just said that the beta sub I W gives us a aspect specific sentiment of W. 2 computer this alpha value. The second is the ratings on each aspect, such as value and room or service. In the second stage, or in a second step, we're going to assume that the overall rating is simply weighted combination of these aspect ratings. But if you can see if we can uncover these parameters, that would be nice because also R of D is precisely the aspect ratings that we want to get, and these are decomposer ratings on different aspects of our sub ID is precisely the aspect weights that we hope to get. But as always when we make this assumption, we have a formal way to model the problem, and that allows us to compute interesting quantities. And so we have seen such cases before in, for example, PLSA, where we predict the text data
This lecture is a continued discussion of latent aspect rating analysis. Topics and then we can use a topic model to model the generation of the review text. We have also shown that generative models are powerful for mining latent user preferences, in particular in the generating model for letting the rating regression, we embed some interesting preference information and sentiment weights of words in the model. And then we can then plug in the latent regression model to use the text to further predict the Overall rating and that means we first predict the aspect rating and then combine them with aspect weights to predict the overall rating. And shows that by doing text mining we can understand the users better. For example, direct application would be the generator rated aspect, the summary. Now these ratios are computer based on the inferred weights from the model. So given an entity, we can assume there are aspects that are described by word distributions. And here are some results to validate the preference weights. Our assumed the words in the review text are drawn from these distributions. Here are some interesting results on analyzing users rating behavior. And this is what we call a personalized or rather query specific recommendation. So this would give us a unified generative model where we model both the generation of text and the overall rating condition on text. The next two papers are about the generative models for letting the aspect rating analysis. Here are also mother results about the aspects discovered from reviews with low ratings. These are MP3 three reviews an these results show that the model can discover some interesting aspects commented on low overall ratings versus those high overall ratings, and they care more about the different aspects. So to summarize our discussion of opinion mining in general, this is a very important topic and with a lot of applications. You can compare different hotels, compare the opinions from different consumer groups in different locations, and of course the model is general. It can be applied to any reviews with overall ratings, so this is very useful technique that can support a lot of text mining applications. And that's not surprising as also another way to validate the inferred weights. Opinions of different reviewers and such a detailed understanding can help us understand better about reviews and also better about their feedback on the hotel. Finally, there is also some result on applying this model for personalized ranking or recommendation of entities. The positive sentence is negative sentences about each aspect. And as a task sentiment analysis can be usually done by using just text categorization, but standard techniques tend not to be enough and so we need to have enriched feature representation. Most approaches have been proposed and evaluated for product reviews, and that was the cause in such a context of the opinion holder an opinion target or clear and they are easy to analyze and there of course also have a lot of practical applications, but opinion mining from news and social media is also important, but that's more difficult than analyzing review data, mainly because the opinion holders and opinion targets are all. The non personalized recommendation results are shown on the top. To solve the problem using a unified model. The 2nd result is to compare different reviewers on the same hotel so the table shows the decompose ratings for two reviewers about same hotel again their high level overall ratings are the same. implicit and so that calls for natural language processing techniques to uncover them accurately. It's more informative than original review that has just overall rating and review test. And once we can end users better, we can serve these users better. Now, it's also possible to develop a unified generative model for solving this problem, and that is we not only modeling, we not only model the generation of overrating based on text, we also model the generation of text and so a natural solution would be to use topic model. How can we know that we can infer the weights of those reviewers on different aspects? We can find the reviewers whose weights or more precise whose inferred weights or similar to yours and then use those reviews to recommend the hotels for you. As a result, we can learn those useful information when fitting the model to the data. And you can see the highly weighted words versus the negatively lower weighted words here for each of the four dimensions. In the same way as we assumed for a generative model like PSA. And such knowledge can be useful for manufacturers to design their next generation of products. You can't really tell much difference between these hotels, but by decomposing these ratings into aspect ratings we can see some hotels have higher ratings for some. And we also need to consider the order of those categories and we talk about the ordinal regression. So this shows that the model can reveal differences in. We did not really have to do this, but the design of the generative model has this component and these are sentiment waits for words in different aspects. First it's about rating decomposition. And, what's also interesting that since this is an almost computer and supervised, assuming that the reviews with overall ratings are available, and then this can allow us to learn from potentially a large amount of data on the Internet to reach sentiment lexicon. An because of the decomposition, we can now generate the summaries for each aspect. Now, how do we know whether the inferred weights are correct and this poses a very difficult challenge for evaluation. An you can see the top results generally have much higher price than the low Group, and that's because when reviewers cared more about the value as dictated by this query and they tend to really have favor low price hotels. And this provides some Indirect way of validating the infer wait. The first one is about solving the problem using two stages and the second one is about the unified model where topic model is integrated with the regression model. Even if you read all the reviews, it's very hard to infer such preferences or such emphasis. And that means these reviewers tend to put a lot of weight on value as compared with other dimensions
 Because non text data can provide a context for mining text data. Basically you can see text based prediction character serve as a unified framework to combine many text mining and analysis techniques, including topic mining and content, any content mining techniques or sentiment analysis.This lecture is about text based prediction. And this is because text data can help interpret patterns discovered from non text data. And of course we can also add non text data directly to the predictive model, but then non text data also helps provide context for text analysis that further improves the topic mining and the opinion analysis. So topics can be intermediate representation of text. This helps discover some frequent patterns from non text data. So from text mining perspective and we're interested in text based prediction, of course sometimes text alone can make predictions. So this is very different from content analysis or topic mining where we directly characterize the content of text. In one perspective, we can see non text data can help text mining. Now the other perspective is text data can help non text data mining as well. And that's to mine text in the context defined by non text data. So for example, topic mining can be very useful to generate the patterns or topic based indicators or predictors that can be further fed into a predictive model. And similarly, sentiment analysis can lead to such predictors as well. And of course the data are in the form of non text data and text data. And this is most useful for prediction about human behavior or human preferences or opinions. So those are the data mining or text mining algorithms can be used to generate the predictors. That may not be directly related to the text, or only remotely related to text data. in such a prediction problem set up, we are very much interested in joint mining of non text and text data. But in general text data will be put together with non text data. Humans are the best in consuming or interpreting text data. Provide a way to partition text data in different ways, and this leads to a number of techniques for contextual text mining. As we'll discuss more later, so the join analysis of text and non text data can be actually understood from 2 perspectives. And both can help provide predictors for the prediction problem. These patterns that are generated from text and non text data themselves can sometimes already be useful for prediction, but when they are put together with many other predictors they can really help improving the accuracy of prediction. And the other could be to mine knowledge about the observer so sentiment analysis or opinion analysis. The second is a Qiaozhu Mei dissertation on contextual text mining. And also help us understand the text data because text data are created to be consumed by humans. It maybe, although it's generated from original text data, it provides a much better representation of the problem and it serves as more effective predictors. And this gives us two sets of text data and then we can see what's the difference and this difference in text data is interpretable because text content is easy to digest and that difference might suggest some meaning for this pattern that we've found from non text data, so that helps interpret such patterns. So one sub task could be mine, mine the content of text data like topic mining. In general, we want to collect data that are most useful for learning. Next human also must be involved in predictive model building and adjusting or testing. And this technique is called pattern annotation. And, this is so that we can adjust the sensors to collect the most useful data for prediction. But this only provides limited view of what we can predict. And such improvement often leads to more effective predictors for our problems it would enlarge the space of patterns of opinions or topics that we can mine from text. And then through the analysis, we generally can generate the multiple predictors of this interesting variable to us, and we call these features. But when there are, of course a lot of text data than machines have to help, and that's why we need to do text data mining. And these features can then be put into a predictive model to actually predict the value of any interesting variable. Or from the prediction errors you can also know what additional data we need to acquire in order to improve the accuracy of prediction. So this then allows us to change the world and so this basically is the general process for making a prediction based on data, including text data
 topic. context. And. Indirect text context refers to additional data related to the meta data.This lecture is about the contextual text mining. Contextual text mining is related to multiple kinds of knowledge that we mine from text data. It's related to text based prediction because it allows us to combine non text data with text data to derive sophisticated predictors for the prediction problem. As I'm showing here, is related to topic mining because can make topics associated with context, like a time or location, and similarly it can make opinion mining more contextualized, making opinions connected to context. And also indirect context, so the direct context can include the meta-data such as time, location, authors, and source of the text data. So more specifically, why are we interested in contextual text mining? Well that's, first, because text often has rich context information and this can include direct context such as meta data. For example, comparing topics overtime, we can see trends of topics and comparing topics in different context can also reveal differences about the two contexts. So in general, any related data can be regarded as context, so there could be remotely related to context. And so in general, this enables discovery of knowledge associated with different context as needed. Or we can obtain a set of papers about the text mining, and this can be compared with papers about another topic. And such information is not, in general, directly related to the text yet through the authors we can connect them. And in particular, we can compare different contexts, and this often gives us a lot of useful knowledge. df So, as you can see, the list can go on and on, basically contextual text mining can have many applications. It also in general provides meaning to the discovery topics if we gonna associate the text with context. And so what's the use of, why is text context useful? Well, context can be used to partition text data in many interesting ways. Is there any difference in the opinions about the topic expressed on one social network and another? In this case, the social network of authors and the topic can be the context. So, for example, from authors, we can further obtain additional context, such as social network of the author or the author's age. Is there any difference in the research topics published by authors in the USA and those outside? Now, in this case, the context would include the authors and their affiliation and location. Now, such text data can be partitioning in many interesting ways because we have context. We can get all the SIGIR papers and compare those papers with the rest or compare SIGIR papers with KDD papers with ACL papers. And but perhaps we can include some other variables as well. We can also partition the data to obtain the papers written by authors in the US, and that of course uses additional context
 Firstly it would model the conditional likelihood of text given context.This lecture is about a specific technique for contextual text mining called contextual probabilistic latent semantic analysis. Recall that in contextual text mining we hope to analyze topics in text. The topics, the coverage and variations, etc. The topic coverage also depends on the context. And we're going to introduce contextual probabilistic latent semantic analysis As an extension of PLSA for doing contextual text mining. And this is precisely what we want in contextual text mining. Not necessary exactly the same model, but similar models. And in this case, the estimate premise would naturally contain context variables, and in particular a lot of conditional probabilities of topics given certain context. Before we have those classical probabilistic model logic model, Boolean model etc. Recall that before when we generate the text, we generally assume we will start with some topics and then sample words from some topics. But after 1998 that we see clear dominance of language model as probabilistic models and we see words like a language model, estimation of parameters etc. And this would allow us to do contextual text mining. Or in other words, we can do let the context influence both coverage and content of a topic. Secondly, it makes 2 specific assumptions about the dependency of topics on context. As extension of PLSA model, CPLSA mainly does the following changes. So in this approach contextual probabilistic latent semantic analysis or CPLSA The main idea is to explicitly add interesting context variables into a generated model. And naturally, the model will have more parameters to estimate, but once we can estimate those parameters that involve the context, then we will be able to understand the context of specific views of topics or context of specific coverages of topics. So to generate such a document with context, we first also choose a view. So such a technique would allow us to use location as context to examine variations of topics. Now here, because we consider context so the distribution of topics or the coverage of topics can vary depending on the context that has influenced the coverage. One is to assume that depending on the context depending on different time periods or different locations, we assume that there are different views of the topic or different versions of word distributions that characterize a topic, and this assumption allows us to discover different variations of the same topic in different context. Now, so the main difference is when we obtain the coverage and the word distributions, we let the context influence our choice. But the visualization shows that with this technique that we can have conditional distribution of time given a topic. It's to perform comparative text mining to allow us to extract the common topics shared by multiple collections and their variations in each collection. We pick We've picked the document specifically coverage now with the coverage and these word distributions, we can generate the document in exactly the same way as in PLSA. And the topic we focus on is about the retrieval models and you can see the top word top words with high probability is about this model on the left. Each document has just one coverage distribution. And these are shown as view one, view two and view three Each view is a different version of word distributions. And people talked about these topics. Now you can see some probabilities of words for each topic. Now such context information is what we hope to model as well. In this lecture, we're going to continue discussing contextual text mining. And so one idea here is to model the variations of topic content in different context and this gives us different views of the world distributions. That clearly suggests that the generation of text would then depend on context, and that allows us to bring context into the generative model. Apparently the study of retrieval models have involved a lot of other words that seem to suggest some different retrieval tasks though. The first is paper about simple extension of PLSA to enable cross collection comparison. And these views are tide to some context variables. So here are some sample results from using such a model. And in addition to some other topics. The consequences that this would enable us to discover contextualized topics make the topics more interesting, more meaningful, because we can then have topics that can be interpreted as specific to a particular context that we're interested in. Before is fixed in PLSA and it's hard to a particular document. And of course, the model is completely general, so you can apply this to any other collections of text to reveal spatial temporal patterns. We assume we have got a word distribution associated with each topic, right? And then next to the view we choose a coverage from the bottom. But imagine if you are not familiar with the text collections or have a lot of text articles and such a technique can review the common topics covered in both sets of articles. And that means depending on the time or location, we might cover topics differently. But here we are going to add context variables so that the coverage of topics and also the content of topics will be tight little context. And then again this dependency would then allow us to capture the association of topics with specific context. So as you see here, we can assume there are still multiple topics. This these topics are obtained from block articles about the Hurricane Katrina. Now, as you can see, we assume there are different views associated with the each of the topics. So in this case, the context that is explicitly specified by the topical collection. So this technique here can use event as context. The other is that we assume. So this allows us to plot this conditional probability. This is some additional result on special patterns and this. Now on the bottom you will see the theme coverage or topic coverage might also vary according to these context. So in this case we can see the time can be used as context to reveal trends of topics. The other is the publication of a Seminal paper by Croft and Ponte, and this is about the language modeling approach to information retrieval. This topic is indeed very relevant, to both wars. It can be used to review common topics in multiple sets of articles as well. To review the common topics covered in both sets, articles and the differences or variations of the topic in each of the two collections. So here we are looking at the research articles in information retrieval, IR, particularly SIGIR papers. And then next time we might choose a different topic, an will get donate, etc right until we generate all the words and this is basically the same process as in PLSA. So we're going to choose particular coverage and that coverage. So in this case the results show I've seen that before TREC the study of retrieval models was mostly a vector space model, Boolean model, etc. In consideration of context so that we can associate the topics with appropriate context that we're interested in
 IR information retrieval. a model called Network supervised topic model. The context of a text article can form a network. For example topics. Viewing the regular topic models like PLSA or LDA as solving optimization problem. So we want to maximize the probability of text data given the parameters generated denoted by Lambda here. And that's because we can use network to impose some constraints on topics text. So the general idea of network supervised topic model modeling is the following. So we often use maximum likelihood estimator to obtain the parameters and these parameters would give us useful information that we want to obtained from text data. strongly connected to be similar and we ensure their coverages are more similar. The first is the information retrieval second is data mining. And the prior here is the neighbors on the network must have similar topic distribution. So such context connects the content. So the advantage of this idea is that it's quite general here the topic model can be any generative model for text. In general, the idea is to use the network to impose some constraints on the model parameters Lambda here. So instead of just optimizing the probability of text data given parameters, Lambda. They will share common distribution of the topics or have just slight variations of the topic distributions or topic coverage. But the second equation shows some additional constraints on the parameters. Right, it doesn't have to be PLSA or LDA or the current topic models. And if you just listen to text data alone based on the Co occurrences you won't get such a coherent topics, even though a topic model PLSA or LDA also. For example, the authors of research articles might form a collaboration network. Text also can help characterize the content associated with each subnetwork and this is to say that both kinds of data, the network and text can help each other. So such heuristic can be used to guide us in analyzing topics. Now there is some benefit in jointly analyzing text and its social network context or network context in general. And similarly, the network can be also any network. But still they cannot generate such coherent results as NetPLSA is showing that the network context is very useful here. In this lecture, we're going to continue discussing contextual text mining. So first some motivation for using network context for analysis of text. So a more general view of text mining in the context of network is to treat text as living in the rich information network environment. And so, in general analysis of text there should be using the entire network of information that's related to the text data. Similarly, locations associated with text can also be connected to form geographical network, but in general you can imagine the meta data of the text data can form some kind of network if they have some relations. So can you recognize which part? Is the likelihood for the text data given by a topic model? If you look at it that you will see this part is precisely the PLSA log likelihood that we want to maximize when we estimate the parameters for PLSA alone. It's to measure the difference between the topic coverage at node U and the V. should be able to pick up a Co occurring words, so in general the topics that they generate represent words that Co occur with each other. For example, the text at adjacent nodes of the network can be assumed to cover similar topics. And this, in this case we also use four topics, but NetPLSA would give much more meaningful topics. Indeed, in many cases they tend to cover similar topics. If we use more topics, perhaps will have more coherent topics.This lecture is about how to mine text data with social network as context. So technically what we can do is simply to add a network induced regularizers to the likelihood objective function as shown here. Of course, in this case the optimization objective function is the likelihood function. But the idea is the same. And this general idea clearly can be applied for many problems, but here in this paper referenced here. So this makes it possible to find the parameters that are that are both to maximize the PLSA log likelihood. That means the parameters will fit the data well and also to respect this constraint from the network. This function combines the likelyhood with a regularizer function called r here, and the regularizer is defined the on the parameters Lambda and the network and tells us basically what kind of parameters are preferred from network constraint perspective, so you can easy to see this is in effect implemented the idea of imposing a prior on the model parameters only that will not necessarily having a probabilistic model. So for example, it's reasonable to assume that authors connected in collaboration network tend to write about the similar topics. A similar model could have been also used to characterize the content associated with each subnetwork of collaborations. So here are some sample results from that paper, and this slide shows the regular results of using PLSA and the data here is DBLP data. Or authors of social media content might also form social networks. So here's one suggested reading and this is the paper about the NetPLSA where you can find more details about the model and how to estimate such a model. This is a new parameter to control the influence of network constraints. To read more about the topic. bibliographic data about the research articles
 So non text data provide context for mining text data and we discussed a number of techniques for contextual text mining. But W2 and W4 are negatively correlated. Take the text stream as input and apply regular topic modeling to generate a number of topics. Now we call these topics "causal topics".This lecture is about using a time series as context to potentially discover causal topics in text. And so as a result, joined analysis of text and non text is very necessary. Text data is often combined with non text data for prediction, because for this purpose, for the prediction purpose, we generally would like to combine non text data and text data together as much clues as possible for prediction. And on the other hand, text data can also help interpret the patterns discovered from non text data and this is called a pattern annotation. And, with some extension like a CPLSA or contextual PLSA, then we can discover these topics in the collection and also discover their coverage overtime. For example, PLSA and we can apply this to text streams. So when we apply the topic models we are maximizing the coherence. Then we can further analyze the component words in the topic and then try to analyze word level correlation. The idea is to do a iterative adjustment of topics discovered by topic models using time series to induce a prior. Now, Text based prediction is generally very useful for big data applications that involve text, because, you can help us infer new knowledge about the word and the knowledge can go beyond what's discussed in the text. But a commonly used measure for causality here is Granger causality test. But at least they are correlated topics that might potentially explain the cause, and humans can certainly analyze such topics to understand the issue better. But we hope these topics are not just regular topics. And then when we apply the selected words, as a prior to guide the topic modeling, we again go back to optimize the coherence because topic models will ensure the next generation of topics to be coherent and we can iterate, iterate, and optimize in this way as shown on this picture. And this approach can be regarded as an alternate way to maximize both dimensions. So basically the results show that the approach can effectively discover possibly causal topics based on the time series data. So in the end, let's summarize the discussion of text based prediction. But as I also explained that these topics are likely very good because they are general topics that explained the whole text collection, they're not necessarily the best topics that are correlated with our time series. In particular, we're going to look at the time series as a context for analyzing text to potentially discover causal topics. And we might figure out that topic one and topic four are more correlated and topic two and topic three are not. Those major topics in the news event. And that's a prediction market and the data is the same. In this lecture we're going to continue discussing contextual text mining. Now the ideal is to get the causal topic that's scored high both in topical coherence, and also causal relation. Of course, then we can apply topic models to get another generation of topics, and that can be further ranked based on the time series to select the highly correlated topics. So the idea here is to go back to topic model by using these, each as a prior, to further guide the topic modeling, and that's to say we ask our topic models to now discover topics that are very similar to each of these two subtopics, and this will cause a bias toward more correlated topics with the time series. And the output so would contain topics just like in topic modeling. Why, because we are restricted to the topics that were discovered by PSA or LDA? And that means the choice of topics will be very limited and we know these models try to maximize light role of the text data, so those topics tend to be the major topics that explain the text data well, and they're not necessarily correlated with time series. And it's also very useful now when we analyze text data together with non text data we can see they can help each other. But when we decompose the topic model words into sets of words that are strongly very strongly correlated with time series, we select the most strongly correlated words with the time series we are pushing the model back to the causal dimension to make it better in causal scoring. So all these cases are special cases of a general problem of joint analysis of text and the time series data to discover causal topics. So here in this work site here, a better approach, it was proposed, and this approach is called Iterative causal topic model. That said, four topics shown here. But here you see, these topics are indeed biased toward each time series.  With these topics, we certainly don't have to explain the data the best in text, but rather they have to explain the data in the text, meaning that they have to represent a meaningful topics in texts semantically coherent topics, but also more important they should be correlated with the external time series that is given as a context. Now these subtopics, all these variations of topics based on the correlation analysis, are topics that are still quite related to the original topic topic one, but they already deviating because of the use of time series information, to bias selection of words. It's very frequently used measure, it has many applications. For example, whenever the topic is mentioned, the price tends to go down, etc. And then get the even more correlated subtopics that can be further fed into the process as prior to drive the topic model discovery. And we're going to look at the words in this topic - the Top words. And this is to see this was different from the standard talking models where we have just the text collection. And we're going to look into each word in the top ranked word list for each topic. Let's say we have a topic about government response here and then with topic model, we can get the coverage of the topic overtime. This is the time series data from a presidential prediction market. It's a non text time series, Yt, is the stock prices. In this case, we hope to use text mining, to understand the time series. So as a topic and it's not good to mix these words with different correlations, so we can then further separate these words, we're going to get all the red words that indicate positive correlation W1and W3, and we're going to also get another subtopic, if you  want, that represents a negatively correlated words W2 and W4. As a result, they can also support optimizig of our decision making, and this has widespread applications. So this whole process is just heuristic way of optimizing causality and coherence. Now the output that we want to generate is the topics whose coverage in the text stream has strong correlations with the time series. Here's another scenario where we want to analyze the presidential election. But the results here clearly show that the approach we can uncover some important issues in that presidential election. Now the question here is, does Xt cause Yt? Or in other words, we want to match the causality relation between the two. So that just means the time series has effectively served as a context to bias the discovery of topics. Here are some other results from analyzing presidential election Time series. So the only component that you haven't seen in such a framework is how to measure the causality because the rest is just topic model. And if the topic is correlated with the time series, there must be some words that are highly correlated with the time series. And then we're going to use the external time series to assess which topic is more causally related or correlated with the external time series, so we can certainly rank them. So here, for example, we might discover W1 and W3 are positively correlated with time series. And if you look at these topics and they are indeed quite related to the campaign. And in general this is a very active research topic and there was there new papers being published and there are also many open challenges that have to be solved. Even if we get the best one, the most correlated and the topics might still not be so interesting from causal perspective. Let's say we take topic one as the target to examine. And then we're going to add some history information of X into such a model to see if we can improve the prediction of Y. Basically you're going to have auto regressive model to use the history information of Y to predict itself. If, on the other hand, the difference is insignificant, and that would mean X does not really have a causal relation with Y, and so that's the basic idea
 So this course covered text mining, and there's a companion course called text retrieval and search engines that covers text retrieval. Then we talked about the text clustering and text categorization. Recently, information network mining techniques can also be used to analyze text information network. Moreover, some techniques and information retrieval, for example BM 25, vector space and language models, also very useful for text data mining. And then we talked about the topic mining and analysis and that's where we introduced the probabilistic topic model. So in general, to build a big text data application system, we need two kinds of techniques, text retrieval and text mining. And we then also briefly reviewed some similarity based approaches to text clustering. At that point that we talked about, representing text data with a vector space model, and we talked about some retrieval techniques such as BM25 for measuring similarity of text and for assigning weights to terms, TF-IDF weighting, etc. So given text reviews with overall ratings, the method would allow us to infer the ratings on different aspects. In the case of using non text data to help the text data analysis, we talked about the contextual text mining. We also talked a lot about the text similarity when we discuss how to discover paradigmatically relations, we compare their context of words, discover words that share similar contexts. And this part is well connected to text retrieval. One is generating model and this is a general method for modeling text data and modeling other kinds of data as well. Finally, in the discussion of text based prediction, we mainly talked about the joint mining of text and non text data as they are both very important for prediction. You may recall we mostly used word based representations, and we've relied a lot on statistical techniques, on statistical learning techniques particularly. First in NLP and text representation, you should realize that NLP is always very important for any text applications because it enriches text representation the more NLP, better text representation we can have and this further enables more accurate knowledge discovery, to discover deeper knowledge buried in text. But in practice, PLSA seems as effective as LDA, and it's simpler to implement. And finally, we also recommend you to learn more about the text retrieval information retrieval or search engines. One is generative classifiers, they rely on base rule to infer the conditional probability of a category given text data. And we particularly talked about how text data can help non text data and vice versa. Another technique that's useful is indexing technique that enables quick response of search engine to users query and such techniques can be very useful for building efficient text mining systems as well. I hope you have learned useful knowledge and skills in text mining and analytics. In text clustering we talked about the, how we can solve the problem by using a slightly different than mixture model than the probabilistic topic model. And in particular, we talked about how to use context to analyze topics. And text retrieval as I explained, is to help convert the big text data into a small amount of most relevant data for a particular problem, and can also help providing knowledge prominence, help interpreting patterns later. However, the current state of the art of natural language processing is still not robust enough, and so as a result, the robust text mining technologies today tend to be based on word representation and tend to rely a lot on statistical analysis, as we have discussed in this course. The next point that is about the co- occurrence analysis of text and we introduced some information theory concepts such as entropy, conditional entropy and mutual information. Then we also talk about latent aspect rating analysis. And a search engine would be essential system component in any text based applications, and that's because text data are created for humans, to us to consume. We introduce the contextual PLSA as a generalization or generalized model of PLSA to allow us to incorporate context variables such as time and location and this is a general way to allow us to review a lot of interesting topical patterns in text data. Then we talked about the sentiment analysis and opinion mining, and that's where we introduced sentiment classification problem. So there are many applications of data mining techniques in particular, for example, a pattern discovery would be very useful to generate a interesting features for text analysis. And no serious evaluation has been done yet in, for example, examining the practical value of word embedding other than word similarity based evaluation. And in discussing this, we also discussed the rule of non-text data which can contribute to additional predictors for the prediction problem and also it can provide a context for analyzing text data. And although it's a special case of text categorization, but we talked about how to extend or improve the text categorisation method by using more sophisticated features that would be needed for sentiment analysis. And this is also the basis for understanding LDA, which is a theoretically more appealing model. Text mining has to do with further analyzing the relevant data to discover the actionable knowledge that can be directly useful for decision making or many other tasks. We then talked about how to analyze topic syntax, how to discover topics, and analyze them. This is a practical, useful technique for a lot of text categorization tasks. In Word Association Mining and analysis, the important points are first are we introduced the two concepts for two basic plan complementary relations of words, paradigmatic and syntagmatic relations. We also suggested you to learn more about data mining, and that's simply because general data mining algorithms can always be applied to text data which can be regarded as a special case of general data. In this case we use social network or network in general of text data to help analyzing topics. And the other area that has emerged in statistical learning is the word embedding technique where they can learn vector representation of words and then these vector representations would allow you to compute the similarity of words. We did not talk about that predictive modeling component, but this is mostly about the regression or categorization techniques, and this is another reason why statistical learning is important. One is to effectively reduce the data size from a large collection to a small collection with the most relevant text data that only matter for the particular application. But nevertheless, these are advanced techniques that surely will make impact in text mining in the future. Statistical learning is also key to predictive modeling, which is very crucial for many big data applications. And we talked about the maximum likelihood estimator and the EM algorithm for solving the problem of computing maximum likelihood estimator. And finally we talked about how time series data can be used as context to mine, potentially causal topics in text data. We spend a lot of time to explain the basic topic model PLSA in detail. In the beginning, we talked about the natural language processing and how it can enrich text representation. And finally, we talked about the text based prediction, which has to do with predicting values of other real world variables based on text data. We also briefly introduce some discriminative classifiers, particularly logistical regression K nearest neighbor and SVN.This lecture is a summary of this whole course. So they are very important because they are key to also understanding some advanced NLP techniques and naturally they would provide more tools for doing text analysis in general. A lot of NLP techniques are nowadays actually based on supervised machine learning
