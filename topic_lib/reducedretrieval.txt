 reliably. Consider the sentence. Semantic matching of terms. Consider the word level ambiguity. Pragmatic analysis. And these words can help matching documents where the original query words have not occurred. The same word can have different syntactic categories.This lecture is about natural language content analysis. So as a result, we have robust and general natural language processing techniques that can process a lot of text data. Finally, we're going to cover the relation between natural language processing and text retrieval. This technical code would allow us to add additional words to the query and those additional words could be related to the query words. So that means the NLP techniques must be general, robust, and efficient, and that just implies today we can only use fairly shallow and NLP techniques for text retrieval. In terms of semantic analysis. Such relations can be extracted by using the current natural language processing techniques. For example, we have techniques that can allow us to extract the entities and relations mentioned in text articles. If you see matching of some of the query words in a text document, chances are that that document is about the topic, although there are exceptions. 'cause knowledge graph would contain entities and their relations, and this goes beyond the simple bag of words representation and such technique should help us improve the search engine utility significantly. And common sense reasoning is often required. For example, design can be a noun or a verb. And we'll keep duplicated occurrences of words. Because we have a lot of background and knowledge to help us disambiguate the ambiguity. We also keep a lot of ambiguities because we assume the receiver or the hearer could know how to disambiguate ambiguous word based on the knowledge or the context. There was another reason why we have not used the sophisticated NLP techniques in modern search engines, and that's because some retrieval techniques actually naturally solve the problem of NLP. In terms of inference, we are not there yet, partly because of the general difficulty of inference and uncertainties. So what does that mean for text retrieval? In text retrieval, we're dealing with all kinds of text. So these are examples of semantic analysis and they help us to obtain partial understanding of the sentences. So this achieves to some extent. So this is some extra knowledge that you would infer based on understanding of the text. Another common example of an ambiguous sentence is the following. But yet this representation tends to actually work pretty well for most search tasks, and this is partly because the search task is not all that difficult. That's because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence. topic itself, but because of its relevance to the topic we talk about, it's useful for you to know the background. Although this is still an open topic for research and exploration. There are also syntactical ambiguities, for example. On the other hand, the deeper understanding techniques tend not to scale up well, meaning that they would fail on some unrestricted text. We can also do word sense disambiguation to some extent. An in some evaluation results we have seen above 90% accuracy in terms of partial parsing of sentences. First, what is natural language processing? Which is the main technique for processing natural language to obtain understanding? The second is the state of the art in NLP, which stands for natural language processing. Ambiguity, or PP attachment ambiguity. And we need to figure out the syntactic categories of those words. That is to turn text data into simply a bag of words, meaning we will keep individual words, but will ignore all the orders of words. But we have some techniques that would allow us to do partial understanding of the sentence. So here we show we have noun phrases as intermediate components and then verbal phrases. There's no need to invent the different words for different meanings. Think about the social media data, the accuracy likely is lower. The main topic of this lecture, natural language processing can actually be interpreted in two ways in terms of the structure. Such a representation is often sufficient, and that's also the representation that the major search engines today, like a Google or Bing or using. Computers unfortunately, are hard to obtain such understanding. So that's the topic of this lecture. And, if you don't restrict the text domain or the use of words, then these techniques tend not to work well. This is a general challenging in artificial intelligence. The two users of off may have different syntactic categories. So this is called lexical analysis or part of speech tagging. This is especially useful for review analysis, for example. However, in the long run we still need deeper natural language processing techniques in order to improve the accuracy of the current search engines, and it's particularly needed for complex search tasks. But the fundamental reason why a natural language processing is difficult for computers is simply because natural language has not been designed for computers. Finally we have a sentence. And we also often dealing with a lot of text data. But this structure shows what we might get if we look at the sentence and try to interpret the sentence. In summary in this lecture we talked about what is an NLP and We've talked about the state of the art techniques, what we can do, what we cannot do, and finally, we also explain the why bag of words representation remains the dominant representation used in modern search engines, even though deeper NLP would be needed for future search engines. That means we can get noun phrase structures or verbal phrases structures, or some segment of the sentence understood correctly in terms of the structure. Then you would never match applet or with very small probability, right? So this is the case when some retrieval techniques naturally achieve the goal of word sense disambiguation. If you look at the world alone, it would be ambiguous, but when the user uses the word in the query, usually there are other words. It's very hard to restrict the text to a certain domain. For computers, ambiguity is the main difficulty. To some extent, but in general we cannot really do that. We can also do sentiment analysis, meaning to figure out the weather sentence is positive or negative. It also would have to maintain a large knowledge knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world. Like we say something to basically achieve some goal. Speech Act analysis is also far from being done, and we can only do that analysis for various special cases. We could overload the same word with different meanings without the problem. In fact, most search engines today use something called a bag of words representation. For example, parts of speech tagging or partial parsing or recognizing sentiment, and those are not deep understanding 'cause we're not really understanding the exact meaning of a sentence. And that context can help us naturally prefer documents where Java is referring to program language 'cause those documents would probably match applet as well if Java occurs in the document in a way that it means coffee. It's not giving us a complete understanding as I showed it before for this sentence, but it would still help us gain understanding of the content, and these can be useful. We are far from being able to do a complete understanding of a sentence. So this is how computer would obtain some understanding of this sentence. Now from this representation we could also further infer some other things, and we might indeed naturally think of something else when we read the text, and this is called inference. As you see from this picture, this is really the first step to process any text data, text data in natural languages. We can figure out whether this word in this sentence would have certain meaning in another context. Alright, so this is an example of syntactic ambiguity where we have different structures that can be applied to the same sequence of words. Now, this is probably the simplest representation you can possibly think of. So that makes natural language processing difficult for computers. As a result, we omit a lot of common sense knowledge because we assume everyone knows about that. Of course there are many queries that are not answered well by the current search engines and they do require a representation that would go beyond the bag of words representation that would require more natural language processing to be done. But for a computer would have to use symbols to denote that, right? So we would use a symbol D1 that denote a dog and B1 to denote a boy and then P1 to denote the playground, playground. So this is called a bag of words representation. They may work well based on machine learning techniques on the data that are similar to the training data that the program has been trained on, but generally wouldn't work well. We usually think of this as processing of natural language. So imagine a computer wants to understand all these subtle differences and meanings. Most of the existing work has been evaluated using news data set and so a lot of these numbers are more or less biased toward news data. So for example, here it shows that A and a dog would go together to form a noun phrase. The data that are very different from the training data, so this pretty much summarizes the state of the art of natural language processing. So those techniques also helped us bypass some of the difficulties in natural language processing. It's also hard to do precise deep semantic analysis, so here's example in the sentence. How do we define owns exactly the word own is something that we understand, but it's very hard to precisely describe the meaning of own for computers
 Typical keyword query and the search engine system would return relevant documents to users. This gives excellent discussion of the relationship between information filtering and information retrieval. By the structures documents. The difference is. In the previous lecture we talk about natural language content analysis. As a result, bag of words representation remains very popular in applications like search engines. Push tends to be supported by recommender systems and pull tends to be supported by a search engine. To help users get access to the text data. For example, the user may type in the query and then browse results to find the relevant information. And this is because. Or the user doesn't know what are the key words to use in the query. In querying the user will just enter a query. Again, browsing tends to be more convenient. So to summarize this lecture we've talked about the two high level strategies for text access, push and pull. We explained that the state of the art natural language processing techniques are still not good enough to process a lot of unrestricted text data in a robust manner. So for example, a news filter or news recommender system could monitor the news stream and identify interesting news to you and simply push the news articles to you. Here, information filtering is similar to information recommendation or the push mode of information access. And in this case, a user typically would use a search engine to fulfill the goal. So the main question we will address here is how can a text information system help users get access to the relevant text data we're going to cover two complementary strategies, push versus pull. The relationship between browsing and the query is best understood by making an analogy to sight seeing. In the pull mode we can further distinguish querying and browsing again, we generally want to combine the two ways to help users so that you can support both querying and browsing. Or simply because the user finds it inconvenient to type in a query. In the case of browsing, the users would simply navigate into the relevant information by following the paths supported. So for example, when you search for information on the web, a search engine might infer you might be also interested in some related information. This is also important step to convert raw big text data into small relevant data that are actually needed in a specific application. In such a case, the system can interact with you and can learn your interest and then can monitor the information stream. In the pull mode, we can further distinguish in two ways to help users querying versus browsing. So this is a pull mode in contrast, in the push mode, the system will take the initiative to push the information to the user, or to recommend that information to the user. To search for information there, it's still hard to enter the query in such a case. So how to construct such a topic map is in fact a very interesting research question that likely will bring us more interesting browsing experience on the web or in other applications. But after you have collected information and have purchased your product. So this is about the two high level strategies or two modes of text access. In this lecture we're going to talk about some high level strategies. If it is, the system has seen any relevant items to your interest the system could then take the initiative to recommend information to you. So this is usually appropriate for satisfying a users Ad hoc information need. An ad hoc information need is temporary information need, for example. You want to buy a product so you suddenly have a need to read reviews about related products. Now this would be appropriate if the user has a stable information need. This mode of information access maybe also appropriate when the system has good knowledge about the users need and this happens in the search context
 Query. But nevertheless. the query the documents. The differences between text retrieval and database retrieval. Second, document ranking is generally preferred. These documents for a user to examine. The query and document pair. So text retrieval is basically a task where the system would respond to a user's query with relevant documents. Basically, to support the query. So text queries tend to be ambiguous. As opposed to absolute relevance. And similarly there is a relevant document that's miss classified as non relevant. First, we'll define text retrieval. But in general, documents are longer than the queries. But literally, information retrieval would broadly include retrieval of other non textual information as well. The expected relevant documents may be different. To indicate the relevant documents so we can see the true relevant documents here. Is defined as a sequence of words. Is to compute an approximation of this relevant document. Each is relevant. The SQL query more precisely. Second, we're going to make a comparison between text retrieval and the related task, database retrieval. And this is called absolute relevance.This lecture is about the text retrieval problem. And with the document selection. Unfortunately, this set of relevant documents is generally unknown. Topics you by using specific vocabulary and as a result. They are generally not equally relevant. Alternatively, there's another strategy called document ranking. Finally, we're going to talk about the document selecting versus document ranking as two strategies for responding to a users query. In text retrieval, the data is unstructured free text, but in databases. First text retrieval is an empirical defined problem. So as I said, ranking is generally preferred. A user will typically give a query to the system to express information need and then the system would return relevant documents to users. The probability ranking principle established some solid foundation for ranking as a primary task for search engines. The relevant document in the set is defined as follows, it basically. Keyword queries or natural language queries tend to be incomplete also. And theta is a cut off determined by the user. This preference also has a theoretical justification, and this is given by the probability ranking principle. No relevant documents to return in the case of overly constraint query. That's a function that would take a document and query as input. So clearly the utility of document is dependent on other documents that user has seen. We might have identical documents that have similar content or exactly the same content. 2nd, a user would be assumed to browse the results sequentially. And this would allow us to compute the score for each document independently, and then we're going to rank these documents based on those scores. So the scenario is the following you have a collection of text documents. So this slide shows a formal formulation of the text retrieval problem. Relevant documents refer to those documents that are useful to the user who is in typing the query. To our user. And then give a zero or one as output to indicate whether this document is relevant to the query or not. Now this task is often called information retrieval. Because this is a very important topic for search engines. Each q sub I is a word in the vocabulary. That also means we must rely on empirical evaluation involving users. So to summarize this lecture, the main points. And this is also to bypass the difficulty in determining absolute relevance. And therefore it would make sense to feed the users with the most relevant documents. Next we have the query, which is a sequence of words. And it turns out that ranking is indeed generally preferred to document selection. Match no random documents because in the collection no others have discussed the topic using these vocabularies. And this will help users prioritize examination of search results. We assume it's generally not very useful for the user to see another similar or duplicated one. The first is the classic paper on probability ranking principle. So formally, we can see the task is to compute this r prime of Q, an approximation of the relevant documents. That would simply give us a value that would indicate which document is more likely relevant. And then the goal of text retrieval is to find the set of relevant documents which we denoted by R of Q because it depends on the query and this is in general a subset of all the documents in the collection. because if the documents are independent then we can evaluate the utility of each document separately. Functioning we are going to do basically classify them into two groups, relevant documents and non relevant ones. Relevant relative relevance would be easier to determine their absolute relevance because in the first case, we have to say exactly whether a document is relevant or not. So here we can see in the approximation of the relevant documents we have got some non relevant documents. And this is partly due to the difference in the information. The queries are typically well defined. very similar. Because of this difference, we can also see that text information tends to be more ambiguous, and we talked about that in the natural language processing lecture. Now typically the documents are much longer than queries. We also still want to rank these red documents because. Actually match the users query with the companion text data of the image. First, the utility of the document to a user is independent of the utility of any other document. So this further suggests that the main technical challenge in designing search engine is redesigned effective ranking function. In the case of document selection is unlikely accurate. And that's what ranking is doing. Such a function is the main topic in the following lectures. So this function then can be used to rank the documents. And then you are to compute the answers to this query. And user dependent in the sense that for the same query typed in by different users. Although there is important difference, the principles and methods are. This principle says returning a ranked list of documents in descending order of probability that a document is relevant to the query is the optimal strategy under the following two assumptions. In the case of texy retrieval, we're looking for relevant documents. Or data. Current image search engines. Where collection is so large the user doesn't have complete knowledge about the whole collection. If it's empirically defined. And we can assume that all the documents that are ranked above this threshold are in this set. A document is defined in the same way, so it's also a sequence of words and here d sub ij. So it's not going to make a call whether this document is relevant or not, but rather it would say which document is more likely relevant. So it's unclear what should be the right answers to a query, and this has very important consequences and that is text retrieval is an empirically defined problem. So this is a collective relevance and that also suggests that the value of the document might depend on other documents. Whereas in databases that data tended to have well defined the semantics. In the last lecture we talked about the high level strategies for text access. Be cause in effect, these are the documents that we deliver to the user. But here for simplicity, we just assume there is one kind of language as the techniques used for retrieving data from multiple languages are more or less similar to the techniques used for retrieving documents in one language. It's worth noting that text retrieval is at the core of information retrieval, in the sense that other medias, such as video, can be retrieved by exploiting the companion text data. All the documents that. So For these reasons, ranking is generally preferred now. Because a user cannot digest all the contents at once, the user general would have to look at each document sequentially. Even if the classifier is accurate. is also a word in the vocabulary. The documents may be very short. This picture shows our overall plan for lectures. Which is a set of documents that would be useful to the user. So first it's about the text retrieval problem. Versus the queries that are typed in by users on the search engine. Why? Because the only clue is usually the query, but query may not be accurate in the sense that it could be overly constrained. And so in this case you can see the system must decide if a document is relevant or not. And this is when you thought these words might be sufficient to help you find the relevant documents. The second assumption is to say that the user would indeed follow the ranked list if the user is not going to follow the ranked list is not going to examine the documents sequentially, then obviously the ordering would not be optimal. There is also important difference in the queries. The first strategies will do document selection and that is we're going to have a binary classification function or binary classifier. But it turns out that they are not sufficient, and there are many distraction documents using similar words. Why cause when the user is looking for the information? In general, the user does not have a good knowledge about the information to be found. These Assumptions aren't necessarily true. But if the user only wants to read a few relevant documents, the user might stop at the top position. So in this case the system only needs to decide if one document is more likely relevant than another, and that is it only needs to determine relative relevance. The query given to us by the user is only a hint on which document should be in this set. In the case of document ranking, we can see the system seems like simply ranks all the documents in the descending order of the scores. Up to the time when the book was written, Chapter 6 of this book has in depth discussion of the probability ranking principle and probabilistic retrieval models in general. And in that case the user does not have a good knowledge about what. Now then we have a collection of documents
 And document lengths. Use a probability. To indicate whether a document is relevant to a query. 2nd. That can compute a score based on the query and document. BM25, query likelihood, PL2. the weighting function to determine the overall contribution of matching a word. 1st. Is contributing more to the overall score than matching a common term. The retrieval function. To characterize this information. Queries and documents are all observations from random variables.This lecture is the overview of text retrieval methods. Another is language model, yet another is divergent from randomness model. Interestingly, although these different models are based on different thinking. Trying to find a truly optimal retrieval model. TF and document frequency of words, such information is used in. Bag of words representation remains the main representation. And we achieve this goal by designing a retrieval model which gives us a formalization of relevance. We have a query that has a sequence of words and a document that's also a sequence of words and we hope to define a function F. Pre requires a computational definition of relevance and we achieve this goal by designing appropriate retrieval model. So in this case, the ranking function is defined as the similarity between the query and the document. Pivoted length normalization. We then define the score of document with respect to a query as the probability that this random variable R is equal to 1 given a particular document and query. Tends to be very similar. A good retrieval function. So to summarize. When optimized, these models tend to perform similarly. With respect to a document d here would be based on scores computed based on each individual word. In general. To satisfy. Any term is expected to occur more frequently. Such as presidential campaign and news. First, these models are all based on the assumption of using bag of words to represent text, and we explained this in the natural language processing lecture. We might also denote as c of presidential and d. Now clearly this means our function must be able to measure the likelihood that a document d, is relevant to a query Q. One is classic probabilistic model. The second kind of models are called probabilistic models. We explained that the main problem is to design a ranking function to rank documents for a query in this lecture we will give a overview of different ways of designing this ranking function. The third kind of models are based on probabilistic influence, so here the idea is to associate uncertainity to inference rules, and we can then quantify the probability that we can show that the query follows from the document. So all these are trying to characterize the popularity of the term in the collection in general, matching a rare term in the collection. And these functions tend to also involve similar variables. And we call this document frequency or DF of presidential. In the previous lecture we introduced the problem of text retrieval. There are three different components, each corresponding to how well the document matches each of the query words. And that means the score would depend on the score of each word. This is called a term frequency or TF. Basically, we assume that if a document is more similar to the query, then another document is then we will say the first document is more relevant than the second one. Another factor is how long is the document. So with this assumption, the score of a query, presidential campaign news
 First, we represent a document and query via term vector. Each document vector is also similar. Vector space model is a framework. Therefore, our ranking function is also defined as the similarity between the query vector and document vector. Vector space model is a special case of similarity based models, as we discussed before, which means we assume relevance is roughly similarity between the document and the query. Corresponding to the weights on different terms. Each corresponding to the weight on a particular term. And then we're going to measure the similarity between the query vector and every document vector. It indicates how well the term characterizes the document. Similarly to term weight in the document is also very meaningful. Basically I showed you some examples of query and document vectors, but where exactly should the vector for a particular document point to? So this is equivalent to how to define the term weights. Basically, what we see here is only the vector representation of the document. Basically, we assume that if a document is more similar to a query than another document then the first document will be assumed to be more relevant than the second one, and this is the basis for ranking documents in this approach. Each term is assumed to define one dimension. Finally, how to define the similarity measure is also not given. Of course, the document has other information. Now this is different from another document which might be represented as a different vector D2 here. So this shows that by using this vector space representation we can actually capture the differences between topics of documents. The basic idea of vector space retrieval model is actually very easy to understand. So this is basically the main idea of the vector space model. As we will see later, there are other ways to model relevance.This lecture is about the vector space retrieval model we're going to give a introduction to its basic idea. For example, the orders of words are simply ignored, and that's because we assume that the bag of words with representation. Where each dimension corresponds to a term. So for example, one document might be represented by this vector, D1. So here are term can be any basic concept, for example a word or a phrase. We clearly assume the concepts are orthogonal, otherwise there will be redundancy. And we're going to assume that all our documents and the query will be placed in this vector space. How do you compute those element values in those vectors? Now this is a very important question because term weight in the query vector indicates the importance of term. So the relevance in this case would be assumed to be the similarity between the two vectors. In the last lecture we talked about the different ways of designing a retrieval model. So in this case, for example, we can easily see D2 seems to be the closest to this query vector, and therefore D2 will be ranked above others. In this lecture we're going to talk about this specific way of designing a ranking function called vector space retrieval model. Which would give us a different a ranking function. It has a number of elements and each value of each element is indicating that weight of the corresponding term. So with this representation you can already see D1 seems to suggest a topic about presidential library. Alright, what does this mean terms of representation of document? That just means we're going to look at our document from the perspective of this vector. Imagine a high dimensional space. So in this case, the document covers programming and library, but does not talk about the presidential
 D4 and D3 are relevant documents and D1, D2, and D5 are non relevant. Similarly. When user 0/1 bit vector to represent a document or a query. And 3rd is to define the similarity between two vectors, particularly the query vector and the document vector. Let's compute the similarity. Vector space model is really a framework. And with such a instantiation, and we showed that the scoring function is basically to score a document based on the number of distinct query words matched in the document. Again here the simplest strategy is to use a bit vector to represent both the query and a document. So as a result this scoring function basically matches how many unique query terms are matched in the document. In the vectors space model of course we want to 1st compute the vectors for these documents and the query. And to also place a query in the vector space as a vector. They cover different terms in the query. And finally, it did not say how we should measure the similarity between the query vector and the document vector. As we discussed in the previous lecture. Which is based on the bit vector represntation dot product similarity and bag of words representation. So a commonly used similarity measure here is dot product. So here I show some sample documents and a simple query. And you can easily verify their correct because we're basically counting the number of unique query terms matched in each document. To summarize, in this lecture we talked about how to instantiate a vector space model. So as we discussed in the previous lecture, the Vector Space model is really a framework. And we use the dot product as the similarity function. Many words will only occasionally occur in the document. Now, if I ask you to rank these documents how would you rank them? This is basically our ideal ranking: when humans can examine the documents and then try to rank them. Using a vector space model and then we make assumptions about how we place vectors in the vector space and how we define the similarity. So intuitively D3 is better because matching presidential is more important than matching about even though about and presidential above in the query. The query is news about the presidential campaign and we have 5 documents here. Similarly, if the document has a term, but the term is not in the query, there will be a zero in the query vector, so those don't count. Many words don't really occur in any document. Each word defines one dimension and this is basically the bag of words representation. Right here I show 2 documents D1 and D3 and we have the query also here. And we're going to talk about how we use the general framework of the vector space model as a guidance to instantiate the framework to derive a specific ranking function. Now note that this matching actually that makes sense, right? It basically means if a document matches more unique query terms then the document will be assumed to be more relevant, and that seems to make sense. In this case you can see the result is 3 because D3 matched three distinct query words: news, presidential, campaign. Indeed, that's probably the simplest vector space model that we can derive. They match news, presidential and campaign. However, if we examine this model in detail, we likely will find some problems. Is it actually meaningful? Does it mean something? It's related to how well the document matches the query. The document vector in general will have more ones of course. In this case, we basically only care about word presence or absence. So now we have placed the documents and the query in the vector space. In this case, we use each word to define a dimension. Because if a document, if a term is matched in the document, then there will be 2 ones. When it's one, it means the corresponding word is present in the document or in query. But in the case of D3, it matched news, presidential, and campaign. So this is to continue the discussion of the vector space model, which is one particular approach to design ranking function. So if we can provide a definition of the concepts that would define the dimensions and these Xi's or Yi's, namely weights of terms for query and document, then we will be able to place document vectors and query vector in this well defined space and then if we also specify similarity function then we'll have a well defined the ranking function. So this is our formula, and that's actually particular retrieval function, a ranking function, right? Now we can find the implement this function using a programming language and then rank documents for query. And we're going to cover the simplest instantiation of the framework. A lot of words will be absent in a particular document. forming, forming product and these two will generate another product and these two will generate yet another product and so on so forth. And if you look at the these documents for a moment, you may realize that some documents are probably relevant and some others are probably non relevant. The second is to decide how to place documents as vectors in the vector space. So you can imagine in order to implement this model we have to say, specifically, how we compute these vectors? What is exactly Xi and what is exactly Yi? This will determine where we place a document vector, where we place a query vector. And then X2 * Y2 and then finally XN multiplied by YN and then we take a sum here. We also talk about a very simple way to instantiate vector space model. So intuitively, we would like D3 we ranked above D2. So what do you think is the vector representation for the query? Note that we are assuming that we only use zero and one to indicate whether the term is absent or present in the query or in the document, so these are 0/1 bit vectors. Can we expect this function to actually perform well when we used to rank the documents for users' queries? So it's worth thinking about. But D4 is clearly about the presidential campaign. Whereas D1 only match two. But this model doesn't do that. The dot product of two vectors is simply defined as the sum of the products of the corresponding elements of the two vectors. Now, what about the documents? It's the same, so T1 has two words news and about. So what does that mean? Well, that means this number or the value of this scoring function is simply the count of how many unique query terms are matched in the document. Let's look at how we measure the similarity. This is how we interpret this score. We also show that such a such simple vector space model still doesn't work well and we need to improve it. But it will also have many zeros, since the vocabulary is generally very large
 So now as a result of IDF weighting, we will have D3 to be ranked above D2 becauses it matched rare word whereas D2 matched common word. The query vector is the same because all these words occur exactly once in the query, so the vector is 001 vector. So to summarize this lecture we've talked about how to improve the vector space model, and we've got to improve the instantiation of the vector space model based on TF IDF weighting. Document frequency is the count of documents that contain a particular term. But intuitively we would like D4 to be ranked above D3 and D2 is really non relevant. So suppose we change the vector representation into term frequency vectors. If we can do that, then we can penalize common words which generally have a low IDF and reward rare words which will have high IDF. Now a specific function may not be as important as the heuristic to simply penalize popular terms. By multiplying it by the idea of the corresponding word as shown here. In general, you can see it would give a higher value for a low DF word, a rare word. And we have seen that this improvement model indeed works better than the simplest vector space model. Now, without the IDF weighting before, we just have term frequency vectors, but with IDF weighting, we now can adjust the TF weight by multiplying with the IDF value. In the simplest model, we only model the presence and absence of the time. But D4 would be different. In order to consider the difference between a document where aquarium occurred multiple times and one where the query term occurs just once, we have to consider the term frequency: The count of a term in the document. So that is to say, now the elements of both queries vector and document vector will not be 0 or 1s, but instead they will be the counts of word in the query or the document. So this shows that the IDF weighting can solve problem 2. Here we said inverse document frequency because we actually want to reward a word that doesn't occur in many documents. Those are generally very frequently occur everywhere matching it doesn't really mean anything, but computationally. As a result, now the score for D4 is high. Perhaps we will have to use different ways to instantiate the vector space model. So intuitively we want to focus more on the discrimination of Low DF words rather than these common words. So this can be seen as a more accurate representation of our documents. But the matching of about and presidential with discriminate them. This is the continued discussion of the vector space model. So more specifically, the IDF can be defined as a logarithm of (M + 1) / K, where M is the total number of documents in the collection, K is the DF or document frequency, the total number of documents containing the word W. They can be essentially ignored and this makes sense when the term occurs so frequently, and let's say a term occurs in more than 50% of the documents, then the term is unlikely very important, and it's basically a common term. In fact it looks identical, but inside the sum of course Xi and Yi are now different, and all the counts of word I in the query and in the document.In this lecture we are going to talk about how to improve the instantiation of the vector space model. So how effective is this model in general? When we use TF IDF weighting, let's look at the obvious documents that we have seen before nicely. So if you look at these, the IDF will distinguishing these two words as a result of adjustment here would be larger, would make this weight larger. So the improvement mostly is on the placement of the vector where we give higher weight to a term that occured many times in the document but infrequently in the whole collection. These are the new scores of the new documents, but how effective is this new weighting method and new scoring function? So now let's see overall how effective is this new ranking function with TF IDF weighting? Here we show all the five documents that we have seen before, and these are their scores. We can come up with a simple scoring function that would give us basically a count of how many unique query terms of matching the document. So this means, by using term frequency we can now rank D4 above D2 and D3 as we hope to. We're going to focus on how to improve the instantiation of this model. So how can we fix this problem? Intuitively, we would like to give more credit for matching presidential than matching about, but how can we solve the problem in a general way? Is there any way to determine which word should be treated more importantly, and which word can be basically ignored about is such a word at which does not really care that much content? We can essentially ignore that we sometimes call such a word stop word. We have seen document frequency as one signal used in the modern retrieval functions. Can you see the difference? And it has to do with the consideration of multiple occurrences of the same term in a document. So how can we do this systematically? Again, we can rely on some statistical counts, and in this case the particular idea is called the inverse document frequency. But because of the change of the vector, now the new score has a different interpretation. Because Now D5 here, which did not have a very high score with our simplest vector space model, now actually has a very high score. As shown on this slide, in particular, if you look at these three documents, they will all get the same score because they matched 3 unique query words. So if you count the occurrence of the word in the whole collection, then we would see that about has much higher frequency than presidential which tends to occur only in some documents. At the following heuristics: First, we would like to give more credit to D4 because it matched the presidential more times than these three; Second, Intuitively, matching presidential should be more important than matching about because about this very common word that occurs everywhere. We ignore the actual number of times that term occurs in the document. One natural thought is, in order to consider multiple times of the term in the document, we should consider the term frequency instead of just absence or presence. There's no difference. Because now presidential occured twice here. In the previous lecture, you have seen that with simple same instantiations of the vector space model. This is actually a common phenomenon in designing retrieval functions. For example, here we can see is adjustment, and in particular for about there is adjustment by using the IDF value of about which is smaller than the IDF value of presidential. And so the way to incorporate this into our vector representation is then to modify the frequency count. Basically, when you try to fix one problem, you tend to introduce other problems and that's why it's very tricky how to design effective ranking function and what's the best ranking function
 And inside the sum, each matching query term has a particular weight and this way it is TF IDF weighting. In the previous lecture, we have derived a TF IDF weighting formula using the vector space model. So to summarize this lecture. W in the query and the count of the word in the document. Indeed, it has received the highest score among all these documents, but this document is intuitively non relevant, so this is not desirable. That is a 01 bit transformation and the linear transformation. Before we discuss the details, let's take a look at the formula for this simple TF IDF weighting ranking function and see why this document has received such a high score.In this lecture we continue the discussion of vector space model. Once we see a word in the document, it's very likely that the document is talking about this word. If you plug this function into our TF IDF weighting vector space model. It has upper bound and social robust and effective. The other is the document frequency. So intuitively, in order to lower the score for this document, we need to somehow restrict the contribution of the matching of this term in the document. In Y axis I showed term frequency weight. So we precisely recover the 01 bit transformation. And that is M. The main point is that we need to do some linear TV TF transformation, and this is needed to capture the intuition of diminishing return from higher term counts. And will discuss how we can further improve this formula in the next lecture. And that is the idea of TF transformation. For example, in the 01 bit vector representation, we actually use researcher transformation function as shown here. So the count of campaign in this document is a four which is much higher than the other documents and has contributed to the high score of this document. We probably shouldn't reward multiple occurrences. So this transformation function is going to turn the raw count of word into a term frequency, wait for the word in the document. In other words, this upper bound might also ensure that all terms would be counted when we aggregate the weights to computer score. This is the number of documents that contain this word W. then we would end up having the following ranking function which has a BM 25 TF component. The other variables involved in the formula included the count of the query, term Term. So for example, with the logarithm function, we can have a sub linear transformation that looks like this and this would control the influence of really high weight because it's going to lower its inference, yet it will retain the inference of small counts. BM stands for best matching. Kind of confirmed that it's not a accidental matching of the word. And this upper bound is useful to control the influence of a particular time. So this is the formula and if you look at the formula carefully, then you will see it involves a sum over all the matched query terms. And if you think about the matching of terms in a document carefully, you actually would realize. And so that we can prevent a spammer from just increasing the count of one term to spam all queries that might match this time. a state of the art ranking function called BM 25. By that I mean the first occurrence of term says a lot about the matching of this term because it goes from zero count to a count of 1 and that increase means a lot. But imagine we have seen, let's say, 50 times of the word in the document. Furthermore, 1 interesting property of this function is that as we vary K, we can actually simulate a different transformation functions, including the two extremes that I've shown here
 and IDF weighting and document length normalization. And a query TF component here. Such as IDF inverse document frequency. There is also a query term frequency component. Then long papers generally have higher chance of matching query words. okapi TF transformation here. As we discussed before, and this is to achieve a sub linear transformation. Long documents such as full paper. But obviously we can consider many other choices.This lecture is about document Length normalization. First, in such a model we use the similarity notion relevance, assuming that the relevance of a document with respect to a query is. In the vector space model. These major heuristics are the most important heuristics to ensure such a general ranking function to work well for all kinds of text. If you generate a long document randomly by simply sampling words. But in fact the BM 25 has been derived using probabilistic modeling. Basically proportional to the similarity between the query and document, so naturally that implies that the query an document must be represented in the same way, and in this case we represent them as vectors in high dimensional vector space where the dimensions are defined by words or concepts or terms in general. And finally, BM25 and pivoted normalization seems to be the most effective formulas out of the vector space model. So far in the lectures about the vector space model, we have used various signals from the document to assess the matching of the document, with the query. And there is a reward for short documents. Is empirically an analytically shown to be better than BM25. is seem to be Interpolation of 1 and the normalized document length controlled by a parameter b here. And this is the idea of document length normalization. And we generally need to use a lot of heuristics to design the ranking function. And a reference in the end has details about derivation of this model and here we see that it's basically the TF IDF weighting model that we have discussed. Some examples to show the need for several heuristics, including TF weighting and transformation. Factor here we are adjusting K, but it achieves a similar factor, just because We put a normalizer in the denominator therefore. However, there is another case when the document is long and that is when the document simply has more content. First, the ranking function actually has a nice interpretation in the vector space model, we can easily see it looks very much like a vector space model with a special weighting function. We can even use latent semantic analysis to find some clusters of words that represent a latent concept as one dimension. On the web pages, those are the text fields that describe links to other pages, and these can all be combined with appropriate weights of different fields to help improve scoring for a document. But one might reason that D6 May have matched these query words In a scattered manner. But we have not considered document length. The count of the term in the document. When we use BM25 for such a document. From a distribution of words, Then eventually you probably will match any query. As we explained there is this. So for example, cosine measure can be regarded as the dot product of two normalized vectors. And it's used in all the major search engines. And finally, the last paper has a discussion of improving BM25 to correct the over penalization of long documents. When we compare the matching of words in such a long document with matching of the words in a short abstract. The second reason is because the original BM25 has somewhat different form of IDF. For example, stemmed words. It would use more words than the corresponding abstract. Think about the full text article of a research paper. If you look at the matching of these query words, we see that in D6 there are more matchings of the query words. So here, I show 2 example documents. The first one is called a pivoted length Normalization vector space model. It's an excellent example of using empirical data analysis to suggest the need for length normalization and then further derived length normalization formula. And this is of course controlled by the parameter b here. We can use phrases to define dimensions. right, so this would cause penalization for long document, because the larger the denominator is then the smaller TF weighting is. And these are in fact the state of art vector space model formulas. There is also a possibility to improve the similarity function, and so far we have used the dot product, but one can imagine there are other measures. For example, can we further improve the instantiation of the dimension of the vector space model? Now We've just assumed that the bag of words representation, so each dimension is the word. That means we first normalize each vector and then we take the dot product that would be equivalent to the cosine measure. D6, on the other hand, has 5000 words. And the obvious choice is to apply the BM25 for each field and then combine the scores. And we have. So in this sense we should penalize long documents because they just naturally have better chances for matching any query. And the reason is because a document may be long because of different reasons. In such a case, obviously we don't want to over penalize such a long document. In fact, it's sufficiently general. And then apply BM 25. And this has played an important role in determining the effectiveness of the retrieval function. We can simply add a small constant to the TF normalization formula, But what's interesting is that we can analytically prove that by doing such a small modification. D4 is much shorter with only 100 words. The degree of penalization is controlled by b because if we set B to a larger value than the normalizer would look like this, there's even more penalization for long documents and more reward for the short documents. Now consider another case of a long document where we simply concatenated a lot of abstracts of different papers. But there has been also further development in improving BM. But we also put a document length Normalizer in the bottom. In this lecture, we're going to continue the discussion of the vector space model. Whereas the value of the normalizer would be smaller for shorter documents. That you have seen here, so as effective retrieval function BM25 should probably use a heuristic modification of the IDF to make it even more look like a vector space model. So to summarize, all what we have said about the vector space model. But dot product seems still the best and one reason is because it's very general. On the Y axis we show the normalizer in this case, the pivoted length normalization formula for the normalizer is. So for example you might consider Title Field, the abstract or body or the research article or even anchor text. Anyway, this Normalizer has an interesting property. The 3rd paper has a thorough discussion of BM25 and its extensions. In particular, we're going to discuss the issue of document length normalization. But if a document is longer than the average document length, then there will be some penalization, whereas if it's a shorter, then there's even some reward. Also similar in that it also has a IDF of component here. So far we have talked about the mainly how to place the document vector in the vector space
 That's a disjunctive query. Basically, it's similar to disjunctive Boolean query. This is a typical text retrieval system architecture. What about the multi term keyword query? We talked about vector space model for example and we would match such query with document generated score and the score is based on aggregated term weights. So I want the relevant document to match both term A and the term B right? So that's one conjunctive query? Or I want the relevant documents to match term a or term b. And they would contain information such as document IDs, term frequencies or term positions etc. That means, although the general phenomenon is applicable or is observed in many cases, the exact words that are common may vary from context to context. So they tend to be often used in queries, and they also tend to have high TF IDF weights, these intermediate frequency words. Tokenization is to normalize lexical units into the same form so that semantically similar words can be matched with each other. Boolean query is basically boolean expression like this. Those words don't occur very frequently and there are many such words. But mostly words would occur just rarely. And these patterns are basically characterized by the following pattern of few words, like the common words the, a, or we occur very frequently in text. But it's also to help improving speed. It's often true that the users are unnecessary interested in those words, but retain them would allow us to match such a document accurately, and they generally have very high IDFs. The X axis is basically the world rank and this is r(w), Y axis is a word frequency or F(w). And the queries representation would then be given to the scorer which would use the index to quickly answer a users query by scoring the documents and then ranking them.This lecture is about the implementation of text retrieval systems. These are the highest frequency words they occur very frequently. So these interaction signals can be used by the system to improve the ranking accuracy by assuming the view of the documents are better than the skiped ones. So formally, if we use F(w) to denote the frequency, r(w) to denote the rank of a word, then this is the formula. The basic idea is do pre-compute as much as we can basically. So here's some common phenomenon of word distribution in text. The main challenge is to manage a lot of text data and to enable a query to be answered very quickly and to respond to many queries. These words tend to occur in quite a few documents, but they are not like those most frequent words and they're also not very rare. We take the union of all the documents that match at least one query term, and then we would aggregate the term weights. Those words are very, very frequent. Just look up a sequence of document IDs and frequencies for all the documents that match a query term, so we would read those entries sequentially. This is the document ID, document one, and the frequency is 1, the TF is 1 for news. The first part is the indexer, and the second part is a scorer that responds to the users query, and the third part is a feedback mechanism. Those words are actually very useful for search. And this data structure can then be used by the online module, which is a scorer to process users query dynamically and quickly generate search results. So here, of course we have to use some language specific natural language processing techniques. This word occured in only one document, document three, so the document frequency is one. Also, if a user happens to be interested in such a topic, but because they're rare. Now, because they are very large, compression is often desirable. In this lecture we will discuss how we can implement text retrieval method to build a search engine. So for example, computer computation and computing can all be matched to the root form compute. The results will be given to the user and then the user can look at the results and provide some feedback that can be expressed judgments about which documents are good, which documents are bad or implicit feedback such as click slows so the user doesn't have to do any, anything extra the user would just look at the results and skip some and click on some results to view. It's actually pretty simple, right? With this structure we can easily fetch all the documents that match a term and this will be the basis for scoring documents for a query. The feedback mechanism can be done online or offline depending on the method. Basically, it's like a or b. And so, for example news. This law says that the rank of word multiplied by the frequency of the word is roughly a constant. Normally this is good idea to increase the coverage of documents that matched with this query, but it's also not always beneficial because sometimes the subtle of this difference between computer and computation might still suggest the difference in the coverage of the content, but in most cases stemming seems to be beneficial. And this has been used to in many search engines to support basically search algorithms, sometimes other indices, for example, a document index might be needed in order to support the feedback. For example, the number of documents that match the term or the total number of frequency of the term, which means we would count the duplicated occurrences of the term. But it occurred twice in this document, so the frequency count is 2 and the frequency count is useful in some other retrieval method where we might use the frequency to assess the popularity of a term in the collection, and similarly will have a pointer to the postings here. And you might also realize we need this count of documents or document frequency for computing some statistics to be used in the vector space model. Can you think of that? So what weighting heuristic would need this count? That's IDF, right? Inverse document frequency. Basically why is it more efficient than sequentially just scanning documents? Like, this is the obvious approach. Now the position information is very useful for checking whether the matching of query terms is actually within a small window of let's say 5 words or 10 words or whether the matching of the two query terms is in fact a phrase of two words. So in this case it matched three documents and store information about these three documents here. So the count of documents is 3. So that's the fastest way to respond to a single term query. Imagine the web it has a lot of documents. They are in fact too frequent to be discriminated and they are generally not very useful for retrieval. And in general we don't have to have direct access to a specific entry we generate with. The document ID is 3 an it occured it twice. So if the query has just one word news and we can easily look up this table to find the entry and go quickly to the postings and fetch all the documents that match news. Stemming is often used and this is where map all the inflectional forms of words into the same root form. So the most commonly used indexes, is called inverted index. We have three documents here and these are the documents that you have seen in some previous lectures. You can just compute the score for each document and then you can then score them. The implementation of the indexer, and the scorer is fairly standard and this is the main topic of this lecture. Now how can we answer such a query by using inverted index? If you think a bit about it would be obvious cause we had simply fetch all the documents that match term a, and also fetch all the documents that match term B and then just take the intersection to answer a query A&amp;B or to take the Union to answer the query A or B. Once we do tokenization then we would index the text documents and that is to convert the documents into some data structure that can enable fast search. What about the multiple term queries? Let's first look at the some special cases of the Boolean query. So they account for a large percent of occurrences of words. This term ocurred in all the three documents. We can see the documents are first processor via tokenizer to get that tokenized units, for example words, and then these words or tokens will be processed by a indexer that would create the index which is a data structure for the search engine to use to quickly answer query. Suppose we want to create inverted index for these documents
 And on the right, we have shown Term Lexicon and DocumentID Lexicon. And this will be followed by document IDs 2s. These lexicons are to map stream-based representations of document IDs or terms into integer representations or to map back from integers to the stream representation. And this is also possible because in order to uncover or uncompress these document IDs. Why? Think about the distribution of words and this is due to the Zipf's law, and many words occur rarely. And we can imagine if a term has matched many documents, then there will be long list of document IDs. And there are many different methods for encoding. We're going to scan these documents sequentially, and then parse the document, and count the frequencies of terms. 10 encodes 2 in unary coding. Whereas if a term according only a few documents, then the gap would be large. Once we look up a term we fetch all the document IDs that match the term. In this stage, we generally sort the frequencies by document IDs because we process each document sequentially. Basically you will not count the terms in a small set of documents. Because we store the difference and in order to recover the exact the document ID, we have to 1st recover the previous document IDs and then we can add the difference to the previous document ID to restore the current document ID. But because we tend to see many small values, they are very frequent. And that also means. Similarly, 5 is encoded as 110 followed by 01 and in this case, the unary code encodes 3. Therefore, the document IDs are all 1s in this case. For example, 3 is encoded as 101. First, you collect the local term ID, document ID, and frequency tuples. And on the top we can see these are the old entries for the documents that match term ID1. Those are the frequencies of terms in all those documents. And what about the document IDs that might be compressed using d-gap. There are many approaches to solve the problem and sorting based method is quite common. We have to sequential process the data. Unary code, Gamma code, and Delta code are all possibilities and there are many other possibilities. Unfortunately, in most retrieval applications, that data set would be large and they generally cannot be loaded into memory at once. In this case, we can see IDs of documents that match the term one will be grouped together. The large numbers will not be frequent. We first decode x1 to obtain the first document ID ID1. Notice that here we're using the term IDs as a key to sort. The reason why we are interested in using integers to represent these IDs is because integers are often easier to handle. The idea of compression in general is to leverage skewed distributions of values, and we generally have to use variable length encoding instead of the fixed length encoding as we use by default in program languages like C++. So how can we leverage the skewed distributions of values to compress these values? In general, we would use fewer bits to encode those frequently awards at the cost of using longer bits to encode those rare values. In this lecture we will continue the discussion of system implementation. So when we take the gap, I'm going to take the difference between adjacent the document IDs. So how do I uncompressed invert index when we just talked about this first, you decode those encode integers and we just I think discussed how we decode unary coding and gamma coding. And then once you collect those counts, you can sort those counts based on terms so that you build a local partial inverted index, and these are called rounds. Now we mentioned earlier that because of postings are very large, it's desirable to compress them. On the left, you see some documents. And that means we are going to compute the difference between 5 and 2 to the two, and that's one, and so we now have again one at the end. Now this was possible because we only need to have sequential access to those documents IDs. So this creates some skewed distribution that would allow us to compress these values. And this is basically precise X - 1 to the floor of the log of X. So we first encounter all the terms in the first document. And we're going to do that for all the documents. Therefore we can use fewer bits for the small but highly frequent integers at the cost of using more bits for large integers. So this is where you can see the advantage of converting document IDs into integers, and that allows us to do this kind of compression and we just repeat until we decode all the documents every time we use the document ID in the previous position to help you recover the document ID in the next position. We can save on average, even though sometimes when we see a large number we have to use a lot of bits. They are also easy to compress. So in our case, let's think about how we can compress the TF (term frequency). We can sort them and then this time we're going to sort based on term IDs. For inverted index compression, people have found that gamma coding seems to work well. Eventually we will get a single inverted index with their entries are sorted based on term IDs. Now, if you think about what kind of values are most frequent there, you probably will be able to guess that small numbers tend to occur far more frequently than large numbers. They would be appropriate for a certain distribution, but none of them is perfect for all distributions. And we're going to write this into the disk as a temporary file, and that would allow us to use the memory to process the next batch of documents. What about the document IDs that we also saw in postings? They are not distributed in the skewed way right. Binary coding is really equal length encoding, and that's our property for randomly distributed values. For example, the binary code is a commonly used code in programming languages where we use basically fixed length encoding. If you will likely see some large numbers, imagine if you occasionally see a number like 1000. There are variable lengths, so you'll have to rely on some mechanism. But three is still larger than two to the one, so the difference is 1, and then one is encoded here at the end. But the remaining part will be using uniform code to actually code the difference between the X and this two to the log of X. This is easy to understand, if the difference is too large then we would have a higher floor of log of X. Now you can easily see zero with signal, the end of encoding. And so this is the unary code 110. In this case, for unary you can see it's very easy to see the boundary. The unary coding is a variable length encoding method. This will be followed by a uniform code or binary code, and this is basically the same uniform code and binary code are the same, and we're going to use this code to code the remaining part of the value of X. So the magnitude of this value is much lower than the original X. So for example, three would be encoded as two ones, followed by zero, whereas five will be encoded as 4 ones, followed by zero etc
 when we consider query terms. For example, documents normalization. Basically to exploit the inverted index to accumulate the scores for documents matching or query term. Some general techniques include the caching. For example document lengths. So we'll compute the weighted contribution of matching this query term in this document. That's the key to enable faster response to a user's query. The raw TF. These are adjustment factors of document and query so they are at the level of a document and query. And h is the aggregation function of all the matched query terms. The function g gives us the weight of a match query term t(i) in document d. Of course, for the query we have the computed at query time, but for document, for example, document lengths can be precomputed. This general form covers actually most state of the art retrieval functions. And then finally, this adjustment function would then consider the document level or query level factors to further adjust the score. And we explore the Zipf's law to avoid attaching many documents that don't match any query term. First these query level and document level factors can be precomputed in the indexing time. A rare term would match fewer documents and then the score contribution would be higher because the idea of value will be higher. But conceptually we have these score accumulatorseventually allocated. This would give us all the documents that match this query term. So to summarize this you can see we first processed the query term information. We can also keep only the most promising accumulators because a user generally doesn't want to examine so many documents. d2 score is increased because it matched both the information and security. The count of a term in the document. So keeping them in the memory would help and these are general techniques for improving efficiency. We only need to process the document that matched at least one query term. And then we'll maintain a score accumulator for each document d to compute h. 3 is the occurrences of information in this document. We see this scoring function of document d and query q is defined as first a function of f(a). And these as I just said, are the scoring factors at the level of the whole document and query. Now this simplification would help showing the algorithm clearly it's very easy to extend the computation to include other weights, like the transformation of TF or document length normalization or IDF weighting. Now inside this edge function there are functions that would compute the weights of the contribution of a matched query term t(i). Now of course, the vector space model is a special case of this, but we can imagine many other retrieval functions of the same form. So this general form would cover many state of the art retrieval functions. You can also see how we can incorporate the IDF weighting so they can easily be incorporated when we process each query term. Finally, we process d5 and 3. And this h function would then aggregate all these weights, so it will for example, take a sum of all the matched query terms. Note that we don't have to touch any document that didn't match any query term. We allocate another score cumulative for d3 and add 1 to it. Then for each entry dj and fj are particular match of the term in this particular document dj. Since our scoring function assumes that the score is just a sum of these raw counts, we just need to add 3 to the score accumulator to account for the increase of score due to matching this term information in document d1. So each pair is document ID and the frequency of the term in the document. MeTA is the tool kit that we will use for the programming assignment and this is a new tool kit that has a combination of both text retrieval algorithms and text mining algorithms. Imagine we have all these score accumulators to store the scores for these documents. Also their security occurred in three documents. So these score accumulators obviously will be initialized at 0. And then we're going to update the score accumulator for this document. So these basic techniques have great potential for further scaling up using distributed file system, parallel processing and caching. We only need to return high quality subset of documents that likely are ranked on the top. And then finding the d4 gets a 5 because the term information occurred five times in this document. That's adjustment function that would consider two factors that are shown here at the end f sub d of (d) and f sub q of (q). And so topic of all those models are implemented there. When we fetch the inverted index, we can fetch the document frequency and then we can compute the IDF. The second one is a newer textbook that has a nice discussion of implementing and evaluating search engines. Right, so those are all heuristics for further improving the accuracy here. So first we iterate over all the query terms. So there are some tricks to further improve the efficiency. The first one is a classic textbook about the efficiency of Inverted index and compression techniques and how to in general build efficient search engine in terms of the space, overhead and speed. Would you process a common term first or would you process a rare term first? The answer is we should process the rare term first. It processed all the contributions of matching information in these four documents. So to summarize, all the discussion about the system implementation, here are the major takeaway points. And this aggregate functioning would then combine all these. That's d4 and 1 so we would update the score for d4 and again we add 1 to d4. So the form of this function is as follows. And the basic idea is to preprocess the data as much as we can, and we want to do compression when appropriate so that we can save disk space and can speed up IO and processing of inverted index. Another technique is to do parallel processing and that's needed for really processing such a large data set like the web data set and to scale up to the web scale, we need a special to have special techniques to do parallel processing and to distribute the storage of files on multiple machines. And this would allow us to add this to a accumulator that would incrementally compute the function h. Inverted index is the primary data structure for supporting a search engine. And I show some entries of the inverted index on the right side information occurred four documents and their frequencies. That's d2 and 4 and then we added 4 to the score accumulator of d2. We're going to compute the function g
 And d3 is judged as non relevant. And R sub B is system B's approximation of relevant documents. This is to simulate users queries.This lecture is about the evaluation of text retrieval systems. There we mentioned that text retrieval is empirically defined problem. These are basically. The binary judgments of documents with respect to a query. One is effectiveness or accuracy. So evaluation must rely on users. And here I listed the three major aspects. Here I show R sub A as results from system A. Task of computing approximation of the relevant document set R sub A is system A's approximation here. And we might need to define multiple measures. But how to evaluate a search engines quality or accuracy is something unique to text retrieval, and we're going to talk a lot about this. The third aspect is usability. Basically, the question is how useful is a system for real user tasks. So, for example d1 is judged as being relevant to Q1, D2 is judged as being relevant as well. But in this case, the user doesn't have to read many and the user would see most of the relevant documents. Thus they don't have to accurately reflect the exact utility to users. The main idea that people have proposed for using a test set to evaluate text retrieval algorithm is called the Cranfield evaluation methodology. We compared it with database retrieval. The measures actually only to be correlated with the utility to actual users. This has been very important for comparing different algorithms and for improving search engine system in general. How accurate the other search results. If the user is not interested in getting all the relevant document. We also need the documents that's called a document collection and on the right side you see we need relevance judgments. Because. Now in this course we are going to talk mostly about effectiveness and accuracy measures because the efficiency and usability dimensions are not really unique to search engines and so. So which one is better and how do we quantify this? Obviously this question highly depends on the users task and it depends on users as well. The second reason is basically what I just said, but there is also another reason which is to assess the actual utility of text retrieval system. We can also have a sample set of queries or topics. And there are some differences, and there are some documents that are returned by both systems. So this is the main topic of this lecture. These are judgments of which documents should be returned for which queries. In the previous lectures we have talked about a number of text retrieval methods, different kinds of ranking functions. Ideally they have to be made by users who formulated the queries, 'cause those are the people that know exactly what documents would be useful, and then finally we have to have measures to quantify how well systems result matches the ideal ranked list that would be constructed based on users relevance judgments. The second is efficiency. And typically this has to be done by using user studies and using the real search engine. On the one hand, one can also imagine the user might need to have as many relevant documents as possible. So as I said, we need the queries that are shown here. So in this case, matches must reflect the utility to the actual users in a real application. Then we'll have to have relevance judgments. And for example, in natural language processing or in other fields where the problem is empirically defined, we typically would need to use such a methodology. In this case, we're measuring systems capability of ranking relevant documents on top of non random ones
 Cranfield evaluation methodology. Compute the. To quantitatively compared to retrieval systems. Are the retrieval results all relevant. We can have a test collection that consists of queries, documents and relevance judgments. That's the number of retrieval relevant documents.This lecture is about the basic measures for evaluation of text retrieval systems. A document can be also relevant or non relevant depending on whether the user thinks this is useful document. Depending on a parameter beta. B for documents that are not retrieved but relevant, etc. There are fundamental measures in text retrieval and many other tasks. To summarize, we talked about precision. These two are the two basic measures in text retrieval evaluation. We talked about F measure as a way to combine precision and recall. Suppose we have a total of 10 relevant documents in the collection for this query. We also talked about the tradeoff between precision and recall and. This measures the completeness of coverage of relevant documents in your retrieval. So precision and recall are the basic measures and we need to use them to further evaluate search engine. And this shows that system A is better by precision. OK, so now let's define these two measures more precisely, and these measures are to evaluate a set of retrieval documents. But in system B. In this case is a sum of precision and recall. Result, so we just assume that there are 10 relevant documents in the collection. Now the relevance judgments shown on the right. So intuitively it looks like system A is more accurate and this intuition can be captured by a measure called precision where we simply compute: to what extent, all the retrieval results are relevant if you have 100% precision, that would mean all the retrieval documents are relevant. We often are interested in the precision at 10 documents for web search. We can have A to represent the number of documents that are retrieved and relevant. This is a popular measure that's often used little combine precision and recall, and formula looks very simple. We can have perfect recall easily. And in any case, this is just a combination of precision and recall. But such results are clearly not very useful for users, even though the average using this formula would be relatively high. That's why although these two measures are defined for a set of retrieval documents, they are actually very useful for evaluating a ranked list. The relevant retrieved documents A to the total number of retrieval documents, so this is just A divided by the sum of A&amp;C. In reality, however, high record tends to be associated with low precision. But we also talked about System B might be preferred by some other users who like to retrieve as many relevant documents as possible. And, it's a harmonic mean of precision and recall is defined on this slide. So this means F1 encodes different. So that means we are considering that approximation of the set of relevant documents. Now it's easy to see that if you have a larger precision or larger recall than F measure would be high. The tradeoff between precision and recall is captured in the interesting way in F1. We also talked about the recall. Did not include all the ten, obviously and we have only seen three relevant documents there, but we can imagine there are other relevant documents in judge before this query. We can then run two systems on these datasets to quantitatively evaluate their performance. OK, So what would be an ideal result? You can easily see in the ideal case we have precision and recall, or to be 1. Now we can see by recall system B is better and these two measures turn out to be the very basic measures for evaluating search engines, and they are very important because they also widely used in. But what's interesting is that. Which addresses the question have all the relevant documents being retrieved. This means we look at the how many documents among the top 10 results are actually relevant. And here's one measure that's often used called F measure. But we're going to use different denominators. So in that case will have to compare the number of relevant documents, then retrieve and there is another measure called recall. For this example, shows actually a very important methodology here. We just said that there tends to be a tradeoff between precision and recall, so naturally it would be interesting to combine them. Beta is a parameter that's often set to one. It can control the emphasis on precision or recall when we set Beta to one, we end up having a special case of F measure, often called F1. Now this is a very meaningful measure because it tells us how many relevant documents the User can expect to see on the first page of search results where they typically show 10 results
 Precision and Recall.3 and 1. The total number of relevant documents. In general. But as a convenience, we often assume that the precision is 0. Now, so that also means it depends on whether the user cares about the high recall or low recall, but high Precision. But as we talked about Ranking before, we framed the tax retrieval problem as a ranking problem. The user doesn't care about high recall. In contrast, the average precision is a much better measure. So that means which one is better actually depends on users, and more precisely user's task. In that case, you emphasize high recall, so you want to see as many relevant documents as possible. These are the provisions that had the different points corresponding to retrieving the first relevant document. And this is basically computed in this way, and it's called Average Precision. Some users might like system A, some users might like system B.This lecture is about how we can evaluate a ranked list. So it combines precision and recall, and furthermore you can see this measure is sensitive to a small change of a position of a relevant document. Note that it actually combines recall and precision, but first we have precision numbers here. So the precision levels are marked as 0. What's the difference here? The difference is just that in the low level of recall in this region, system B is better, there's higher precision, but in high recall reading system A is better. We have got one document and that's relevant. The actual precision would be higher, but we may make this assumption in order to have an easy way to compute another measure called average precision that we will discuss later. So we divided by 10 and which is a total number of relevant documents in the collection. In this lecture we will continue the discussion of evaluation. In contrast, if we look at the precision at the 10 documents. And this will basically compute the area underneath the curve. Therefore, you might favor system A. What about the recall ? Note that we assume that there are 10 relevant documents for this query in the collection, so it's one out of 10. How can we use precision and recall to evaluate a ranked list? Naturally, we have to look at the precision and recall at different cut offs because in the end the approximation of relevant documents set given by a ranked list is determined by where the user stops browsing, right? If we assume the user sequentially browses the list of results, the user would stop at some point and that point will determine the set, and then that's the most important cut off that will have to consider when we compute the precision recall without knowing where exactly the user would stop, then we have to consider all the positions where the user could stop. These are the two basic measures for quantitatively measuring the performance of search result. That would be the ideal system. So this is very good because it's a very sensitive to the ranking of every relevant document. And the Y axis also has different amounts that for precision. So we this is one precision and this is another with different recall. Note that here we are not dividing this sum by 4, which is a number of retrieved relevant documents. But if we use this measure to compare systems, it wouldn't be good because it wouldn't be sensitive to where these four relevant documents are ranked. So, we also need to evaluate the quality of a ranked list. And on the Y axis we show the precision. So that's pretty useful, right? So it's a meaningful measure from a user's perspective. And this is the standard method used for evaluating a ranked list. Why? Because for the same level of recall, it's the same level of recall here, and you can see the precision point by system is better than system B, so there's no question. So this is the different levels of recall. So of course this is a pessimistic assumption. An we can do then look at this number and that's the precision at a different recall level, etc. In the previous lecture we talked about. And then look after the precision. Because there are eight documents, and four of them are relevant and the recall is a four out of 10. Let's say if I move this relevant document up a little bit, now it would increase this average precision, whereas if I move any relevant document down, let's say I move this random document down, then it would decrease the average precision. But the precision is lower because we've now got a random number. And then finally we take the average. The precision is zero at all the other levels of Recall that are beyond the search results. Indeed, you can imagine what does the curve look like for ideal search system. So here again, it's the precision recall curve, right. What if the user stops at third position? Well, this is interesting because in this case we have not got any additional relevant document. But this is usually OK for the purpose of comparing to text retrieval methods, and this is for the relative comparison, so it's OK if the actual measure or actually actual number deviates a little bit from the true number as soon as the deviation is not biased toward any particular retrieval method and we are OK, we can still accurately tell which method works better, and this is an important point to keep in mind. It can tell small differences between 2 ranked lists and that's what we want. Basically, we're going to take a look at every different recall point. Which one is better? What do you think? In this case, clearly system B is better because the user is unlikely examining a lot of results. In particular, we're going to look at how we can evaluate the ranked list of results. Now we missed the mini random documents. It can tell the difference of different difference in ranked lists in subtle ways
 It combines precision and recall and it's sensitive to the rank of every relevant the document. So to summarize, we show the Precision recall curve, can characterize the overall accuracy of a ranked list. The average precision is low. But we generally experiment with many different queries and this is to avoid the variance across queries. So again, the answer depends on your users, your user's tasks, and their preferences. Some users will examine more than others and average precision is the standard measure for comparing two ranking methods. Also the average would be then dominated by where those relevant documents are ranked in the lower portion of the ranked list, but from a user's perspective we care more about the highly ranked documents. And we emphasized that the actual utility over ranking list that depends on how many top rankings results are user would actually examine. As a special case of the mean average precision, we can also think about the case where there is precisely one relevant document. But as I just mentioned in another lecture, is this good? Recall that we talked about the different ways of combining precision and recall.So average precision is computed for just one query. So what are those values? Those are basically large values that indicate the lowly ranked results. And then we can also take a average of all these average position or reciprocal rank over a set of topics and that would give us something called Mean Reciprocal Rank. So in this sense, R is also meaningful measure and the reciprocal rank will take the reciprocal of R instead of using R directly. Now again, here you can see this R actually is meaningful here, and this R is basically indicating how much effort an user would have to make in order to find that relevant document. You can have a high average precision, where as gMAP tends to be affected more by lower values and those are the queries that don't have good performance. But here it's the same. In that case, there is precisely one relevant document, or in another application like a question answering. So if that document is ranked on the very top, R is 1 and then it's one for reciprocal rank. And we conclude that the arithmetic mean is not as good as the F measure. So by taking this transformation by using reciprocal rank, here we emphasize more on the difference on the top and think about the difference between one and two. Depending on the queries you use, you might make different conclusions, so it's better to use more queries. That means the relevant item is ranked very low down on the list and the sum, the audacity. And we called this kind of average gMAP map. So if you think about improving the search engine for those difficult queries than gMAP would be preferred. So what does a large menu value here mean? It means the query is relatively easy. It's the same larger R with correspond to a small one overall, but the difference would only show when show up when you have many topics. And this happens often. So this would give us what is called Mean Average Precision or MAP. In this case we take arithmetic mean of all the average precisions over set of queries or topics. And that's called the known item search. To think about the multiple ways to solve the same problem and then compare them and think carefully about differences and which one makes more sense. Maybe there's only one answer there, so if you rank the answers, then your goal is ranked at one particular answer on top right? So in this case, you can easily verify, the average precision will basically boil down two reciprocal rank, that is one over R, where R is the rank position of that single relevant document. So one natural question here is, why not simply using R? Now imagine if you are to design a measure to measure performance of the ranking system when there is only one relevant item. Now you test it on multiple topics. So again think about average of mean reciprocal rank versus average of just R
 Overall these 10 documents. Rated 3 here. That means a document is judged as being relevant or non relevant. And further from non relevant documents, those are not useful. So this gain intuitively matches the utility of a document from a user's perspective. And this ideal DCG will be used as a normalizer. Those are very useful documents from your moderately relevant documents. And we only know there is one highly relevant document one marginally relevant document, two non relevant documents. It's useful for measuring ranked list based on multiple level relevance judgments. Cumulative gain. What is the ideal DCG? This is the DCG of ideal ranking. So when we take a such a sum than a lower rank document will not contribute contribute that much as a highly ranked document. Precision and recall doesn't work because they rely on binary judgments. So this gain basically can measure how much gain of relevant information the user can obtain by looking at each document. And then we can compute the DCG for this ideal ranked list. And that means in total we have 9 documents rated 3.This lecture is about the how to evaluate the text retrieval system when we have multiple levels of judgments. For example, very relevant document here, as opposed to here. Right? And we marked the reading levels or relevance levels for these documents as shown here, 32113, etc. And finally it will do normalization to ensure comparability across queries. The difference however is when we have multiple topics. For a topic like this one we have 9 highly relevant documents. Two is the rank position of this document. Now how do we evaluate search engine system using these judgments? Obviously the map doesn't work. Then the highest DCG that any system could achieve for such a topic will not be very high. By looking at the moderately relevant or marginal relevant documents, the user would get two points. But earlier we also talk about the relevance as a matter of degree, so we often can distinguishing very high relative documents. And The scale of the judgments can be multiple. And that means the contribution of the gain from different positions has to be weighted by their position, and this is the idea of discounting, basically. And the main idea of this measure I just to summarize is to measure the total utility of the top K documents. We're going to look at the how to evaluate the text retrieval system when we have multiple level of judgments. So you always choose a cut off and then you measure the total utility and it would discount the contribution from a lower ranked document. And we call these "Gain". So more in the more general way, this is basically a measure that can be applied to any rank the task with multiple level of judgments. In this lecture we will continue the discussion of evaluation. Looking at the non random document, the user would only gain one point. So this would be given by this formula that you see here, and so this ideal DCG would then be used as the normalizer DCG. Those are again easy queries, so by doing the normalization we can avoid the avoid the problem making all the queries contribute equally to the average. So you can imagine now normalization essentially is to compare the actual DCG with the best DCG you can possibly get for this topic. The DCG can get really high, but imagine in another case, There are only two very relevant documents in total, in the whole collection. because if we don't do normalization, different topics will have different scales of DCG. So cumulative gain gives us some idea about the how much total gain the user would have if the user examines all these documents. For example here I show example of three levels, 3 for relevant sorry 3 for very relevant, two for marginally relevant and one for non relevant. If the user looks at the more documents, then the cumulative gain is more. And the reason why we call it "Gain" is because the measure that we're introducing is called nDCG(Normalized Discounted Cumulative Gain). So imagine you can have ratings for these pages
2 and 0. These are text retrieval systems. And the other unjudged documents are usually just assumed to be non relevant. You intuitively. So the goal is to figure out the relevant documents we want to make judgments on relevant documents, because those are the most useful documents from users perspective. Commercial search engines. And we hope these methods can help us nominate likely relevant documents. First, the documents and queries must be representative. They must represent the real queries, and real documents that the users handle, and we also have to use many queries and many documents in order to avoid biased conclusions. Another way to evaluate IR or Text retrieval is user studies, and we haven't covered that. For the matching of relevant documents, with the queries we also need to ensure that there exists a lot of relevant documents for each query. The "K" can vary from systems right? But the point is to ask them to suggest the most likely relevant documents. So to summarize, the whole part of text retrieval evaluation, it's extremely important because the problem is empirically defined problem. The main methodology is Cranfield evaluation methodology and this is still the main paradigm used in all kinds of empirical evaluation tasks, not just the search engine evaluation. But we assume that because of random fluctuations depending on the queries. For measures, It's also challenging because we want the measures that would accurately reflect the perceived utility of users. System A is better. And then the users would. Proceeding at the 10 documents is easier to interpret from the user's perspective, so that's also often useful. And then design measures to measure that. In this case probability is 1. So the idea of statistical significance test is basically to assess the variance across these different queries. Now, of course there are many documents that are duplicated bcause many systems might have retrieved the same relevamnt documents. In terms of relevance judgments, the challenge is to ensure complete judgments of all the documents for all the queries, yet minimizing human effort because we have to use human labor to label these documents, it's very labor intensive and as a result it's impossible to actually label all the documents for all the queries, especially considering a giant dataset like the web. So there will be some duplicate documents and there are also unique documents that are only returned by one system and so the idea of having diverse set of ranking methods is to ensure the pool is broad and can include as many possible relevant documents as possible. Actually System A is better and this is another case. MAP and nDCG are the two main measures that should definitely know about and they are appropriate for comparing ranking algorithms. In this case, a new system might be penalized because it might have nominated some relevant documents that have not been judged, so those documents might be assumed to be non relevant. Of course the users don't see and which result is from which method the users would judge those results or click on those documents in search engine application. So, In order to create the test collection, we have to create a set of queries, a set of documents and a set of relevance judgments. So the fact that the average is larger doesn't tell us anything and we can reliably conclude that, and this can be quantitatively measured by a P value and that basically, means the probability that this result is infected from random fluctuation. It's called A-B Test, and it's a strategy that's often used by the modern search engines. In this lecture we will continue the discussion of evaluation we'll cover some practical issues that you have to solve in actual evaluation of text retrieval systems. But how can we quantitatively answer this question? And, This is why we need to do statistical significance test.This lecture is about some practical issues that you would have to address in evaluation of text retrieval systems. We have to consider carefully what the users care about. These are three mini books about evaluation and they all excellent in covering a broad review of information retrieval, evaluation, and discovered some of the things that we discussed. If a query has only one, let's say rather than the document in the collection, then you know it's not very informative to compare different methods using such a query, because there's not that much room for us to see difference, so ideally there should be more relevant documents in the collection, but yet the queries also should represent the real queries that we care about. We actually have 4 cases where system B is better, but 3 cases System A is better. If there is a A big variance that means the results could fluctuate a lot according to different queries. We assume that the mean is 0 here. Yet if you look at the these exact average precisions for different queries.4 and then this is twice as much as 0. And the question here is how sure can you be that observed difference that doesn't simply result from the particular queries you choose, so here are some sample results of average position for system A and system B in two different experiments. What's not covered is Some other evaluation strategy like A-B Test where the system would mix 2, the results of two methods randomly and then will show the mixed results to users. We would first choose a diverse set of ranking methods. But the if the pool is not very large, this actually has to be reconsidered and we might use other strategies to deal with them, and there are indeed other methods to handle such cases, and such a strategy is generally ok for comparing systems that contributed to the pool. So this is actually a major challenge. In this case, because these numbers seem to be consistently better for system B . However, this is problematic for evaluating a new system that may have not contributed to the pool. But yet if we look at the only average, System B is better. And then we simply combine all these top K sets to form a pool of documents for human assessors, to judge.4 for system B and again here, It's also 0. through the clicked documents, if the user tends to click on one, the results from one method, then it's just that the method may may be better, so this is the leverage the real users of a search engine to do evaluation. One is a statistical significance test and this also is the reason why we have to use a lot of queries. That's unfair. So to illustrate this, let's think about the such a distribution and this is called the null distribution
 Often queries and documents.This lecture is about a probabilistic retrieval model. Similarly. probability of relevance. The likelihood of queries given each document. Called a language modeling approaches to retrieval. And that is a user formula to query based on an imaginary relevant document. Being probabilistic models, we define the ranking function based on the probability that this document is relevant to this query. So intuitively this probability. Inquiry likelihood Our assumption is that this probability of relevance can be approximated by the probability of query given a document and relevance. Model called the Language model and this kind of models are proposed to model text. And we also assume that the query and the documents are observations from random variables. In particular, we're going to discuss the query likelihood retrieval model. This document how likely the user would oppose this query. Imagine the relevance of status values or relevance judgments. Observing this query if a particular document is infected, imaginary relevant document in the user's mind. Compute the ratio. Here you can see we compute all these query likelihood probabilities. And in this case we have a ranking function that's basically based on the probability of a query given the document, and this probability should be interpreted as the probability that a user who likes document D would pose queria Q. So we ask this question and we quantify the probability and this probability is conditional probability of. Note that in the vector space model we assume they are vectors, but here we are assumed. Because it's a form is actually similar to objectives space model. In other words, D2 is assumed to be relevant to Q1. Maybe data collected from a different user. Which is one of the most effective models in probabilistic models. Just captures the following probability and that is if a user likes document D. is A query that the user tightly and the D1 is a document the user has seen and one means the user thinks the one is relevant to Q1. For this query we can simply rank them based on these probabilities, and so that's the basic idea of probabilistic retrieval model, and you can see it makes a lot of sense. So obviously this approach won't work if we apply it to unseen queries or unseen documents. We assume that the user likes the document because we have seen that the user clicked on this document. But D2 is also relevant, ET cetera. The classical problem is model has led to the BM 25 retrieval function which we discussed in the vector space model. So to summarize, the general idea of modern relevance in the probabilistic model is to assume that we introduce a binary random variable R here, and then let's a scoring function be defined based on this conditional probability. In general we only collected data from the documents that we have shown to the users. We assume they are the data observed from random variables. Nevertheless, this shows the basic idea of problems control model and it makes sense intuitively. On the other hand, D3 is non relevant. And this part. Ann In the next lecture working through, give an introduction to language models that we can see how we can model text with the probabilistic model in general. And try to give your estimate of the probability. We also talked about the approximate in this by using the query likelihood. In other words, we actually make the following assumption. Not to understand this idea, let's take a look at the general idea or the basic idea of probabilistic retrieval models. Suppose we're trying to compute this probability for D1D2 and D3 for Q1. And this part shows that we're interested in how likely the user would actually enter this query. There will be a lot of unseen documents. So more specifically, we would be very interested in the following conditional probability as issuing this here if the user. In contrast here we see it's relevant. How likely would the user enter query Q in order to retrieve documenting? So assume that the user likes D. Because we have a relevance value here and then we asked the question about the how likely will see this particular query from this user. There are even more unseen queries because you cannot predict what queries would be typing by users. These probabilities would give us some sense about which document might be more relevant or more useful to a user who typing this query. To use this new conditional probability to help us score, then this knew conditional probability will have to somehow be able to estimate this conditional probability without relying on this big table. Once we have these values, we can then rank these documents based on these values. So divine is actually non relevant. Now the question of course, is how do we compute this conditional probability? At this, in general has to do with how to compute the probability of text, because Q is attached. We have made an interesting assumption here. Basically, we can do assume that whether the user types in this query has something to do with whether user likes the document. So let's look at how this model work for our example, and basically what we are going to do in this case is to ask the following question which of these documents is most likely the imaginary relevant document in the user's mind when the user formulates this query. And here we can see more data
 Distribution. Language model, which is basically probability distribution over text. This model gives. These common words. We assume that we generate the text by generating each word independently. Text mining etc would have. The first one clearly suggests a topic of text mining, because the high probability words are all related to this topic. The Mortal Council be regarded as a probabilistic mechanism for generating text. Whereas these common words will not have a high probability. Therefore it's clearly context dependent. Help. Normalized frequency. This is the background language model. But in general, high probability words with likely show up more often. But this model now only needs N parameters to characterize. Assuming that the text that has been generated by sampling words from the distribution. Relatively higher probabilities, in contrast in this distribution, will text has relatively small probability. So we call this document language model and we call this collection languagemodel. But it's a non zero probability if we assume none of the words have non zero probability. We can use this text to estimate the language model and the model might look like this. We can ask the question which model which word distribution has been used to generate this text. That doesn't mean we cannot generate this paper from text mining. This has to do with how do you model text data with probabilistic models so it's related to how we model query based on a document. Also, given that a user is interested in Sports News, how likely would the user used baseball in a query? Now this is clearly related to the query likelihood that we discussed in the previous matching. These words have. It looks similar because these words occur everywhere. So here I show some general English background text. Some text will have higher probabilities than others. What is the language model? It's just a probability distribution over word sequences. All these words are related to computer. So this assumption is not necessarily true, but we make this assumption to simplify the model. So the background language model precisely tells us this. And finally, we discussed possible uses of language model. Here I show two unigram language models with some probabilities and these are high probability words that are shown on top. You can see the probability is given to these sentences or sequences of words can vary a lot depending on the model. So this means again based on different attacks today or we can have a different model and model captures the topic. Why 'cause that would take away probability mass for these ovbserved words? And that obviously wouldn't maximize the probability of this particular observer text data. It gives the best model based on this particular data. For example, we can simply take the ratio of these two probabilities or normalize the topic language model by the probability of the world in the background language model.This lecture is about the statistical language model. Information tells us what words are common in general. This means it can be used to represent the topic of the text. There can be expected to occur on the top, but soon we will see text mining Association clustering. Another example, given that we observe baseball 3 times and game once in a news article, how likely is it about sports? This obviously is related to text categorization, an information retrieval. Right, this is the background model. We talked about the simplest language model called unigram them model which is also just a word distribution. The former document. That's visualized hands here as a stochastic system that can generate the sequences of words. text has a probability of 10 out of 100 because I've seen text 10 times an there are in total 100 words, so we simply not simply normalize these counts. And This is why it's also often called a generating model. So we can imagine what gender the text that looks like a text mining. So this means the probability of a sequence of words. Similarly from the second topic, we can imagine we can generate the folder nutrition paper. As I said, text can be assumed to be assembled drawn from this world distribution. That's in fact the word justified, and your intuition is consistent with mathematical derivation, and this is called a maximum likelihood estimator. But we also see computer itself and software will have relatively high probabilities. So we're going to cover this a little more later in discussing the query likelihood retrieval model model. We can then ask the question, how likely will observe a particular text from each of these three models? I suppose we sample words. But it could have generated any other sequences. The first is textbook on statistical natural language processing. One use is simply to use it to represent the topics. So why is such a model useful? So many because it can quantify the uncertainties in natural language. In this estimator we assume that the parameter settings are those that would give our observed data the maximum probability. if we have not oveserve a word there will be no incentive to assign a non zero probability using this approach. And so, although here we might also see these words, for example computer. That means if we change these probabilities, then the probability of observing the particular text data would be somewhat smaller. In the extreme case, you might imagine we might be able to generate the attacks paper text mining paper that would be accepted by a major conference. As we always do so in this case, this language model gives us the conditional probability of seeing the world in the context of computer and these common words will naturally have high probabilities. Suppose we now have available of particular document. But we can imagine the probability here is much smaller than the probability here, and we will see many other words here that would be more common in General English. Given that we see John and Fields. We have one probability for each word, and all these probabilities must sum to one. Or consequences of this is, of course we're going to assign zero probabilities to unseen words. But imagine in the context of discussing applied math, maybe the eigenvalues positive would have a higher probability. We're going to talk about what is the language model and then we're going to talk about the simplest language model called a unigram language model, which also happens to be the most useful model for text retrieval. So in this case, let's look at the text mining paper. Because they occur frequently in the context of computer, but not frequently in the whole collection. As you can see, even though we have not asked the model to generate the sequence, it actually allows us to compute the probability for all the sequences. So for example there are many possibilities, right? So this in this sense we can view our data as basically a sample observable from such a generating model. Basically we just need to look at the count of a word in the document and then divided by the total number of words in the document or document length. So now the model has precisely in parameters wherein is vocabulary size. But if we just use this model, we cannot just say all these words are semantically related to computer. In fact, there was a small probability you might be able to actually generate the actual text mining paper that would actually be meaningful, although the probability would be very very small. So if we use this background model, we would know that these words are common words in general, so it's not surprising to observe them in the context of computer
 In the query likelihood retrieval model.This lecture is about the query likelihood probabilistic retrieval model. Here different ways to estimate these document language model would lead to a different ranking function for query likelihood. And then use this as a query word. Although the model has to be estimated based on the document. We assume that this document is generated using this unigram language model. And then given a query and I get data mining algorithms. So that we can compute the probability of each query word given by this document. So according to our assumption that a user would pick a word from a document to generate a query, then the probability of obtaining a word like update. In this lecture we continue the discussion of probabilistic retrieval model. And we make the assumption that each word is generated independently, so as a result, the probability of the query is just a product of the probability of each query word. That works as follows, where we assume that the query is generated by sampling words from the document. The ranking function is. Now, how do we compute the probability of each query word Well based on the assumption that a word is picked from the document. Well, since we are computing the query likelihood. And different estimation methods here would lead to different ranking functions. And we call it the document language model here. As we derive this ranking function. And of course, a word might occur multiple times in the query. Our idea is to model how likely a user who likes a document would pose a particular query. Probability that we observe this query given that the user is thinking of this document. So the user can again generate the query using a similar process, namely pick a word. So for example, the probability of presidential given the document. Then we can assume the user would use this document as a basis to post a query to try to retrieve this document. So as you see, the assumption that we've made here is each query word is independently sampled and also each word is basically obtained from the document. Maybe a different question, but this assumption has allowed us to formulate characterize this conditional probability. Doesn't necessarily assign zero probability for update. So that's the basic idea of this query, likelihood retrieval function. So for example, a user might pick a word like presidential from this document. And then this part is log of the probability of the word given by the document language model. And similarly we can get probabilities for the other two documents. However. And another word, campaign. In this sum, We have it all over the query words N query words. And this is assumed to be product of probabilities of all individual words. Therefore, we have converted the retrieval problem, include the problem of estimating this document language model. Now the difference is that this time we can also pick a word like update even though update does not occur in the document to potentially generate the query word like update so that a query with update want to have zero probabilities. Now this model. Then the probability here is just the probability of this particular query, which is a sequence of words. None of these documents has mentioned update. So I showed model here. The second step would just compute the likelihood of this query and by making independent assumptions we could then have this probability as a product of the probability of each query word. That is unigram language model instead of a fixed document. So in order to fix this, we have to assume that the user could have drawn a word not necessarily from the document, so let's improve the model and the improvement here is to say that instead of drawing a word from the document, let's imagine that the user would actually draw a word from a document model. For example, I've shown two possible language models here is made based on two documents. Now we actually often score the document for this query by using log of the query likelihood as shown on the second line. The value is log of the probability of this word given by the document. Essentially we are only considering the words in the query because if a word is not in the query, the count would be 0. In fact that we consume this model does not assign zero probability for any word. In particular, we're going to talk about the query likelihood retrieval function. For example, presidential. Instead of this particular document. So more generally then, this ranking function would look like the following right here we assume that the query has N words. This is based on the independence assumption
 To formulate a query. maximum likelihood estimate P sub seen here. These are the query words that are matching the document. This is the ranking function based on the query likelihood. These seen words have a different probability. match the query terms in document but inside the sum it's different. There is a log of probability of word given by the document or document language model. So more formally, we will be estimating the probability of a word given a document as follows. All query words that are not matched in the document. All the words match the query words matched in the document and with this kind of terms. So the main task now is to estimate this document language model. And here the coefficient Alpha. And this is the sum over all the query words that are matching the document. So to make this transformation and to improve the maximum likelihood estimate by assigning non zero probabilities to words that are not observed in the data. And this makes sense, because here we are considering all query words and then we subtract the query words that are matched in the document. Here we assume that the independence of generating each query word. One sum is over all the query words that are matching the document. Then the probability would be a discounted. It will tell us which unseen words will have likely higher probability. If the word is seen in the document.This lecture is about the smoothing of language models. It's not matched in the document. That would give us the query words that not matched in the document. And the estimated probability would look like this. And the key question here is what probability should be assigned to those unseen words. And just like in the vector space model, we take a sum of terms that are in the intersection of query vector and the document vector. Obviously this sum has extra terms that are. One idea here that's very useful for retrieval is let the probability of unseen word be proportional to its probability given by reference language model. Over the query words matched in the document in the one sum here. In the case of retrieval, a natural choice would be to take the collection language model as the reference language model. Due to this this term, but they don't occur in the document. This is over all the query words that are not matched in the document. As we said before, different methods for estimating this model would lead to different retrieval functions. unseen words in this abstract. In our smoothing method, we assume that the words that are not observed in the document we have somewhat different form of probability namely it's for this form. Is to control the amount of probability mass that we assign to unseen words. But imagine a user who is interested in the topic of this subject. Over all the query words, so we take a sum over all the query words. In this case, these words have this probability because of our assumption about the smoothing. In this lecture, we're going to continue talking about probabilistic retrieval model. We're going to assume that the probability of this word would be proportional to the probability of the word in the whole collection. The user might actually choose a word that's not in the abstract to use as query. In particular, we're going to talk about the smoothing of language model in the query likelihood retrieval method. We would have to take away some probability mass from the words that are observed in the document. So they occur in the query. That means in this sum, all the words have a non-zero probability in the document, sorry it's the non 0 count of the word in the document. Obviously, all these probabilities must sum to one, so Alpha sub D is constrained in some way. So how do we estimate this language model? The obvious choice would be the maximum likelihood estimate that we have seen before, and that is we're going to normalize the word frequencies in the document. If the word is not seen in the document, we're going to let its probability be proportional to the probability of the word in the collection. And they also have to of course have a non 0 count in the query, so these are the words that are matched. Right, this is another frequent account that has a different probability. For the document, the author might have written other words. But that here. If you think about this factor, then a smoother language model would be more accurate representation of the actual topic. So how do we improve this, well? In order to assign a non zero probability to words that have not been observed in the document. Note that for words that have not occurred in the document here they all have zero probability, so we this know this is just like the model that we assumed earlier in the lecture, where we assume that the user would sample word from the document. As a difference of two other sums, basically the first sum is actually sum, over all the query words. They all occurred in the document
 Based on the collection language model. Normalization in particular alpha sub d might be related to document. That looks like a TF IDF weighting and documents length normalization. But intuitively it achieves a similar fact. With this assumption, we've shown that we can derive a general ranking formula for query likelihood that has the effect of TF IDF weighting and document length normalization. used logarithm of query likelihood for scoring. And it's also necessary in general to improve the accuracy of estimating the model represents the topic of this document. But note that the alpha sub d also occurs here. because it looks like a TF IDF weighting. So to summarize, we talked about the need for smoothing document language model, otherwise would give zero probability for unseen words in the document. I just say that this term is related to IDF weighting. That is, the probability of an unseen word is assumed to be proportional to its probability in the collection. for a long document. And so naturally, this pot would correspond to the vector element from the document vector, and here indeed we can see it actually encodes a weight that has similar factor to TF IDF weighting. Inside the sum, We also see that each matched query term would contribute. The general idea of smoothing in retrieval is to use the collection language Model to Give us some clue about the which unseen word should have a higher probability. weight And this weight actually is very interesting. We actually have a smaller weight and this is precisely what IDF weighting is doing. This collection probability, but it turns out that this term here is actually related to the document length. How exactly we should use the reference language model based on the collection to adjust the probability of the maximum likelihood estimate? And this is the topic of the next lecture. The probability in the collection is larger, then the weight is actually smaller, and this means a popular term. But as we will see later when we consider some specific smoothing methods, it turns out that they do penalize long documents just like in TF IDF weighting and document length normalization formulas in the vector space model. After we make the assumption about the smoothing the language model. We also see that through some rewriting, the scoring of such a ranking function is primarily based on sum of weights on match query terms, just like it in the vector space model. It's also interesting to note that the last term here is actually independent of the document, since our goal is to rank the documents for the same query, we can ignore this term for ranking. In particular, we see that the main part of the formula is a sum over the matched query terms. But the actual ranking function is given us automatically by the probability rules and the assumptions that we have made and unlike inthe vector space model where we have to heuristically. And that's not good for scoring a query with such an unseen world. In particular, we're going to show that from this formula we can see smoothing with the collection language model will give us something like a TF IDF weighting and length normalization. Interestingly, we also have something related to the length normalization. But what's nice with probabilistic modeling is that we are automatically given a logarithm function here. length, so it encodes how much probability mass we want to give to unseen words. Only that we now have a different form of TF and IDF. Imagine if we drop this logarithm, we would still have TF and IDF weighting. After we smooth the document language model, we send you to have non zero probabilities for all the words. The second benefit is that it also allows us to compute the query likelihood more efficiently. So a nice property of probabilistic modeling is that by following some assumptions and probability rules we will get a formula automatically. So this term appears to penalize long documenting in that the alpha sub d would tend to be longer than larger than. Is actually achieving the effect of IDF? Why? Because this is the popularity of the term in the collection
 To the observed text. Alpha sub D does depend on the document.This lecture is about the specific smoothing methods for language models used in probabilistic retrieval model. It is often called the Dirichlet Prior or Bayesian smoothing. Document the language model by using maximum likelihood estimate that gives us word counts normalized by the total number of words in the text. Is to maximize the probability of the observed text as a result, if a word like network. Maximum likelihood estimate and the collection language model as before as in the JM smoothing method. For the term in the document. For example, if we compute the smooth probability for text. In this lecture we will continue the discussion of language models for information retrieval, particularly the query likelihood retrieval method, and we're going to talk about the specifics smoothing methods used for such a retrieval function. Of the word in the document. This picture shows how we estimate. The word network which used to have zero probability now is getting a non zero probability. So in this approach in what we do is we do a linear interpolation between the maximum likelihood estimate here and the collection language model and this is controlled by the smoothing parameter Lambda. And inside the sum is the count of the term in the query and some weight. So this is a slide from a previous lecture where we show that with the query likelihood ranking and smoothing with the collection language model we end up having a retrieval function that looks like the following. As a result, we have more counts. Something related to the probability of the word in the collection, and we multiply that by the parameter mu. As pseudo count to this data pretend we actually augment the data by including some pseudo data defined by the collection language model. The larger lambda is, the more smoothing we will have. And so the probability of the text would be of this form naturally. So by mixing them together we achieve the goal of assigning non zero probabilities to a word network. So this is the retrieval function based on these assumptions that we have discussed, you can see it's a sum over all the matched The query terms here. Is not observed in the text, it's going to get zero probability as shown here. So this is a smoothing parameter. Because. This length depends on the document, whereas in the linear interpolation the JM smoothing method. So the idea of smoothing then is to rely on collection language model where this word is not going to have a zero probability to help us decide what non zero probability should be assigned to such a word. So we can note that Network has a non zero probability here. So the total counts would be the sum of these pseudo counts and the actual count. A second one is similar, but it has a dynamic coefficient for linear interpolation. We can rewrite the smoothing method in this form. Now the maximum likelihood estimate gives us 10 / 100 and that's going to be here. We pretend. If you think about this and you can easily see now the Alpha sub D in this smoothing method is basically Lambda. And so here you can also see what's alpha sub d here. In particular, we're going to need to know how to estimate the probability of a word Exactly and. Here, but this part is non zero, and that's basically how this method works. Have you notice that this part is basically alpha sub D? So we can see this case. And this is also called a Jelinek-Mercer smoothing. The first is the simple linear interpolation with a fixed coefficient. Every word has got this many pseudo count
 And does TF-IDF weighting and document length normalization. And. But intuitively, is still implements TF IDF weighting and document length normalization. vector element. And then the form dictated by probabilistic model. sorry, general ranking function for smoothing with collection language model. And this is the probability of unseen word, or in other words, lambda is basically the alpha here. So overall this shows by using probabilistic model we follow very different strategy than the vector space model. So, this is a general smoothing. Yet in the end, we naturally implemented TF IDF weighting and document length normalization. Let's look at the Dirichlet Prior Smoothing. Although it's TF IDF weighting and stopping the length normalization , for example, it's unclear whether we have sub-linear transformation. And we again see the query term frequency here. We also see IDF weighting which is given by this. And in this case, you can easily see this is precisely a vector space model, because this part is the sum over all the matched query terms. And with our document length normalization here. It is a query words are generated independently that allows us to decompose the probability of the whole query. The first assumption is that the relevance can be modeled by the query likelihood and the second assumption we've made. And you can interpret this as the element of a document vector. So we either use JM smoothing or the Dirichlet smoothing. Dirichlet Prior, this is to add pseudocounts to every word and is doing adaptive interpolation in that the coefficient would be larger for shorter documents. But Fortunately we can see here. Now, this also concludes our discussion of the query likelihood problems model. And we can also see it has TF here and then IDF. If we're going to draw a word from the collection language model and we want to draw as many as the number of words in the document. The actual count of the word in the document with the expected count given by this product. The larger the count is, the higher the weight will be. Into a product of probabilities of all the words in the query. So Lambda is parameter and let's look and this is a TF. As a result, what we get is the following function here, and this is again a sum over all the matched query words. Again, the form of the function is dictated by the probabilistic reasoning and assumptions that we have made. Now we see very clearly this TF weighting here. And in this case note that there is a specific form and we can see whether this form actually makes sense. And intuitively it makes a lot of sense. So that's also why, for example, BM 25 remains very competitive and still open challenge how to use probabilistic model to derive a better model than BM25. So this ratio basically is comparing the actual count here. In this case, of course, we still need to set the smoothing parameter, but there are also methods that can be used to estimate these parameters. Experiment results also show that these retrieval functions also are very effective, and they are comparable to BM 25 or pivoted length normalization. So what do you think Is the denominator here? This is the length of document, total number of words multiplied by the probability of the word given by the collection. Where we have made explicit assumptions, and we know precisely why we have a logarithm here and why we have these probabilities here. We're comparing the actual count with the expected count of the word if we sample Mu words according to the collection of the probability. That is tight to users. Each of these functions also has precisely one smoothing parameter. And then the third assumption that we have made is if a word is not seen in the document that we're going to let its probability with proportional to its probability in the collection of the smoothing with the collection language model, and finally we've made one of these two assumptions about the smoothing. So this actually can be interpreted as expected count of the word. Fortunately, the function has a nice property in that implements TF IDF weighting and documents length normalization And, these functions also work very well, so in that sense these functions are less heuristic compared with a vector space model. We basically have made four assumptions that I listed here. If the word is in fact the following, the distribution in the collection this. And note that n is the length of the query. So that just means if we score this function we have to take a sum over all the query words and then do some adjustment of the score based on the document. In this case, the smoothing parameter is Mu and that's different from lambda that we saw before, but the format looks very similar. So to summarize this part we've talked about the two smoothing methods. This is the element of the query vector what do you think is the element of the document vector? It's this, so that's our document. Yet in the end we end up with some retrieval functions that look very similar to a vector space model with some advantages in having assumptions clearly stated
 Assumed documents to learn and to improve the query. Relevance feedback, where the user makes explicit judgments. And also use the document collection to try to improve ranking. The feedback is based on relevance judgments made by the users. And we simply assume that the top ranked documents to be relevant.This lecture is about the feedback in text retrieval. The text is probably relevant, is interesting to user. The user can actually make judgments. We talk about pseudo feedback where we simply assume top-ranked documents to be relevant. But of course, pseudo relavance feedback is completely unreliable. And then the query would be sent to a retrieval engine or search engine. And we can again use the information to update the query. These results will be shown to the user. They look like relevant documents, so it's possible to learn some related terms to the query from this set. We can assume these display. Now this is called a relevant judgment or relevance feedback because we've got some feedback information from the user based on the judgments. You can imagine these top ranked documents are actually similar to relevant documents, even if they are not relevant. Typically it would involve updating the query so the system can now rank the results more accurately for the user. There was another form of feedback called pseudo relevance feedback or blind feedback, also called automatic feedback. We can see the user with typing the query. So this is all relevance feedback. So this is the effective for improving the search result. It takes some user effort, but the judgment the information is reliable. So we can learn from such information and this is called implicit feedback. So if we make a contrast between these two, what we can find these that would learn some related terms to the word computer? As we have seen before and these related words can then be added to the original query to expand the query and this would help us bring documents that don't necessarily match computer but match other words like a program and software. This is a diagram that shows the retrieval process. What exactly is interesting to the user. After the user has seen these results. And the engine will return results. The text that's actually seen by the user instead of the actual document of this entry
 and effective. And these pluses are relevant documents. And these are relevant documents. And, this average basically is the centroid vector of relevant documents. The task of a Text Retrieval system is to learn from examples to improve retrieval accuracy. This newer query vector will then reflect the move of this original query vector toward this relevant centroid vector and away from the non relevant centroid vector. Two relevant the documents here. They are not as reliable as the relevance feedback. I only showed the vector representation of these documents. Or the document that are viewed by users. Now this method can be used for both relevance feedback and pseudo relevance feedback. And this is especially true for pseudo relevance feedback. The general method in the vector space model for feedback. These documents would be basically the top ranked of the documents. We might adjust weights of all terms or assign weights to new terms. So when we use the query vector and use a similarity function to find the most similar documents, we are basically drawing a circle here and then. Those are the documents that are assumed to be relevant or judgement be relevant. And these are all the documents.This lecture is about the feedback in the vector space model. And as a result, in general the query will have more terms, so we often call this query expansion. Those are documents known in non-relevant, and they can also be the document that escaped by users. Here you can see this is original query vector. Is to modify our query vector and we want to place the query vector in a better position to make the accurate. We have zero weights of course and these are negative documents. We have 5 documents here. Look at the centroid vectors of the positive documents. So our goal here is trying to move this query vector to some position to improve the retrieval accuracy. An the original query terms are still very important. That means we have to keep relatively high weight on original query terms. When we take the average of these vectors, then we are computing the centroid of these vectors and similarly this is the average non relevant document vectors. In this lecture we continue talking about feedback in text retrieval. For example is relevant, etc. Right? They are displayed in red and these are the term vectors and I have just assumed some TF IDF weights. So this is precisely what we want from feedback. We illustrate this idea by using a 2 dimensional display of all the documents in the collection and also the query vector. In practice, we often truncate this vector, and only retain the terms with highest weights. And we're going to use this new query vector. That's basically this. Those terms of typing by the user and the user has decided that those terms are most important. This is basically the same. Of course you can consider the centroid of negative documents, and we want to move away from the negative documents. So that means we can move the query vector towards the centroid of all the relevant locking vectors. But note that in some cases in difficult queries where most top ranking results are negative, negative feedback factor is very useful. From of course the negative centroid here. And that is the original query has only four terms that are. In the case of pseudo feedback up, the parameter beta should be set to a smaller value because the relevant examples are assumed to be relevant. A lot of terms. And the root Rocchio. And we also have this negative weight here, controlled by Gamma here and this weight that has come. I saw we have a parameter alpha controlling the original period weight, that's one, and then we have beta to control the influence of the positive centroid weight that's 1. And then these minuses are negative documents like this. So it's essentially of non relevant documents and we have these three parameters here of our alpha beta and gamma they're controlling the amount of movement. And we have. non-zero But after we do query expansion, you can imagine it would have many terms that would have nonzero weights. And this is basically the idea of Rocchio you. So in order to prevent us from overfitting or drifting the topic with preventing topic drifting due to bias toward the feedback examples, we generally would have to keep a pretty high weight on the original terms. And we do exactly the same for other terms. So this is the main idea of Rocchio feedback and after we have done this we will get a new query vector which can be used to store documents. I just mentioned that we often truncated adapter and see the only a small number of words that have highest weights in the centroid vector. That means ideally you want to place the query vector somewhere here, or you want to move the query vector closer to this point
 Document set. Whichever model is basically to generalize. Using a linear interpolation. But we assume that the query is generated by assembling words from a language model in the query likelihood method. And will the basis is the feedback documents. There are many other methods, for example, the relevance model is a very effective model for estimating query model. One is the query model denoted by this distribution. This is the user generated mixable. The feedback model that we want to estimate. As a result, this topic model must assign high probabilities, so the high probability words according to the topic model would be those that are common here, but rare in the background. Of course this approach is based on generated model. But not having high probability here. We are going to use maximum likelihood estimator to adjust this model to estimate the parameters. And this model is actually going to make the query likelihood retrieval function much closer to vector space model. We talked about the three major feedback scenarios, relevance feedback, sooner feedback, and in principle feedback. These are the click the documents by users or relevant documents judged by users or simply top ranked blocking that we assume to be relevant. So we derive the query likelihood ranking function by making various assumptions. Approach is simply to assume these documents are generated from this language model as we did before. Given by the probabilistic model here to characterize what the user is looking for versus the count of query words there. It's kind of a natural to sample words that form feedback documents as a result, then researchers proposed a way to generalize query like hold function and it's called a callback labeler divergance retrieval model. And then this model can be combined with the original query model. Then there's no burden on explaining those common words in the feedback documents by the topic model. And this is of course a pseudo feedback. One is important research paper that's about relevance based language models, and it's a very effective way of computing query model. Now imagine how we can compute the centroid for these documents by using language Model 1. And in this case, then, feedback can be achieved through simple query model estimation or updating. The here and in this case it's not appropriate to use the background language model to achieve this goal, because this model would assign high probabilities to these common words. We consider two distributions. But this basically means this is exactly like a vector space model 'cause we computer vector for the document the computer, another vector for the query and then we compute the distance only that these vectors are of special forms their probability distributions. So here are some examples of the feedback model learned from a Web document collection, and we do sudo feedback. Then we estimate the query name model and we compute the KL diversions, as is often denoted by AD here. In general, feedback is to learn from examples. And if they are common, they would have to have high probabilities according to maximum likelihood estimator. And here the sum is over all the words that are in the document and also with the non zero probability for the query model. In this lecture we will continue the discussion of feedback in text retrieval. As a result, it would then assign higher probabilities to other words that are common here. Sorry, mostly positive documents, although we could also consider both kinds of documents. And then we got the results and we can find some feedback documents. Into a language model.This lecture is about the feedback in the language modeling approach. And note that we also have another parameter Lambda here, but we assume that the Lambda denotes the noise in the feedback document. When we don't use the background more often, remember Lambda can use the probability of using the background model to generate the text. If we don't rely much on background model, we still have to use this topic model to account for the common words, whereas if we set Lambda to a very high value, we will use the background model very often to explain these words. And if we use maximum likelihood estimator, note that if all the words here must be generated from this model, then. So this can be added to the original query to achieve feedback. And these are the words learned using this approach. So this is how we can use the care divergent model to them the feedback the picture shows that we first estimate the document language model. So these are relevant to this topic and they if combined with the original query. And that's also why this care divergent model can be regarded as a generalization of query, like whole because we can cover query like Rd as a special case. Yet, this form of the language model can be regarded as a generalization of query like hold in the sense that it can cover query likelihood as a special case. And that means. This query words still show up as high probabilities in each case naturally becausw they occur frequently in the top ranked documents, but we also see beverage, alcohol, bomb, terrorists, etc. Perform feedback because a lot of times the feedback information is additional information about the query. Issues that this model really works and picks up some related words to the query. But rather, we're going to ask this whole model mixture model to explain the data becauses there has got some help from the background model. So as a result of the topic model, here is very discriminant if it contains all the relevant words without common words. These words would work as follows. We talked about how to use Rock You to do feedback in vector space model and how to use query model is missing for feedback in language model and we briefly talked about the mixture model and the basic idea. So basically it's the difference. In this model. So to summarize, in this lecture we have talked about the feedback in language model approach. These documents that should not belong to this topic model, right? So now what we can do is to assume that, well, those words are generated from background language model, so they were generated those words like the. Example. So it's kind of again a generalization of some overall the match query words. Here smoothly with collection language model, of course an. So in both cases you can see the highest probability words include very relevant words to the query, so airport security, for example. The first one is a book that has a systematic review and discussion of language models for information retrieval and signal. Note that in order to reduce its probability in this model. This is very similar to Rock Hill which updates the query vector. This is the probability of award given by the feedback model in both cases. It's very similar to the likelihood function local. Topic model here that we would like to estimate and we're going to then generate award here. Let's say we are observing the positive documents. Can help us match more accurately documents and also they can help us bring up a documents that only imagine the some of these other words. Now you can also easy to see we can recover the query like hold retrieval function by simply setting this query model to the relative frequency of award in the query. But you can imagine. Then We only have these probabilities as parameters, just like in the simplest unigram language model. In the case of learning the associations with words, words that are related to the water computer. If we ignore the original query and this is generally not desirable, right? So this unless you are absolutely sure you have seen a lot of relevant documents and the query terms are not important. But anyway for. The coin shows up as head and we're going to use the background language model and we can do that simple word from that model with probability of 1 minus them. This model is forced to assign high probabilities to award like that because it occurs so frequently here
 So spam detection we have to prevent those spam pages from being ranked high. And in general we can classify two scenarios. For example, using links that we can leverage to improve scoring. There are many additional heuristics. So to summarize, web search is one of the most important applications of text retrieval, and there are some new challenges, particularly scalability, efficiency, quality information. And these are techniques that can allow us to improve search results by leveraging extra information. And there are also techniques to achieve robust ranking, and we're going to use a lot of signals to rank pages so that it's not easy to spam the search engine with a particular trick. And the third line of techniques is link analysis. A crawler is an essential component of web search applications. Now the algorithm that we talked about, such as a vector space model are general algorithms. So for example, robustness. And there can be applied to any search applications, so that's the advantage. Web pages are linked with each other, so obviously the link information is something that we can also leverage. Firstly, there is a scalability challenge. The second problem is that this low quality information and there are often spams. And then there is another scenario that's incremental updating of the crawled data or incremental crawling. So what are the major crawling strategies in general? Breadth first is most common becauses naturally balance balance is the server load. So here are some general challenges. On the other hand, there are also some interesting opportunities that we can leverage to improve search results. And in general, in web search we're going to use multiple features for ranking, not just a link analysis, but also exploit in all kinds of clues, like the layout of web pages or anchor text that describes a link to another page. In this lecture we're going to talk about one of the most important applications of text retrieval: web search engines. And this is typically going to start with a query and then you can use the query to get some results from a major search engine and then you can start with those results and gradually crawl more.This lecture is about the web search. And finally, you may be interested in the discover hidden urls. On the other hand, they also don't take advantage of special characteristics of pages or documents in the specific applications such as web search. If it is, then it means it's a high utility page and then that's it's more important to ensure such a page to be fresh. One is parallel indexing and searching and this is to address the issue of scalability. And there are some variations of the crawling task, and one interesting variation is called focused crawling. So here's a picture showing the basic search engine technology is. The third component that is a retriever that would use inverted index to answer users query by talking to the users browser and then the search results will be given to the user and then the browser would show those results and to allow the user to interact with the web so we are gonna talk about each of these components. In this case you need to optimize the resource. Also, parallel crawling is very natural because this task is very easy to parallelize. And then the second component is indexer that would take these pages and create a inverted index. Basically this is the web on the left, and then user on the right side, and we're going to help this user to get the access for the web information and the first component is the crawler. The third challenge is dynamics of the web. There are also new opportunities, particularly rich link information and layout, etc. But in general, you can imagine you can learn from the past experience. So because of these challenges and opportunities, there are new techniques that have been developed for web search or due to the need for web search. In this case, we're going to crawl just some pages about a particular topic
 And the counts. Each key is a word. Our scalability and efficiency. Procedure Map and procedure Reduce. And these counts represent the occurrences of this word in different lines. Or search. From this map function.This lecture is about the web indexing. And the output that we want to generate is the number of occurrences of each word, so it's the word account. This is a general software framework for supporting parallel computation. We know this kind of counting would be useful to, for example, assess the popularity of a word in a large collection, and this is useful for achieving effect of IDF weiging. Of course the new key is usually different from the old key that's given to the map as input. In general, we can use the standard information retrieval techniques for creating the index, and that is what we talked about in the previous lecture. Basically we have got the counts, but what's missing is the document IDs and the specific frequency counts of words in those documents, so we can modify this slightly to actually build inverted index in parallel. Note that the post the GFS and MapReduce frameworks are very general so they can also support many other applications. And then finally output the key and the total count, and that's precisely what we want as the output of this whole program. So in this case we can assume the input to map function is a pair of a key, which denotes the document ID and the value denoting the stream for that document. And once the GFS client obtained the. On top of the Google File System and Google also proposed map reduce as a general framework for parallel programming. It simply groups all the counts of this word in this document together and it would then generate the set of key value pairs. But there are new challenges that we have to solve for web scale indexing and the two main challenges. Now of course, each Reduce function will handle a different key, so we will send these output values to multiple, reduce functions, each handling unique key. Hadoop is the most known open source implementation of map reduce, now used in many applications. So it's all the words in that document, and so the map function will do something very similar to what we have seen in the word count example. And so some of the low level details hidden in the framework, including the specific network communications or load balancing or where the tasks are executed. And the value is the count of this orld in this document, plus the document ID. key value pairs from different Map functions, so the function is very simple and the programmer specifies this function as a way to process each part of the data. How the input is files containing words. Right, so this is the general framework of MapReduce. As a result, the programmer can make a minimum effort to create a application that can be run on large cluster in parallel. So more specifically, we can assume the input to each map function is key value pair that represents the line number and the stream on that line. And with such a framework the input data can be partitioned into multiple parts, each is processed in parallel, first by map, and then in the process after we reach the reduce stage, then multiple reduce functions can also further process the different keys and their associated values in parallel, so it achieves (some) It achieves the purpose of parallel processing of large data set. So to summarize, web scale indexing requires some new techniques that go beyond the standard traditional indexing techniques, mainly will have to store the index on multiple machines, and this is usually done by using file system like a Google File System, a distributed file system. So now you can see the reduce function has already got input that looks like a inverted index entry, right? So it's just the word and all the documents that contain the word and the frequencies of the word in those documents. A reduce function would then process the input. Of course, the second line will be handled by a different map function, which will produce a similar output. So the counts of Java in those documents. And this will be collected together and this will be also fed into the reduce function. The map function would then just count the words in this line, and in this case of course there are only four words. Which is a key and a set of values to produce another set of key values as the output. Now the programmer only needs to write the Map function and the Reduce function. In the case of map, it's going to count the occurrences of word using associative array and will output the old accounts together with the document ID here. So this framework is hiding a lot of low level features from the program. All these details are hidden from the programmer. (And) Another feature is that the data transfers directly between application and chunk servers, so it's efficient in this sense. And these key value pairs are the output of the map function and all the outputs of all the map functions would be then collected. And then there will be for the sorted based on the key, and the result is that all the values that are associated with the same key would be then grouped together. And a program with the specify these two functions to program on top of map reduce and you can see basically they are doing what I just described. So once we see all these pairs then we can sort them based on the key which is the word. Now you can easily see why we need to add document ID here. So this is a very simple MapReduce function, yet it would allow us to construct the inverted index at very large scale and the data can be processed by different machines. Similarly, another document D2 can be processed in the same way, so in the end that again there was a sorting mechanism that would group them together and then we will have just a key like "Java" associated with all the documents that match this key or the documents where "Java" occurred. Now, what exactly is in the value will depend on the data, and it's actually a fairly general framework to allow you to just partition the data into different parts, and each part can be then processed in parallel. An similarly we do that for other words like "Hadoop", "Hello" etc. So basically the reduce function is going to do very minimum work. These chunks are replicated to ensure reliability, so this is something that the programmer doesn't have to worry about
 The query would match this anchor text. Or query likelihood to score different parts of documents or to provide additional features based on content matching, but link information is also very useful so they provide additional scoring. A case where we would like to consider indirect links and Pagerank does that. And that's the main topic of this lecture. Linear algebra technique. Now this description text is called anchor text. And particularly focusing on how to do link analysis and use the results to improve search. Of the document to improve scoring and finally information quality varies a lot, so that means we have to consider many factors to improve the ranking algorithm. One is to provide extra text for matching and the other is to provide some additional scores for the web pages to characterize how likely a pages have, how likely a pages authority. We want to see how we can improve ranking of pages. One is it would consider indirect citations. So for such queries we might benefit from using link information.This lecture is about link analysis for web search. Secondly, documents have additional information and on the web web pages are well format. So this kind of query is often called navigational queries. The main topic of this lecture is to look at the ranking algorithms for web search. One line is to exploit links to improve scoring. The other idea is it's going to smooth the citations or assume that basically every page is having a non zero pseudo citation count. People have also proposed algorithms to exploit the large scale implicit feedback information in the form of click throughs, and that's of course in the category of feedback techniques. There are a lot of other clues such as the layout, title or link information. And machine learning is often used there. In general in web search the ranking algorithms are based on machine learning algorithms to combine all kinds of features. So of course, then if a page is cited often, then we can assume this page to be more useful, in general. Here and then, this actually provides evidence for matching the page that's being pointed to. For example, people might search for a web page or entry page and this is different from the traditional library search where people are primarily interested in collecting literature information. That means many other pages are pointing to this page and this shows that this page is quite useful. This means the link information can help in two ways. This would give us a more robust way to rank the pages, making it harder for any spammer to just manipulate the one signal to improve the ranking of a page. Many of them are based on the standard visual models such as BM25 that we talked about. So people then of course proposed ideas to leverage these in this link information. So if you match the anchor text that describes a link to a page, actually that provides good evidence for the relevance of the page being pointed to, so anchor text is very useful. If you look at the bottom part of this picture you can also see there are some patterns of links, and these links might indicate the utility of a document. Right, so the description here actually is very similar to what the user will type in the query box when they are looking for such a page, and that's why it's very useful for ranking pages. This is very similar to one paper citing another paper. So we can call the first case authority page and the second case half page. First, on the web we tend to have very different information needs. Now Google's Pagerank, which was the main technique that they used in early days, is a good example and that is an algorithm to capture page popularity, basically to score authority. Again, so this has provided the opportunity to use extra context information
 And this probability is precisely what Pagerank computes. And these are linear equations. And multiply by the vector again. Now, intuitively, this would basically capture the in link account. And here I also showed the equations in the matrix form.5 for d3 and d4. It's the vector p here. So this is a nice way to capture both indirect and direct links. So this is assumed random surfing model. One extension is to do topic specific Pagerank. Noted that Pagerank doesn't really use the query information so. So the Pagerank score of the document is the average probability that the surfer visits a particular page. Randomly initialized vector p and then we repeatedly just updated this p by multiplying the matrix here by this p vector. The sum of the scores to this document d1. Right? That includes the graph, the actual links, and we have this smoothing transition matrix uniform transition matrix representing random jumping and we can combine them together with a linear interpolation to form another matrix. This also allows us to see why Pagerank essentially assume the smoothing of the transition matrix. The Pagerank scores of different pages and in this iterative approach or power approach, we simply start with. Inside the sum is a product of two probabilities. Basically, that's to say alpha would be 1. So we can make Pagerank query specific, however, so for example in the topic specific Pagerank we can simply assume when the surfer is bored the surfer is not going to randomly jump to any page on the web. And one possible solution is simply to use a page specific damping factor and that could easily fix this. The other is the transition probability from the d,i to d,j. By doing this, then we can bias and Pagerank to topic like sports and then if you know the current query is about sports and then you can use this specialized Pagerank score to rank documents that would be better than if you use a generic Pagerank score. And this is just the metrics with values indicating how likely I ran. Now we can imagine if we want to compute the average probabilities, the average probabilities probably would satisfy this equation without considering the time index. So Interestingly, this updating formula can be also interpreted as propagating scores over the graph. Sometimes you may also normalize the equation and that would give you a somewhat different form of the equation, but then the ranking of pages will not change. One is just the matrix multiplication and repeatedly multiply the vector by this matrix, but the other is to just think of it as propagating the scores repeatedly on the web. So the probability is the probability of being at d,i at time t multiplied by the probability of going from that page to the target page. Now, how can we compute the probability of a surfer visiting a page? If you look at the surf model then basically we can compute the probability of reaching a page as follows. And because they are of the same form, we can imagine there's a different matrix that's a combination of this M and that uniform matrix, where every element is 1 / N, and in this sense Pagerank uses this idea of smoothing and ensuring that there's no zero entry in such a transition matrix. So mathematically, how can we compute this probability in order to see that we need to take a look at how this probability is computed. So this is how we updated the vector. And this is why. And this can be solved by using iterative algorithm. Instead it's going to jump to only those pages that are relevant to a query. Pagerank is also a general algorithm that can be used in many other applications for network analysis, particularly for example social networks. Then With some probability that random surfer will follow the links. So the first part captures the probability that the random surfer would reach this page by following a link, and you can see the random surfer chooses this strategy with probability 1 minus alpha as we assume and so there is a factor of 1 minus alpha here, but the main part is really sum over all the possible pages that the surfer could have been at time t. Mainly because we have lost some probability mass when we assume there's some probability that the surfer will try to follow links, but then there's no link to follow. So the Pagerank algorithm would just initialize the p vector first and then just compute the updating of this p vector by using this matrix multiplication. Note that it also considers the indirect in links. And this is Basically the updating formula for this particular pages Pagerank score so you can also see you if you want to compute the value of this updated score for d1 you basically multiply this rule. So you can see this now if we assume alpha is . Right, so the form is exactly the same and is. But it also assumes that the random surfer might get bored sometimes, so the random surfer will decide to ignore the actual links and simply randomly jump to any page on the web. Now if you rewrite this matrix model multiplication in terms of just, individual equations, you will see this. So the equations here on above are basically taken from the previous slide, so you see the relation between the. In general, the element in this matrix M sub i, j is the probability of going from d,i to d,j and obviously for each row the values should sum to 1 because the surfer would have to go to precisely one of these other pages, right? So this is the transition matrix. Or follow a link and then visit the next page. Basically, we're going to combine the scores of the pages that possibly would lead to reaching this page, so we'll look at all the pages that are pointing to this page and then combine their scores and propagate the score. You can imagine if you compute the Pagerank scores for social network where a link might indicate friendship relation, you'll get some meaningful scores for people. Equals a metrics of the transverse of the matrix here. Now of course this is time dependent calculation of probabilities. That's of course oversimplification of the complicated web, but let's say there are four documents here, d1 d2 d3 and d4, and let's assume that a random surfer or random walker can be on any of these pages. For example, row one would indicate the probability of going to any other 4 pages from d1, and here we see there are only two non zero entries, each is 1 over 2. Then we can ask the question how likely on average the surfer would actually reach a particular page like d1 or d2 or d3 really, that's the average probability of visiting a particular page. The only difference is that now the transition probability is a uniform transition probability of 1 / N and this part of captures the probability of reaching this page through random jumping. In that case, if the page does not have any out link then the probability of these pages would not sum to one basically the probability of reaching the next page from this page will not sum to one. At any page would assume random surfer would choose the next page to visit, so this is a small graph here
 Intuitions are pages that are wider. The text representation of a page and we also talk about page rank and hits on as two major link analysis algorithms. So the algorithm is exactly the similar page rank, but here because the adjacency matrix is not normalized. Both can generate scores for web pages that can be used in the ranking function. And these scores can then be used in ranking just like a page rank scores. The difference between this and page rank is that now the matrix is actually a multiplication of the edges in the matrix and its transpose, so this is different from page rank. It can have many applications in graph and network analysis. And this is basically the first equation right? And similarly the second equation can be returned as the authoritie vector is equal to the product of A transpose multiplied by the hub vector and these are just different ways of expressing these equations. Note that page rank and it's also very general algorithms, so they have many applications in analyzing other graphs or networks. It's going to use reinforcement mechanism to kind of help improve the scoring for hubs and authorities, and here. So to summarize, in this lecture we have seen that link information is very useful.So we talked about page rank as a way to. In particular, the anchor text is very useful to increase the. And similarly, we can do a transformation to have equation for just the authority scores. But mathematically, then we will be computing the same problem. We first also construct the matrix, but this time we're going to construct the adjacency matrix and we're not going to normalize the values. I saw what we get here is then the hub vector is equal to the product of the edges of the adjacency matrix and the authority vector. It would assume that good authorities are cited by good hubs. And if we do that and then we'll basically get hits algorithm to compute the hub scores an authority scores for all the pages. So if you do that, you can actually then eliminate the authoritie vector completely and you get the equation of only hub scores, right? The Hub score vector is equal to A * A transpose multiplied by the hub score vector again
 The query, the document and judgment. So we will assume that the given a query document appear Q and D. They can generate the content based scores for matching documents with a query and we also talked about the link based approaches like page rank that can give additional scores to help us improve ranking. And these features can vary from content based features such as a score of the document with respect to the query according to a retrieval function such as BM25, or query likelihood or pivot length normalization PL2 etc. And this information can be based on real judgments by users, or this can also be approximated by just using click through information where we can assume the clicked documents are better than the skiped documents or clicked documents are relevant and skiped documents are non relevant. So the general idea of learning to rank is to use machine learning to combine these features to optimize the weights, different features to generate the optimal ranking function. But by hypothesising that relevance is related to these features in a particular way, we can then combine these features to generate the potentially more powerful ranking function and more robust ranking function.This lecture is about learning to rank. These parameters can control the influence of different features on the final relevance. It can also be linked based score like page rank score. So in general, we would fit such a hypothesize the ranking function to the training data, meaning that we will try to optimize its retrieval accuracy on the training data, we can adjust these parameters to see how we can optimize the performance of the function on the training data in terms of some measures such as MAP or nDCG. So we can hypothesize that the probability of relevance is related to these features through a particular form of the function that has some parameters. So the question that we address in this lecture is how we can combine many features to generate a single ranking function to optimize search results. It can be also application of retrieval models to the anchor text of the page, those are the text descriptions of links that point to this page. We have talked about some retrieval models, like BM25 or query like code. We can define a number of features
 Why? Because It's a non relevant document. So to compute the non relevance from relevance we just. We can predict the relevance well. The beta values are still unknown, but this gives us the probability that this document is relevant if we assume such a model. We might have a topic sensitive Pagerank that would depend on query. And observing a zero here. Is actually the probability of relevance. OK, we would want to maximize this probability, since this is a relevant document. For that pair and then we just use this formula to generate the ranking score and this scoring function can be used to rank documents for a particular query. Of course, this probability depends on the beta values. This is similar to expressing the probability of document. So in this approach, we simply assume that the relevance of document with respect to query is related to a linear combination of all the features. And we assume that these features can be combined in a linear manner. Would be completely specified, so for any new query and new documents, we can simply compute the features. What we do for the second document? Well, we want to compute the probability that the prediction is non relevant. One is BM25 score of the document and query one is the page rank score of the document, which might or might not depend on the query. And beta is a parameter that's a weighting parameter, a larger value would mean the feature would have a higher weight and would contribute more to the scoring function. Otherwise the general page rank doesn't really depend on query and then we have BM25 score on the anchor text of the document. And each feature is controlled by a parameter here. And this is precisely what we want. So what we can do is we use the maximum likelihood estimator to actually estimate the parameters. This expression. Make it as large as possible. Basically it would mean if this combination gives us a high value, then the documents more likely relevant. Now this form is clearly non negative and it still involves the linear combination of features. Here are used Xi to denote the feature, so Xi of Q and D is a feature. Basically, we are going to predict the relevance status of the document that based on the feature values that is given that we observe these feature values here. Can we predict the relevance? Yeah, and of course the prediction will be using this function. The beta is just the parameter values that would maximize this whole likelihood expression. So this is the probability of relevance. The specific form of the function actually also involves a transformation of the probability of relevance. But in this case it's not relevant. And we can have as many features as we would like. These are then the feature values for a particular Doc document query pair. An in this case, the document is D1, and the judgment that says that is relevant. So what's the probability of this document? The relevant if it has these feature values. Do 1 minus the probability of relevance. Also, as large as possible, which is equivalent to, say, make this the part as small as possible. The next task is to see how we to estimate the parameters so that the function can actually be applied without knowing the beta values, it's harder to apply this function, so let's see how we can estimate beta values. Again, this is not necessarily the best hypothesis, but this is a simple way to connect these features with the probability of relevance. Both this and this. But it is sufficient to illustrate the point. So that's the basic idea of learning to rank. And we hypothesize that the probability of relevance is related to features in this way, so we're going to see for what values of beta
There are many more advanced learning algorithms, then the regression based approaches and they generally attempt to direct the optimizer retrieval measure. To summarize this lecture we have talked about the using machine learning to combine multiple features to improve ranking results. Note that the optimization objective function that we have seen on the previous slide is not directly related to retrieval measure By maximizing the prediction of one or zero, we don't necessarily optimize the ranking of those documents. Modern search engines are all used, some kind of machine learning techniques combined with many features to optimize ranking and this is a major feature of these commercial engines such as Google or Bing. Now these learning to rank approaches are actually general, so they can also be applied to many other ranking problems, not just the retrieval problem. like MAP or nDCG. Such data won't available before, so the data can provide a lot of useful knowledge about the relevance and machine learning methods can be applied to leverage this. Secondly, it's also driven by the need for combining many features and this is not only just because there are more features available on the web that can be naturally used to improve scoring, it's also because by combining them we can improve the robustness of ranking, so this is desired for combating spams. But the ranking can be wrong, so we might have got a larger value for D2 and than D1. So, for example, the Rocchio feedback approaches that we talked about earlier was machine learning approach applied to relevance feedback But the most recent use of machine learning has been driven by some changes in the environment of applications of retrieval systems, and first, it's mostly driven by the availability of a lot of training data in the form of click throughs. So that won't be good from retrieval perspective, even though by likelihood function is not bad
 Users and mostly model users based on keyword queries. And the support search and browsing. Look at the user search history and then further model the user completely to understand the users task, environment, task need context or other information. And furthermore the knowledge would be used to help user to improve productivity in finishing a task. That means the current search engines basically provides search support. For example, a decision making task. So search navigation and recommendation or filtering might be combined to form a full fledged information management system. On the document side. Intelligent Information systems will provide intelligent and interactive task support. Search engines will become better and better and this is already happening because search engines can learn from the implicit feedback, more users use it and the quality of the search results for the popular queries that are typed in by many users will likely become better. Sometimes browsing, clicking on links. These vertical search engines can be expected to be more effective than the current general search engines because they could assume that the users are a special group of users that might have a common information need, and then the search engine can be customized to serve such users. For example, consumers might search for opinions about products in order to purchase a product and choose a good product to buy. But that's what the current system does. We can also see we can go beyond bag of words representation to have entity relation representation. So this is pushing for personalization and complete the user modeling and this is a major direction in research. So this calls for large scale semantic analysis, and perhaps this is more feasible for vertical search engines. Because we have a better understanding of the users. We have to find the patterns or converted the text information into your knowledge that can be used in application or actionable knowledge that can be used for decision making. We can imagine the system could provide task support by automatically generating a response to the customer email, or maybe intelligently attach also promotion. In order to further improve the accuracy of search engines, it's important to consider special cases of information need, so one particular trend could be to have more and more specialized and customized search engines and they can be called vertical search engines. But it does not provide a good task support for many other tasks. If you connect the scientists with literature information to provide all kinds of service, including search, browsing or alert of new relevant documents, or mining, analyzing research trends, or provide the task support or decision support, for example. And we can imagine a system that can provide analysis of these emails to find the major complaints of the customers. You might be might be able to provide a support for automatically generating related work section for research paper, and this would be closer to task support, right? So then we can imagine this would be a literature assistant if we connect to online shoppers with blog articles or product reviews, then we can help these people to improve shopping experience so we can provide for example data mining capabilities to analyze the reviews to Compare products, compare sentiment of products and to provide a task support or decision support to help them choose what product to buy. Once the information is found and this step has to do with analysis of information or data mining. Sometimes we've got some information recommended, although most of the cases information recommended is because of advertising. Because of the restriction of the domain, we also have some advantages in handling the documents because we can have better understanding of documents. Now I should also emphasize interactive here because it's important to optimize the combined intelligence of the users and the system so we can get some help from users in some natural way and we don't have to assume the system has to do everything when the human user and the machine can collaborate in an intelligent way in an efficient way, then the combined intelligence will be high and in general we can minimize the user's overall effort in solving problem. Automatically generate some generic response first and tell the customer that he or she can expect the detailed response later, etc. These are different modes of information access, but these modes can be combined. This means we'll recognize peoples names their relations, locations etc and this is already feasible with today's natural language processing technique and Google's recent initiative on the Knowledge Graph. Recommender systems and push and pull are different ways to get access to relevant information, but going beyond access we also need to help people digest information. For example, particular words may not be ambiguous in such a domain, so we can bypass the problem of ambiguity. So in this case it would be beneficial to support the whole workflow of purchasing a product or choosing a product. And it sees the data through a bag of words representation. In this lecture we're going to talk about some possible future trends of web search and intelligent information retrieval systems in general. So it's a very simple approximation of the actual information in the documents. And this is of course very attractive, because that means the search engine will self improve itself as more people are using it. So this is a trend and so basically in this dimension we anticipate in the futur. And it doesn't really understand it that much information in the documents
 Text document and profile description of the user. A search engine for information filtering. We can measure the similarity between profile text, description and document, and then we can use the score threshold for the filtering decision. Then we are talking about the content based filtering or content based recommendation. So in this case, one commonly used strategies is user utility function to evaluate the system. Frst it has to make a filtering decision so it has to be a binary decision maker binary classifier given a text. If it says yes and then the documents will be sent to the user and then the user could give some feedback and the feedback information would have been ,would be used to both adjust to the threshold and to adjust the vector representation so the vector learning is essentially the same as query modification or feedback. And it's called a user interest profile. Three basic problems in content based filtering are the following. And the system Clock collect a lot of information about these users interest and this can then be used to improve the classifier. So here's the basic idea for extending a retrieval system for information filtering. If we look at the second strategy, then it will compare users and in this case will exploit the user similarity and the technique is often called collaborative filtering. And we have to initialize the system based on only very limited text description or very few examples from the user. And another component that we have to add is for is of course to learn from the history and here we can use the traditional feedback techniques to learn to improve scoring. And then there was a utility function to guide the user to make decisions, and I explained the utility function in the moment. And we can accumulate a lot of documents and learn from the entire history and all these modules would have to be optimized to maximize the utility. So let's first look at the content based filtering system. Right, so you can see the document vector could be fed into a scoring module which is already exists in a search engine that implements a vector space model and the profile will be treated as a query essentially and then the profile vector can be matched with the document vector to generate the score. An we mentioned that recommender systems are the main systems to serve users in the push mode where the systems would take initiative to recommend the information to user or push a relevant information to the user. This is probably because of we know that web search engines are by far the most important applications of text retrieval and they are the most useful tools to help people convert big raw text data into a small set of relevant documents. So this utility function has to be designed based on the specific application. Now what's the criteria for evaluating such a system? How do we know this filtering system actually performs well? Now, in this case we cannot use the ranking evaluation measures like a map because we can afford waiting for a lot of documents and then rank the documents to make a decision for the user. And we know Rocchio can be used for scoring improvement. And then the accepted document will be those that have passed the threshold according to the classifier. And the third component is a learning module which he had to be able to learn from limited relevance judgments. So how can we build such a system? And there are many different approaches. So here's what the system might look like if we just generalize the vector space model for filtering problems. We have talked about the problem of search and ranking problem, different methods for ranking implementation of a&lt;br&gt; search engine and how to evaluate the search engine etc. And you have some knowledge about this user's interest and then the system would make a delivery decision whether this item is interesting to the user and then if he's interested in, then the system would recommend the article to the user. Note that in this case typical users information need is stable, so the system would have a lot of opportunities to observe the users, if the user has taken a recommended item has viewed that, and this is the signal to indicate that the recommended item may be relevant if the user discarded it, it's not relevant, and so such feedback can be a long term feedback and can last for a long time. Inside the system there will be a binary classifier that would have some knowledge above the users interest. So a recommender system is sometimes called a filtering system and it's because recommending useful items to people is like discarding or filtering out the useless articles. In the case of recommending text objects. Another reason why we spend so many lectures on search engines is because many techniques used in search engines are actually also very useful for recommender systems, which is the topic of this lecture. or like most of those users.This lecture is about the recommender systems. And so overall, the two systems are actually well connected and there are many techniques that are shared by them. First we can reuse a lot of retrieval techniques to do scoring, right, so we know how to score documents against queries, etc. We do retrieval, and then we kind of find the scores of documents and then we apply a threshold to see whether document is passing the threshold or not, and if it's passing the threshold we are going to say it's relevant, and we're going to deliver it to the user. And these strategies can be combined if we follow the first strategy that look at item similarity. There is also typically a learning module that will learn from users feedback overtime. And then this score would be fed into a threshold module that would say yes or no, and then the evaluation would be based on utility for the filtering results. When the system has a good knowledge about what the user wants. So in other words, we're trying to decide the absolute relevance. It helps the system decide where to set the threshold
5 and it's relevant. In content-based filtering system, We generally have to solve several problems related to filtering decision and learning etc. So it's not as high as the optimal utility. And such a system can actually be built based on a search engine system by adding a threshold mechanism, and adding adaptive learning algorithm to allow the system to learn from long-term feedback from the user. And the problem is, of course this approach is purely heuristic. So to summarize, there are two strategies for recommender systems or filtering systems. The second one is non-relevant and etc. The other is collaborative filtering, which is looking at the user similarity. In this lecture, we've covered the content based filtering approach in the next lecture we're going to talk about collaborative filtering. And so this can be just adjustment factor. And there is also the difficulty of bias training sample as we mentioned. The optimal point theta optimal is the point when we would achieve the maximum utility if we had chosen this threshold. So you can just compute the utility on the training data for each candidate score threshold. Let's say we can use alpha to control the deviation from the optimal utility point so you can see the formula of the threshold would be just the interpolation of the zero utility threshold and the optimal utility threshold. As I just explained, it's desirable to explore the interest space, so it's desirable to lower the threshold based on your training data. And there is also zero threshold zero utility threshold, and you can see at this cut off the utility is 0. What if I can cut at a different scoring threshold point what would happen, what's utility? Since these are training data, we can kind of compute the utility, right? We know their relevance status or we assume that we know relevant status that's based on approximation of clickthroughs. Now how do we solve these problems? In general, I think one can use the empirical utility optimization strategy, and this strategy is basically to optimize the threshold based on historical data, just as you have seen on the previous slide. One is content based which is looking at the item similarity. But what's more interesting is this gamma parameter here and you can see in this formula, gamma is controlling the influence of the number of examples in training dataset. And on the y-axis, We show the utility. Suppose I cut at this position and that would be the utility. So there will be a beta parameter to control the deviation from the optimal threshold, and this can be based on for example can be accounting for the overfitting to the training data let's say. And also can work on arbitrary utility with a proper lower bound. So here I show the historical data that you can collect in a filtering system so you can see the scores and the status of relevance. So here I show a ranked list of all the training documents that we have seen so far, and they are ranked by their positions. Now the question is how should we set alpha? And when should we deviate more from the optimal utility point? Well this can depend on multiple factors and one way to solve the problem is to encourage this threshold mechanism to explore up to the zero point, and that's a safe point, but we're not going to necessarily reach all the way to the zero point, but rather we're going to use other parameters to further define alpha, and this specifically is as follows. And the zero utility lower bound is also often too conservative. Now this approach, it actually has been working well in some evaluation studies empirically effective. So then we can just choose the threshold that gives the maximum utility on the training data. Now this means we also want to explore the document space a little bit and to see if the user might be interested in documents that we haven't delivered.There are some interesting challenges in threshold learning in the filtering problem. And that's also a difficult problem to solve. Of course this function depends on how you specify the coefficients in the utility function, but we can then imagine. And there are of course more advanced machine learning approaches that have been proposed for solving these problems, and this is the active research area. So in other words, we're going to explore the space of user interests by testing whether the user might be interested in some other documents that currently are not matching the users' interests so well
 That's content-based filtering. 1st. And then we're going to predict the user preferences based on the preferences of these similar users. Users with the same interest where have similar preferences. If the interest of the user is in information retrieval, then we can infer the user probably favor SIGIR papers. This is a different strategy called a collaborative filtering. Now the user similarity here can be judged based on their similarity in preferences on a common set of items. We can also assume that if we see people favor SIGIR papers, then we can infer their interest is probably information retrieval. So this is in general a sparse matrix. And that is, we say, we will infer individual's interest or preferences from that of other similar users. In this lecture, we will look at the user similarity. Given a user U, we're going to 1st find the similar users u_1 through u_m.This lecture is about the collaborative filtering. So first, what is collaborative filtering? It is to make filtering decisions for individual user based on the judgments of other users. Second, the users with similar preferences probably share the same interest. And in many cases, such assumption actually does make sense. So the general idea is the following. So this means this approach is very general. And how can we figure out the function based on the observed ratings? So this is the setup. R is the major conference devotes to the problem. And what's interesting here is we could potentially infer the value of an element in this matrix based on other values, and that's actually the central question in collaborative filtering, and that is we assume there's unknown function here F that would map a pair of a user and an object to the rating. So another assumption we have to make is that there are sufficiently large number of user preferences available to us. Alright, so those who are interested in information retrieval research probably all favor SIGIR papers. Could be products and then the users would give ratings 1 through 5 let's say. That's the assumption that we make and if this assumption is true, Then it would help collaborative filtering to work well
 They can be text documents. And then these ratings are weighted by their similarity. Basically the normalized rating that's more meaningful, but when they evaluate these collaborative filtering approaches. Weight Controls the influence of user on the prediction. Can be based on the average ratings of similar users. Because some users might be more generous and they generally given high ratings. And then use the predicted values to improve the similarity. There are new approaches proposed, but this memory based approach. This would recover a meaningful rating for this user. But not all users contribute equally to the average, and this is controlled by the weights. So this basically measures whether the two users tend to all give higher ratings to similar items, or lower ratings to similar items. Using the ratings of similar users to this active user, this is called a memory based approach. Another measure is the cosine measure, and this is to treat the rating vectors as vectors in the vector space. And of course, naturally, this way that should be related to the similarity between u_a and this particular user u_i. We could also combine that with content based filtering.And here we are going to talk about basic strategy and that would be based on similarity of users and then predicting the rating of. Because it's a little similar to storing all the user information and when we are considering a particular user, we're going to try to retrieve the relevant users or the similar users to this user case and then try to use that user information about those users to predict the preference of this user. Now this is it will normalize these ratings so that the ratings from different users would be comparable. So indeed there are many different ways to compute this function or this weight, w and specific approaches generally differ in how this is computed. In that case, obviously the systems prediction would have to be adjusted to match the actual ratings of the user, and this is what's happening here, basically. So, in the actual filtering system using collaborative filtering. And mainly we would like to improve the user similarity measure and there are some practical issues to deal with here as well. So mathematically, this is to say the predicted the rating of this user on this object user A on object o_j is basically combination of the normalized ratings of different users. And inside the sum, We have their ratings, well, their normalized ratings as I just explained, the ratings need to be normalized in order to be comparable with each other. Item to a user. Now the prediction of the rating on the item by another user or active user. And n_i is the average rating of all objects by this user. We also would like to map back to the rating that the user would actually make and this is to Further add the mean rating or average rating of this user u_a to the predicted value. Right, so this is basically the main idea of memory based approaches for collaborative filtering. And then you compare your system's predictions with the actual ratings. Here is a way to solve the problem and the strategy obviously would affect the performance of collaborative filtering. So you can imagine W of a an I is just a similarity of user A and user I. So if this user is generous than the average would be somewhat high and when we add that the rating will be adjust to a relatively high rating. And it's just a normalization strategy so that you get this predicted rating in the same range as the these ratings that we use to make the prediction. And then we're going to measure the angle and then compute the cosine of the angles of the two vectors, and this measure has been used in the vector space model for retrieval as well. This n_i is needed because we would like to normalize the ratings of objects by this user. Just like any other heuristics to improve these similarity functions, another idea which is actually very similar to the idea of IDF that we have seen in Text research is called inverse user frequency. In all these cases, note that the user similarity is based on their preferences on Items and we did not actually use any content information of these items. This would be a sum over commonly rated items and the formula is a standard Pearson correlation coefficient formula as shown here. So there are some obvious ways to also improve this approach
 etc and. The other is collaborative filtering where we look at the user similarity and they obviously can be combined in a practical system. And we also talked about the two strategies for filtering task one is content based, where we look at item similarity. You can use for example, more user information to assess their similarity instead of using the preferences of these users on these items, there may be additional information available about the user. Now the context here could be the context of the user and that it could be also context of documents or items. In particular, those new algorithms tend to use a lot of context information.So to summarize, our discussion of recommender systems in some sense, the filtering task or recommending task is easy and in some other senses, and the task is actually difficult, so it's easy because the users expectations, though in this case the system it takes initiative to push the information to the user so the user doesn't really make. And we also could recall that we talked about push versus pull as two strategies for getting access to the text data and recommended system is to help users in the push mode and search engines are certain users in the pull mode. Obviously the tool should be combined and they can be combined to have a system that can support the user with multiple mode information access. Even if you can make accurate recommendation of the most relevant news, the utility is going to be significantly decreased. Articles that can give you an overview of a number of specific approaches to recommended systems. So in future we could anticipate the such a system to be more useful to user. The thing about the news filtering as soon as you see the news and you have to decide whether the news would be interesting to a user. This is actually a very serious serious problem, but of course there are strategies that have been proposed to solve the problem and there are different strategies that you can use to alleviate the problem
 Mainly introduced the Cranefield evaluation methodology. And we then talked about how to evaluate the text retrieval system. We only managed to cover some basic topics in text retrieval and search engines. Most important ones are TF-IDF weighting, document length normalization, and TF is often transformed through a sublinear Transformation function. We talked about the major evaluation measures, so the most important measures for search engine--map(mean average precision), nDCG(normalized, discounted cumulative gain), an also precision and recall are the two basic measures. and we also later talked about the language modeling approaches, and that's a probabilistic model and here The main takeaway messages is that modern retrieval functions tend to look similar, and they generally use various heuristics. And we then talked about the feedback techniques and we talked about the Rocchio in the vector space model and the mixture model in the language modeling approach. This map shows the major topics we have covered in this course. So the text mining course, or rather text mining analytics course will be dealing with what to do once the user has found the information. These techniques are also essential in any text mining system to help provide provenance, stand to help users interpret in the patterns that user would find through text data mining. We started with the overview of vector space model and the probabilistic model, and then we talked about the vector Space Model in depth.This lecture is a summary of this course. And here are some key high level takeaway messages. Here the main takeaway message is natural language processing is the foundation for text retrieval. We talked about page rank and HITS as the major algorithms to analyze links on the web. And in this course we have covered various strategies to help users get access to the most relevant data. And it's often sufficient for the most of the search tasks, but obviously for more complex such tasks than we need a deeper natural language processing techniques. First we talked about the natural language content analysis. But current NLP isn't robust enough, so the bag of words representation is generally the main method used in modern search engines. This is a very important evaluation methodology that can be applied to many tasks. And this has to do with helping users to further digest the found information or to find the patterns and to reveal knowledge buried in text and such knowledge can then be used in application system to help decision making or to help user finish a task. We then talked about learning to rank. Some features to promote a page. In indexing we introduce the map reduce and then we talked about how to use linking information on the web to improve search. Where you can see a lot of short textbook or textbooks or long tutorials, they tend to provide a lot of information to explain a topic and there are multiple serieses that are related to this course, and one is the information concepts retrieval and services and another is Human language technology and yet another is artificial intelligence and machine learning. Human plays an important role in mining any text data becausw text data is written for humans to consume, so involving humans in the process of data mining is very important. So if you have not taken the text mining course in this data mining specialization series, then naturally the next step is to take that course as this picture shows to mine big text data we generally need two kinds of techniques, one is text retrieval, which is covered in this course and these techniques would help us convert the raw big text data into small relevant text data which are actually needed in the specific application. This is the use of machine learning to combine multiple features for improving scoring not only the effectiveness can be improved using this approach, but we can also improve the robustness of the ranking function so that it's not easy to spam the search engine with just
