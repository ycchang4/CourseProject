This lecture is about natural language content analysis. As you see from this picture, this is really the first step to process any text data, text data in natural languages. So computers have to understand natural language to some extent in order to make use of the data. So that's the topic of this lecture. We're going to cover three things. First, what is natural language processing? Which is the main technique for processing natural language to obtain understanding? The second is the state of the art in NLP, which stands for natural language processing. Finally, we're going to cover the relation between natural language processing and text retrieval. First what is NLP? Well the best way to explain it is to think about if you see a text in a foreign language that you can understand. Now what do you have to do in order to understand that text? This is basically what computers are facing, right? So looking at the simple sentence like a dog is chasing a boy on the playground. We don't have any problem with understanding this sentence. But imagine what the computer would have to do in order to understand it, or in general it would have to do the following. First we have to know dogs are a noun chasing is a verb etc. So this is called lexical analysis or part of speech tagging. And we need to figure out the syntactic categories of those words. So that's the first step. After that, we're going to figure out the structure of the sentence. So for example, here it shows that A and a dog would go together to form a noun phrase. And we won't have dog and is to go first, and there are some structures that are not just right. But this structure shows what we might get if we look at the sentence and try to interpret the sentence. Some words would go together 1st and then they will go together with other words. So here we show we have noun phrases as intermediate components and then verbal phrases. Finally we have a sentence. And to get this structure we need to do something called a syntactic analysis or parsing, and we may have a parser. A computer program that would automatically create this structure. Now at this point you would know the structure of this sentence, but still you don't know the meaning of the sentence, so we have to go further to semantic analysis. In our mind, we usually can map such a sentence to what we already know in our knowledge base. And for example, you might imagine a dog that looks like that there's a boy and there's some activity here. But for a computer would have to use symbols to denote that, right? So we would use a symbol D1 that denote a dog and B1 to denote a boy and then P1 to denote the playground, playground. Now there is also chasing activity that's happening here, so we have a relation chasing here that connects all these symbols. So this is how computer would obtain some understanding of this sentence. Now from this representation we could also further infer some other things, and we might indeed naturally think of something else when we read the text, and this is called inference. So for example, if you believe that if someone is being chased and this person might be scared with this rule, you can see computers could also infer that this boy may be scared. So this is some extra knowledge that you would infer based on understanding of the text. You can even go further to understand why the person said this sentence, so this has reduced the use of language. This is called. Pragmatic analysis. In order to understand the speech actor of a sentence. Like we say something to basically achieve some goal. There's some purpose there, and this has to do with the use of language. In this case, the person who said this sentence might be reminding another person to bring back the dog. That could be one possible intent to reach this level of understanding would require all these steps. And a Computer would have to go through all these steps in order to completely understand this sentence. Yet we humans have no trouble with understanding that, we instantly will get everything. And there is a reason for that. That's because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence. Computers unfortunately, are hard to obtain such understanding. They don't have such a knowledge base, they are still incapable of doing reasoning under uncertainties. So that makes natural language processing difficult for computers. But the fundamental reason why a natural language processing is difficult for computers is simply because natural language has not been designed for computers. They natural languages are designed for us to communicate. There are other languages designed for computers. For example program languages. Those are harder for us, so natural languages is designed to make our communication efficient. As a result, we omit a lot of common sense knowledge because we assume everyone knows about that. We also keep a lot of ambiguities because we assume the receiver or the hearer could know how to disambiguate ambiguous word based on the knowledge or the context. There's no need to invent the different words for different meanings. We could overload the same word with different meanings without the problem. Because of these reasons, this makes every step in natural language processing difficult. For computers, ambiguity is the main difficulty. And common sense reasoning is often required. That's also hard. So let me give you some examples of challenges here. Consider the word level ambiguity. The same word can have different syntactic categories. For example, design can be a noun or a verb. The word root may have multiple meanings, so square root in math sense, or the root of a plant. You might be able to think of other meanings. There are also syntactical ambiguities, for example. The main topic of this lecture, natural language processing can actually be interpreted in two ways in terms of the structure. Think for a moment to see if you can figure that out. We usually think of this as processing of natural language. But you could also think of this as you say, language processes is natural. Alright, so this is an example of syntactic ambiguity where we have different structures that can be applied to the same sequence of words. Another common example of an ambiguous sentence is the following. A man saw a boy with a telescope. Now in this case, the question is who had the telescope? Right, this is called a prepositional phrase attachment. Ambiguity, or PP attachment ambiguity. Now we generally don't have a problem with these ambiguities. Because we have a lot of background and knowledge to help us disambiguate the ambiguity. Another example of difficulties is anaphora resolution, so think about the sentence like John persuaded Bill to buy a TV for himself. The question here is does himself refer to John or Bill? So again, this is something that you have to use some background or the context to figure out. Finally, presupposition is another problem. Consider the sentence. He has quit smoking. This obviously implies that he smoked before. So imagine a computer wants to understand all these subtle differences and meanings. It would have to use a lot of knowledge to figure that out. It also would have to maintain a large knowledge knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world. So This is why it's very difficult. So as a result, we are still not perfect, in fact, far from perfect in understanding natural language using computers. So this slide sort of gives simplified view of state of the art technologies. We can do part of speech tagging pretty well, so I showed 97% accuracy here. Now this number is obviously based on a certain data set, so don't take this literally. It just shows that we can do it pretty well, but it's still not perfect. In terms of parsing, we can do partial parsing pretty well. That means we can get noun phrase structures or verbal phrases structures, or some segment of the sentence understood correctly in terms of the structure. An in some evaluation results we have seen above 90% accuracy in terms of partial parsing of sentences. Again, I have to say these numbers are relative to the data set in some other data sets. The numbers might be lower. Most of the existing work has been evaluated using news data set and so a lot of these numbers are more or less biased toward news data. Think about the social media data, the accuracy likely is lower. In terms of semantic analysis. We are far from being able to do a complete understanding of a sentence. But we have some techniques that would allow us to do partial understanding of the sentence. So I could mention some of them. For example, we have techniques that can allow us to extract the entities and relations mentioned in text articles. For example, recognizing the mentions of people, locations, organisations, etc in text. So this is called entity extraction. We may be able to recognize the relations, for example this person visited that place or this person met that person, or this company acquired another company. Such relations can be extracted by using the current natural language processing techniques. They're not perfect, but they can do well for some entities. Some entities are harder than others. We can also do word sense disambiguation to some extent. We can figure out whether this word in this sentence would have certain meaning in another context. The computer could figure out it has a different meaning. Again, it's not perfect, but you can do something in that direction. We can also do sentiment analysis, meaning to figure out the weather sentence is positive or negative. This is especially useful for review analysis, for example. So these are examples of semantic analysis and they help us to obtain partial understanding of the sentences. It's not giving us a complete understanding as I showed it before for this sentence, but it would still help us gain understanding of the content, and these can be useful. In terms of inference, we are not there yet, partly because of the general difficulty of inference and uncertainties. This is a general challenging in artificial intelligence. That's partly also because we don't have complete semantic representation for natural language text, so this is hard yet in some domains, perhaps in limited domains, when you have a lot of restrictions on the word uses, you maybe do may be able to perform inference. To some extent, but in general we cannot really do that. reliably. Speech Act analysis is also far from being done, and we can only do that analysis for various special cases. So this roughly gives you some idea about the state of the art. And then we also talk a little bit about what we can't do. And so we can't even do one hundred percent part of speech tagging. Now this looks like a simple task, but think about the example here. The two users of off may have different syntactic categories. If you try to make a fine grained distinguishing, it's not that easy to figure out such differences. It's also hard to do general, complete parsing, and again this same sentence that you saw before is example. This ambiguity can be very hard to disambiguate, and you can imagine example where you have to use a lot of knowledge in the context of the sentence or from the background in order to figure out who actually had the telescope. So although the sentence looks very simple, it actually is pretty hard, and in cases when the sentence is very long. Imagine it has four or five prepositional phrases, and there are even more possibilities to figure out. It's also hard to do precise deep semantic analysis, so here's example in the sentence. John owns a restaurant. How do we define owns exactly the word own is something that we understand, but it's very hard to precisely describe the meaning of own for computers. So as a result, we have robust and general natural language processing techniques that can process a lot of text data. In a shallow way, meaning we only do superficial analysis. For example, parts of speech tagging or partial parsing or recognizing sentiment, and those are not deep understanding 'cause we're not really understanding the exact meaning of a sentence. On the other hand, the deeper understanding techniques tend not to scale up well, meaning that they would fail on some unrestricted text. And, if you don't restrict the text domain or the use of words, then these techniques tend not to work well. They may work well based on machine learning techniques on the data that are similar to the training data that the program has been trained on, but generally wouldn't work well. The data that are very different from the training data, so this pretty much summarizes the state of the art of natural language processing. Of course, within such a short amount of time, we can't really give you a complete view of NLP, which is big field an either expect to see multiple courses on natural language processing. topic itself, but because of its relevance to the topic we talk about, it's useful for you to know the background. In case you haven't been exposed to that. So what does that mean for text retrieval? In text retrieval, we're dealing with all kinds of text. It's very hard to restrict the text to a certain domain. And we also often dealing with a lot of text data. So that means the NLP techniques must be general, robust, and efficient, and that just implies today we can only use fairly shallow and NLP techniques for text retrieval. In fact, most search engines today use something called a bag of words representation. Now, this is probably the simplest representation you can possibly think of. That is to turn text data into simply a bag of words, meaning we will keep individual words, but will ignore all the orders of words. And we'll keep duplicated occurrences of words. So this is called a bag of words representation. When you represent the text in this way, you ignore a lot of other information and that just makes it harder to understand the exact meaning of a sentence, because we've lost the order. But yet this representation tends to actually work pretty well for most search tasks, and this is partly because the search task is not all that difficult. If you see matching of some of the query words in a text document, chances are that that document is about the topic, although there are exceptions. So in comparison, some other tasks, for example machine translation, would require you to understand the language accurately, otherwise the translation would be wrong. So in comparison, search task is all relatively easy. Such a representation is often sufficient, and that's also the representation that the major search engines today, like a Google or Bing or using. Of course I put in parentheses is here, but not all. Of course there are many queries that are not answered well by the current search engines and they do require a representation that would go beyond the bag of words representation that would require more natural language processing to be done. There was another reason why we have not used the sophisticated NLP techniques in modern search engines, and that's because some retrieval techniques actually naturally solve the problem of NLP. So one example is word sense disambiguation. Think about the world like Java. It could mean coffee, or could mean program language. If you look at the world alone, it would be ambiguous, but when the user uses the word in the query, usually there are other words. For example, I'm looking for usage of Java applet. When I have applet there that implies. Java Means program language. And that context can help us naturally prefer documents where Java is referring to program language 'cause those documents would probably match applet as well if Java occurs in the document in a way that it means coffee. Then you would never match applet or with very small probability, right? So this is the case when some retrieval techniques naturally achieve the goal of word sense disambiguation. Another example is. Some technical code feedback which we will talk about later in some of the lectures. This technical code would allow us to add additional words to the query and those additional words could be related to the query words. And these words can help matching documents where the original query words have not occurred. So this achieves to some extent. Semantic matching of terms. So those techniques also helped us bypass some of the difficulties in natural language processing. However, in the long run we still need deeper natural language processing techniques in order to improve the accuracy of the current search engines, and it's particularly needed for complex search tasks. Or for question answering. Google has recently launched Knowledge Graph and this is one step toward that goal. 'cause knowledge graph would contain entities and their relations, and this goes beyond the simple bag of words representation and such technique should help us improve the search engine utility significantly. Although this is still an open topic for research and exploration. In summary in this lecture we talked about what is an NLP and We've talked about the state of the art techniques, what we can do, what we cannot do, and finally, we also explain the why bag of words representation remains the dominant representation used in modern search engines, even though deeper NLP would be needed for future search engines. If you want to know more, you can take a look at some additional readings. I only cited one here and that's a good starting point. Thanks. 
In this lecture we're going to talk about text access. In the previous lecture we talk about natural language content analysis. We explained that the state of the art natural language processing techniques are still not good enough to process a lot of unrestricted text data in a robust manner. As a result, bag of words representation remains very popular in applications like search engines. In this lecture we're going to talk about some high level strategies. To help users get access to the text data. This is also important step to convert raw big text data into small relevant data that are actually needed in a specific application. So the main question we will address here is how can a text information system help users get access to the relevant text data we're going to cover two complementary strategies, push versus pull. And then we're going to talk about the two ways to implement the pull mode: querying versus browsing. So first push versus pull. These are two different ways to connect users with the right information at the right time. The difference is. Which takes the initiative. Which party takes the initiative? In the pull mode, the users would take the initiative. To start the information access process. And in this case, a user typically would use a search engine to fulfill the goal. For example, the user may type in the query and then browse results to find the relevant information. So this is usually appropriate for satisfying a users Ad hoc information need. An ad hoc information need is temporary information need, for example. You want to buy a product so you suddenly have a need to read reviews about related products. But after you have collected information and have purchased your product. You generally no longer need such information, so it's a temporary information need. In such a case, it's very hard for a system will predict your need and it's more appropriate for the users to take the initiative, and that's why search engines are very useful today because many people have many ad hoc information needs all the time. So as we're speaking Google is probably processing many queries from us and those are all or mostly all ad hoc information needs. So this is a pull mode in contrast, in the push mode, the system will take the initiative to push the information to the user, or to recommend that information to the user. So in this case this is usually supported by a recommender system. Now this would be appropriate if the user has a stable information need. For example, you may have a research interest in some topic and that interest tends to stay for awhile, so it's relatively stable. Your hobby is another example of a stable information need. In such a case, the system can interact with you and can learn your interest and then can monitor the information stream. If it is, the system has seen any relevant items to your interest the system could then take the initiative to recommend information to you. So for example, a news filter or news recommender system could monitor the news stream and identify interesting news to you and simply push the news articles to you. This mode of information access maybe also appropriate when the system has good knowledge about the users need and this happens in the search context. So for example, when you search for information on the web, a search engine might infer you might be also interested in some related information. And they would recommend the information to you, so that should remind you, for example advertisement placed on search page. So this is about the two high level strategies or two modes of text access. Now let's look at the pull mode in more detail. In the pull mode, we can further distinguish in two ways to help users querying versus browsing. In querying the user will just enter a query. Typical keyword query and the search engine system would return relevant documents to users. And this works when the user knows what exactly are the keywords to be used. So if you know exactly what you're looking for, you tend to know the right keywords, and then querying would work very well and we do that all the time. But we also know that sometimes it doesn't work so well, when you don't know the right keywords to use in the query or you want to browse information in some topic area. In this case browsing would be more useful. So in this case. In the case of browsing, the users would simply navigate into the relevant information by following the paths supported. By the structures documents. So the system would maintain some kind of structures and then the user could follow these structures to navigate. So this really works well when the user wants to explore the information space. Or the user doesn't know what are the key words to use in the query. Or simply because the user finds it inconvenient to type in a query. So even if the user knows what query to type in, if the user is using a cell phone. To search for information there, it's still hard to enter the query in such a case. Again, browsing tends to be more convenient. The relationship between browsing and the query is best understood by making an analogy to sight seeing. Imagine if you are touring the city now. If you know the exact address of the attraction then taking a taxi, there is perhaps the fastest way you can go directly to the site, but if you don't know the exact address you may need to walk around, or you can take a taxi to a nearby place and then walk around. It turns out that we do exactly the same in the information space. If you know exactly what you're looking for, then you can use the right keywords in your query to find the information directly. That's usually the fastest way to do find information. But what if you don't know the exact keywords to use? Your query probably won't work, so you'll land on some related pages, and then you need to also walk around in the information space, meaning by following the links or by browsing. You can then finally get into the relevant page. If you want to learn about the topic again you will likely do a lot of browsing. So just like you are looking around in some area and you want to see some interesting attractions in a related- in the same region. So this is analogy also tells us that today. We have very good spot for query but we don't really have good support for browsing. And this is because. In order to browse effectively, we need a map to guide us. Just like you need a map of Chicago to tour the city of Chicago, you need a topic map to tour the information space. So how to construct such a topic map is in fact a very interesting research question that likely will bring us more interesting browsing experience on the web or in other applications. So to summarize this lecture we've talked about the two high level strategies for text access, push and pull. Push tends to be supported by recommender systems and pull tends to be supported by a search engine. Of course in a sophisticated intelligent information system we should combine the two. In the pull mode we can further distinguish querying and browsing again, we generally want to combine the two ways to help users so that you can support both querying and browsing. If you want to know more about the relationship between pull and push. You can read this article. This gives excellent discussion of the relationship between information filtering and information retrieval. Here, information filtering is similar to information recommendation or the push mode of information access. 
This lecture is about the text retrieval problem. This picture shows our overall plan for lectures. In the last lecture we talked about the high level strategies for text access. We talked about push versus pull. Search engines are the main tools for supporting the pull mode. Starting from this lecture, we're going to talk about how the search engines work in detail. So first it's about the text retrieval problem. We are going to talk about three things in this lecture. First, we'll define text retrieval. Second, we're going to make a comparison between text retrieval and the related task, database retrieval. Finally, we're going to talk about the document selecting versus document ranking as two strategies for responding to a users query. So what is text retrieval? It should be a task that's familiar to most of us because we're using web search engines all the time. So text retrieval is basically a task where the system would respond to a user's query with relevant documents. Basically, to support the query. As one way to implement the pull mode of information access. So the scenario is the following you have a collection of text documents. These documents could be all the web pages on the web. Or all the literature articles in the digital library. Or maybe all the text files in your computer. A user will typically give a query to the system to express information need and then the system would return relevant documents to users. Relevant documents refer to those documents that are useful to the user who is in typing the query. Now this task is often called information retrieval. But literally, information retrieval would broadly include retrieval of other non textual information as well. For example audio, video etc. It's worth noting that text retrieval is at the core of information retrieval, in the sense that other medias, such as video, can be retrieved by exploiting the companion text data. So for example. Current image search engines. Actually match the users query with the companion text data of the image. This problem is also called the search problem. And the technology is often called search technology in industry. If you have to take a course in databases. It will be useful to pause the lecture at this point. And think about. The differences between text retrieval and database retrieval. Are these two tasks are similar in many ways? But there are some important differences. So spend a moment to think about the differences between the two. Think about the data and information managed by search engine versus those that are managed by a database system. Think about the difference between the queries that you typically specify for a database system. Versus the queries that are typed in by users on the search engine. And then finally, think about the answers. What's the difference between the two? OK, so if we think about the information or data managed by the two systems, we will see that. In text retrieval, the data is unstructured free text, but in databases. They are structured data where there is a clear definer schema to tell you. This column is the names of people in that column is ages, etc. In unstructured text it's not obvious what are the names of people mentioned in the text. Because of this difference, we can also see that text information tends to be more ambiguous, and we talked about that in the natural language processing lecture. Whereas in databases that data tended to have well defined the semantics. There is also important difference in the queries. And this is partly due to the difference in the information. Or data. So text queries tend to be ambiguous. Where as in database search. The queries are typically well defined. Think about the SQL query that would clearly specify what records to be returned so it has very well defined semantics. Keyword queries or natural language queries tend to be incomplete also. In that it doesn't really fully specify what document should be retrieved. Where as in the database search. The SQL query can be regarded as a computer specification for what should be returned. And because of these differences, the answers will be also different. In the case of texy retrieval, we're looking for relevant documents. In the database search we are retrieving records or match records with. The SQL query more precisely. Now, in the case of tax retrieval, what should be the right answers to a query is not very well specified as we just discussed. So it's unclear what should be the right answers to a query, and this has very important consequences and that is text retrieval is an empirically defined problem. So this is. A problem because. If it's empirically defined. Then we cannot mathematically prove one method is better than another method. That also means we must rely on empirical evaluation involving users. To know which method works better. And that's why we have a lecture, actually more than one lectures to cover the issue of evaluation. Because this is a very important topic for search engines. Without knowing how to evaluate the algorithm appropriately, there's no way to tell whether we have got a better algorithm or whether one system is better than another. So now let's look at the problem in a formal way. So this slide shows a formal formulation of the text retrieval problem. First, we have our vocabulary set. Which is just a set of words in a language. Now here. We are considering just one language, but in reality on the web there might be multiple natural languages. We have text data in all kinds of languages. But here for simplicity, we just assume there is one kind of language as the techniques used for retrieving data from multiple languages are more or less similar to the techniques used for retrieving documents in one language. Although there is important difference, the principles and methods are. very similar. Next we have the query, which is a sequence of words. And so here. You can see. The. Query. Is defined as a sequence of words. Each q sub I is a word in the vocabulary. A document is defined in the same way, so it's also a sequence of words and here d sub ij. is also a word in the vocabulary. Now typically the documents are much longer than queries. But there are also cases where. The documents may be very short. So you can think about what might be an example of that case. I hope you can think of Twitter search right tweets are very short. But in general, documents are longer than the queries. Now then we have a collection of documents. And this collection can be very large, so think about the web. It could could be very large. And then the goal of text retrieval is to find the set of relevant documents which we denoted by R of Q because it depends on the query and this is in general a subset of all the documents in the collection. Unfortunately, this set of relevant documents is generally unknown. And user dependent in the sense that for the same query typed in by different users. The expected relevant documents may be different. The query given to us by the user is only a hint on which document should be in this set. And indeed, the user is generally unable to specify what exactly should be in this set, especially in the case of web search. Where collection is so large the user doesn't have complete knowledge about the whole collection. So the best a search system can do. Is to compute an approximation of this relevant document. set so we denote by r prime of Q. So formally, we can see the task is to compute this r prime of Q, an approximation of the relevant documents. So how can we do that? Imagine if you are now asked to write a program to do this. What would you do now? Think for a moment. Right, so these are your input. the query the documents. And then you are to compute the answers to this query. Which is a set of documents that would be useful to the user. So how would you solve the problem? Now, in general. There are two strategies that we can use. The first strategies will do document selection and that is we're going to have a binary classification function or binary classifier. That's a function that would take a document and query as input. And then give a zero or one as output to indicate whether this document is relevant to the query or not. So in this case you can see the document. The relevant document in the set is defined as follows, it basically. All the documents that. Have a value of 1 by this function. And so in this case you can see the system must decide if a document is relevant or not. Basically it has to say whether it's one or zero. And this is called absolute relevance. Basically it needs to know exactly whether it's going to be useful to the user. Alternatively, there's another strategy called document ranking. Now, in this case the system is not going to make a call whether a document is relevant or not, but rather the system is going to use the real value function F here. That would simply give us a value that would indicate which document is more likely relevant. So it's not going to make a call whether this document is relevant or not, but rather it would say which document is more likely relevant. So this function then can be used to rank the documents. And then we're going to let the user decide where to stop when the user looks at the documents. So we have a threshold. See down here. To determine what documents should be in this approximation, set. And we can assume that all the documents that are ranked above this threshold are in this set. Be cause in effect, these are the documents that we deliver to the user. And theta is a cut off determined by the user. So here we've got some collaboration from the user in some sense, because we don't really make a cut off and the user kind of helped the system make a cut off. So in this case the system only needs to decide if one document is more likely relevant than another, and that is it only needs to determine relative relevance. As opposed to absolute relevance. Now you can probably already sense that. Relevant relative relevance would be easier to determine their absolute relevance because in the first case, we have to say exactly whether a document is relevant or not. And it turns out that ranking is indeed generally preferred to document selection. So let's look at this. These two strategies in more detail. So this picture shows how it works. So on the left side we see these documents and we use the pluses. To indicate the relevant documents so we can see the true relevant documents here. This is. This set of two random documents consist of these pluses these documents. And with the document selection. Functioning we are going to do basically classify them into two groups, relevant documents and non relevant ones. Of course the classifier will not be perfect so it will make mistakes. So here we can see in the approximation of the relevant documents we have got some non relevant documents. And similarly there is a relevant document that's miss classified as non relevant. In the case of document ranking, we can see the system seems like simply ranks all the documents in the descending order of the scores. And then we're going to let the users stop wherever the user wants to stop. So if a user wants to examine more documents, then the user would go down the list to examine more and stop at the lower position. But if the user only wants to read a few relevant documents, the user might stop at the top position. So in this case the user stops at d4 so in effect we have delivered these four documents. To our user. So as I said, ranking is generally preferred. An one of the reasons is because the classifier. In the case of document selection is unlikely accurate. Why? Because the only clue is usually the query, but query may not be accurate in the sense that it could be overly constrained. For example, you might expect the relevant documents to talk about all these. Topics you by using specific vocabulary and as a result. You might. Match no random documents because in the collection no others have discussed the topic using these vocabularies. So in this case you will see there is this problem of. No relevant documents to return in the case of overly constraint query. On the other hand, if the query is underconstrained, for example. If the query does not have sufficient discriminating words to find the relevant documents, you may actually end up having over delivery. And this is when you thought these words might be sufficient to help you find the relevant documents. But it turns out that they are not sufficient, and there are many distraction documents using similar words. Right so. This is the case of over delivery. Unfortunately, it's very hard to find the right position between these two extremes. Why cause when the user is looking for the information? In general, the user does not have a good knowledge about the information to be found. And in that case the user does not have a good knowledge about what. Vocabularies will be used in those random documents. So it's very hard for user to pre specify the right level of constraints. Even if the classifier is accurate. We also still want to rank these red documents because. They are generally not equally relevant. Relevance is often a matter of degree. So we must prioritize. These documents for a user to examine. And this note that this prioritization is very important. Because a user cannot digest all the contents at once, the user general would have to look at each document sequentially. And therefore it would make sense to feed the users with the most relevant documents. And that's what ranking is doing. So For these reasons, ranking is generally preferred now. This preference also has a theoretical justification, and this is given by the probability ranking principle. In the end of this lecture there is reference for this. This principle says returning a ranked list of documents in descending order of probability that a document is relevant to the query is the optimal strategy under the following two assumptions. First, the utility of the document to a user is independent of the utility of any other document. 2nd, a user would be assumed to browse the results sequentially. Now it's easy to understand why these two assumptions are needed in order to justify for the ranking strategy. because if the documents are independent then we can evaluate the utility of each document separately. And this would allow us to compute the score for each document independently, and then we're going to rank these documents based on those scores. The second assumption is to say that the user would indeed follow the ranked list if the user is not going to follow the ranked list is not going to examine the documents sequentially, then obviously the ordering would not be optimal. So under these two assumptions. We can theoretically justify the rankings strategy is in fact the best you could do. Now I've put one question here. Do these two assumptions hold? Now I suggest you to pause the lecture for a moment to think about this. Now, can you think of some examples? That would suggest. These Assumptions aren't necessarily true. Now, if you think for a moment you may realize. None of the assumptions is actually true. For example, in the case of independence assumption. We might have identical documents that have similar content or exactly the same content. If you look at each of them alone. Each is relevant. But if the user has already seen one of them. We assume it's generally not very useful for the user to see another similar or duplicated one. So clearly the utility of document is dependent on other documents that user has seen. In some other cases you might see a scenario where one document that may not be useful to the user, but when three particular documents are put together, they provide answer to the users question. So this is a collective relevance and that also suggests that the value of the document might depend on other documents. Sequential browse in general would make sense if you have a ranked list there. But even if you have a ranked list. There was evidence showing that users don't always just go strictly sequentially through the entire list. There sometimes would look at the bottom, for example, or skip some. And if you think about the more complicated interface that we could possibly use, like 2 dimensional interface where you can put the additional information on the screen, then seek when you're browsing is a very restrictive assumption. So the point here is that. None of these assumptions is really true. But nevertheless. The probability ranking principle established some solid foundation for ranking as a primary task for search engines. This has actually been the basis for a lot of research work in information retrieval, and many algorithms have been designed based on this assumption Despite that. The assumptions aren't necessarilly true and we can address this problem by doing post processing of a ranked list, for example to remove redundancy. So to summarize this lecture, the main points. That you can take away are the following. First text retrieval is an empirical defined problem. And that means which algorithm is better must be judged by the users. Second, document ranking is generally preferred. And this will help users prioritize examination of search results. And this is also to bypass the difficulty in determining absolute relevance. Because we can get some help from users in determining where to make the cut off. It's more flexible. So this further suggests that the main technical challenge in designing search engine is redesigned effective ranking function. In other words, we need to define what is the value of this function F on. The query and document pair. The whole design. Such a function is the main topic in the following lectures. There are two suggested additional readings. The first is the classic paper on probability ranking principle. The second is a must read for anyone doing research information travel. It's a classic IR book. Which has excellent coverage of the main research results in early days. Up to the time when the book was written, Chapter 6 of this book has in depth discussion of the probability ranking principle and probabilistic retrieval models in general. 
This lecture is the overview of text retrieval methods. In the previous lecture we introduced the problem of text retrieval. We explained that the main problem is to design a ranking function to rank documents for a query in this lecture we will give a overview of different ways of designing this ranking function. So the problem is the following. We have a query that has a sequence of words and a document that's also a sequence of words and we hope to define a function F. That can compute a score based on the query and document. So the main challenge here is to design a good ranking function that can rank all the relevant documents on top of all the non relevant ones. Now clearly this means our function must be able to measure the likelihood that a document d, is relevant to a query Q. That also means we have to have some way to define relevance. In particular, in order to implement the program to do that, we have to have a computational definition of relevance. And we achieve this goal by designing a retrieval model which gives us a formalization of relevance. Now, over many decades, researchers have designed many different kinds of retrieval models. And they fall into different categories. 1st. One thing many of the models are based on the similarity idea. Basically, we assume that if a document is more similar to the query, then another document is then we will say the first document is more relevant than the second one. So in this case, the ranking function is defined as the similarity between the query and the document. One well known example in this case is vectors space model, which we will cover more in detail later in the lecture. The second kind of models are called probabilistic models. In this family of models we follow a very different strategy, where we assume that. Queries and documents are all observations from random variables. And we assume there is a Binary Random variable called R here. To indicate whether a document is relevant to a query. We then define the score of document with respect to a query as the probability that this random variable R is equal to 1 given a particular document and query. There are different cases of such a general idea. One is classic probabilistic model. Another is language model, yet another is divergent from randomness model. In the later lecture we will talk more about one case which is language model. The third kind of models are based on probabilistic influence, so here the idea is to associate uncertainity to inference rules, and we can then quantify the probability that we can show that the query follows from the document. Finally, there is also a family of models that are using axiomatic thinking. Here the idea is to define a set of constraints that we hope. A good retrieval function. To satisfy. So in this case, the problem is to seek A good ranking function that can satisfy all the desired constraints. Interestingly, although these different models are based on different thinking. In the end. The retrieval function. Tends to be very similar. And these functions tend to also involve similar variables. So now let's take a look at the common form of a state of the retrieval model. And to examine some of the common ideas used in all these models. First, these models are all based on the assumption of using bag of words to represent text, and we explained this in the natural language processing lecture. Bag of words representation remains the main representation. Used in all the search engines. So with this assumption, the score of a query, presidential campaign news. With respect to a document d here would be based on scores computed based on each individual word. And that means the score would depend on the score of each word. Such as presidential campaign and news. Here we can see. There are three different components, each corresponding to how well the document matches each of the query words. Inside these functions. We see a number of heuristics used. So for example, one factor that affects the. Function G Here is how many times does the world presidential occur in the document. This is called a term frequency or TF. We might also denote as c of presidential and d. In general, if. The word occurs more frequently in the document then the value of this function would be larger. Another factor is how long is the document. And This is to use the document length for scoring. In general. If a term occurs in a long document that many times. It's not as significant as. If it occurred the same number of times in a short document, because in a long document. Any term is expected to occur more frequently. Finally, there is this factor called the document frequency. That is, we also want to look at the how often presidential occurs in the entire collection. And we call this document frequency or DF of presidential. And in some other models we might also. Use a probability. To characterize this information. So here is show the probability of presidential in the collection. So all these are trying to characterize the popularity of the term in the collection in general, matching a rare term in the collection. Is contributing more to the overall score than matching a common term. So this captures some of the main ideas used in pretty much all the state of art retrieval models. So the natural question is which model works the best? Now it turns out that many models work equally well, so here are a list of the four major models that are generally regarded as a state of the art retrieval models. Pivoted length normalization. BM25, query likelihood, PL2. When optimized, these models tend to perform similarly. And this was discussed in detail in this reference at the end of this lecture. Among all these BM25 is probably the most popular. It's most likely that this has been used in virtually all the search engines and you will also often see this method discussed in research papers. And we'll talk more about this method later in. Some other letures. So to summarize. The main points made in this lecture are first the design of a good ranking function. Pre requires a computational definition of relevance and we achieve this goal by designing appropriate retrieval model. Second, many models are equally effective, but we don't have a single winner yet. Researchers are still actively working on this problem. Trying to find a truly optimal retrieval model. Finally, the state of art ranking functions tend to rely on the following ideas. First bag of words representation. 2nd. TF and document frequency of words, such information is used in. the weighting function to determine the overall contribution of matching a word. And document lengths. These are often combined in interesting ways and we'll discuss how exactly they are combined to rank documents in the lectures later. There are two suggested the additional readings. If you have time. The first is a paper where you can find a detailed discussion and comparison of multiple state of the art models. The second is a book with a chapter that gives a broad review of different retrieval models. 
This lecture is about the vector space retrieval model we're going to give a introduction to its basic idea. In the last lecture we talked about the different ways of designing a retrieval model. Which would give us a different a ranking function. In this lecture we're going to talk about this specific way of designing a ranking function called vector space retrieval model. And we're going to give a brief introduction to the basic idea. Vector space model is a special case of similarity based models, as we discussed before, which means we assume relevance is roughly similarity between the document and the query. Now, whether this assumption is true is actually a question. But in order to solve a search problem, we have to convert the vague notion of relevance into a more precise definition that can be implemented with the programming language. So in this process will have to make a number of assumptions. This is the first assumption that we make here. Basically, we assume that if a document is more similar to a query than another document then the first document will be assumed to be more relevant than the second one, and this is the basis for ranking documents in this approach. Again, it's questionable whether this is really the best definition for relevance. As we will see later, there are other ways to model relevance. The basic idea of vector space retrieval model is actually very easy to understand. Imagine a high dimensional space. Where each dimension corresponds to a term. So here I show a 3 dimensional space. With three words, programming, library and presidential. So each term here defines one dimension. Now we can consider vectors in this 3 dimensional space. And we're going to assume that all our documents and the query will be placed in this vector space. So for example, one document might be represented by this vector, D1. Now this means this document probably covers library, and presidential, but it doesn't really talk about programming. Alright, what does this mean terms of representation of document? That just means we're going to look at our document from the perspective of this vector. We're going to ignore everything else. Basically, what we see here is only the vector representation of the document. Of course, the document has other information. For example, the orders of words are simply ignored, and that's because we assume that the bag of words with representation. So with this representation you can already see D1 seems to suggest a topic about presidential library. Now this is different from another document which might be represented as a different vector D2 here. So in this case, the document covers programming and library, but does not talk about the presidential. So what does this remind you? You can probably guess the topic is likely about programming language, and the library is software library. So this shows that by using this vector space representation we can actually capture the differences between topics of documents. Now you can also imagine there are other vectors, for example D3 is pointing to that direction. That might be about present in your program. And in fact that we can place all the documents in this vector space. And they will be pointing to all kinds of directions. And similarly, we're going to place our query also in this space as another vector. And then we're going to measure the similarity between the query vector and every document vector. So in this case, for example, we can easily see D2 seems to be the closest to this query vector, and therefore D2 will be ranked above others. So this is basically the main idea of the vector space model. So to be more precise. To be more precise. Vector space model is a framework. In this framework, we make the following assumptions. First, we represent a document and query via term vector. So here are term can be any basic concept, for example a word or a phrase. Or even N-gram of characters. Those are just sequence of characters Inside the word. Each term is assumed to define one dimension. Therefore, N terms in our vocabulary would define an N dimensional space. A query vector would consist of a number of elements. Corresponding to the weights on different terms. Each document vector is also similar. It has a number of elements and each value of each element is indicating that weight of the corresponding term. Here you can see we assume there are N dimensions. Therefore there are N elements. Each corresponding to the weight on a particular term. So the relevance in this case would be assumed to be the similarity between the two vectors. Therefore, our ranking function is also defined as the similarity between the query vector and document vector. Now, if I ask you to write a program to implement this approach in the search engine, you would realize that this is far from clear, right? We haven't said a lot of things in detail, therefore it's impossible to actually write the program to implement this. That's why I said this is a framework. And this has to be refined in order to actually suggest a particular ranking function that you can implement on your computer. So what does this framework not say? It actually hasn't set up many things that would be required in order to implement this function. First, it did not say how we should define or select the basic concepts exactly. We clearly assume the concepts are orthogonal, otherwise there will be redundancy. For example, if two synonyms are somehow distinguished as two different concepts, then there would be defining two different dimensions and that would clearly cause redundancy here, or over emphasizing of matching this concept. Because it would be as if you match the two dimensions when you actually match one semantic concept. Secondly, it did not say how exactly should place documents and query in this space. Basically I showed you some examples of query and document vectors, but where exactly should the vector for a particular document point to? So this is equivalent to how to define the term weights. How do you compute those element values in those vectors? Now this is a very important question because term weight in the query vector indicates the importance of term. So depending on how you assign the weights, you might prefer some terms to be matched over others. Similarly to term weight in the document is also very meaningful. It indicates how well the term characterizes the document. If you got it wrong, then you clearly don't represent this document accurately. Finally, how to define the similarity measure is also not given. So these questions must be addressed before we can have a operational function that we can actually implement using a program language. So how do we solve these problems? Is the main topic of the next lecture. 
In this lecture we're going to talk about how to instantiate vector space model so that we can get a very In this lecture, we are going to talk about how to instantiate vector space model so that we can get a very specific ranking function. So this is to continue the discussion of the vector space model, which is one particular approach to design ranking function. And we're going to talk about how we use the general framework of the vector space model as a guidance to instantiate the framework to derive a specific ranking function. And we're going to cover the simplest instantiation of the framework. So as we discussed in the previous lecture, the Vector Space model is really a framework. It didn't say. As we discussed in the previous lecture. Vector space model is really a framework. It doesn't say many things. So for example, here it shows that it did not say how we should define the dimension. It also did not say how we place a document vector in this space. It did not say how we place a query vector in this vector space. And finally, it did not say how we should measure the similarity between the query vector and the document vector. So you can imagine in order to implement this model we have to say, specifically, how we compute these vectors? What is exactly Xi and what is exactly Yi? This will determine where we place a document vector, where we place a query vector. And of course, we also need to say exactly what should be the similarity function. So if we can provide a definition of the concepts that would define the dimensions and these Xi's or Yi's, namely weights of terms for query and document, then we will be able to place document vectors and query vector in this well defined space and then if we also specify similarity function then we'll have a well defined the ranking function. So let's see how we can do that and think about the simplest instantiation. Actually, I would suggest you to pause the lecture at this point, spend a couple of minutes to think about. Suppose you are asked to implement this idea. You come up with the idea of vector space model. But you still have to figure out how to compute these vectors. Exactly how to define the similarity function? What would you do? So think for. A couple of minutes and then proceed. So let's think about the some simplest ways of instantiating this vector space model? First, how do we define dimension where the obvious choice is to use each word in our vocabulary to define the dimension? And here we show that there are N words in our vocabulary, therefore there are N dimensions. Each word defines one dimension and this is basically the bag of words representation. Now let's look at how we place vectors in this space. Again here the simplest strategy is to use a bit vector to represent both the query and a document. And that means each element Xi and Yi would be taking a value of either zero or one. When it's one, it means the corresponding word is present in the document or in query. When it's zero it's going to mean that it's absent. So you can imagine if the user types in a few words in the query, then the query vector will only have a few ones, many many zeros. The document vector in general will have more ones of course. But it will also have many zeros, since the vocabulary is generally very large. Many words don't really occur in any document. Many words will only occasionally occur in the document. A lot of words will be absent in a particular document. So now we have placed the documents and the query in the vector space. Let's look at how we measure the similarity. So a commonly used similarity measure here is dot product. The dot product of two vectors is simply defined as the sum of the products of the corresponding elements of the two vectors. So here we see that it's the product of X1 and Y1. So here. And then X2 * Y2 and then finally XN multiplied by YN and then we take a sum here. So that's the dot product. Now we can represent this in a more general way using a sum here. So this is only one of the many different ways of measuring the similarity. So now we see that we have defined the the dimensions we have defined the vectors and we have also defined the similarity function, so now we finally have the simplest of vector space model. Which is based on the bit vector represntation dot product similarity and bag of words representation. And the formula looks like this. So this is our formula, and that's actually particular retrieval function, a ranking function, right? Now we can find the implement this function using a programming language and then rank documents for query. Now at this point you should again pause the lecture. So think about how we can interpret this score. So we have gone through the process of modeling the retrieval problem. Using a vector space model and then we make assumptions about how we place vectors in the vector space and how we define the similarity. So in the end that we've got a specific retrieval function, shown here. Now the next step is to think about whether this retrieval function actually makes sense. Can we expect this function to actually perform well when we used to rank the documents for users' queries? So it's worth thinking about. What is this value that we'll calculate? So in the end we get a number, but what does this number mean? Is it meaningful? So spend a couple of minutes to think about that. And of course the general question here is: Do you believe this is a good ranking function would they actually work? So again, think about how to interpret this value. Is it actually meaningful? Does it mean something? It's related to how well the document matches the query. So in order to assess whether this simplest vector space model actually works well, let's look at the example. So here I show some sample documents and a simple query. The query is news about the presidential campaign and we have 5 documents here. They cover different terms in the query. And if you look at the these documents for a moment, you may realize that some documents are probably relevant and some others are probably non relevant. Now, if I ask you to rank these documents how would you rank them? This is basically our ideal ranking: when humans can examine the documents and then try to rank them. So think for a moment and take a look at this slide and perhaps by pausing the lecture. So I think most of you would agree that d4 and d3 are probably better than others because they really cover the query well. They match news, presidential and campaign. So it looks like these documents are probably better than the others, so they should be ranked on top. An the other three D2, D1, and D5 are really non relevant, so we can also say differently. D4 and D3 are relevant documents and D1, D2, and D5 are non relevant. So now let's see if our simplest vector space model could do the same or could do something closer. So let's first think about how we actually use this model to score documents. Right here I show 2 documents D1 and D3 and we have the query also here. In the vectors space model of course we want to 1st compute the vectors for these documents and the query. Now I show the vocabulary here as well, so these are the N dimensions that will be thinking about. So what do you think is the vector representation for the query? Note that we are assuming that we only use zero and one to indicate whether the term is absent or present in the query or in the document, so these are 0/1 bit vectors. So what do you think is the query vector? The query has four words here, so for these four words there will be one and for the rest will be 0. Now, what about the documents? It's the same, so T1 has two words news and about. So there are two ones here and the rest of zeros. Similarly. So. Now that we have the two vectors. Let's compute the similarity. And we're going to use dot product so you can see when we use dot product we just multiply the corresponding elements, right? So these two will be. forming, forming product and these two will generate another product and these two will generate yet another product and so on so forth. Now you can easy to see if we do that. We actually don't have to care about. These zeros. Because if whenever we have zero the product will be 0. So when we take a sum over all these pairs then the zero entries will be gone. As long as you have 1 zero then the product will be 0. So in effect we're just counting how many Pairs of one and one, but in this case we have seen two, so the result would be two. So what does that mean? Well, that means this number or the value of this scoring function is simply the count of how many unique query terms are matched in the document. Because if a document, if a term is matched in the document, then there will be 2 ones. If it's not then there will be 0 on the document side. Similarly, if the document has a term, but the term is not in the query, there will be a zero in the query vector, so those don't count. So as a result this scoring function basically matches how many unique query terms are matched in the document. This is how we interpret this score. Now we can also take a look at the D3. In this case you can see the result is 3 because D3 matched three distinct query words: news, presidential, campaign. Whereas D1 only match two. Now in this case, it seems reasonable to rank D3 on top of D1 and this simplest vector space model indeed does that, so that looks pretty good. However, if we examine this model in detail, we likely will find some problems. So here I'm going to show all the scores for these five documents. And you can easily verify their correct because we're basically counting the number of unique query terms matched in each document. Now note that this matching actually that makes sense, right? It basically means if a document matches more unique query terms then the document will be assumed to be more relevant, and that seems to make sense. The only problem is here we can notice that there are three documents D2, D3, and D4 and they tied with a 3 as a score. So that's a problem, because if you look at them carefully, it seems that D4 should be ranked above D3 because D3 only mentioned presidential once, but D4 mentioned it multiple times. In the case of D3 presidential, could be an extended matching. But D4 is clearly about the presidential campaign. Another problem is that D2 and D3 also have the same score. But if you look at the three words that are matched in the case of D2 it matched the news, about, and campaign. But in the case of D3, it matched news, presidential, and campaign. So intuitively D3 is better because matching presidential is more important than matching about even though about and presidential above in the query. So intuitively, we would like D3 we ranked above D2. But this model doesn't do that. So that means this model is still not good enough. We have to solve these problems. To summarize, in this lecture we talked about how to instantiate a vector space model. We may need to do three things. One is to define the dimension. The second is to decide how to place documents as vectors in the vector space. And to also place a query in the vector space as a vector. And 3rd is to define the similarity between two vectors, particularly the query vector and the document vector. We also talk about a very simple way to instantiate vector space model. Indeed, that's probably the simplest vector space model that we can derive. In this case, we use each word to define a dimension. When user 0/1 bit vector to represent a document or a query. In this case, we basically only care about word presence or absence. We ignore the frequency. And we use the dot product as the similarity function. And with such a instantiation, and we showed that the scoring function is basically to score a document based on the number of distinct query words matched in the document. We also show that such a such simple vector space model still doesn't work well and we need to improve it. And this is a topic that we're going to cover in the next lecture. 
In this lecture we are going to talk about how to improve the instantiation of the vector space model. This is the continued discussion of the vector space model. We're going to focus on how to improve the instantiation of this model. In the previous lecture, you have seen that with simple same instantiations of the vector space model. We can come up with a simple scoring function that would give us basically a count of how many unique query terms of matching the document. We also have seen that this function has a problem. As shown on this slide, in particular, if you look at these three documents, they will all get the same score because they matched 3 unique query words. But intuitively we would like D4 to be ranked above D3 and D2 is really non relevant. So the problem here is that this function could not capture. At the following heuristics: First, we would like to give more credit to D4 because it matched the presidential more times than these three; Second, Intuitively, matching presidential should be more important than matching about because about this very common word that occurs everywhere. It doesn't really carry that much content. So in this natural, let's see how we can improve the model to solve these two problems. It's worth thinking at this point about the "why do we have these pro problems?" If we look back at assumptions we have made while instantiating the vector space model, we will realize that the problem is really coming from some of the assumptions. In particular, it has to do with how we place the vectors in the vector space. So then, naturally, in order to fix these problems, we have to revisit those assumptions. Perhaps we will have to use different ways to instantiate the vector space model. In particular, we have to place the vectors in a different way. So let's see how can improve this. One natural thought is, in order to consider multiple times of the term in the document, we should consider the term frequency instead of just absence or presence. In order to consider the difference between a document where aquarium occurred multiple times and one where the query term occurs just once, we have to consider the term frequency: The count of a term in the document. In the simplest model, we only model the presence and absence of the time. We ignore the actual number of times that term occurs in the document. So let's add this back so we can do then represent a document by a vector with term frequency as elements. So that is to say, now the elements of both queries vector and document vector will not be 0 or 1s, but instead they will be the counts of word in the query or the document. So this would bring in additional information about the document. So this can be seen as a more accurate representation of our documents. So now let's see what the formula would look like if we change this representation. So as you see on this slide we still use DOT product, and so the formula looks very similar in the form. In fact it looks identical, but inside the sum of course Xi and Yi are now different, and all the counts of word I in the query and in the document. Now at this point I also suggest you to pause the lecture for a moment and just think about how we can interpret the score of this new function. It's doing something very similar to what the simplest VSM is doing. But because of the change of the vector, now the new score has a different interpretation. Can you see the difference? And it has to do with the consideration of multiple occurrences of the same term in a document. More important, they would like to know whether this would fix the problems of the simplest of vector space model. So let's look at this example again. So suppose we change the vector representation into term frequency vectors. Now let's look at these three documents again. The query vector is the same because all these words occur exactly once in the query, so the vector is 001 vector. And in fact, D2 is also essentially representing the same way, because none of these words has been repeated many times as a result of the score is also the same, still 3. The same goes for D3. And we still have a 3. But D4 would be different. Because now presidential occured twice here. So the element for presidential in the document factor would be 2 instead of 1. As a result, now the score for D4 is high. It's 4 now. So this means, by using term frequency we can now rank D4 above D2 and D3 as we hope to. So this solved the problem with D4. But we can also see that, D2 and D3 are still treated in the same way. They still have identical scores, so it did not fix the problem here. So how can we fix this problem? Intuitively, we would like to give more credit for matching presidential than matching about, but how can we solve the problem in a general way? Is there any way to determine which word should be treated more importantly, and which word can be basically ignored about is such a word at which does not really care that much content? We can essentially ignore that we sometimes call such a word stop word. Those are generally very frequently occur everywhere matching it doesn't really mean anything, but computationally. How can we capture that? So again, I encourage you to think a little bit about this. Can you come up with any statistical approaches to somehow distinguish presidential from about? If you think about it for a moment, you will realize that what differences that words like about occurs everywhere. So if you count the occurrence of the word in the whole collection, then we would see that about has much higher frequency than presidential which tends to occur only in some documents. So this idea suggest that we could somehow use global statistics of terms or some other information to try to down weight the element that for about in the vector representation of D2. At the same time, we hope to somehow increase the weight of presidential in the vector of these three. If we can do that, then we can expect the D2 will get the overall score to be less than 3. While D3 will get the score above 3, then we would be able to rank this lead on top of D2. So how can we do this systematically? Again, we can rely on some statistical counts, and in this case the particular idea is called the inverse document frequency. We have seen document frequency as one signal used in the modern retrieval functions. We discussed this in previous lecture, so here's a specific way of using it. Document frequency is the count of documents that contain a particular term. Here we said inverse document frequency because we actually want to reward a word that doesn't occur in many documents. And so the way to incorporate this into our vector representation is then to modify the frequency count. By multiplying it by the idea of the corresponding word as shown here. If we can do that, then we can penalize common words which generally have a low IDF and reward rare words which will have high IDF. So more specifically, the IDF can be defined as a logarithm of (M + 1) / K, where M is the total number of documents in the collection, K is the DF or document frequency, the total number of documents containing the word W. Now if we plot this function by varying k, then you will see the curve would look like this. In general, you can see it would give a higher value for a low DF word, a rare word. You can also see the maximum value of this function is log of M + 1. Will be interesting for you to think about what's minimum value for this function? This could be interesting exercise. Now a specific function may not be as important as the heuristic to simply penalize popular terms. But it turns out that this particular function form has also worked very well. Now, whether there is a better form of function here is still open research question, but it's also clear that if we use a linear panelization like what's shown here with this line, then it may not be as reasonable as the standard IDF. In particular, you can see the difference. In the standard IDF, and we somehow have a turning point here. After this point they were gonna say these terms are essentially not very useful. They can be essentially ignored and this makes sense when the term occurs so frequently, and let's say a term occurs in more than 50% of the documents, then the term is unlikely very important, and it's basically a common term. It's not very important to match this word, so with the standard idea you can see it's basically assuming that they all have lower weights. There's no difference. But if you look at the linear panelization at this point that there is some difference. So intuitively we want to focus more on the discrimination of Low DF words rather than these common words. Of course, which one works better still has to be validated by using the empirical created dataset and we have to use users to judge which results are better. So now let's see how this can solve problem 2. Alright, so now let's look at the two documents again. Now, without the IDF weighting before, we just have term frequency vectors, but with IDF weighting, we now can adjust the TF weight by multiplying with the IDF value. For example, here we can see is adjustment, and in particular for about there is adjustment by using the IDF value of about which is smaller than the IDF value of presidential. So if you look at these, the IDF will distinguishing these two words as a result of adjustment here would be larger, would make this weight larger. So if we score with these new vectors, then what would happen is that of course they share the same weights for news and campaign. But the matching of about and presidential with discriminate them. So now as a result of IDF weighting, we will have D3 to be ranked above D2 becauses it matched rare word whereas D2 matched common word. So this shows that the IDF weighting can solve problem 2. So how effective is this model in general? When we use TF IDF weighting, let's look at the obvious documents that we have seen before nicely. These are the new scores of the new documents, but how effective is this new weighting method and new scoring function? So now let's see overall how effective is this new ranking function with TF IDF weighting? Here we show all the five documents that we have seen before, and these are their scores. Now we can see the scores for the first 4 documents here seem to be quite reasonable. They are as we expected. However, we also see a new problem. Because Now D5 here, which did not have a very high score with our simplest vector space model, now actually has a very high score. In fact, it has the highest score here. So this creates a new problem. This is actually a common phenomenon in designing retrieval functions. Basically, when you try to fix one problem, you tend to introduce other problems and that's why it's very tricky how to design effective ranking function and what's the best ranking function. Is there open research questions researchers are still working on that? But in the next few lectures, we'll are also gonna talk about some additional ideas to further improve this model and try to fix this problem. So to summarize this lecture we've talked about how to improve the vector space model, and we've got to improve the instantiation of the vector space model based on TF IDF weighting. So the improvement mostly is on the placement of the vector where we give higher weight to a term that occured many times in the document but infrequently in the whole collection. And we have seen that this improvement model indeed works better than the simplest vector space model. But it also still has some problems. In the next lecture, we're going to look at the how to address these additional problems. 
In this lecture we continue the discussion of vector space model. In particular, we're going to talk about the TF transformation. In the previous lecture, we have derived a TF IDF weighting formula using the vector space model. And we have shown that this model actually works pretty well for these examples, as shown on this slide except for D5, which has received very high score. Indeed, it has received the highest score among all these documents, but this document is intuitively non relevant, so this is not desirable. In this lecture we're gonna talk about how we can use TF transformation to solve this problem. Before we discuss the details, let's take a look at the formula for this simple TF IDF weighting ranking function and see why this document has received such a high score. So this is the formula and if you look at the formula carefully, then you will see it involves a sum over all the matched query terms. And inside the sum, each matching query term has a particular weight and this way it is TF IDF weighting. So it has an idea of component where we see two variables. One is the total number of documents in the collection. And that is M. The other is the document frequency. This is the number of documents that contain this word W. The other variables involved in the formula included the count of the query, term Term. W in the query and the count of the word in the document. If you look at this document again, now it's not hard to realize that the reason why it hasn't received the highest score is be cause it has a very high count of campaign. So the count of campaign in this document is a four which is much higher than the other documents and has contributed to the high score of this document. So intuitively, in order to lower the score for this document, we need to somehow restrict the contribution of the matching of this term in the document. And if you think about the matching of terms in a document carefully, you actually would realize. We probably shouldn't reward multiple occurrences. So generously. By that I mean the first occurrence of term says a lot about the matching of this term because it goes from zero count to a count of 1 and that increase means a lot. Once we see a word in the document, it's very likely that the document is talking about this word. If we see a extra occurrence on top of the first occurrence, that is to go from one to two. Then we also can say that was the 2nd occurrence. Kind of confirmed that it's not a accidental matching of the word. Now we are more sure that this document is talking about this word. But imagine we have seen, let's say, 50 times of the word in the document. Then adding one extra occurrence is not good to tell us more about the evidence 'cause we already sure that this document is about this word. So if you think in this way, it seems that we should restrict the contribution of high count of term. And that is the idea of TF transformation. So this transformation function is going to turn the raw count of word into a term frequency, wait for the word in the document. So here I show in X axis the raw count. In Y axis I showed term frequency weight. So in the previous ranking functions we actually have implicitly used some kind of transformation. For example, in the 01 bit vector representation, we actually use researcher transformation function as shown here. Basically, if the count is 0, then it has zero weight, otherwise it would have a weight of 1. It's a flat. Now, what about using term count as TF wait, that's the linear function, right? So it has just exactly the same way as the count. We have just seen that this is not desirable. So what we want is something like this. So for example, with the logarithm function, we can have a sub linear transformation that looks like this and this would control the influence of really high weight because it's going to lower its inference, yet it will retain the inference of small counts. Or we might want to even bend the curve more by applying logarithm twice. Now people have tried all these methods and they are indeed working better than the linear form of the transformation. But so far what works the best seems to be this special transformation called BM25 transformation. BM stands for best matching. Now in this transformation you can see there's a parameter K here. And this K controls the upper bound of this function. It's easy to see this function has upper bound. Because if you look at the x / x + K where K is non negative number then the numerator will never be able to exceed the denominator, right? So it's upper bounded by K + 1. This is also difference between this transformation function and the logarithm transformation which doesn't have upper bound. Furthermore, 1 interesting property of this function is that as we vary K, we can actually simulate a different transformation functions, including the two extremes that I've shown here. That is a 01 bit transformation and the linear transformation. So for example, if we set K to 0 now you can see, the function value would be 1. So we precisely recover the 01 bit transformation. If you set K to a very large number, on the other hand is going to look more like the linear transformation function. So in this sense, this transformation is very flexible. It allows us to control the shape of the transformation. It also has a nice block of the upper bound. And this upper bound is useful to control the influence of a particular time. And so that we can prevent a spammer from just increasing the count of one term to spam all queries that might match this time. In other words, this upper bound might also ensure that all terms would be counted when we aggregate the weights to computer score. As I said, this transformation function has worked well so far. So to summarize this lecture. The main point is that we need to do some linear TV TF transformation, and this is needed to capture the intuition of diminishing return from higher term counts. It's also to avoid dominance by one single term over all others. This BM25 transformation that we talked about is very interesting. It's so far one of the best performing TF transformation formulas. It has upper bound and social robust and effective. If you plug this function into our TF IDF weighting vector space model. then we would end up having the following ranking function which has a BM 25 TF component. Now this is already very close to 2. a state of the art ranking function called BM 25. And will discuss how we can further improve this formula in the next lecture. 
This lecture is about document Length normalization. In the vector space model. In this lecture, we're going to continue the discussion of the vector space model. In particular, we're going to discuss the issue of document length normalization. So far in the lectures about the vector space model, we have used various signals from the document to assess the matching of the document, with the query. In particular, we have considered the term frequency. The count of the term in the document. We have also considered its global statistics. Such as IDF inverse document frequency. But we have not considered document length. So here, I show 2 example documents. D4 is much shorter with only 100 words. D6, on the other hand, has 5000 words. If you look at the matching of these query words, we see that in D6 there are more matchings of the query words. But one might reason that D6 May have matched these query words In a scattered manner. So maybe the topic of D6 is not really about the topic of the query. So the discussion of campaign at the beginning of the document may have nothing to do with the mention of presidential at the end. In general, if you think about long documents, they would have a higher chance to match any query in fact. If you generate a long document randomly by simply sampling words. From a distribution of words, Then eventually you probably will match any query. So in this sense we should penalize long documents because they just naturally have better chances for matching any query. And this is the idea of document length normalization. We also need to be careful in avoiding to over penalize long documents. On the one hand, we want to penalize a long document, but on the other hand we also don't want to over penalize them. And the reason is because a document may be long because of different reasons. In one case, the document may be long because it uses more words. So for example. Think about the full text article of a research paper. It would use more words than the corresponding abstract. So this is the case where we probably should penalize the matching of. Long documents such as full paper. When we compare the matching of words in such a long document with matching of the words in a short abstract. Then long papers generally have higher chance of matching query words. Therefore we should penalize them. However, there is another case when the document is long and that is when the document simply has more content. Now consider another case of a long document where we simply concatenated a lot of abstracts of different papers. In such a case, obviously we don't want to over penalize such a long document. Indeed, we probably don't want to penalize such a document because it's long. So that's why we need to be careful about. Using the right degree of penalization. A method that has been working well based on research results is called pivotal length normalization, and in this case the idea is to use the average document length as a pivot as a reference point. That means we'll assume that for the average length documents, the score is about right, so the normalizer would be 1. But if a document is longer than the average document length, then there will be some penalization, whereas if it's a shorter, then there's even some reward. So this is illustrated using this slide. On the axis, X-axis, you can see the length of document. On the Y axis we show the normalizer in this case, the pivoted length normalization formula for the normalizer is. is seem to be Interpolation of 1 and the normalized document length controlled by a parameter b here. So you can see here, when we first divide the length of the document by the average document length, This not only gives us some sense about how this document is compared with the average document length, but also gives us a. Benefits of not worrying about the unit of. Length, we can measure the length by words or by characters. Anyway, this Normalizer has an interesting property. First we see that if we set the parameter B to 0 then the value would be one, so there's no normalization at all. So b in this sense controls the length normalization. Whereas if we set b to a nonzero value then the normalizer would look like this so the value would be higher for documents that are longer than the average document length. Whereas the value of the normalizer would be smaller for shorter documents. So in this sense we see there is a panelization for long documents. And there is a reward for short documents. The degree of penalization is controlled by b because if we set B to a larger value than the normalizer would look like this, there's even more penalization for long documents and more reward for the short documents. By adjusting b which varies from zero to one, we can control the degree of Length normalization. So if we plug in this length normalization factor into the vector space model ranking functions that we have already examined. Then we will end up having the following formulas. And these are in fact the state of art vector space model formulas. So let's look at this. Take a look at each of them. The first one is called a pivoted length Normalization vector space model. And a reference in the end has details about derivation of this model and here we see that it's basically the TF IDF weighting model that we have discussed. The idea of component should be very familiar now to you. There is also a query term frequency component. Here. And then in the middle, there is the normalized TF. And in this case we see we used a double logarithm. As we discussed before, and this is to achieve a sub linear transformation. But we also put a document length Normalizer in the bottom. right, so this would cause penalization for long document, because the larger the denominator is then the smaller TF weighting is. And this is of course controlled by the parameter b here. And you can see again if b is set to zero and there is no length normalization. OK, so this is one of the two most effective vector space model formulas. The next one, called BM25 or okapi. Is. Also similar in that it also has a IDF of component here. And a query TF component here. But in the middle, the normalization is a little bit different. As we explained there is this. okapi TF transformation here. That does sublinear transformation with the upper bound. In this case, we have put the length normalization. Factor here we are adjusting K, but it achieves a similar factor, just because We put a normalizer in the denominator therefore. Again, if a document is longer than the term weight of this model So you can see after we have gone through all the analysis that we talked about. And we have. In the end, reached basically the state of the art retrieval functions. So. So far we have talked about the mainly how to place the document vector in the vector space. And this has played an important role in determining the effectiveness of the retrieval function. But there are also other dimensions where we did not really examine in detail. For example, can we further improve the instantiation of the dimension of the vector space model? Now We've just assumed that the bag of words representation, so each dimension is the word. But obviously we can consider many other choices. For example, stemmed words. Those are the words that are have been transformed into the same root form. So that the computation and computing will all become the same and they can be matched. We can do stop word removal. This is to remove Some very common words that don't carry any content like "the", "a" or "of". We can use phrases to define dimensions. We can even use latent semantic analysis to find some clusters of words that represent a latent concept as one dimension. We can also use smaller units, like a character N-grams. Those are sequences of N characters for dimensions. However, in practice, people have found that the bag of words representation with phrases is still the most effective one, and it's also efficient. So this is still so far the most popular dimension instantiation method. And it's used in all the major search engines. I should also mention that sometimes we need to do language-specific and domain-specific tokenization, and this is actually very important as we might have variations of terms. That might prevent us from matching them with each other, even though they mean the same thing in some languages like Chinese There is also the challenge in segmenting Text to obtain word boundaries because it's just a sequence of characters. a word might correspond to 1 character or two characters or even 3 characters. So it's Easier in English when we have a space to separate the words, but in some other languages we may need to do some natural language processing to figure out where all the boundaries for words. There is also a possibility to improve the similarity function, and so far we have used the dot product, but one can imagine there are other measures. For example, we can measure the cosine of the angle between two vectors, or we can use Euclidean distance measure. And these are all possible. But dot product seems still the best and one reason is because it's very general. In fact, it's sufficiently general. If you consider the possibilities of doing weighting in different ways. So for example, cosine measure can be regarded as the dot product of two normalized vectors. That means we first normalize each vector and then we take the dot product that would be equivalent to the cosine measure. I just mentioned that the BM 25 seems to be one of the most effective formulas. But there has been also further development in improving BM. 25, Although none of these works have changed the BM25 fundamentally. So in one line work people have derived BM25F. Here F stands for Field and this is to use BM25 for documents with the structures. So for example you might consider Title Field, the abstract or body or the research article or even anchor text. On the web pages, those are the text fields that describe links to other pages, and these can all be combined with appropriate weights of different fields to help improve scoring for a document. When we use BM25 for such a document. And the obvious choice is to apply the BM25 for each field and then combine the scores. Basically the idea of BM25F is to first combine the frequency counts of terms in all the fields. And then apply BM 25. Now this has advantage of avoiding over counting the first occurrence of the term. Remember, in the sub linear transformation of TF, the first occurrence is very important and it contributes a large weight and if we do that for all the fields than the same term might have gained a lot of advantage in every field. But when we combine these word frequencies together, we just do the transformation. A one time at that time, then the extra occurrences will not be counted as fresh first occurrences. And this method has been working very well for scoring structure documents. The other line of extension is called a PM 25 plus. In this line, researchers have addressed the problem of over penalization of long documents by BM 25. So to address this problem, the fix is actually quite simple. We can simply add a small constant to the TF normalization formula, But what's interesting is that we can analytically prove that by doing such a small modification. We will fix the problem of over penalization of long documents by the original BM25 so the new formula, called BM25+. Is empirically an analytically shown to be better than BM25. So to summarize, all what we have said about the vector space model. Here are the major takeaway points. First, in such a model we use the similarity notion relevance, assuming that the relevance of a document with respect to a query is. Basically proportional to the similarity between the query and document, so naturally that implies that the query an document must be represented in the same way, and in this case we represent them as vectors in high dimensional vector space where the dimensions are defined by words or concepts or terms in general. And we generally need to use a lot of heuristics to design the ranking function. We use. Some examples to show the need for several heuristics, including TF weighting and transformation. and IDF weighting and document length normalization. These major heuristics are the most important heuristics to ensure such a general ranking function to work well for all kinds of text. And finally, BM25 and pivoted normalization seems to be the most effective formulas out of the vector space model. Now I have to say that I've put BM 25 in the category of vector space model. But in fact the BM 25 has been derived using probabilistic modeling. So the reason why I've put it in the vector space model is. First, the ranking function actually has a nice interpretation in the vector space model, we can easily see it looks very much like a vector space model with a special weighting function. The second reason is because the original BM25 has somewhat different form of IDF. And that form of IDF actually doesn't really work so well as the standard IDF. That you have seen here, so as effective retrieval function BM25 should probably use a heuristic modification of the IDF to make it even more look like a vector space model. There are some additional readings. The first is a paper about the pivoted length normalization. It's an excellent example of using empirical data analysis to suggest the need for length normalization and then further derived length normalization formula. The second is the original paper where BM25 was proposed. The 3rd paper has a thorough discussion of BM25 and its extensions. Particularly BM25F. And finally, the last paper has a discussion of improving BM25 to correct the over penalization of long documents. 
This lecture is about the implementation of text retrieval systems. In this lecture we will discuss how we can implement text retrieval method to build a search engine. The main challenge is to manage a lot of text data and to enable a query to be answered very quickly and to respond to many queries. This is a typical text retrieval system architecture. We can see the documents are first processor via tokenizer to get that tokenized units, for example words, and then these words or tokens will be processed by a indexer that would create the index which is a data structure for the search engine to use to quickly answer query. And the query will be going through a similar process in step, so that tokenizer would be applied to the query as well so that the text can be processed in the same way the same units will be matched with each other. And the queries representation would then be given to the scorer which would use the index to quickly answer a users query by scoring the documents and then ranking them. The results will be given to the user and then the user can look at the results and provide some feedback that can be expressed judgments about which documents are good, which documents are bad or implicit feedback such as click slows so the user doesn't have to do any, anything extra the user would just look at the results and skip some and click on some results to view. So these interaction signals can be used by the system to improve the ranking accuracy by assuming the view of the documents are better than the skiped ones. So a search engine system then can be divided into 3 parts. The first part is the indexer, and the second part is a scorer that responds to the users query, and the third part is a feedback mechanism. Now typically the indexer is done in the offline manner, so you can preprocess the collected data and to build the inverted index which we will introduce in the moment. And this data structure can then be used by the online module, which is a scorer to process users query dynamically and quickly generate search results. The feedback mechanism can be done online or offline depending on the method. The implementation of the indexer, and the scorer is fairly standard and this is the main topic of this lecture. In the next few lectures, the feedback mechanism, on the other hand, has variations depends on which method is used. So that is usually down in the algorithm specific way. Let's first talk about the tokenizer. Tokenization is to normalize lexical units into the same form so that semantically similar words can be matched with each other. In the language like English. Stemming is often used and this is where map all the inflectional forms of words into the same root form. So for example, computer computation and computing can all be matched to the root form compute. This way all these different forms of computing can be matched with each other. Normally this is good idea to increase the coverage of documents that matched with this query, but it's also not always beneficial because sometimes the subtle of this difference between computer and computation might still suggest the difference in the coverage of the content, but in most cases stemming seems to be beneficial. When we tokenize the text in some other languages, for example Chinese, we might face some special challenges in segmenting the text to find the word boundaries, because it's not obvious where the boundary is as there's no space to separate them. So here, of course we have to use some language specific natural language processing techniques. Once we do tokenization then we would index the text documents and that is to convert the documents into some data structure that can enable fast search. The basic idea is do pre-compute as much as we can basically. So the most commonly used indexes, is called inverted index. And this has been used to in many search engines to support basically search algorithms, sometimes other indices, for example, a document index might be needed in order to support the feedback. Like I said, in this kind of techniques are not really standard in that they vary a lot according to feedback methods. To understand why we want to use inverted index, it would be useful for you to think about how you would respond to a single term query quickly. So if you want to use more time to think about that, pause the video. So think about how you can preprocess the text data so that you can quickly respond to a query with just one word? If you have thought about that question, you might realize that what the best is to simply create a list of documents that match every term in the vocabulary. In this way you can basically pre construct the answers. So when you see a term, you can simply just fetch the ranked list of documents for that term and return the list to the user. So that's the fastest way to respond to a single term query. Now the idea of inverted indexes actually basically like that, we're going to pre construct the such a index that would allow us to quickly find the all the documents that match a particular term. So let's take a look at this example. We have three documents here and these are the documents that you have seen in some previous lectures. Suppose we want to create inverted index for these documents. Then we would maintain a dictionary in the dictionary we will have one entry for each term, and we're going to store some basic statistics about the term. For example, the number of documents that match the term or the total number of frequency of the term, which means we would count the duplicated occurrences of the term. And so, for example news. This term ocurred in all the three documents. So the count of documents is 3. And you might also realize we need this count of documents or document frequency for computing some statistics to be used in the vector space model. Can you think of that? So what weighting heuristic would need this count? That's IDF, right? Inverse document frequency. So IDF is the property over turn and we can compute it right here. So with the document account here, it's easy to compute the IDF, either at this time or when we build index or running time when we see a query. Now, in addition to these basic statistics, we also store all the documents that match the news, and these entries are stored in a file called postings. So in this case it matched three documents and store information about these three documents here. This is the document ID, document one, and the frequency is 1, the TF is 1 for news. In the second document, it's also 1 etc. So from this list that we can get all the documents that match at the term news and we can also know the frequency of news in these documents. So if the query has just one word news and we can easily look up this table to find the entry and go quickly to the postings and fetch all the documents that match news. So let's take a look at another term. This time, let's take a look at the word presidential. This word occured in only one document, document three, so the document frequency is one. But it occurred twice in this document, so the frequency count is 2 and the frequency count is useful in some other retrieval method where we might use the frequency to assess the popularity of a term in the collection, and similarly will have a pointer to the postings here. And in this case there is only one entry here, because, the term occured in just one document, and that's here. The document ID is 3 an it occured it twice. So this is the basic idea of inverted index. It's actually pretty simple, right? With this structure we can easily fetch all the documents that match a term and this will be the basis for scoring documents for a query. Now sometimes we also want to store the positions of these terms. So, in many of these cases, the term occured just once in the document, so there's only one position. For example in this case. But in this case, the time occurred twice, so we will store two positions. Now the position information is very useful for checking whether the matching of query terms is actually within a small window of let's say 5 words or 10 words or whether the matching of the two query terms is in fact a phrase of two words. This can all be checked quickly by using the position information. So why is inverted index good for fast search? We just talked about the possibility of using it to answer a single word query. And that's very easy. What about the multiple term queries? Let's first look at the some special cases of the Boolean query. Boolean query is basically boolean expression like this. So I want the relevant document to match both term A and the term B right? So that's one conjunctive query? Or I want the relevant documents to match term a or term b. That's a disjunctive query. Now how can we answer such a query by using inverted index? If you think a bit about it would be obvious cause we had simply fetch all the documents that match term a, and also fetch all the documents that match term B and then just take the intersection to answer a query A&amp;B or to take the Union to answer the query A or B. So this is all very easy to answer. It's going to be very quick now. What about the multi term keyword query? We talked about vector space model for example and we would match such query with document generated score and the score is based on aggregated term weights. So in this case it's not a Boolean query. But the scoring can be acted out in a similar way. Basically, it's similar to disjunctive Boolean query. Basically, it's like a or b. We take the union of all the documents that match at least one query term, and then we would aggregate the term weights. So this is a basic idea of using that inverted index for scoring documents in general, and we're going to talk about this in more detail later, but for now, let's just look at the question why is inverted index a good idea. Basically why is it more efficient than sequentially just scanning documents? Like, this is the obvious approach. You can just compute the score for each document and then you can then score them. Sorry, you can then sort them. This is a straight forward method, but this is going to be very slow. Imagine the web it has a lot of documents. If you do this then it would take a long time to answer your query. So the question now is why would the inverted index be much faster it has to do with the word distribution in text. So here's some common phenomenon of word distribution in text. There are some language independent patterns that seem to be stable. And these patterns are basically characterized by the following pattern of few words, like the common words the, a, or we occur very frequently in text. So they account for a large percent of occurrences of words. But mostly words would occur just rarely. There are many words that occur just once, let's say in the document or once in the collection. There are many such singletons. It's also true that most frequently words in one corpus may have to be rare in another. That means, although the general phenomenon is applicable or is observed in many cases, the exact words that are common may vary from context to context. So this phenomenon is characterized by what's called Zipf's law. This law says that the rank of word multiplied by the frequency of the word is roughly a constant. So formally, if we use F(w) to denote the frequency, r(w) to denote the rank of a word, then this is the formula. It basically says the same thing, just mathematical term we will see is basically a constant, right? So as so there is also parameter Alpha that might be adjusted to better fit any empirical observations. So if I plot the word frequencies in sorted order, then you can see this more easily. The X axis is basically the world rank and this is r(w), Y axis is a word frequency or F(w). Now, this curve basically shows that the product of the two is roughly the constant. Now if you look at these words, we can see they can be separated into three groups. In the middle it's the intermediate frequency words. These words tend to occur in quite a few documents, but they are not like those most frequent words and they're also not very rare. So they tend to be often used in queries, and they also tend to have high TF IDF weights, these intermediate frequency words. But if you look at the left part of the curve. These are the highest frequency words they occur very frequently. They are usually stop words like the, a, we, of, etc. Those words are very, very frequent. They are in fact too frequent to be discriminated and they are generally not very useful for retrieval. So they, are often removed and this is called stop words removal, so you can use pretty much just the count of words in the collection to kind of infer what words might be stop words. Those are basically the highest frequency words and they also occupy a lot of space in the inverted index. You can imagine the posting entries for such a word would be very long, and therefore if you can remove such words, you can save a lot of space in the inverted index. We also show that the tail part, which has a lot of rare words. Those words don't occur very frequently and there are many such words. Those words are actually very useful for search. Also, if a user happens to be interested in such a topic, but because they're rare. It's often true that the users are unnecessary interested in those words, but retain them would allow us to match such a document accurately, and they generally have very high IDFs. So what kind of data structures should we use to store inverted index? It has two parts, right? If you recall, we have a dictionary and we also have postings. The dictionary has modest size, although for the web it's still going to be very large, but compared with postings, it's modest. And we also need to have fast random access to the entries 'cause we want to look up with a query term very quickly. So therefore we prefer to keep such a dictionary in memory if it's possible, or if the collection is not very large, this is feasible; but the collection is very large, then it's in general not possible if the vocabulary size is very large. Obviously we can't do that so, but in general that's our goal, so the data structures that we often use for storing dictionary would be directly accessed data structures like hash table, or B-tree if we can't store everything in memory, we can use this and try to build a structure that would allow you to quickly look up entries. For postings, they're huge. And in general we don't have to have direct access to a specific entry we generate with. Just look up a sequence of document IDs and frequencies for all the documents that match a query term, so we would read those entries sequentially. And therefore, because it's large, we generally have store postings on disk, so they have to stay on disk. And they would contain information such as document IDs, term frequencies or term positions etc. Now, because they are very large, compression is often desirable. Now this is not only to save disk space, and this is of course one benefit of compression. It's not going to occupy that much space. But it's also to help improving speed. Can you see why? We know that input and output would cost a lot of time in comparison with the time taken by CPU, so CPU is much faster. But IO takes time, and so by compressing the inverted index, the posting files will become smaller and the entries that we have to read into memory to process query time would be smaller and then so we can reduce the amount of trafficing IO and that can save a lot of time. Of course we have to then do more processing of the data when we uncompress the data in the memory. But as I said in the CPU is fast, so overall we can still save time. So compression here is both to save disk space and through speed up the loading of inverted index. 
This lecture is about the inverted index construction. In this lecture we will continue the discussion of system implementation. In particular, we're going to discuss how to construct the inverted index. The construction of the inverted index is actually very easy if the data set is very small, it's very easy to construct dictionary and then store the postings in a file. The problem is that when our data is not able to fit to the memory then we have to use some special methods to deal with it. Unfortunately, in most retrieval applications, that data set would be large and they generally cannot be loaded into memory at once. There are many approaches to solve the problem and sorting based method is quite common. It works in four steps as shown here. First, you collect the local term ID, document ID, and frequency tuples. Basically you will not count the terms in a small set of documents. And then once you collect those counts, you can sort those counts based on terms so that you build a local partial inverted index, and these are called rounds. And then you write them into a temporary file on the disk, and then you merge in step three would do pair-wise merging of these runs until you eventually merge all the runs to generate a single inverted index. So this is an illustration of this method. On the left, you see some documents. And on the right, we have shown Term Lexicon and DocumentID Lexicon. These lexicons are to map stream-based representations of document IDs or terms into integer representations or to map back from integers to the stream representation. The reason why we are interested in using integers to represent these IDs is because integers are often easier to handle. For example, integers can be used as index for array. They are also easy to compress. So this is one reason why we tend to map these strings into integers. So that well, so that we don't have to carry these strings around. So how does this approach work? Well, it's very simple. We're going to scan these documents sequentially, and then parse the document, and count the frequencies of terms. In this stage, we generally sort the frequencies by document IDs because we process each document sequentially. So we first encounter all the terms in the first document. Therefore, the document IDs are all 1s in this case. And this will be followed by document IDs 2s. They are naturally sorted in this order just because we process the data in the sequential order at some point we will run out of memory and then we would have to write them into the disk. But before we do that, we can sort them just use whatever memory we have. We can sort them and then this time we're going to sort based on term IDs. Notice that here we're using the term IDs as a key to sort. So all the entries that share the same term would be grouped together. In this case, we can see IDs of documents that match the term one will be grouped together. And we're going to write this into the disk as a temporary file, and that would allow us to use the memory to process the next batch of documents. And we're going to do that for all the documents. So we are going to write a lot of temporary files into the disk. And then the next stage is merge sort. We are going to merge them and sort them. Eventually we will get a single inverted index with their entries are sorted based on term IDs. And on the top we can see these are the old entries for the documents that match term ID1. So this is basically how we can do the construction of inverted index even though the data cannot be loaded into the memory. Now we mentioned earlier that because of postings are very large, it's desirable to compress them. So let's now talk a little bit about how we compress inverted index. The idea of compression in general is to leverage skewed distributions of values, and we generally have to use variable length encoding instead of the fixed length encoding as we use by default in program languages like C++. So how can we leverage the skewed distributions of values to compress these values? In general, we would use fewer bits to encode those frequently awards at the cost of using longer bits to encode those rare values. So in our case, let's think about how we can compress the TF (term frequency). Now if you can picture what the inverted index would look like, and you see in postings, there are a lot of term frequencies. Those are the frequencies of terms in all those documents. Now, if you think about what kind of values are most frequent there, you probably will be able to guess that small numbers tend to occur far more frequently than large numbers. Why? Think about the distribution of words and this is due to the Zipf's law, and many words occur rarely. So we see a lot of small numbers. Therefore we can use fewer bits for the small but highly frequent integers at the cost of using more bits for large integers. This is a trade off, of course. If the values are distributed uniformly, this one will save us a lot of space. But because we tend to see many small values, they are very frequent. We can save on average, even though sometimes when we see a large number we have to use a lot of bits. What about the document IDs that we also saw in postings? They are not distributed in the skewed way right. So how can we deal with that? Well, it turns out that you can use a trick called d-gap and that is to restore the difference of this term IDs. And we can imagine if a term has matched many documents, then there will be long list of document IDs. So when we take the gap, I'm going to take the difference between adjacent the document IDs. Those gaps will be small, so we will again see a lot of small numbers. Whereas if a term according only a few documents, then the gap would be large. The large numbers will not be frequent. So this creates some skewed distribution that would allow us to compress these values. And this is also possible because in order to uncover or uncompress these document IDs. We have to sequential process the data. Because we store the difference and in order to recover the exact the document ID, we have to 1st recover the previous document IDs and then we can add the difference to the previous document ID to restore the current document ID. Now this was possible because we only need to have sequential access to those documents IDs. Once we look up a term we fetch all the document IDs that match the term. Then we sequentially process them. So it's very natural. That's why this trick works. And there are many different methods for encoding. For example, the binary code is a commonly used code in programming languages where we use basically fixed length encoding. Unary code, Gamma code, and Delta code are all possibilities and there are many other possibilities. So let's look at some of them in more detail. Binary coding is really equal length encoding, and that's our property for randomly distributed values. The unary coding is a variable length encoding method. In this case, an integer that's at least one would be encoded as x-1, one bits followed by zero. So for example, three would be encoded as two ones, followed by zero, whereas five will be encoded as 4 ones, followed by zero etc. So now you can imagine how many bits do we have to use for a large number like 100. So how many bits do we have to use exactly the full number like 100? Exactly, we have to use 100 bits, right? So it's the same number of bits as the value of this number, so this is very inefficient. If you will likely see some large numbers, imagine if you occasionally see a number like 1000. You have to use 1000 bits, so this only works well if you are absolutely sure that there will be no large numbers, mostly very, very often using very small numbers. How do you decode this code? Since these are variable length encoding methods and you can't just count how many bits, and then they just stop. You can say 8 bits or 32 bits, then you will start another code. There are variable lengths, so you'll have to rely on some mechanism. In this case, for unary you can see it's very easy to see the boundary. Now you can easily see zero with signal, the end of encoding. So you just count how many ones you have seen and until you hit zero you have finished one number. You will start another number. Now we just saw that the unary coding is too aggressive in rewarding small numbers. If you occasionally, you can see a very big number. It would be a disaster. So what about some other, less aggressive method? Well, Gamma coding is one of them. And in this method we are going to use unary coding for a transformed form of the value, so it's one plus the floor of the log of X. So the magnitude of this value is much lower than the original X. So that's why we can afford using unary code for that. So we will first have the Unary code for this log of X. This will be followed by a uniform code or binary code, and this is basically the same uniform code and binary code are the same, and we're going to use this code to code the remaining part of the value of X. And this is basically precise X - 1 to the floor of the log of X. So the unary code basically coded the floor of log of X, add one there. But the remaining part will be using uniform code to actually code the difference between the X and this two to the log of X. And it's easy to show that for this value this difference, we only need to use up to this many bits and floor of log of X bits. This is easy to understand, if the difference is too large then we would have a higher floor of log of X. So here are some examples. For example, 3 is encoded as 101. The first 2 digits are the unary code, right. So this is for the value 2. 10 encodes 2 in unary coding. And so that means log of X, the floor of log of X is 1 because we want actually use unary code to encode one plus the floor of log of X. Since this is 2, then we know that the floor of log of X is actually one. But three is still larger than two to the one, so the difference is 1, and then one is encoded here at the end. So that's why we have 101 for 3. Similarly, 5 is encoded as 110 followed by 01 and in this case, the unary code encodes 3. And so this is the unary code 110. And so the floor of log of X is 2. And that means we are going to compute the difference between 5 and 2 to the two, and that's one, and so we now have again one at the end. But this time we're going to use 2 bits, cause with this level floor of log of X we could have more numbers 5, 6, 7 they would all share the same prefix here,110. So in order to differentiate them we have to use 2 bits in the end to differentiate them. So you can imagine six would be 10 here in the end instead of 01 after 110. It's also true that the form of gamma code is always: first odd number of bits and in the center in there is a zero. That's the end of the unary code. And before that, on the left side of this zero, there will be all ones an on the right side of this zero, it's binary coding or uniform coding. So how can you decode such a code? You again first do unary coding right Once you hit zero, you have got the unary code. And this also would tell you how many bits you have to read further to decode the uniform code. So this is how you can decode Gamma code. There was also the error code that's basically the same as gamma code, except that you replace the unary prefix with the gamma code, so that's even less conservative than gamma code in terms of rewarding the small integers. So that means it's OK if you occasionally see a large number. It's OK with delta code. It's also fine with gamma code, it's really a big loss for Unary code and they are all operating of course at different degrees of favoring small integers. And that also means. They would be appropriate for a certain distribution, but none of them is perfect for all distributions. Which method works the best with have to depend on the actual distribution in your data set. For inverted index compression, people have found that gamma coding seems to work well. So how do I uncompressed invert index when we just talked about this first, you decode those encode integers and we just I think discussed how we decode unary coding and gamma coding. So I won't repeat. And what about the document IDs that might be compressed using d-gap. We're going to do sequential decoding. So suppose the encoded ID list is x1, x2, x3, etc. We first decode x1 to obtain the first document ID ID1. Then we would decode x2, which is actually the difference between the second ID and the first one. So we have to add the decoder value of x2 to ID1 to recover the value of the ID at this second position, right. So this is where you can see the advantage of converting document IDs into integers, and that allows us to do this kind of compression and we just repeat until we decode all the documents every time we use the document ID in the previous position to help you recover the document ID in the next position. 
This lecture is about how to do fast search by using inverted index. In this lecture, we're going to continue the discussion of system implementation. In particular, we're going to talk about how to support fast search by using inverted index. So let's think about what a general scoring function might look like. Now of course, the vector space model is a special case of this, but we can imagine many other retrieval functions of the same form. So the form of this function is as follows. We see this scoring function of document d and query q is defined as first a function of f(a). That's adjustment function that would consider two factors that are shown here at the end f sub d of (d) and f sub q of (q). These are adjustment factors of document and query so they are at the level of a document and query. And then inside of this function we also see there's another function called h. So this is the main part of the scoring function. And these as I just said, are the scoring factors at the level of the whole document and query. For example document lengths. And this aggregate functioning would then combine all these. Now inside this edge function there are functions that would compute the weights of the contribution of a matched query term t(i). So this is g. The function g gives us the weight of a match query term t(i) in document d. And this h function would then aggregate all these weights, so it will for example, take a sum of all the matched query terms. But it can also be a product or could be another way of aggregating them. And then finally, this adjustment function would then consider the document level or query level factors to further adjust the score. For example, documents normalization. So this general form would cover many state of the art retrieval functions. Let's look at how we can score documents with such a function using inverted index. So here's a general algorithm that works as follows. First these query level and document level factors can be precomputed in the indexing time. Of course, for the query we have the computed at query time, but for document, for example, document lengths can be precomputed. And then we'll maintain a score accumulator for each document d to compute h. And h is the aggregation function of all the matched query terms. So how do we do that? Well, for each query term we're going to fetch the inverted list from the inverted index. This would give us all the documents that match this query term. And that includes d1, f1 through dn, fn. So each pair is document ID and the frequency of the term in the document. Then for each entry dj and fj are particular match of the term in this particular document dj. We're going to compute the function g. That would give us something like a TF if weights of this term. So we'll compute the weighted contribution of matching this query term in this document. And then we're going to update the score accumulator for this document. And this would allow us to add this to a accumulator that would incrementally compute the function h. So this is basically a general way to allow us to do computer all functions of this form by using inverted index. Note that we don't have to touch any document that didn't match any query term. But this is why it's fast. We only need to process the document that matched at least one query term. In the end, then, we're going to adjust the score to compute this function Fa, and then we can sort. So let's take a look at the specific example. In this case, let's assume the scoring function is very simple while it just takes the sum of TF. The raw TF. The count of a term in the document. Now this simplification would help showing the algorithm clearly it's very easy to extend the computation to include other weights, like the transformation of TF or document length normalization or IDF weighting. So let's take a look at a specific example where the queries information security. And I show some entries of the inverted index on the right side information occurred four documents and their frequencies. Also their security occurred in three documents. So let's see how the algorithm works. So first we iterate over all the query terms. An we fetch the first query them what is that? That's information. Imagine we have all these score accumulators to store the scores for these documents. We can imagine there will be allocated but then they will only be allocated as needed. So before we do any weighting of terms, we don't even need a score accumulator. But conceptually we have these score accumulatorseventually allocated. Let's fetch the entries from the inverted list for information first. That's the first one. So these score accumulators obviously will be initialized at 0. So the first answer is d1 and 3. 3 is the occurrences of information in this document. Since our scoring function assumes that the score is just a sum of these raw counts, we just need to add 3 to the score accumulator to account for the increase of score due to matching this term information in document d1. And then we go to the next entry. That's d2 and 4 and then we added 4 to the score accumulator of d2. Of course, at this point that we will allocated the score accumulator as needed. And so, at this point we allocated d1 and d2. The next is d3 and we add 1. We allocate another score cumulative for d3 and add 1 to it. And then finding the d4 gets a 5 because the term information occurred five times in this document. OK, so this completes the processing of all the entries in the inverted index for information. It processed all the contributions of matching information in these four documents. So now our algorithm will go to the next query term that security. So we're going to fetch all the inverted index entries for security. So in this case there are three entries and we're going to go through each of them. The first is d2 and 3, and that means security occurs three times in d2. And what do we do? Well, we do exactly the same as what we did for information, so this time we're going to change the score accumulator d2, since it's already allocated. And what we do is to add 3 to the existing value which is 4. So we now get the 7 for d2. d2 score is increased because it matched both the information and security. Go to the next entry. That's d4 and 1 so we would update the score for d4 and again we add 1 to d4. So d4 now goes from 5 to 6. Finally, we process d5 and 3. Since we have not yet allocated a score accumulator for d5. At this point, we're going to allocate 1 for d5, and we're going to add 3 to it, So those scores on the last row are the final scores for these documents if our scoring function is just a simple sum of TF values. Now, what if we actually would like to do length normalization? We can do the normalization at this point for each document. So to summarize this you can see we first processed the query term information. We processed all the entries in the inverted index for this term. Then we processed the security. It's worth thinking about what should be the order of processing here. when we consider query terms. It might make difference, especially if we don't want to keep all the score accumulators. Let's say we only want to keep the most promising score accumulators. What do you think it would be a good order to go through? Would you go. Would you process a common term first or would you process a rare term first? The answer is we should process the rare term first. A rare term would match fewer documents and then the score contribution would be higher because the idea of value will be higher. And then it allows us to touch the most promising documents first, so it helps pruning some non promising ones if we don't need to so many documents to be returned to the user. Right, so those are all heuristics for further improving the accuracy here. You can also see how we can incorporate the IDF weighting so they can easily be incorporated when we process each query term. When we fetch the inverted index, we can fetch the document frequency and then we can compute the IDF. Or maybe perhaps the IDF value has already been precomputed. When we index the documents at that time, we already computed the IDF value that we can just fetch it. So all these can be done at this time, so that would mean when we process all the entries for information, these weights will be adjusted by the same IDF, which is IDF for information. So this is the basic idea of using inverted index for faster search and it works well for all kinds of formulas that are of the general form. This general form covers actually most state of the art retrieval functions. So there are some tricks to further improve the efficiency. Some general techniques include the caching. This is just to store some results of popular queries so that next time when you see the same query, you simply return the stored results. Similarly, you can also store the list of inverted index in the memory for popular term and if the query terms are popular, likely you will soon need to fetch the inverted index for the same term again. So keeping them in the memory would help and these are general techniques for improving efficiency. We can also keep only the most promising accumulators because a user generally doesn't want to examine so many documents. We only need to return high quality subset of documents that likely are ranked on the top. For that purpose, we can then prune the accumulators. We don't have to store all the accumulators. At some point, we just keep the highest value accumulators. Another technique is to do parallel processing and that's needed for really processing such a large data set like the web data set and to scale up to the web scale, we need a special to have special techniques to do parallel processing and to distribute the storage of files on multiple machines. So here, as here's a list of some text retrieval tool kits, it's not a complete list. You can find more information at this URL on the bottom. Here is the four here Lucene is one of the most popular toolkits that can support a lot of applications and it has very nice support for applications. You can use it to build a search engine application very quickly. The downside is that it's not that easy to extend it and algorithms implemented there also not the most advanced algorithms. Lemur/Indri is another tool kit that does not have such a nice support for application as lucene, but it has many advanced search algorithms. And it's also easy to extend. Terrier is yet another tool kit that also has good support for application capability and some advanced algorithms, so that's maybe in between Lemur or Lucene or maybe rather combining the strength of both, so that's also a useful tool kit. MeTA is the tool kit that we will use for the programming assignment and this is a new tool kit that has a combination of both text retrieval algorithms and text mining algorithms. And so topic of all those models are implemented there. There are a number of text analysis algorithms implemented in the toolkit as well as basic search algorithms. So to summarize, all the discussion about the system implementation, here are the major takeaway points. Inverted index is the primary data structure for supporting a search engine. That's the key to enable faster response to a user's query. And the basic idea is to preprocess the data as much as we can, and we want to do compression when appropriate so that we can save disk space and can speed up IO and processing of inverted index. In general we talked about how to construct the inverted index when the data can't fit into the memory, and then we talk about the fast search using inverted index. Basically to exploit the inverted index to accumulate the scores for documents matching or query term. And we explore the Zipf's law to avoid attaching many documents that don't match any query term. And this algorithm can for his support a wide range of ranking algorithms. So these basic techniques have great potential for further scaling up using distributed file system, parallel processing and caching. Here are two additional readings that you can take a look if you have time and you're interested in learning more about this. The first one is a classic textbook about the efficiency of Inverted index and compression techniques and how to in general build efficient search engine in terms of the space, overhead and speed. The second one is a newer textbook that has a nice discussion of implementing and evaluating search engines. 
This lecture is about the evaluation of text retrieval systems. In the previous lectures we have talked about a number of text retrieval methods, different kinds of ranking functions. But how do we know which one works the best? In order to answer this question, we have to compare them and that means we have to evaluate these retrieval methods. So this is the main topic of this lecture. First, let's think about the why do we have to do evaluation? I already give one reason and that is we have to use evaluation to figure out which retrieval method works better. Now this is very important for advancing our knowledge, otherwise we wouldn't know whether a new idea works better than old idea. In the beginning of this course, we talked about the problem of text retrieval. We compared it with database retrieval. There we mentioned that text retrieval is empirically defined problem. So evaluation must rely on users. Which system works better would have to be judged by our users So this becomes a very challenging problem. Because. How can we get users involved in the evaluation? How can we do a fair comparison of different methods? So just go back to the reasons for evaluation. I listed two reasons here. The second reason is basically what I just said, but there is also another reason which is to assess the actual utility of text retrieval system. Now imagine you're building your own search engine applications. It would be interested in knowing how well your search engine works for your users. So in this case, matches must reflect the utility to the actual users in a real application. And typically this has to be done by using user studies and using the real search engine. In the second case, or for the second reason. The measures actually only to be correlated with the utility to actual users. Thus they don't have to accurately reflect the exact utility to users. So the measure only needs to be good enough to tell which method works better. And this is usually done through a test collection, and this is the main idea that we'll be talking about in this course. This has been very important for comparing different algorithms and for improving search engine system in general. So next we talk about what to measure right? There are many aspects of a search engine that we can measure we can evaluate. And here I listed the three major aspects. One is effectiveness or accuracy. How accurate the other search results. In this case, we're measuring systems capability of ranking relevant documents on top of non random ones. The second is efficiency. How quickly can a user get some results? How much computing resources are needed to answer query? So in this case we need to measure the space and time overhead of the system. The third aspect is usability. Basically, the question is how useful is a system for real user tasks. Here, obviously interfaces and many other things are also important, and we typically would have to do user studies. Now in this course we are going to talk mostly about effectiveness and accuracy measures because the efficiency and usability dimensions are not really unique to search engines and so. They are needed for evaluating any other software systems, and there is also good coverage of such materials in other courses. But how to evaluate a search engines quality or accuracy is something unique to text retrieval, and we're going to talk a lot about this. The main idea that people have proposed for using a test set to evaluate text retrieval algorithm is called the Cranfield evaluation methodology. This one actually was developed a long time ago, developed in 1960s. It's a methodology for laboratory test. Of system components, it's actually methodology that has been very useful not just for search engine evaluation, but also for evaluating virtually all kinds of empirical tasks. And for example, in natural language processing or in other fields where the problem is empirically defined, we typically would need to use such a methodology. And today with the Big Data Challenge with use of machine learning everywhere, this methodology has been very popular, but it was first developed for search engine application in 1960s. So the basic idea of this approach is to build a reusable test collections and define measures. One such a test collection is build. It can be used again and again to test the different algorithms, and we're going to define measures that would allow you to quantify the performance of a system or an algorithm. So how exactly would this work? We're going to have a sample collection of documents and this is just to simulate the real document collection in search application. We can also have a sample set of queries or topics. This is to simulate users queries. Then we'll have to have relevance judgments. These are judgments of which documents should be returned for which queries. Ideally they have to be made by users who formulated the queries, 'cause those are the people that know exactly what documents would be useful, and then finally we have to have measures to quantify how well systems result matches the ideal ranked list that would be constructed based on users relevance judgments. So this methodology is very useful for starting retrieval algorithms because the tested connection can be reused many times and it would also provide a fair comparison for all the methods. We have the same criteria, same data set to be used to compare different algorithms. This allows us to compare a new algorithm with an older algorithm that was developed many years ago by using the same standard. So this is an illustration of how this works. So as I said, we need the queries that are shown here. We have Q1Q2, etc. We also need the documents that's called a document collection and on the right side you see we need relevance judgments. These are basically. The binary judgments of documents with respect to a query. So, for example d1 is judged as being relevant to Q1, D2 is judged as being relevant as well. And d3 is judged as non relevant. The two, Q1, etc. These would be created by users. But once we have these and then we basically have a text collection and then if you have two systems you want to compare them then you can just run each system on these queries and documents and each system would then return results. Let's say if the query is Q1 and then we would have results. Here I show R sub A as results from system A. So this is remember we talked about. Task of computing approximation of the relevant document set R sub A is system A's approximation here. And R sub B is system B's approximation of relevant documents. Now let's take a look at these results. So which is better now? Imagine for a user, which one would you like? Now let's take a look at the both results. And there are some differences, and there are some documents that are returned by both systems. But if you look at the results, you would feel that well, maybe A is better in the sense that we don't have many non relevant documents and among the three documents returned, two of them are relevant, so that's good, it's precise. On the other hand, one can also say, maybe B is better because we've got more relevant documents. We've got 3 instead of two. So which one is better and how do we quantify this? Obviously this question highly depends on the users task and it depends on users as well. You might be able to imagine for some users may be system A is better. If the user is not interested in getting all the relevant document. But in this case, the user doesn't have to read many and the user would see most of the relevant documents. On the one hand, one can also imagine the user might need to have as many relevant documents as possible. For example, if you are doing a literature survey, you might be in the segment category and you might find that system B is better. So in that case we will have to also define measures to quantify them. And we might need to define multiple measures. Because users have different perspectives of looking at the results. 
This lecture is about the basic measures for evaluation of text retrieval systems. In this lecture we're going to discuss how we design basic measures. To quantitatively compared to retrieval systems. This is a slide that you have seen earlier in the lecture where we talked about. Cranfield evaluation methodology. We can have a test collection that consists of queries, documents and relevance judgments. We can then run two systems on these datasets to quantitatively evaluate their performance. And we raised the question about which set of results is better. Is system a better or system B better? So let's now talk about how to actually quantify their performance. Suppose we have a total of 10 relevant documents in the collection for this query. Now the relevance judgments shown on the right. Did not include all the ten, obviously and we have only seen three relevant documents there, but we can imagine there are other relevant documents in judge before this query. So now intuitively we thought that system A is better because it did not have much noise, and in particular we have seen that among the three results, two of them are relevant. But in system B. We have 5 results and only three of them are relevant. So intuitively it looks like system A is more accurate and this intuition can be captured by a measure called precision where we simply compute: to what extent, all the retrieval results are relevant if you have 100% precision, that would mean all the retrieval documents are relevant. So in this case the system A has a precision of two out of three, system B has 3 / 5. And this shows that system A is better by precision. But we also talked about System B might be preferred by some other users who like to retrieve as many relevant documents as possible. So in that case will have to compare the number of relevant documents, then retrieve and there is another measure called recall. This measures the completeness of coverage of relevant documents in your retrieval. Result, so we just assume that there are 10 relevant documents in the collection. An here we've got two of them in system A, so the record is 2 out of 10 Whereas System B has got three. So it's a 3 out of 10. Now we can see by recall system B is better and these two measures turn out to be the very basic measures for evaluating search engines, and they are very important because they also widely used in. Many other task evaluation problems, for example, if you look at the applications of machine learning, you tend to see precision recall numbers being reported for all kinds of tasks. OK, so now let's define these two measures more precisely, and these measures are to evaluate a set of retrieval documents. So that means we are considering that approximation of the set of relevant documents. We can distinguish it into 4 cases depending on the situation of the document. A document that can be retrieved or not retrieved, right? Because we're talking about the set of results. A document can be also relevant or non relevant depending on whether the user thinks this is useful document. So we can now have counts of documents in each of the four categories. We can have A to represent the number of documents that are retrieved and relevant. B for documents that are not retrieved but relevant, etc. Now with this table, then we could define precision as the. Ratio of. The relevant retrieved documents A to the total number of retrieval documents, so this is just A divided by the sum of A&amp;C. sum of this column. Similarly, recall is defined by dividing A by the sum of A&amp;B, so that's again the divide A by the sum of the rule instead of the column. Right, so we can see precision and recall is all focused on looking at A. That's the number of retrieval relevant documents. But we're going to use different denominators. OK, So what would be an ideal result? You can easily see in the ideal case we have precision and recall, or to be 1.0 that means we have got 1% of all the relevant documents in our results and all the results that we return are relevant. At least there's no single nonrelevant document in return. In reality, however, high record tends to be associated with low precision. And you can imagine why that's the case as you go down the list to try to get as many relevant documents as possible, you tend to encounter a lot of non relevant documents, so the precision would go down. Note that this set can also be defined by a cut off in the ranked list. That's why although these two measures are defined for a set of retrieval documents, they are actually very useful for evaluating a ranked list. There are fundamental measures in text retrieval and many other tasks. We often are interested in the precision at 10 documents for web search. This means we look at the how many documents among the top 10 results are actually relevant. Now this is a very meaningful measure because it tells us how many relevant documents the User can expect to see on the first page of search results where they typically show 10 results. So precision and recall are the basic measures and we need to use them to further evaluate search engine. But there are the building blocks really. We just said that there tends to be a tradeoff between precision and recall, so naturally it would be interesting to combine them. And here's one measure that's often used called F measure. And, it's a harmonic mean of precision and recall is defined on this slide. So you can see. It first. Compute the. Inverse of R and P here and then it would interpret the two by using the coefficients. Depending on a parameter beta. And after some transformation you can easily see it would be of this form. And in any case, this is just a combination of precision and recall. Beta is a parameter that's often set to one. It can control the emphasis on precision or recall when we set Beta to one, we end up having a special case of F measure, often called F1. This is a popular measure that's often used little combine precision and recall, and formula looks very simple. It's just this here. Now it's easy to see that if you have a larger precision or larger recall than F measure would be high. But what's interesting is that. The tradeoff between precision and recall is captured in the interesting way in F1. So in order to understand that, we can first look at the natural question, why not just combine them using a simple arithmetic mean as shown here? That would be likely the most natural way of combining them. So what do you think? If you want to think more, you can pause the video. So why is this not as good as F1? Or what's the problem with this? Now. If you think about the arithmetic mean, you can see this is the sum of multiple terms. In this case is a sum of precision and recall. In the case of a sum, the total value tends to be dominated by the large values. That means if you have a very high P or very high R, then you really don't care about the weather. The other value is low, so the whole sum would be high. Now this is not desirable because one can easily have a perfect recall. We can have perfect recall easily. Can you imagine how? It's probably very easy to imagine that we simply retrieve all the document in the collection. Then we have a perfect recall. And this will give us point of five as the average. But such results are clearly not very useful for users, even though the average using this formula would be relatively high. But in contrast, you can see F1 would reward the case where precision and recall are roughly similar, so it would penalize a case where you have extremely high value for one of them. So this means F1 encodes different. tradeoffs between them. For this example, shows actually a very important methodology here. When you try to solve a problem. You might naturally think of 1 solution, let's say in this case it's this arithmetic mean. But it's important not to settle on this solution. It's important to think whether you have other ways to combine them. And once you think about multiple variants, it's important to analyze their difference. And then think about which one makes more sense in this case. If you think more carefully, you will feel that F1 probably makes more sense than the simple arithmetic mean, although in other cases there may be different results, but in this case the arithmetic mean seems not reasonable. But if you don't pay attention to these subtle differences, you might just take a easy way to combine them and then go ahead with it. And there later you will find that measure. It doesn't seem to work well. so at this methodology is actually very important in general in solving problem and try to think about the best solution. Try to understand the problem very well and then know why you need this measure and why you need to combine precision and recall and then use that to guide you in finding a good way to solve the problem. To summarize, we talked about precision. Which addresses the question. Are the retrieval results all relevant. We also talked about the recall. Which addresses the question have all the relevant documents being retrieved. These two are the two basic measures in text retrieval evaluation. They are useful for many other tasks as well. We talked about F measure as a way to combine precision and recall. We also talked about the tradeoff between precision and recall and. This turns out to depend on the users search tasks and will discuss this point more in the later lecture. 
This lecture is about how we can evaluate a ranked list. In this lecture we will continue the discussion of evaluation. In particular, we're going to look at how we can evaluate the ranked list of results. In the previous lecture we talked about. Precision and Recall. These are the two basic measures for quantitatively measuring the performance of search result. But as we talked about Ranking before, we framed the tax retrieval problem as a ranking problem. So, we also need to evaluate the quality of a ranked list. How can we use precision and recall to evaluate a ranked list? Naturally, we have to look at the precision and recall at different cut offs because in the end the approximation of relevant documents set given by a ranked list is determined by where the user stops browsing, right? If we assume the user sequentially browses the list of results, the user would stop at some point and that point will determine the set, and then that's the most important cut off that will have to consider when we compute the precision recall without knowing where exactly the user would stop, then we have to consider all the positions where the user could stop. So let's look at these positions. Look at this slide and then let's look at the what if the user stops at the first document? What's the precision and recall at this point? What do you think? It's easy to see, that this document is relevant, so the Precision is one out of one. We have got one document and that's relevant. What about the recall ? Note that we assume that there are 10 relevant documents for this query in the collection, so it's one out of 10. What if the user stops at the second position? Top 2. The precision is the same - 100% two out of two in the record 2 out of 10. What if the user stops at third position? Well, this is interesting because in this case we have not got any additional relevant document. So the recall doesn't change. But the precision is lower because we've now got a random number. So what exactly the Precision? It's two out of three, right? And recall is the same - 2 out of 10. So when would we see another point where the recall would be different? Now if you look down the list it won't happen until we have seen another relevant document. In this case D5. At that point, the recall is increased to three out of 10. And the precision is a three out of five. So you can see if we keep doing this we can also get to D8 and then we will have a Precision of four out of eight. Because there are eight documents, and four of them are relevant and the recall is a four out of 10. Now when can we get a recall of five out of 10? Well, in this list we don't have it. So we have to go down on the list. We don't know where it is. But as a convenience, we often assume that the precision is 0. The precision is zero at all the other levels of Recall that are beyond the search results. So of course this is a pessimistic assumption. The actual precision would be higher, but we may make this assumption in order to have an easy way to compute another measure called average precision that we will discuss later. Now I should also say now here you see, we make these assumptions that are clearly not accurate. But this is usually OK for the purpose of comparing to text retrieval methods, and this is for the relative comparison, so it's OK if the actual measure or actually actual number deviates a little bit from the true number as soon as the deviation is not biased toward any particular retrieval method and we are OK, we can still accurately tell which method works better, and this is an important point to keep in mind. When you compare different algorithms, the keys to avoid any bias toward each method, and as long as you can avoid that, it's OK if you do transformation of these measures in anyway, so you can preserve the order. OK, so we just talked about that we can get a lot of precision recall numbers at different positions, so now you can imagine we can plot a curve and this just shows on the X axis we show recalls. And on the Y axis we show the precision. So the precision levels are marked as 0.1, 0.2, 0.3 and 1.0 . So this is the different levels of recall. And the Y axis also has different amounts that for precision. So we plotted these precision recall numbers that we have got as points on this picture. Now we can further link these points to form a curve as you see, we assumed that all the other precision that start high level records to be 0 and that's why they are down here, right, So they are zero in this. The actual curve probably will be something like this, but as we just discussed, it doesn't matter that much for comparing two methods. 'cause this would be an underestimate for all the methods. OK, so now that we have this precision recall curve, how can we compare 2 ranked lists right? So that means we have to compare two PR curves. And here I show 2 cases where the system A is shown in red, System B is showing blue with crosses. Alright, so which one is better? I hope you can see here System A is clearly better. Why? Because for the same level of recall, it's the same level of recall here, and you can see the precision point by system is better than system B, so there's no question. Indeed, you can imagine what does the curve look like for ideal search system. It has to have perfect precision at all the recall points, so it has to be this line. That would be the ideal system. In general. The higher the curve is, the better, right The problem is that we might see a case like this actually happens often like the two curves across each other. Now in this case, which one is better? What do you think? Now this is a real problem that you actually might face. Suppose you build a search engine and you have old algorithm that's shown here in blue or system B and you have come up with a new idea and you test it and the results are shown in red curve A. Now your question is - is your new method better than the older method? Or more practically, do you have to replace the algorithm that you are already using your in your search engine with another new algorithm? So should we use system method A to replace method B? This is going to be a real decision that you have to make. If you make the replacement, the search engine would behave like system made here, whereas if you don't do that, It will be like a system B. So what do you do? Now, if you want to spend more time to think about this, pause the video and it's after a very useful to think about that. As I said, it's a real decision that you have to make if you are building your own search engine, or if you're working for a company that cares about search. Now if you have thought about this for a moment, you might realize that, well in this case it's hard to say there was. Some users might like system A, some users might like system B. What's the difference here? The difference is just that in the low level of recall in this region, system B is better, there's higher precision, but in high recall reading system A is better. Now, so that also means it depends on whether the user cares about the high recall or low recall, but high Precision. And imagine if someone is just going to check out what's happening today and you want to find some random in the news. Which one is better? What do you think? In this case, clearly system B is better because the user is unlikely examining a lot of results. The user doesn't care about high recall. On the other hand, if you think about a case where a user is doing, it's a literature survey, you're starting a problem. You want to find whether your idea has been started before. In that case, you emphasize high recall, so you want to see as many relevant documents as possible. Therefore, you might favor system A. So that means which one is better actually depends on users, and more precisely user's task. So this means you may not necessarily be able to come up with one number that would accurately depict the performance. You have to look at the overall picture yet as I said, when you have a practical decision to make whether you replace the algorithm with another, then you may have to actually come up with a single number to quantify each method. Or when we compare many different methods in research, ideally we have one number to compare them with, so that we can easily make a lot of comparisons. So for all these reasons it's desirable to have one single number to measure that. So how do we do that? And that needs a number to summarize a range. So here again, it's the precision recall curve, right. And one way to summarize this whole ranked list for this whole curve is look at the area underneath the curve. Right, so this is one way to measure that, there are other ways to measure that, but it just turns out that this particular way of measuring it has been very popular and has been used since a long time ago for text retrieval evaluation. And this is basically computed in this way, and it's called Average Precision. Basically, we're going to take a look at every different recall point. And then look after the precision. So we this is one precision and this is another with different recall. Now this we don't count this one because the recall level is the same. An we can do then look at this number and that's the precision at a different recall level, etc. So we have all these, add it up. These are the provisions that had the different points corresponding to retrieving the first relevant document. The 2nd and then the third, the fourth, etc. Now we missed the mini random documents. So in all those cases we just assumed they have zero precisions. And then finally we take the average. So we divided by 10 and which is a total number of relevant documents in the collection. Note that here we are not dividing this sum by 4, which is a number of retrieved relevant documents. Now imagine if I divide by 4, what would happen? Now think about this for a moment. It's a common mistake that people sometimes overlook. So if we you divide this by 4, it's actually not very good. In fact, you are favoring a system that would retrieve very few rather than documents, as in that case the denominator would be very small, so this would be not a good measure. So note that this dinominator is 10. The total number of relevant documents. And this will basically compute the area underneath the curve. And this is the standard method used for evaluating a ranked list. Note that it actually combines recall and precision, but first we have precision numbers here. But second, that we also consider recall because if you miss the many, there will be many zeros here. So it combines precision and recall, and furthermore you can see this measure is sensitive to a small change of a position of a relevant document. Let's say if I move this relevant document up a little bit, now it would increase this average precision, whereas if I move any relevant document down, let's say I move this random document down, then it would decrease the average precision. So this is very good because it's a very sensitive to the ranking of every relevant document. It can tell small differences between 2 ranked lists and that's what we want. Sometimes one algorithm only works slightly better than another, and we want to see this difference. In contrast, if we look at the precision at the 10 documents. If you look at this whole set. What's the precision? What do you think? Well, it's easy to see. That's four out of 10, right? So that precision is very meaningful because it tells us what user would see. So that's pretty useful, right? So it's a meaningful measure from a user's perspective. But if we use this measure to compare systems, it wouldn't be good because it wouldn't be sensitive to where these four relevant documents are ranked. if I move them around the precision at 10 is still the same. Right, so this is not a good measure for comparing different algorithms. In contrast, the average precision is a much better measure. It can tell the difference of different difference in ranked lists in subtle ways. 
So average precision is computed for just one query. But we generally experiment with many different queries and this is to avoid the variance across queries. Depending on the queries you use, you might make different conclusions, so it's better to use more queries. If you use more queries then you would also have to take average of the average precision over all these queries. So how can we do that? You can naturally think of just doing arithmetic mean as we know. Always tend to think in this way. So this would give us what is called Mean Average Precision or MAP. In this case we take arithmetic mean of all the average precisions over set of queries or topics. But as I just mentioned in another lecture, is this good? Recall that we talked about the different ways of combining precision and recall. And we conclude that the arithmetic mean is not as good as the F measure. But here it's the same. We can also think about the alternative ways of aggregating the numbers. Don't just automatically assume that. Let's just take the arithmetic mean of the average precision over these queries. Let's think about what's the best way of aggregating. If you think about different ways, naturally you would probably be able to think about another way, which is geometric mean. And we called this kind of average gMAP map. This is another way. So now, once you think about the two different ways of doing the same thing, the natural question to ask is which one is better so. So do you use MAP or gMAP? Again, that's important question. Imagine you are again testing a new algorithms by comparing it with your old algorithm in the search engine. Now you test it on multiple topics. Now you've got the average precisions for all these topics. Now you are thinking of looking at the overall performance you have to take average. But which which strategy would you use? Now first you should also think about the question, would it make a difference? Can you think of scenarios where using one of them would make a difference? That is, they would give different the rankings of those methods. And that also means depending on the way you average, or you take the average of these average precisions, you will get different conclusions. This makes the question become even more important. So which one would you use? Again, if you look at the difference between these different ways of aggregating the average position, you will realize in arithmetic mean the sum is dominant by large values. So what does a large menu value here mean? It means the query is relatively easy. You can have a high average precision, where as gMAP tends to be affected more by lower values and those are the queries that don't have good performance. The average precision is low. So if you think about improving the search engine for those difficult queries than gMAP would be preferred. On the other hand, that if you just want to have improvement over all the kinds of queries or particular popular queries, that might be easy and you want to make the perfect and maybe MAP would be them preferred. So again, the answer depends on your users, your user's tasks, and their preferences. So the point that here is. To think about the multiple ways to solve the same problem and then compare them and think carefully about differences and which one makes more sense. Often in one of them might make sense in one situation and another might make more sense in a different situation, so it's important to figure out under what situations one is preferred. As a special case of the mean average precision, we can also think about the case where there is precisely one relevant document. And this happens often. For example, in what's called a known item search, where you know a target page. Let's say you want to find the Amazon home page, you have one relevant document there, and you hope to find it. And that's called the known item search. In that case, there is precisely one relevant document, or in another application like a question answering. Maybe there's only one answer there, so if you rank the answers, then your goal is ranked at one particular answer on top right? So in this case, you can easily verify, the average precision will basically boil down two reciprocal rank, that is one over R, where R is the rank position of that single relevant document. So if that document is ranked on the very top, R is 1 and then it's one for reciprocal rank. If it's ranked at the second, then it's 1 / 2 etc. And then we can also take a average of all these average position or reciprocal rank over a set of topics and that would give us something called Mean Reciprocal Rank. It is a very popular value for known item search or any ranking problem where you have just one relevant item. Now again, here you can see this R actually is meaningful here, and this R is basically indicating how much effort an user would have to make in order to find that relevant document. If it's ranked on the top is no effort that you have to make or little effort, but if it's ranked at 100 then you actually have to read presumably 100 documents in order to find it. So in this sense, R is also meaningful measure and the reciprocal rank will take the reciprocal of R instead of using R directly. So one natural question here is, why not simply using R? Now imagine if you are to design a measure to measure performance of the ranking system when there is only one relevant item. You might have thought about using r directly as the measure. After all that measures the users effort, right? But think about, if you take the average of this over a large number of topics, again, it would make a difference right, for one single topic using R or using one overall wouldn't make any difference. It's the same larger R with correspond to a small one overall, but the difference would only show when show up when you have many topics. So again think about average of mean reciprocal rank versus average of just R. What's the difference? Do you see any difference? And would this difference change the order of systems in our conclusion? And it turns out that there is actually a big difference, and if you think about it, if you want to think about it and then yourself, then pause the video. Basically the difference is if you take some of R directly, then again will be dominated by large values of R. So what are those values? Those are basically large values that indicate the lowly ranked results. That means the relevant item is ranked very low down on the list and the sum, the audacity. Also the average would be then dominated by where those relevant documents are ranked in the lower portion of the ranked list, but from a user's perspective we care more about the highly ranked documents. So by taking this transformation by using reciprocal rank, here we emphasize more on the difference on the top and think about the difference between one and two. It will make a big difference. In one over R, but think about 100 and 101 and one it won't make much difference if you use this. But if you use this, there will be a big difference. Being 100 and let's say 1000. Right, so this is not the desirable. On the other hand, one and two won't make much difference, so this is yet another case where there may be multiple choices of doing the same thing, and then you need to figure out which one makes more sense. So to summarize, we show the Precision recall curve, can characterize the overall accuracy of a ranked list. And we emphasized that the actual utility over ranking list that depends on how many top rankings results are user would actually examine. Some users will examine more than others and average precision is the standard measure for comparing two ranking methods. It combines precision and recall and it's sensitive to the rank of every relevant the document. 
This lecture is about the how to evaluate the text retrieval system when we have multiple levels of judgments. In this lecture we will continue the discussion of evaluation. We're going to look at the how to evaluate the text retrieval system when we have multiple level of judgments. So, so far we have talked the about binary judgments. That means a document is judged as being relevant or non relevant. But earlier we also talk about the relevance as a matter of degree, so we often can distinguishing very high relative documents. Those are very useful documents from your moderately relevant documents. They are ok, they are useful perhaps. And further from non relevant documents, those are not useful. So imagine you can have ratings for these pages. Then you would have multiple levels of ratings. For example here I show example of three levels, 3 for relevant sorry 3 for very relevant, two for marginally relevant and one for non relevant. Now how do we evaluate search engine system using these judgments? Obviously the map doesn't work. Average precision doesn't work. Precision and recall doesn't work because they rely on binary judgments. So let's look at some top ranked results when using these judgments, right? Imagine the user would be mostly care about the top 10 results here. Right? And we marked the reading levels or relevance levels for these documents as shown here, 32113, etc. And we call these "Gain". And the reason why we call it "Gain" is because the measure that we're introducing is called nDCG(Normalized Discounted Cumulative Gain). So this gain basically can measure how much gain of relevant information the user can obtain by looking at each document. Alright, so looking at the first document that the user can gain three points. Looking at the non random document, the user would only gain one point. By looking at the moderately relevant or marginal relevant documents, the user would get two points. Etc. So this gain intuitively matches the utility of a document from a user's perspective. Of course, if we assume the user stops at the 10 documents and we're looking at the cut off at 10, we can look at the total game of the user. And what's that? Well, that's simply the sum of these and we call it a cumulative gain. So if the user stops at the position one where there's just three, if the user looks at the another document, that's 3 + 2. If the user looks at the more documents, then the cumulative gain is more. Of course, this is at the cost of spending more time to examine the list. So cumulative gain gives us some idea about the how much total gain the user would have if the user examines all these documents. Now in nDCG we also have another letter here, D discounted. Cumulative gain. So why do we want to do discounting? Well, if you look at this cumulative gain, there is one deficiency, which is it did not consider the rank position of these documents. So, for example, looking at the this sum here. And we only know there is one highly relevant document one marginally relevant document, two non relevant documents. We don't really care where they are ranked. Ideally we want these two to be ranked on the top and which is the case here. But how can we capture that intuition? Well, we have to say this is 3 here. is not as good as this three on the top. And that means the contribution of the gain from different positions has to be weighted by their position, and this is the idea of discounting, basically. So we're going to say well, the first one doesn't need to be discounted, because the user can be assumed to always see this document, but the second one, this one will be discounted a little bit because there's a small possibility that the user wouldn't notice it. So we divide this gain by the weight based on the position, so log of 2. Two is the rank position of this document. And when we go to the third position, we discount even more because the normalizes log of three and so on, so forth. So when we take a such a sum than a lower rank document will not contribute contribute that much as a highly ranked document. So that means if you for example switch the position of this, let's say this position and this one, and then you would get more discount if you put. For example, very relevant document here, as opposed to here. Imagine if you put three here, then it would have to be discounted, so it's not as good as if we would put the three here. So this is the idea of discounting. OK, so now at this point that we have got that discounted cumulative gain for measuring the utility of this ranked list with multiple levels of judgments. So are we happy with this? Well, We can use this rank systems. Now, We still need to do a little bit more in order to make this measure comfortable across different topics. And this is the last step. And, By the way, here we just showed the DCG at the ten right? So this is the total sum of DCG. Overall these 10 documents. So the last step is called the N normalization and if we do that then we will get a normalized DCG. So how do we do that? Well the idea here is we're going to normalize DCG by the ideal DCG at the same cut off. What is the ideal DCG? This is the DCG of ideal ranking. So imagine if we have 9 documents in the whole collection. Rated 3 here. And that means in total we have 9 documents rated 3. Then our ideal rank, the Lister would have put all these nine documents on the very top. So all these would have to be 3 and then this will be followed by a two here because that's the best we could do after we have run out of threes. But all these positions would be threes. Right? So this will be an ideal ranked list. And then we can compute the DCG for this ideal ranked list. So this would be given by this formula that you see here, and so this ideal DCG would then be used as the normalizer DCG..., here. And this ideal DCG will be used as a normalizer. So you can imagine now normalization essentially is to compare the actual DCG with the best DCG you can possibly get for this topic. Now, why do we want to do this? Well, by doing this will map the DCG values into a range of zero through one, so the best value or the highest value for every query would be one. That's when your ranked list is in fact the ideal list. But otherwise, in general you will be lower than one. Now, what if we don't do that? Well, you can see this transformation or this normalization doesn't really affect the relative comparison of systems for just one topic, because this ideal DCG is the same for all the systems, so the ranking of systems based on only DCG would be exactly the same as if you rank them based on the normalized DCG. The difference however is when we have multiple topics. because if we don't do normalization, different topics will have different scales of DCG. For a topic like this one we have 9 highly relevant documents. The DCG can get really high, but imagine in another case, There are only two very relevant documents in total, in the whole collection. Then the highest DCG that any system could achieve for such a topic will not be very high. So again, we face the problem of different scales of DCG values, and we take an average. We don't want the average to be dominated by those high values. Those are again easy queries, so by doing the normalization we can avoid the avoid the problem making all the queries contribute equally to the average. So this is the idea of nDCG. It's useful for measuring ranked list based on multiple level relevance judgments. So more in the more general way, this is basically a measure that can be applied to any rank the task with multiple level of judgments. And The scale of the judgments can be multiple. Can be more than binary, not only more than binary. They can be multiple levels like a 1 through 5 or even more depending on your application. And the main idea of this measure I just to summarize is to measure the total utility of the top K documents. So you always choose a cut off and then you measure the total utility and it would discount the contribution from a lower ranked document. And finally it will do normalization to ensure comparability across queries. 
This lecture is about some practical issues that you would have to address in evaluation of text retrieval systems. In this lecture we will continue the discussion of evaluation we'll cover some practical issues that you have to solve in actual evaluation of text retrieval systems. So, In order to create the test collection, we have to create a set of queries, a set of documents and a set of relevance judgments. It turns out that each is actually challenging to create. First, the documents and queries must be representative. They must represent the real queries, and real documents that the users handle, and we also have to use many queries and many documents in order to avoid biased conclusions. For the matching of relevant documents, with the queries we also need to ensure that there exists a lot of relevant documents for each query. If a query has only one, let's say rather than the document in the collection, then you know it's not very informative to compare different methods using such a query, because there's not that much room for us to see difference, so ideally there should be more relevant documents in the collection, but yet the queries also should represent the real queries that we care about. In terms of relevance judgments, the challenge is to ensure complete judgments of all the documents for all the queries, yet minimizing human effort because we have to use human labor to label these documents, it's very labor intensive and as a result it's impossible to actually label all the documents for all the queries, especially considering a giant dataset like the web. So this is actually a major challenge. It's a very difficult challenge. For measures, It's also challenging because we want the measures that would accurately reflect the perceived utility of users. We have to consider carefully what the users care about. And then design measures to measure that. If your measure is not measuring the right thing, then your conclusion would be misled. So it's very important. So, we're going to talk about a couple of issues here. One is a statistical significance test and this also is the reason why we have to use a lot of queries. And the question here is how sure can you be that observed difference that doesn't simply result from the particular queries you choose, so here are some sample results of average position for system A and system B in two different experiments. And you can see in the bottom we have mean average precision. So the mean if you look at the mean average precision. The mean average precisions are exactly the same in both experiments. So you can see this is 0.2. This is 0.4 for system B and again here, It's also 0.2 and 0.4, so they are identical. Yet if you look at the these exact average precisions for different queries. If you look at these numbers in detail, You will realize that, in one case you would feel that you can trust the conclusion here given by the average. In another case, in the other case, you will feel that well, I'm not sure. So why don't you take a look at all these numbers for a moment? Pause the video. So if you look at the average, the mean average precision, we can easily say that, well, system B is better, right? So it's afterwards 0.4 and then this is twice as much as 0.2, so that's a better performance. But if you look at these two experiments. Look at the detail results, You will see that would be more confident to say that in the case one in experiment one. In this case, because these numbers seem to be consistently better for system B . Whereas in experiment 2, We're not sure because looking at some results like this. Actually System A is better and this is another case. System A is better. But yet if we look at the only average, System B is better. So, What do you think? You know how reliable Is our conclusion, if we only look at the average? Now, in this case, intuitively we feel experiment one is more reliable. But how can we quantitatively answer this question? And, This is why we need to do statistical significance test. So the idea of statistical significance test is basically to assess the variance across these different queries. If there is a A big variance that means the results could fluctuate a lot according to different queries. Then we should believe that unless you have used a lot of queries, the results might change if we use another set of queries. Right, so this is not. So, If you have see high variance then it's not very reliable. So let's look at these results again in the second case, right? So here we show two different ways to compare them. One is signed test where we just look at the sign. If system B is better than system A, we have a plus sign when System A is better, we have a minus sign, etc. Using this case, If you see this, well, there are seven cases. We actually have 4 cases where system B is better, but 3 cases System A is better. You intuitively. This is almost like a random result, right? So if you just take a random sample of two to flip 7 coins, and if you use plus to denote the head and then minus to denote the tail, and that could easily be the results of just randomly flipping these 7 coins. So the fact that the average is larger doesn't tell us anything and we can reliably conclude that, and this can be quantitatively measured by a P value and that basically, means the probability that this result is infected from random fluctuation. In this case probability is 1. It means it surely is random fluctuation. Now in Wilcoxon test, the nonparametric test and we would be not only looking at the science will be also looking at the magnitude of the difference, but we can draw a similar conclusion where you say it's very likely to be from random. So to illustrate this, let's think about the such a distribution and this is called the null distribution. We assume that the mean is 0 here. This say we start with the assumption that there's no difference between the two systems. But we assume that because of random fluctuations depending on the queries. We might observe a difference, so the actual difference might be on the left side here or on the right side here, right, so? And this curve kind of shows the probability that we will actually observe values that are deviating from zero here. Now, so if we look at this picture, then we see that. If a difference is observed here, then the chance is very high that this is in fact a random observation, right? We can define a region of you know likely observation because of random fluctuation, and this is 95% of all the outcomes and in this interval then the observed values may still be from random fluctuation. But if you observe a value in this region or a difference on this side, then the difference is unlikely from random fluctuation, right? So there's a very small probability that you will observe such a difference just because of random fluctuation. So in that case we can then conclude the difference must be real. So, System B is indeed better. So this is the idea of statistical significance test. The takeaway message here is that you have to use many queries to avoid jumping into a conclusion, as in this case to say System B is better. There are many different ways of doing this statistical significance test. So, now let's talk about the other problem of making judgments. And as we said earlier, it's very hard to judge all the documents completely unless it's a very small data set. So the question is if we can afford judging all the documents in the collection, which is subset, should we judge? And the solution here is pulling and this is a strategy that has been used in many cases to solve this problem. So the idea of pulling is the following. We would first choose a diverse set of ranking methods. These are text retrieval systems. And we hope these methods can help us nominate likely relevant documents. So the goal is to figure out the relevant documents we want to make judgments on relevant documents, because those are the most useful documents from users perspective. So then we're going to have each to return top K documents. The "K" can vary from systems right? But the point is to ask them to suggest the most likely relevant documents. And then we simply combine all these top K sets to form a pool of documents for human assessors, to judge. So imagine you have many systems, each will return K documents, will take the top K documents and we formed the union. Now, of course there are many documents that are duplicated bcause many systems might have retrieved the same relevamnt documents. So there will be some duplicate documents and there are also unique documents that are only returned by one system and so the idea of having diverse set of ranking methods is to ensure the pool is broad and can include as many possible relevant documents as possible. And then the users would. Human assistance would make a completely judgment on this data set this pool. And the other unjudged documents are usually just assumed to be non relevant. Now if the pool is large enough, this assumption is OK. But the if the pool is not very large, this actually has to be reconsidered and we might use other strategies to deal with them, and there are indeed other methods to handle such cases, and such a strategy is generally ok for comparing systems that contributed to the pool. That means if you participated in contributing to the pool, then it's unlikely that it will penalize your system because the top ranked documents have all been judged. However, this is problematic for evaluating a new system that may have not contributed to the pool. In this case, a new system might be penalized because it might have nominated some relevant documents that have not been judged, so those documents might be assumed to be non relevant. That's unfair. So to summarize, the whole part of text retrieval evaluation, it's extremely important because the problem is empirically defined problem. If we don't rely on users, there's no way to tell whether one method works better. If we have inappropriate experiment design, we might misguide our research or applications and we might just draw wrong conclusions. And we have seen this in some of our discussion, so make sure to get it right for your research or application. The main methodology is Cranfield evaluation methodology and this is still the main paradigm used in all kinds of empirical evaluation tasks, not just the search engine evaluation. MAP and nDCG are the two main measures that should definitely know about and they are appropriate for comparing ranking algorithms. You will see them often in research papers. Proceeding at the 10 documents is easier to interpret from the user's perspective, so that's also often useful. What's not covered is Some other evaluation strategy like A-B Test where the system would mix 2, the results of two methods randomly and then will show the mixed results to users. Of course the users don't see and which result is from which method the users would judge those results or click on those documents in search engine application. In this case then the search engine can keep track of the, clicked documents and see if one method has contributed more. through the clicked documents, if the user tends to click on one, the results from one method, then it's just that the method may may be better, so this is the leverage the real users of a search engine to do evaluation. It's called A-B Test, and it's a strategy that's often used by the modern search engines. Commercial search engines. Another way to evaluate IR or Text retrieval is user studies, and we haven't covered that. I've put some references here that you can look at if you want to know more about that. So there are three additional readings here. These are three mini books about evaluation and they all excellent in covering a broad review of information retrieval, evaluation, and discovered some of the things that we discussed. But they also have a lot of others to offer. 
This lecture is about a probabilistic retrieval model. In this lecture, we're going to continue the discussion of tax retrieval methods. We can do look at the another kind of very different way to design ranking functions than the vector space model that we discussed before. Being probabilistic models, we define the ranking function based on the probability that this document is relevant to this query. In other words, we introduce a binary random variable here. This is the variable R here. And we also assume that the query and the documents are observations from random variables. Note that in the vector space model we assume they are vectors, but here we are assumed. We assume they are the data observed from random variables. And so the problem of retrieval now becomes two estimated. probability of relevance. In this category of models there are different variants. The classical problem is model has led to the BM 25 retrieval function which we discussed in the vector space model. Because it's a form is actually similar to objectives space model. In this lecture, we will discuss another subclass in this. Big class. Called a language modeling approaches to retrieval. In particular, we're going to discuss the query likelihood retrieval model. Which is one of the most effective models in probabilistic models. There is also another line called a divergent from randomness model which has led to. The PL-2 function. It's also one of the most effective state of the other travel functions. Inquiry likelihood Our assumption is that this probability of relevance can be approximated by the probability of query given a document and relevance. So intuitively this probability. Just captures the following probability and that is if a user likes document D. How likely would the user enter query Q in order to retrieve documenting? So assume that the user likes D. Because we have a relevance value here and then we asked the question about the how likely will see this particular query from this user. So this is the basic idea. Not to understand this idea, let's take a look at the general idea or the basic idea of probabilistic retrieval models. So here are listed at some. Imagine the relevance of status values or relevance judgments. Often queries and documents. For example, in this line it shows that query one. is A query that the user tightly and the D1 is a document the user has seen and one means the user thinks the one is relevant to Q1. So this R here can be also approximated by the click through data that a search engine can collect by watching how you interact with the search results. So in this case, let's say the user clicked on this document, so there's one here. Similarly. The user clicked on D2 also, so there is 1 here. In other words, D2 is assumed to be relevant to Q1. On the other hand, D3 is non relevant. There's a 0 here. At the voice down relevant and then D5 is again relevant. And so on so forth. And this part. Maybe data collected from a different user. So this user typing Q1 and then found that D1 is actually not useful. So divine is actually non relevant. In contrast here we see it's relevant. And all this could be the same query typing by. The same user at different times. But D2 is also relevant, ET cetera. And here we can see more data. Then what about other queries? Now we can imagine we have a lot of such data. We can ask the question, how can we then estimate the probability of relevance? Right, so how can we compute this probability of relevance? Or intuitively that just means? If we look at the all the entries where we see this particular D and this particular Q, how likely will see a one on the third column? So basically that just means we can just collect those accounts. We can first count the how many times we have seen Q&amp;D as a pair. in this table and then count how many times we actually have also seen one in the third column. So and then we just. Compute the ratio. So let's take a look at some specific examples. Suppose we're trying to compute this probability for D1D2 and D3 for Q1. What is the estimated probability? Now think about that. You can pause the video if needed. Try to take a look at the table. And try to give your estimate of the probability. Have you seen that if we are interested in Q1 and D1 will be looking at these two pairs? And in both cases. Actually, in one of the cases. The user has said This is why this is relevant, so R is equal to 1 in only one of the two cases. In the other case it's 0. So that's one out of two. What about the D1 and D2? They are here. in both cases. In this case R is equal to 1, so it's two out of two. And so on, so forth. So you can see with this approach, we can actually score these documents for the query, right? We now have a score for D1D2 and D3. For this query we can simply rank them based on these probabilities, and so that's the basic idea of probabilistic retrieval model, and you can see it makes a lot of sense. In this case it's going to rank D2 above all the other documents, because in all the cases when you have seen D1 and D2. Eyes equals one the user clicked on this document. So this also. Should. Show that with a lot of click through data, a search engine can learn a lot from the data to improve their search engine. This is a simple example that shows that with even a small number of entries here we can already estimate some probabilities. These probabilities would give us some sense about which document might be more relevant or more useful to a user who typing this query. Now of course the problems that we don't observe all the queries and all the documents and all the relevance values. There will be a lot of unseen documents. In general we only collected data from the documents that we have shown to the users. There are even more unseen queries because you cannot predict what queries would be typing by users. So obviously this approach won't work if we apply it to unseen queries or unseen documents. Nevertheless, this shows the basic idea of problems control model and it makes sense intuitively. So what do we do in such a case when we have a lot of unseen documents and then some queries where the solutions that we have to approximate in somewhere, right? So in this particular case code query like whole retrieval model, we just approximate this by another conditional probability. P of Q given D an R is equal to 1. So the condition part. We assume that the user likes the document because we have seen that the user clicked on this document. And this part shows that we're interested in how likely the user would actually enter this query. How likely will see this query in the same role? So no data here. We have made an interesting assumption here. Basically, we can do assume that whether the user types in this query has something to do with whether user likes the document. In other words, we actually make the following assumption. And that is a user formula to query based on an imaginary relevant document. If you just look at this is conditional probability. It's not obvious we're making this assumption. So what I really meant is that. To use this new conditional probability to help us score, then this knew conditional probability will have to somehow be able to estimate this conditional probability without relying on this big table. Otherwise we would be having similar problems as before an by making this assumption, we have some way to bypass this big table and try to just model how the user formulates the query. OK, so this is how you can simplify the general model so that we can derive a specific Iranian function later. So let's look at how this model work for our example, and basically what we are going to do in this case is to ask the following question which of these documents is most likely the imaginary relevant document in the user's mind when the user formulates this query. So we ask this question and we quantify the probability and this probability is conditional probability of. Observing this query if a particular document is infected, imaginary relevant document in the user's mind. Here you can see we compute all these query likelihood probabilities. The likelihood of queries given each document. Once we have these values, we can then rank these documents based on these values. So to summarize, the general idea of modern relevance in the probabilistic model is to assume that we introduce a binary random variable R here, and then let's a scoring function be defined based on this conditional probability. We also talked about the approximate in this by using the query likelihood. And in this case we have a ranking function that's basically based on the probability of a query given the document, and this probability should be interpreted as the probability that a user who likes document D would pose queria Q. Now the question of course, is how do we compute this conditional probability? At this, in general has to do with how to compute the probability of text, because Q is attached. And this has to do with. Model called the Language model and this kind of models are proposed to model text. So more specifically, we would be very interested in the following conditional probability as issuing this here if the user. This document how likely the user would oppose this query. Ann In the next lecture working through, give an introduction to language models that we can see how we can model text with the probabilistic model in general. 
This lecture is about the statistical language model. In this lecture we're going to give an introduction to statistical language model. This has to do with how do you model text data with probabilistic models so it's related to how we model query based on a document. We're going to talk about what is the language model and then we're going to talk about the simplest language model called a unigram language model, which also happens to be the most useful model for text retrieval. And finally, we discussed possible uses of language model. What is the language model? It's just a probability distribution over word sequences. So here I show 1. This model gives. The sequence today is Wednesday, a probability of 0.001. It gave today. Wednesday is a very very small probability. Becauses amount for the medical. You can see the probability is given to these sentences or sequences of words can vary a lot depending on the model. Therefore it's clearly context dependent. In ordinary conversation, probably today is Wednesday is most popular among these sentences. But imagine in the context of discussing applied math, maybe the eigenvalues positive would have a higher probability. This means it can be used to represent the topic of the text. The Mortal Council be regarded as a probabilistic mechanism for generating text. And This is why it's also often called a generating model. So what does that mean? We can imagine this is a mechanism. That's visualized hands here as a stochastic system that can generate the sequences of words. So we can ask for a sequence and it's too simple sequence from the device if you want, and it might generate. For example, today is Wednesday. But it could have generated any other sequences. So for example there are many possibilities, right? So this in this sense we can view our data as basically a sample observable from such a generating model. So why is such a model useful? So many because it can quantify the uncertainties in natural language. Where do unvertainties Come from it. One source is simply the ambiguity in natural language that we discussed earlier in Lab 2. Another source is because we don't have complete understanding. We lack all the knowledge to understand language. In that case there will be uncertainties as well, so let me show some examples of questions that we can answer with the language model that would have interesting application in different ways. Given that we see John and Fields. How likely we see happy as opposed to habit as the next word in a sequence of words? Obviously this would be very useful for speech recognition, because happy and happy it would have similar acoustical sound acoustic signals. But if we look at the language model, will know that John feels happy would be far more likely than John feels habit. Another example, given that we observe baseball 3 times and game once in a news article, how likely is it about sports? This obviously is related to text categorization, an information retrieval. Also, given that a user is interested in Sports News, how likely would the user used baseball in a query? Now this is clearly related to the query likelihood that we discussed in the previous matching. So now let's look at the simplicity language model, called a unigram language model. In such a case. We assume that we generate the text by generating each word independently. So this means the probability of a sequence of words. Will be then the product of the probability of each world. And normally they're not independent. Right, so if you have seen a word like language that would make them far more likely to observe model than if you haven't seen the language. So this assumption is not necessarily true, but we make this assumption to simplify the model. So now the model has precisely in parameters wherein is vocabulary size. We have one probability for each word, and all these probabilities must sum to one. So strictly speaking we actually have N-1 parameters. As I said, text can be assumed to be assembled drawn from this world distribution. So for example, now we can ask the device or the model to stochastic in general words for us instead of sequences. So instead of giving a whole sequence like today's Wednesday, it now gives us just one word and we can get all kinds of words, and we can assemble these words in a sequence. So that would still allows little computer the probability of today's Wednesday as the product of the three probabilities. As you can see, even though we have not asked the model to generate the sequence, it actually allows us to compute the probability for all the sequences. But this model now only needs N parameters to characterize. That means if we specify all the probabilities for all the words, then the models behavior is completely specified, whereas if we don't make this assumption we would have to specify probabilities for all kinds of combinations of words. In sequences. So by making this assumption, it makes it much easier to estimate these parameters, so let's see a specific example here. Here I show two unigram language models with some probabilities and these are high probability words that are shown on top. The first one clearly suggests a topic of text mining, because the high probability words are all related to this topic. The second one is more related to health. We can then ask the question, how likely will observe a particular text from each of these three models? I suppose we sample words. The former document. Let's say we take the first distribution, which had a simple words. What words do you think it would be generated? Well, maybe text or maybe mining. Maybe another word even fooled, which has a very small probability, might still be able to show up. But in general, high probability words with likely show up more often. So we can imagine what gender the text that looks like a text mining. In fact, there was a small probability you might be able to actually generate the actual text mining paper that would actually be meaningful, although the probability would be very very small. In the extreme case, you might imagine we might be able to generate the attacks paper text mining paper that would be accepted by a major conference. And in that case, the public in it would be even smaller. But it's a non zero probability if we assume none of the words have non zero probability. Similarly from the second topic, we can imagine we can generate the folder nutrition paper. That doesn't mean we cannot generate this paper from text mining. Distribution. We can, but the probability would be very, very small, maybe smaller than even generating a paper that can be accepted by a major conference on text mine. So the point here is that given distribution. We can talk about the probability of observing a certain kind of text. Some text will have higher probabilities than others. Now let's look at the problem in a different way. Suppose we now have available of particular document. In this case, maybe the abstract of a text reminding payroll. And we see these world accounts here. The total number of words is 100. Now the question will ask here is estimation question. We can ask the question which model which word distribution has been used to generate this text. Assuming that the text that has been generated by sampling words from the distribution. So what would be your guess? Have to decide what probability is. Text mining etc would have. So pause the video for a second and try to think about your best guess. If you're like a lot of people, you would have guessed that my best guess is. text has a probability of 10 out of 100 because I've seen text 10 times an there are in total 100 words, so we simply not simply normalize these counts. That's in fact the word justified, and your intuition is consistent with mathematical derivation, and this is called a maximum likelihood estimator. In this estimator we assume that the parameter settings are those that would give our observed data the maximum probability. That means if we change these probabilities, then the probability of observing the particular text data would be somewhat smaller. So you can see this has a very simple formula. Basically we just need to look at the count of a word in the document and then divided by the total number of words in the document or document length. Normalized frequency. Or consequences of this is, of course we're going to assign zero probabilities to unseen words. if we have not oveserve a word there will be no incentive to assign a non zero probability using this approach. Why 'cause that would take away probability mass for these ovbserved words? And that obviously wouldn't maximize the probability of this particular observer text data. But one can still question whether this is our best estimate. Well, the answer depends on what kind of model you want to find, right? This is made. It gives the best model based on this particular data. But if you're interested in a model that can explain the content of the four paper of this abstract, then you might have a second thought, right? So for one thing, there should be other words in the body of that article. So they should not have zero probabilities even though they are not observed in abstract. So we're going to cover this a little more later in discussing the query likelihood retrieval model model. So let's take a look at the some possible uses of this language. One use is simply to use it to represent the topics. So here I show some general English background text. We can use this text to estimate the language model and the model might look like this. So on the top will have those all common words like the is way etc and then we'll see some common words like these and then some very very rare words in the bottom. This is the background language model. It represents the frequency of words in English in general. Right, this is the background model. Now let's look at the another text. Maybe this time we'll look at the computer science research papers. So we have a collection of computer science research papers we do estimation again. Again, we can just use the maximum microarrays better, where we simply normalize the frequencies. Now, in this case we will get the distribution that looks like this. On the top. It looks similar because these words occur everywhere. They are very common, but as we go down we will see words that are more related to computer science, computer software, text, etc. And so, although here we might also see these words, for example computer. But we can imagine the probability here is much smaller than the probability here, and we will see many other words here that would be more common in General English. So you can see this distribution characterizes the topic of the corresponding tents. We can look at the even the smaller text. So in this case, let's look at the text mining paper. Now if we do the same, we have another distribution again. There can be expected to occur on the top, but soon we will see text mining Association clustering. These words have. Relatively higher probabilities, in contrast in this distribution, will text has relatively small probability. So this means again based on different attacks today or we can have a different model and model captures the topic. So we call this document language model and we call this collection languagemodel. And later you will see how they are used in retrieval function. But now let's look at the another use of this model. Can we statistically find what words are semantically related to computer? Now how do we find the such words? Well, our first thought is that let's take a look at the text that match computer so we can take a look at all the documents that contain the word computer. Let's build a language model. We can see what would we see there. Not surprisingly, we see these common words on top. As we always do so in this case, this language model gives us the conditional probability of seeing the world in the context of computer and these common words will naturally have high probabilities. But we also see computer itself and software will have relatively high probabilities. But if we just use this model, we cannot just say all these words are semantically related to computer. So intuitively we would like to get rid of these. Help. These common words. How can we do that? It turns out that it's possible to use, langage model to do that. I suggested you don't think about that. So how can we know what words are very common so that we want to kind of get rid of them? What model would tell us that? Maybe you can think about that. So the background language model precisely tells us this. Information tells us what words are common in general. So if we use this background model, we would know that these words are common words in general, so it's not surprising to observe them in the context of computer. Where is the computer has a very small probability in general, so it's very surprising that we have seen computer with this probability, and the same is true for software. So then we can use these two models to somehow figure out the words that are related to the computer. For example, we can simply take the ratio of these two probabilities or normalize the topic language model by the probability of the world in the background language model. So if we do that, we take the ratio, will see that, then on the top computer is ranked and then followed by software program. All these words are related to computer. Because they occur frequently in the context of computer, but not frequently in the whole collection. Whereas these common words will not have a high probability. In fact they have ratio about one down there because they are not really related to computer. By taking the sample of text that contains the computer, we don't really see more occurrences of them than in general. So this shows that the even with these simple language models we can do some limited analysis of semantics. So in this lecture we talked about. Language model, which is basically probability distribution over text. We talked about the simplest language model called unigram them model which is also just a word distribution. We talked about the two uses of a language model one is represented topic in a document in the collection or in general the other is rediscovered water associations. In the next lecture, we're going to talk about how, then which model can be used to design retrieval function. Here are two additional readings. The first is textbook on statistical natural language processing. The second is article that has a survey of statistical language models with a lot of pointers to research work. 
This lecture is about the query likelihood probabilistic retrieval model. In this lecture we continue the discussion of probabilistic retrieval model. In particular, we're going to talk about the query likelihood retrieval function. In the query likelihood retrieval model. Our idea is to model how likely a user who likes a document would pose a particular query. So in this case you can imagine if a user likes this particular document about the presidential campaign news. Then we can assume the user would use this document as a basis to post a query to try to retrieve this document. So we can imagine the user could use a process. That works as follows, where we assume that the query is generated by sampling words from the document. So for example, a user might pick a word like presidential from this document. And then use this as a query word. And then the user would pick another word like "campaign" and that will be the second query word. Now this of course is assumption that we have made about how a user would pose a query. Whether user actually followed this process. Maybe a different question, but this assumption has allowed us to formulate characterize this conditional probability. And this allows us to also not rely on the big table that I showed you earlier to use empirical data to estimate this probability. And this is why we can use this idea to them. Further derive retrieval function that we can implement with the program language. So as you see, the assumption that we've made here is each query word is independently sampled and also each word is basically obtained from the document. So now let's see how this works exactly. Well, since we are computing the query likelihood. Then the probability here is just the probability of this particular query, which is a sequence of words. And we make the assumption that each word is generated independently, so as a result, the probability of the query is just a product of the probability of each query word. Now, how do we compute the probability of each query word Well based on the assumption that a word is picked from the document. That the user has in mind. Now we know the probability of each word is just to the relative frequency of the word in the document. So for example, the probability of presidential given the document. Would be just the count of presidential in the document divided by the total number of words in the document or document length. So with this these assumptions, we now have actually simple formula for retrieval, right? We can use this to rank our documents. So does this model work? Let's take a look. Here are some example documents that you have seen before. Suppose now the query is presidential campaign and we see the formula here on the top. So how do we score these documents? It's very simple, right? We just count how many times we have seen "presidential" or how many times we have seen campaign et cetera and within here for d4 and we have seen presidential twice that's two over the length of Document 4 multiplied by 1 over length of document 4 for probability of campaign. And similarly we can get probabilities for the other two documents. Now if you look at this, these numbers or these formulas for scoring all these documents. It seems to make sense be cause if we assume D3 and D4 have about the same length than looks like we're going to rank D4 above D3, and which is above D2 as we would expect, looks like it did capture the TF heuristic. And so this seems to work well. However. If we try a different query like this one presidential campaign update. Then we might see a problem. What problem? Well think about the update now. None of these documents has mentioned update. So according to our assumption that a user would pick a word from a document to generate a query, then the probability of obtaining a word like update. Would be what? Would be 0, right? So that caused a problem because we cause all these documents to have zero probability of generating this query. Now, while it's fine to have zero probability for D2 which is non relevant, it's not OK to have zero for D3 and D4, because now we no longer can distinguish them. What's worse, we can't even distinguish them from D2, right? So that's obviously not desirable. Now, whenever we've had such result. We should think about what has caused this problem. So we have to examine what assumptions have been made. As we derive this ranking function. Now, if you examine those assumptions carefully, you would realize what has caused this problem. Right? So take a moment to think about what do you think is the reason why update has zero probability. And how do we fix it? Right, so if you think about this for a moment, you realize that that's because we have made assumption that every query word must be drawn from the document in the user's mind. So in order to fix this, we have to assume that the user could have drawn a word not necessarily from the document, so let's improve the model and the improvement here is to say that instead of drawing a word from the document, let's imagine that the user would actually draw a word from a document model. So I showed model here. We assume that this document is generated using this unigram language model. Now this model. Doesn't necessarily assign zero probability for update. In fact that we consume this model does not assign zero probability for any word. Now if we think in this way, then the generation process is a little bit different. Now the user has this model in mind. Instead of this particular document. Although the model has to be estimated based on the document. So the user can again generate the query using a similar process, namely pick a word. For example, presidential. And another word, campaign. Now the difference is that this time we can also pick a word like update even though update does not occur in the document to potentially generate the query word like update so that a query with update want to have zero probabilities. So this will fix our problem, and it's also reasonable because we're now thinking of what the user is looking for in a more general way. That is unigram language model instead of a fixed document. So how do we compute this query likelihood? If we make this assumption? Well, it involves 2 steps, right? The first is to compute this model. And we call it the document language model here. For example, I've shown two possible language models here is made based on two documents. And then given a query and I get data mining algorithms. The second step would just compute the likelihood of this query and by making independent assumptions we could then have this probability as a product of the probability of each query word. But we do this for both documents and then we're going to score these two documents and then rank them. So that's the basic idea of this query, likelihood retrieval function. So more generally then, this ranking function would look like the following right here we assume that the query has N words. W one through WN, and then the scoring function. The ranking function is. Probability that we observe this query given that the user is thinking of this document. And this is assumed to be product of probabilities of all individual words. This is based on the independence assumption. Now we actually often score the document for this query by using log of the query likelihood as shown on the second line. Now we do this. To avoid having a lot of small probabilities. We multiply together and this could cause underflow and we might lose precision by transforming the value with a logarithm function. We maintain the order of these documents, yet we can avoid the underflow problem. So if we take logarithm transformation, of course the product that would become a sum as shown on the second line here. So it's a sum over all the query words inside the sum. The value is log of the probability of this word given by the document. And then we can further rewrite the sum into a different form. So in the first sum here. In this sum, We have it all over the query words N query words. And in this sum we have a sum over all the possible words, but we put a count here of each word in the query. Essentially we are only considering the words in the query because if a word is not in the query, the count would be 0. So we're still considering only these N words. But we are using a different form, as if we're going to take sum over all the words in the vocabulary. And of course, a word might occur multiple times in the query. That's why we have a count here. And then this part is log of the probability of the word given by the document language model. So you can see in this retrieval function we actually know the count of the word in the query. So the only thing that we don't know is this document language model. Therefore, we have converted the retrieval problem, include the problem of estimating this document language model. So that we can compute the probability of each query word given by this document. And different estimation methods here would lead to different ranking functions. Now this is just like a different ways to place a document vector in the vector space would lead to a different ranking function in the vector space model. Here different ways to estimate these document language model would lead to a different ranking function for query likelihood. 
This lecture is about the smoothing of language models. In this lecture, we're going to continue talking about probabilistic retrieval model. In particular, we're going to talk about the smoothing of language model in the query likelihood retrieval method. So you have seen this slide from the previous lecture. This is the ranking function based on the query likelihood. Here we assume that the independence of generating each query word. And the formula would look like the following where we take a sum over all the query words and inside the sum. There is a log of probability of word given by the document or document language model. So the main task now is to estimate this document language model. As we said before, different methods for estimating this model would lead to different retrieval functions. So in this lecture we're going to look into this in more detail. So how do we estimate this language model? The obvious choice would be the maximum likelihood estimate that we have seen before, and that is we're going to normalize the word frequencies in the document. And the estimated probability would look like this. But this is a step function here. Which means all the words that have the same frequency count will have identical probability. Right, this is another frequent account that has a different probability. Note that for words that have not occurred in the document here they all have zero probability, so we this know this is just like the model that we assumed earlier in the lecture, where we assume that the user would sample word from the document. To formulate a query. And there's no chance of sampling any word that's not in the document, and we know that's not good. So how do we improve this, well? In order to assign a non zero probability to words that have not been observed in the document. We would have to take away some probability mass from the words that are observed in the document. So for example here we have to take away some probability mass because we need some extra probability mass for the unseen words. Otherwise they want to sum to one. So all these probabilities must sum to one. So to make this transformation and to improve the maximum likelihood estimate by assigning non zero probabilities to words that are not observed in the data. We have to do smoothing and smoothing has to do with improving the estimate by considering the possibility that if the author had been written had been asked to write more words. For the document, the author might have written other words. If you think about this factor, then a smoother language model would be more accurate representation of the actual topic. Imagine you have seen an abstract of a research article. Let's say this document is abstract. Right, if we assume. unseen words in this abstract. We have all probability of zero. That would mean there is no chance of sampling a word outside the abstract to formulate a query. But imagine a user who is interested in the topic of this subject. The user might actually choose a word that's not in the abstract to use as query. So obviously if we had asked this author to write more, the author would have written the full text of that article. So smoothing of the language model is attempt to try to recover the model for the whole article and then of course we don't have really knowledge about any words that are not observed in the abstract there. So that's why smoothing is actually tricky problem. So let's talk a little more about how to smooth the language model. And the key question here is what probability should be assigned to those unseen words. And there are many different ways of doing that. One idea here that's very useful for retrieval is let the probability of unseen word be proportional to its probability given by reference language model. That means if we don't observe the word in the data set, we're going to assume that it's probability is kind of governed by another reference language model that we will construct. It will tell us which unseen words will have likely higher probability. In the case of retrieval, a natural choice would be to take the collection language model as the reference language model. That is to say if we don't observe a word in the document. We're going to assume that the probability of this word would be proportional to the probability of the word in the whole collection. So more formally, we will be estimating the probability of a word given a document as follows. If the word is seen in the document. Then the probability would be a discounted. maximum likelihood estimate P sub seen here. Otherwise. If the word is not seen in the document, we're going to let its probability be proportional to the probability of the word in the collection. And here the coefficient Alpha. Is to control the amount of probability mass that we assign to unseen words. Obviously, all these probabilities must sum to one, so Alpha sub D is constrained in some way. So what if we plug in this smoothing formula into our query likelihood running function? This is what we will get. Right, in this formula you can see. Right, we have. This as a sum over all the query words and note that we have written in the form of a sum over all the vocabulary. Can see here this is a sum over all the words in the vocabulary, but note that we have a count of the word in the query. So in effect we are just taking sum of query words right? This is now. A common way that we will use. Because of its convenience. In some transformations. So this is as I said, this is some of all the query words. In our smoothing method, we assume that the words that are not observed in the document we have somewhat different form of probability namely it's for this form. So we're going to then decompose this sum into two parts. One sum is over all the query words that are matching the document. That means in this sum, all the words have a non-zero probability in the document, sorry it's the non 0 count of the word in the document. They all occurred in the document. And they also have to of course have a non 0 count in the query, so these are the words that are matched. These are the query words that are matching the document. But on the other hand, in this sum we are taking sum over all the words that are not. All query words that are not matched in the document. So they occur in the query. Due to this this term, but they don't occur in the document. In this case, these words have this probability because of our assumption about the smoothing. But that here. These seen words have a different probability. Now we can go further by rewriting the second sum. As a difference of two other sums, basically the first sum is actually sum, over all the query words. We know that the original sum is not over all the query words. This is over all the query words that are not matched in the document. So here we pretend that they are actually. Over all the query words, so we take a sum over all the query words. Obviously this sum has extra terms that are. This sum has extra terms that are not in this sum. Because here we are taking sum over all the query words there. It's not matched in the document. So in order to make them equal, we will have to then subtract another sum here. And this is the sum over all the query words that are matching the document. And this makes sense, because here we are considering all query words and then we subtract the query words that are matched in the document. That would give us the query words that not matched in the document. And this is almost reverse process of the first step here. And you might want to, why do we want to do that? Well, that's cause. If we do this, then we have different forms of terms inside these sums. So now you can see in this sum we have. All the words match the query words matched in the document and with this kind of terms. Here we have another sum. Over the same set of terms. match the query terms in document but inside the sum it's different. But these two sums can clearly be merged. So if we do that, we'll get another form of the formula that looks like the following. At the bottom here. And note that this is a very interesting formula because here we combined these two. That our sum. Over the query words matched in the document in the one sum here. And the other sum now is decomposed into two parts. And these two parts look much simpler just because these are the probabilities of unseen worlds. Now this formula is very interesting because you can see the sum is now over all the matched query terms. And just like in the vector space model, we take a sum of terms that are in the intersection of query vector and the document vector. So it all already looks a little bit like the vector space model. In fact, there's even more similarity here as we explain on this slide. 
So I showed you how we rewrite the query likelihood retrieval function into a form that looks like the formula of this slide. After we make the assumption about the smoothing the language model. Based on the collection language model. If you look at the this rewriting it actually would give us two benefits. The first benefit is it helps us better understand this ranking function. In particular, we're going to show that from this formula we can see smoothing with the collection language model will give us something like a TF IDF weighting and length normalization. The second benefit is that it also allows us to compute the query likelihood more efficiently. In particular, we see that the main part of the formula is a sum over the matched query terms. So this is much better than if we take a sum over all the words. After we smooth the document language model, we send you to have non zero probabilities for all the words. So this new form of the formula is much easier to score or to compute. It's also interesting to note that the last term here is actually independent of the document, since our goal is to rank the documents for the same query, we can ignore this term for ranking. Because it's going to be the same for all the documents. Ignoring it wouldn't affect the order of the documents. Inside the sum, We also see that each matched query term would contribute. weight And this weight actually is very interesting. because it looks like a TF IDF weighting. First, we can already see it has a frequency of the word in the query, just like in the vector space model. When we take a dot product we see the word frequency in the query to show up in such a sum. And so naturally, this pot would correspond to the vector element from the document vector, and here indeed we can see it actually encodes a weight that has similar factor to TF IDF weighting. I'll let you examine it. Can you see it? Can you see which part is capturing TF? and which part is capturing IDF weighting? So if you want you can pause the video to think more about it. So, have you noticed that this p of seen is related to the term frequency? In the sense that if a word occurs very frequently in the document, then the estimated probability here would tend to be larger. So this means this term is really doing something like TF weighting. Have you also notice that? This term in the denominator. Is actually achieving the effect of IDF? Why? Because this is the popularity of the term in the collection. But it's in the denominator, so if. The probability in the collection is larger, then the weight is actually smaller, and this means a popular term. We actually have a smaller weight and this is precisely what IDF weighting is doing. Only that we now have a different form of TF and IDF. Remember, IDF has a log logarithm of document frequency. But here we have something different. But intuitively it achieves a similar fact. Interestingly, we also have something related to the length normalization. Again, can you see which factor is related to the document length? In this formula. I just say that this term is related to IDF weighting. This. This collection probability, but it turns out that this term here is actually related to the document length. Normalization in particular alpha sub d might be related to document. length, so it encodes how much probability mass we want to give to unseen words. How much smoothing do we want to do ? Intuitively, if a document is long then we need to do less smoothing because we can assume that data is large enough. We probably have observed all the words that the author could have written, but the document is short. Then alpha sub d could be expected to be to be large. We need to do more smoothing. It's like that there are words that have not been written yet by the other. So this term appears to penalize long documenting in that the alpha sub d would tend to be longer than larger than. for a long document. But note that the alpha sub d also occurs here. And so this may not actually be necessary. Penalizing long documents effect is not so clear here. But as we will see later when we consider some specific smoothing methods, it turns out that they do penalize long documents just like in TF IDF weighting and document length normalization formulas in the vector space model. So that's a very interesting observation, because it means we don't even have to think about the specific way of doing smoothing. We just need to assume that if we smooth with this collection language model, then we would have a formula. That looks like a TF IDF weighting and documents length normalization. What's also interesting is that we have very fixed form of the ranking function. And see we have not heuristically put a logarithm here. In fact, you can think about why we will have a logarithm here. If you look at the assumptions that we have made, it will be clear it's because we have. used logarithm of query likelihood for scoring. And we turned the product into a sum of logarithm of probability and that's why we have this logarithm. Note that if we only want to heuristically implement the TF weight and IDF weighting, we don't necessarily have to have a logarithm here. Imagine if we drop this logarithm, we would still have TF and IDF weighting. But what's nice with probabilistic modeling is that we are automatically given a logarithm function here. And that's basically a fixed form of the formula that we did not really have to heuristically design, and in this case, if you try to drop this logarithm, the more of probably one work as well as if you keep logarithm. So a nice property of probabilistic modeling is that by following some assumptions and probability rules we will get a formula automatically. And the formula would have a particular form, like in this case. And if we heuristically design the formula, we may not necessary end up having such a specific form. So to summarize, we talked about the need for smoothing document language model, otherwise would give zero probability for unseen words in the document. And that's not good for scoring a query with such an unseen world. And it's also necessary in general to improve the accuracy of estimating the model represents the topic of this document. The general idea of smoothing in retrieval is to use the collection language Model to Give us some clue about the which unseen word should have a higher probability. That is, the probability of an unseen word is assumed to be proportional to its probability in the collection. With this assumption, we've shown that we can derive a general ranking formula for query likelihood that has the effect of TF IDF weighting and document length normalization. We also see that through some rewriting, the scoring of such a ranking function is primarily based on sum of weights on match query terms, just like it in the vector space model. But the actual ranking function is given us automatically by the probability rules and the assumptions that we have made and unlike inthe vector space model where we have to heuristically. Think about the form of the function. However, we still need to address the question how exactly we should smooth the document language model. How exactly we should use the reference language model based on the collection to adjust the probability of the maximum likelihood estimate? And this is the topic of the next lecture. 
This lecture is about the specific smoothing methods for language models used in probabilistic retrieval model. In this lecture we will continue the discussion of language models for information retrieval, particularly the query likelihood retrieval method, and we're going to talk about the specifics smoothing methods used for such a retrieval function. So this is a slide from a previous lecture where we show that with the query likelihood ranking and smoothing with the collection language model we end up having a retrieval function that looks like the following. So this is the retrieval function based on these assumptions that we have discussed, you can see it's a sum over all the matched The query terms here. And inside the sum is the count of the term in the query and some weight. For the term in the document. And we have TF IDF weight here and then we have another constant here in the end So clearly, if we want to implement this function using a program language, we will still need to figure out a few variables. In particular, we're going to need to know how to estimate the probability of a word Exactly and. How do we set alpha? So in order to answer these questions, we have to think about this very specifically, smoothing methods and that is the main topic of this lecture. We are gonna talk about the two smoothing methods. The first is the simple linear interpolation with a fixed coefficient. And this is also called a Jelinek-Mercer smoothing. So the idea is actually very simple. This picture shows how we estimate. Document the language model by using maximum likelihood estimate that gives us word counts normalized by the total number of words in the text. The idea of using this method. Is to maximize the probability of the observed text as a result, if a word like network. Is not observed in the text, it's going to get zero probability as shown here. So the idea of smoothing then is to rely on collection language model where this word is not going to have a zero probability to help us decide what non zero probability should be assigned to such a word. So we can note that Network has a non zero probability here. So in this approach in what we do is we do a linear interpolation between the maximum likelihood estimate here and the collection language model and this is controlled by the smoothing parameter Lambda. Which is. Between zero and one. So this is a smoothing parameter. The larger lambda is, the more smoothing we will have. So by mixing them together we achieve the goal of assigning non zero probabilities to a word network. So let's see how it works for some of the words here. For example, if we compute the smooth probability for text. Now the maximum likelihood estimate gives us 10 / 100 and that's going to be here. But the collection probability is this, so we just combine them together with this simple formula. We can also see. The word network which used to have zero probability now is getting a non zero probability. Of this value, and that's because the count is going to be 0 for network. Here, but this part is non zero, and that's basically how this method works. If you think about this and you can easily see now the Alpha sub D in this smoothing method is basically Lambda. Because that's remember the coefficient in front of the probability of the word given by the collection language model here, right? OK, so this is the first smoothing method. A second one is similar, but it has a dynamic coefficient for linear interpolation. It is often called the Dirichlet Prior or Bayesian smoothing. So again, here we face the problem of zero probability for an unseen word like network. Again, we will use the collection language model, but in this case we're going to combine them in somewhat different ways. The formula first can be seen as an interpolation of the. Maximum likelihood estimate and the collection language model as before as in the JM smoothing method. Only at the coefficient now is not the Lambda a fixed number, but that dynamic coefficient in this form where mu is a parameter. It's a non-negative value. And you can see what if we set the mu to a constant. The effect is that a long document would actually get a smaller coefficient here. Because a long document that will have longer length, therefore, the coefficient is actually smaller. And so a long document would have less smoothy as we would expect. So this seems to make more sense than fixed coefficient smoothing. Of course this part. Would be of this form so that the two coefficients would sum to one. Now this is one way to understand that this smoothing Basically it means it's a dynamic coefficient interpolation There is another way to understand this formula. Which is even easier to remember, and that's this side. So it's easy to see. We can rewrite the smoothing method in this form. Now in this form we can easily to see what changes we have made to the maximum likelihood estimate which would be this part right? So normalized count by the document length So in this form we can see what we did is we add this to the count of every word. So what does this mean? This is basically. Something related to the probability of the word in the collection, and we multiply that by the parameter mu. And when we combine this with the count here, essentially we are adding pseudo counts. To the observed text. We pretend. Every word has got this many pseudo count. So the total counts would be the sum of these pseudo counts and the actual count. Of the word in the document. As a result, in total we would have added this many pseudo counts why? Because if you take a sum of this. This one, over all the words that we see, the probability of the words would sum to one, and then give us just mu. So this is the total number of the pseudo count that we added So this probability would still sum to one. So in this case we can either see the method. Is essentially to Add these. As pseudo count to this data pretend we actually augment the data by including some pseudo data defined by the collection language model. As a result, we have more counts. I still. The total counts for a word would be like this and as a result even if a word has zero count here let's says we have zero account here and it would still have non zero count because of this part. So this is how this method works. Let's also take a look at some specific example here. Right, so for text again, we will have 10 as an original count that we actually observe, but we also add some pseudo count. And so the probability of the text would be of this form naturally. The probability of network would be just this part. And so here you can also see what's alpha sub d here. Can you see it if you want to think about, you can pause the video. Have you notice that this part is basically alpha sub D? So we can see this case. Alpha sub D does depend on the document. Because. This length depends on the document, whereas in the linear interpolation the JM smoothing method. This is a constant. 
So let's plug in these smoothing methods into the ranking function to see what we will get. So, this is a general smoothing... sorry, general ranking function for smoothing with collection language model. You have seen this before. And now we have a very specific smoothing method, the JM smoothing method. So now let's see what what's the value for alpha sub D here. And, what's the value for P sub seen here? So we may need to decide this in order to figure out the exact form of the ranking function, and we also need to figure out, of course, Alpha. So let's see, well, this ratio. Is basically this right? So Here this is the probability of seeing word on the top. And this is the probability of unseen word, or in other words, lambda is basically the alpha here. So it's easy to see that this can be rewritten as this. Very simple. So we can plug this into here. And then here, what's the value for alpha? What do you think? It will be just Lambda, right? And, what would happen if we plug in this value here? If this is lambda, what can we say about this? Does it depend on the document? No, so it can be ignored. Right? So we end up having this ranking function shown here. And in this case, you can easily see this is precisely a vector space model, because this part is the sum over all the matched query terms. This is the element of the query vector what do you think is the element of the document vector? It's this, so that's our document. vector element. And let's further examine what's inside this logarithm. So one plus this, so it's going to be a non-negative log of this. It's going to be at least one, right? And this is a parameter. So Lambda is parameter and let's look and this is a TF. Now we see very clearly this TF weighting here. And. The larger the count is, the higher the weight will be. We also see IDF weighting which is given by this. And with our document length normalization here. So all these heuristics are captured in this formula. What's interesting that we kind of have got this weighting function automatically by making various assumptions, whereas in the vector space model we had to go through those heuristic design in order to get this. And in this case note that there is a specific form and we can see whether this form actually makes sense. So what do you think Is the denominator here? This is the length of document, total number of words multiplied by the probability of the word given by the collection. So this actually can be interpreted as expected count of the word. If we're going to draw a word from the collection language model and we want to draw as many as the number of words in the document. If you do that, the expected count of a word W would be precisely given by this denominator. So this ratio basically is comparing the actual count here. The actual count of the word in the document with the expected count given by this product. If the word is in fact the following, the distribution in the collection this. And if this counter is larger than the expected count, this part, this ratio would be larger than one. So that's actually a very interesting interpretation, right? It's very natural. And intuitively it makes a lot of sense. And this is one advantage of using this kind of probabilistic reasoning. Where we have made explicit assumptions, and we know precisely why we have a logarithm here and why we have these probabilities here. And we also have a formula that intuitively makes a lot of sense. And does TF-IDF weighting and document length normalization. Let's look at the Dirichlet Prior Smoothing. It's very similar to the case of JM smoothing. In this case, the smoothing parameter is Mu and that's different from lambda that we saw before, but the format looks very similar. The form of the function looks very similar. So we still have linear interpolation here. And when we compute this ratio, while we defined that is that the ratio is equal to this. But what's interesting here is that we are doing another comparison here now. We're comparing the actual count with the expected count of the word if we sample Mu words according to the collection of the probability. So note that it's interesting we don't even see document length here. Unlike in the JM smoothing. So this of course should be plugged into this part. So you might wonder, where is the document length? Interestingly, the document length is here. In alpha sub d so this would be plugged into this part. As a result, what we get is the following function here, and this is again a sum over all the matched query words. And we again see the query term frequency here. And you can interpret this as the element of a document vector. But this is no longer a simple dot product, right? Because we have this part. And note that n is the length of the query. So that just means if we score this function we have to take a sum over all the query words and then do some adjustment of the score based on the document. But it's still It's still clear that it does document length normalization because this lens is in the denominator, so a longer document will have a lower weight here. And we can also see it has TF here and then IDF. Only that this time the form of the formula is different from the previous one in JM smoothing. But intuitively, is still implements TF IDF weighting and document length normalization. Again, the form of the function is dictated by the probabilistic reasoning and assumptions that we have made. Now there are also disadvantages of this approach, and that is there's no guarantee that such a form of the formula would actually work well. So if you look back at this retrieval function. Although it's TF IDF weighting and stopping the length normalization , for example, it's unclear whether we have sub-linear transformation. But Fortunately we can see here. There is a logarithm function here, so we do have also the here, right? So we do have the sub-linear transformation, but we did not intentionally do that. That means there's no guarantee that will end up in this in this way. Suppose we don't have logarithm, then there's no sub linear transformation. As we discussed before, perhaps the formula is not going to work so well. So that's example of the gap between formal model like this and the relevance that we have to model, which is really a subjective machine. That is tight to users. So it doesn't mean we cannot fix this. For example, imagine if we did not have this logarithm, right? So we can heuristically add one, or we can even add a double logarithm, but then it would mean that the function is no longer probabilistic model. So the consequence of the modification is no longer as predictable as what we have been doing now. So that's also why, for example, BM 25 remains very competitive and still open challenge how to use probabilistic model to derive a better model than BM25. In particular, how do we use query likelihood to derive a model that would work consistently better than BM25? Currently we still cannot do that. It's still an interesting open question. So to summarize this part we've talked about the two smoothing methods. Jelinek-Mercer, which is doing fixed coefficient linear interpolation. Dirichlet Prior, this is to add pseudocounts to every word and is doing adaptive interpolation in that the coefficient would be larger for shorter documents. In both cases we can see by using these smoothing methods we would be able to reach a retrieval function, whether assumptions are clearly articulated, so they're less heuristic. Experiment results also show that these retrieval functions also are very effective, and they are comparable to BM 25 or pivoted length normalization. So this is a major advantage of probabilistic model where we don't have to do a lot of heuristic design. Yet in the end, we naturally implemented TF IDF weighting and document length normalization. Each of these functions also has precisely one smoothing parameter. In this case, of course, we still need to set the smoothing parameter, but there are also methods that can be used to estimate these parameters. So overall this shows by using probabilistic model we follow very different strategy than the vector space model. Yet in the end we end up with some retrieval functions that look very similar to a vector space model with some advantages in having assumptions clearly stated. And then the form dictated by probabilistic model. Now, this also concludes our discussion of the query likelihood problems model. And let's recall what assumptions we have made in order to derive the functions that we have seen in this lecture. We basically have made four assumptions that I listed here. The first assumption is that the relevance can be modeled by the query likelihood and the second assumption we've made. It is a query words are generated independently that allows us to decompose the probability of the whole query. Into a product of probabilities of all the words in the query. And then the third assumption that we have made is if a word is not seen in the document that we're going to let its probability with proportional to its probability in the collection of the smoothing with the collection language model, and finally we've made one of these two assumptions about the smoothing. So we either use JM smoothing or the Dirichlet smoothing. If we make these four assumptions, then we have no choice but to take the form of the retrieval function that we have seen earlier. Fortunately, the function has a nice property in that implements TF IDF weighting and documents length normalization And, these functions also work very well, so in that sense these functions are less heuristic compared with a vector space model. And there are many extensions. This basic model and you can find the discussion of them in the reference at the end of this Lecture. 
This lecture is about the feedback in text retrieval. So in this lecture we're going to continue the discussion of text retrieval methods. In particular, we're going to talk about the feedback impacts retrieval. This is a diagram that shows the retrieval process. We can see the user with typing the query. And then the query would be sent to a retrieval engine or search engine. And the engine will return results. These results will be shown to the user. After the user has seen these results. The user can actually make judgments. So for example, the user had said this is good and this document is not very useful. This is good again, etc. Now this is called a relevant judgment or relevance feedback because we've got some feedback information from the user based on the judgments. This can be very useful to the system we learn. What exactly is interesting to the user. So the feedback module would then take this as input. And also use the document collection to try to improve ranking. Typically it would involve updating the query so the system can now rank the results more accurately for the user. So this is all relevance feedback. The feedback is based on relevance judgments made by the users. Now these judgments are reliable, but the users generally don't want to make extra effort unless they have to. So the downside is that it involves some extra effort by the user. There was another form of feedback called pseudo relevance feedback or blind feedback, also called automatic feedback. In this case you can see. Once the user has got without all in fact we don't have to involve users so you can see there's no user involved here. And we simply assume that the top ranked documents to be relevant. Let's say we can assume top ten as relevant. And then we would then use these. Assumed documents to learn and to improve the query. Now you might wonder how could this help if we simply assume the top ranked documents to be relavant well. You can imagine these top ranked documents are actually similar to relevant documents, even if they are not relevant. They look like relevant documents, so it's possible to learn some related terms to the query from this set. In fact, there you may recall that we talked about using language model to analyze word association to learn related words to the word "computer" right, and then what we did is we first use computer to retrieve all the documents that contain computer. So imagine now the query here is a computer right? And then the results will be those documents that contain computer and what we can do then is to take the top N results. They can match computer very well and we're going to count the terms in this set. And then we're going to then use the background language model to choose the terms that are frequent in this set, but not frequent In the whole collection. So if we make a contrast between these two, what we can find these that would learn some related terms to the word computer? As we have seen before and these related words can then be added to the original query to expand the query and this would help us bring documents that don't necessarily match computer but match other words like a program and software. So this is the effective for improving the search result. But of course, pseudo relavance feedback is completely unreliable. We have to arbitrary set a cut off, so there's also something in between called implicit feedback. In this case, what we do is we do involve users, but we don't have to have asked users to make judgments instead, or even the observe how the user interacts with the search result. In this case, we're going to look at the clickthroughs so the user clicked on this one and the user viewed this one and the user skipped this one and the user view this one again. Now this also is a clue about whether a document is useful to the user. And we can even assume that we're going to use only the snippet here in this document. The text that's actually seen by the user instead of the actual document of this entry. The link there, let's say in web search may be broken, but then it doesn't matter if the user tried to fetch this document, because of the display, the text. We can assume these display. The text is probably relevant, is interesting to user. So we can learn from such information and this is called implicit feedback. And we can again use the information to update the query. This is a very important technique used in modern search engines. Will think about the Google and Bing and they can collect a lot of user activities while they're serving us, right? So they would observe what documents we click on, what documents will skip, and this information is very valuable and they can use this to improve the search engine. So to summarize, we talked about the three kinds of feedback here. Relevance feedback, where the user makes explicit judgments. It takes some user effort, but the judgment the information is reliable. We talk about pseudo feedback where we simply assume top-ranked documents to be relevant. We don't have to involve the user, therefore we could do that actually, before we return the results to the user. And the third is implicit feedback, where we use click clues. We don't we involved users, but the user doesn't have to make explicit effort to make judgment. 
This lecture is about the feedback in the vector space model. In this lecture we continue talking about feedback in text retrieval. Particularly, we're going to talk about feedback in the vector space model. As we have discussed before, in the case of feedback. The task of a Text Retrieval system is to learn from examples to improve retrieval accuracy. We will have positive examples. Those are the documents that are assumed to be relevant or judgement be relevant. Or the document that are viewed by users. We also have negative examples. Those are documents known in non-relevant, and they can also be the document that escaped by users. The general method in the vector space model for feedback. Is to modify our query vector and we want to place the query vector in a better position to make the accurate. And what does that mean exactly? If we think about the query vector, that would mean we have to do something to the vector elements. And in general, that would mean we might add new terms. We might adjust weights of all terms or assign weights to new terms. And as a result, in general the query will have more terms, so we often call this query expansion. The most effective method in the vector space model for feedback is called Walker feedback, which was after he proposed at several decades ago. So the idea is quite simple. We illustrate this idea by using a 2 dimensional display of all the documents in the collection and also the query vector. So now we can see the query vector is here in the center. And these are all the documents. So when we use the query vector and use a similarity function to find the most similar documents, we are basically drawing a circle here and then. These documents would be basically the top ranked of the documents. And these pluses are relevant documents. And these are relevant documents. For example is relevant, etc. And then these minuses are negative documents like this. So our goal here is trying to move this query vector to some position to improve the retrieval accuracy. By looking at this diagram. What do you think? Where should we move the query of after so that we can improve the retrieval accuracy? Intuitively, where do you want to move the query vector? Now, if you want to think more, you can pause the video. Now, if you think about this picture, you can realize that in order to work well in this case you want to query about that to be as close to the positive vectors as possible. That means ideally you want to place the query vector somewhere here, or you want to move the query vector closer to this point. Now, So what exactly is this point? Well, If you want these relevant documents to be ranked on the top, you want this to be in the center of all these relevant documents, right? Because then if you draw a circle around this one, you get all these relevant documents. So that means we can move the query vector towards the centroid of all the relevant locking vectors. And this is basically the idea of Rocchio you. Of course you can consider the centroid of negative documents, and we want to move away from the negative documents. Not geometrically, we're talking about the movie vector closer to some other bath and away from other vectors. Algebraically, it just means we have this formula. Here you can see this is original query vector. And, this average basically is the centroid vector of relevant documents. When we take the average of these vectors, then we are computing the centroid of these vectors and similarly this is the average non relevant document vectors. So it's essentially of non relevant documents and we have these three parameters here of our alpha beta and gamma they're controlling the amount of movement. When we add these two vectors together, we are moving the query closer to the centroid. I said add them together when we subtract this part. We kind of move the query vector away from that Central. So this is the main idea of Rocchio feedback and after we have done this we will get a new query vector which can be used to store documents. This newer query vector will then reflect the move of this original query vector toward this relevant centroid vector and away from the non relevant centroid vector. OK, so let's take a look at the example. This is the example that we have seen earlier, only that I give the display of the actual documents. I only showed the vector representation of these documents. We have 5 documents here. And we have. Two relevant the documents here. Right? They are displayed in red and these are the term vectors and I have just assumed some TF IDF weights. A lot of terms. We have zero weights of course and these are negative documents. There are two here. There was another one here. Now in this Rocchio method we first compute the centroid of each category I so let's see. Look at the centroid vectors of the positive documents. What we simply just, so it's very easy to see. We just add this with this one. The corresponding element, and that's down here and take the average and then we will know added the corresponding elements and then just take the average right? So we do this for all these. In the end what we have is this one. This is the average vector of these two. So it's a centroid of this two. That's also look at the centroid of the negative documents. This is basically the same. we are gonna take the average of three elements. And these are the corresponding elements in the three vectors, and so on so forth. So in the end we have this one. Now in the Rocchio Feedback Method, we're going to combine all these with the original query vector, which is this. So now, let's see how we combine them together. That's basically this. I saw we have a parameter alpha controlling the original period weight, that's one, and then we have beta to control the influence of the positive centroid weight that's 1.5 that comes from here. Alright, so this. Goes here. And we also have this negative weight here, controlled by Gamma here and this weight that has come. From of course the negative centroid here. And we do exactly the same for other terms. Each is for one term. And this is our new of vector. And we're going to use this new query vector. This one to rent the documents. You can imagine what would happen right because of the movement that this one would match these red documents much better because we move this vector closer to them and it's going to penalize these black documents in these non relevant documents. So this is precisely what we want from feedback. Now, of course, if we apply this method in practice. We will see one potential problem. And that is the original query has only four terms that are. non-zero But after we do query expansion, you can imagine it would have many terms that would have nonzero weights. So the calculation would have to involve more terms. In practice, we often truncate this vector, and only retain the terms with highest weights. So let's talk about how we use this method in practice. I just mentioned that we often truncated adapter and see the only a small number of words that have highest weights in the centroid vector. This is for efficiency concern. I also say that here that negative examples or non relevant examples tend not to be very useful, especially compared with positive examples. Now you can think about why. One reason is because negative documents tend to distract the query in all directions. So when you take the average it doesn't really tell you where exactly should be moving too. Where is positive documents tend to be clustered together and they will point you to a consistent direction. So that also means sometimes we don't have to use those negative examples. But note that in some cases in difficult queries where most top ranking results are negative, negative feedback factor is very useful. Another thing is to avoid overfitting. That means we have to keep relatively high weight on original query terms. Why? Because The sample that we see in feedback is a relatively small sample. We don't want to overly trust the small sample. An the original query terms are still very important. Those terms of typing by the user and the user has decided that those terms are most important. So in order to prevent us from overfitting or drifting the topic with preventing topic drifting due to bias toward the feedback examples, we generally would have to keep a pretty high weight on the original terms. It's always safe to do that. And this is especially true for pseudo relevance feedback. Now this method can be used for both relevance feedback and pseudo relevance feedback. In the case of pseudo feedback up, the parameter beta should be set to a smaller value because the relevant examples are assumed to be relevant. They are not as reliable as the relevance feedback. In the case of relevance feedback, we obviously could use a larger value, so those parameters they have to be set in imparallelly. And the root Rocchio. method is usually robust. and effective. It's still very popular method of all feedback. 
This lecture is about the feedback in the language modeling approach. In this lecture we will continue the discussion of feedback in text retrieval. In particular, we're going to talk about the feedback in language modeling approaches. So we derive the query likelihood ranking function by making various assumptions. As a basic retrieval function, that formula of those formulas worked well, but if we think about the feedback information, it's a little bit of awkward to use query like hold too. Perform feedback because a lot of times the feedback information is additional information about the query. But we assume that the query is generated by assembling words from a language model in the query likelihood method. It's kind of a natural to sample words that form feedback documents as a result, then researchers proposed a way to generalize query like hold function and it's called a callback labeler divergance retrieval model. And this model is actually going to make the query likelihood retrieval function much closer to vector space model. Yet, this form of the language model can be regarded as a generalization of query like hold in the sense that it can cover query likelihood as a special case. And in this case, then, feedback can be achieved through simple query model estimation or updating. This is very similar to Rock Hill which updates the query vector. So let's see. What is this care? Divergent switchable model? So on the top, what you see is a query like hold. Retrieval function right this one. And then kill diversions or also called cross entropy. Whichever model is basically to generalize. The frequency part here. Into a language model. So basically it's the difference. Given by the probabilistic model here to characterize what the user is looking for versus the count of query words there. And this difference allows us to plug in various different ways to estimate this, so this can be estimated in many different ways, including using feedback information. Now this is called a care divergens becausw. This can be interpreted as measuring the care divergent of two distributions. One is the query model denoted by this distribution. One is talking the language model. Here smoothly with collection language model, of course an. We're not going to talk about the detail of and then find it in some references. It's also called a cross entropy, because in effect we can ignore some terms in the care divergent function and we will end up having actually cross entropy that both our terms in information theory. But anyway for. Our purpose here you can just receive the two formulas look almost identical, except that here we have a probability of award given by a query language model. All this. And here the sum is over all the words that are in the document and also with the non zero probability for the query model. So it's kind of again a generalization of some overall the match query words. Now you can also easy to see we can recover the query like hold retrieval function by simply setting this query model to the relative frequency of award in the query. This is very easy to see once you plug this into. Here you can eliminate this query Lance. That's a constant and then you get exactly like that. So you can see the equivalence. And that's also why this care divergent model can be regarded as a generalization of query, like whole because we can cover query like Rd as a special case. But it would also allow such rule much more than that. So this is how we can use the care divergent model to them the feedback the picture shows that we first estimate the document language model. Then we estimate the query name model and we compute the KL diversions, as is often denoted by AD here. But this basically means this is exactly like a vector space model 'cause we computer vector for the document the computer, another vector for the query and then we compute the distance only that these vectors are of special forms their probability distributions. And then we got the results and we can find some feedback documents. Let's assume they are most inactive. Sorry, mostly positive documents, although we could also consider both kinds of documents. So what we could do is like in rock you'll ever know computer another language model, coder feedback, language model here. Again, this is going to be another vector, just like a computing central about the in Rock Hill. And then this model can be combined with the original query model. Using a linear interpolation. And this would then give us a update model just like again in Rock Hill. So here we can see the parameter Alpha can control the amount of feedback if it's set to 0, then it says here there's no feedback after set to one, we got 4 feedback. If we ignore the original query and this is generally not desirable, right? So this unless you are absolutely sure you have seen a lot of relevant documents and the query terms are not important. So of course the main question here is how do you compute this single F? This is the big question here, and once you can do that, the rest is easy. So here we will talk about one of the approaches and there are many approaches. Of course this approach is based on generated model. And I'm going to show you how it works. This is the user generated mixable. So this picture shows that we have this model. Here. The feedback model that we want to estimate. And will the basis is the feedback documents. Let's say we are observing the positive documents. These are the click the documents by users or relevant documents judged by users or simply top ranked blocking that we assume to be relevant. Now imagine how we can compute the centroid for these documents by using language Model 1. Approach is simply to assume these documents are generated from this language model as we did before. What we could do is do just normalize the word frequency here and then we got this world distribution. Now the question is whether this distribution is good for feedback. But you can imagine. The top ranked the world would be what? What do you think? Those words would be common words, right? As we always see in a language model, the top ranked words are actually common words like the etc. So it's not very good for feedback because we would be adding a lot of such words to our query when we interpret this with the original query model. So this is not good. So when it do something in particular will are trying to get rid of those common words and we are we have seen actually one way to do that by using background language model. In the case of learning the associations with words, words that are related to the water computer. We could do that and there will be another way to do this, but here we are going to talk about another approach which is more principled approach. In this case, we're going to state, well, you said that there are common words here in this. These documents that should not belong to this topic model, right? So now what we can do is to assume that, well, those words are generated from background language model, so they were generated those words like the. Example. And if we use maximum likelihood estimator, note that if all the words here must be generated from this model, then. This model is forced to assign high probabilities to award like that because it occurs so frequently here. Note that in order to reduce its probability in this model. We have to have another model which is this one to help explain the world. The here and in this case it's not appropriate to use the background language model to achieve this goal, because this model would assign high probabilities to these common words. So in this approach, then we assume this machine that was generated. These words would work as follows. We have a source controller here. Imagine we flip a coin here to decide what distribution to use with probability of Lambda. The coin shows up as head and we're going to use the background language model and we can do that simple word from that model with probability of 1 minus them. Now will do decide to use the unknown. Topic model here that we would like to estimate and we're going to then generate award here. If we make this assumption and this whole thing would be just one model and we call this mixture model 'cause there are two distributions that are mixed together and we actually don't know when each distribution is used. So again, think of this whole thing as one model. And we can still ask for words, and it will still give us a word in a random manner, right? And of course, which word will show up would depend on both this distribution and that distribution. In addition, it would also depend on this Lambda, because if you say Lambda is very high and it's going to always use the background distribution, you'll get different words than if you say lemme's very small, we're going to use this. Right, so all these are parameters. In this model. And then if you think in this way, basically we can do exactly the same as what we did before. We are going to use maximum likelihood estimator to adjust this model to estimate the parameters. Basically we're going to adjust well this parameter. So that we can best explain all the data. The difference now is that we are not asking this model alone to explain this. But rather, we're going to ask this whole model mixture model to explain the data becauses there has got some help from the background model. It doesn't have to assign high probabilities towards like the. As a result, it would then assign higher probabilities to other words that are common here. But not having high probability here. So those will be common here. I. And if they are common, they would have to have high probabilities according to maximum likelihood estimator. An if they are rare here. Right, so if they are rare here. Then you don't get much help from this background model. As a result, this topic model must assign high probabilities, so the high probability words according to the topic model would be those that are common here, but rare in the background. OK, so this is basically a little bit a IDF waiting here. But this would allow us to achieve the effect of removing this top awards that are meaningless in the feedback. So mathematically, what we have is to compute the like hold again local, like hold of the feedback documents and. And note that we also have another parameter Lambda here, but we assume that the Lambda denotes the noise in the feedback document. So we are going to, let's say set this to a parameter that say 50% of the words are noise or 9% are noise and this can be assumed to be fixed if we assume this is fixed. Then We only have these probabilities as parameters, just like in the simplest unigram language model. We have end parameters and is the number of words. And then the likelihood function would look like this. It's very similar to the likelihood function local. I can hold a function we see before, except that inside the logarithm there's a some here, and this sum is becausw. We consider two distributions. And which one is used would depend on Lambda and that's why we have this form. But mathematically, this is their function with theater as unknown variables, right? So this is just a function or the other values are known except for this guy. So we can then choose this probability distribution to maximize this locali code. The same idea as the maximum, like Horace made it as a mathematical problem. We just we just have to solve this optimization problem. We essentially would try all the theater values and until we find one that gives this whole thing the maximum probability. So it's a well defined math problem. Once we have done that, will obtain the serial F that can be there, interpreted with the original query model to do feedback. So here are some examples of the feedback model learned from a Web document collection, and we do sudo feedback. Are we just use the top ten documents and we use this mixture model so the query is airport security? What we do is we first retrieve 10 documents from the web database. And this is of course a pseudo feedback. I and then we're going to feed that mixture model to this 10. Document set. And these are the words learned using this approach. This is the probability of award given by the feedback model in both cases. So in both cases you can see the highest probability words include very relevant words to the query, so airport security, for example. This query words still show up as high probabilities in each case naturally becausw they occur frequently in the top ranked documents, but we also see beverage, alcohol, bomb, terrorists, etc. So these are relevant to this topic and they if combined with the original query. Can help us match more accurately documents and also they can help us bring up a documents that only imagine the some of these other words. Maybe for example just the airport and then bomb for example this. So this is how single feedback works. Issues that this model really works and picks up some related words to the query. What's also interesting is that if you look at the two tables here and you compare them and you see in this case when Lambda is set to a small value and we still see some common words here. And that means. When we don't use the background more often, remember Lambda can use the probability of using the background model to generate the text. If we don't rely much on background model, we still have to use this topic model to account for the common words, whereas if we set Lambda to a very high value, we will use the background model very often to explain these words. Then there's no burden on explaining those common words in the feedback documents by the topic model. So as a result of the topic model, here is very discriminant if it contains all the relevant words without common words. So this can be added to the original query to achieve feedback. So to summarize, in this lecture we have talked about the feedback in language model approach. In general, feedback is to learn from examples. These examples can be assumed, examples can be sued, examples like. Assume that the top ten documents that are assumed to be random in there could be based on using fractions like a feedback based on pixels or implicit feedback. We talked about the three major feedback scenarios, relevance feedback, sooner feedback, and in principle feedback. We talked about how to use Rock You to do feedback in vector space model and how to use query model is missing for feedback in language model and we briefly talked about the mixture model and the basic idea. There are many other methods, for example, the relevance model is a very effective model for estimating query model. So you can read more about these methods in the references that listed at the end of this lecture. So there are two additional readings here. The first one is a book that has a systematic review and discussion of language models for information retrieval and signal. One is important research paper that's about relevance based language models, and it's a very effective way of computing query model. 
This lecture is about the web search. In this lecture we're going to talk about one of the most important applications of text retrieval: web search engines. So let's first look at the some general challenges and opportunities in web search. Now, many information retrieval algorithms had been developed before the web was born, so when the web was born, it created the best opportunity to apply those algorithms to major application problem that everyone would care about. So naturally there had to be some further extensions of the classical search algorithms. To address some new challenges encountered in web search. So here are some general challenges. Firstly, there is a scalability challenge. How to handle the size of the web an ensure completeness of coverage of all the information. How to serve many users quickly and by answering all their queries? So that's one major challenge before the web was born, the scale of search was relatively small. The second problem is that this low quality information and there are often spams. The third challenge is dynamics of the web. The new pages are constantly created and some pages may be updated very quickly, so it makes it harder to keep the index fresh so these are some of the challenges that we have to solve in order to build a high quality web search engine. On the other hand, there are also some interesting opportunities that we can leverage to improve search results. There are many additional heuristics. For example, using links that we can leverage to improve scoring. Now the algorithm that we talked about, such as a vector space model are general algorithms. And there can be applied to any search applications, so that's the advantage. On the other hand, they also don't take advantage of special characteristics of pages or documents in the specific applications such as web search. Web pages are linked with each other, so obviously the link information is something that we can also leverage. So because of these challenges and opportunities, there are new techniques that have been developed for web search or due to the need for web search. One is parallel indexing and searching and this is to address the issue of scalability. In particular, Googles's imagine of MapReduce is very influential and has been very helpful in that Aspect. Second, there are techniques that are developed for addressing the problem of spams. So spam detection we have to prevent those spam pages from being ranked high. And there are also techniques to achieve robust ranking, and we're going to use a lot of signals to rank pages so that it's not easy to spam the search engine with a particular trick. And the third line of techniques is link analysis. And these are techniques that can allow us to improve search results by leveraging extra information. And in general, in web search we're going to use multiple features for ranking, not just a link analysis, but also exploit in all kinds of clues, like the layout of web pages or anchor text that describes a link to another page. So here's a picture showing the basic search engine technology is. Basically this is the web on the left, and then user on the right side, and we're going to help this user to get the access for the web information and the first component is the crawler. That would crawl pages. And then the second component is indexer that would take these pages and create a inverted index. The third component that is a retriever that would use inverted index to answer users query by talking to the users browser and then the search results will be given to the user and then the browser would show those results and to allow the user to interact with the web so we are gonna talk about each of these components. First we're going to talk about the crawler. Also called a spider or software robot that would do something like a crawling pages on the web. To build a toy crawler is relatively easy 'cause you just need to start with a set of seed pages and then fetch pages from the web and pause these pages or and figure out the new links and then add them to the priority queue and then just explore those additional links. But to build a real crawler actually is tricky and there are some complicated issues that you have to deal with. So for example, robustness. What if the server doesn't respond? What if there's a trap that generates dynamically generated web pages that might attract your crawler to keep crawling the same site and to fetch dynamically generated pages? The results with this issue of crawling courtesy and you don't want to overload one particular server with many crawling requests. You have to respect the robot exclusion protocol. You also need to handle different types of files. There are images, PDF files, all kinds of formats on the web. Ann, you have to also consider UI or extension, so sometimes those are CGI script an there are internal references etc and sometimes you have JavaScripts on the page that they also create the challenges and you ideally should also recognize redundant pages 'cause you don't have to duplicate the those pages. And finally, you may be interested in the discover hidden urls. Those are URLs that may not be linked to any page, but if you truncate the URL to a shorter path that you might be able to get some additional pages. So what are the major crawling strategies in general? Breadth first is most common becauses naturally balance balance is the server load. You would not keep probing a particular server with many requests. Also, parallel crawling is very natural because this task is very easy to parallelize. And there are some variations of the crawling task, and one interesting variation is called focused crawling. In this case, we're going to crawl just some pages about a particular topic. For example, all pages about the automobiles. And this is typically going to start with a query and then you can use the query to get some results from a major search engine and then you can start with those results and gradually crawl more. So one challenge in crawling is to find the new pages that people have created and people probably are creating new pages all the time. And this is very challenging if the new pages have not been actually linked to any old page. If they are, then you can probably find them by re-crawling the old page. So these are also some interesting challenges that have to be solved. And finally, we might face the scenario of incremental crawling or repeated crawling, right? So your first, let's say if you want to build a web search engine and you're the first crawl a lot of data from the web. And then but then, once you have collected all the data and in the future we just need to crawl the updated pages. In general you don't have to re-crawl everything right? Or it's not necessary. So in this case you your goal is to minimize the resource overhead by using minimum resources to just still crawl the updates to pages. So this is actually very interesting research question here, and it's still open research question in that there aren't many standard algorithms established yet for doing this task. But in general, you can imagine you can learn from the past experience. So the two major factors that you have to consider are first will this page be updated frequently and do I have to crawl this page again if the page is a static page that hasn't been changed for months, you probably don't have to re-crawl it everyday. Because it's unlikely that it will be changed frequently. On the other hand, if it's a sports score page that gets updated very frequently and you may need to re-crawl the maybe even multiple times on the same day and the other factor to consider is this page frequently accessed by users. If it is, then it means it's a high utility page and then that's it's more important to ensure such a page to be fresh. Compared with another page that has never been fetched by any users for a year, then even though that page has been changed a lot, then it's probably not necessary to crawl that page, or at least it's not as not as urgent as to maintain the freshness of frequently accessed page by users. So to summarize, web search is one of the most important applications of text retrieval, and there are some new challenges, particularly scalability, efficiency, quality information. There are also new opportunities, particularly rich link information and layout, etc. A crawler is an essential component of web search applications. And in general we can classify two scenarios. One is initial crawling and here we want to have complete crowding. Of the web. If you are doing a general search engine or focused crawling, if you want to just to target at a certain type of pages. And then there is another scenario that's incremental updating of the crawled data or incremental crawling. In this case you need to optimize the resource. Try to use minimum resource to get the needed fresh information. 
This lecture is about the web indexing. In this lecture we will continue talking about the web search and we're going to talk about how to create a web scale index. So once we crawled the web, we've got a lot of web pages. The next step is to use the indexer to Create the inverted index. In general, we can use the standard information retrieval techniques for creating the index, and that is what we talked about in the previous lecture. But there are new challenges that we have to solve for web scale indexing and the two main challenges. Our scalability and efficiency. The index would be so large that it cannot actually fit in into any single machine or a single disk, so we have to store the data on multiple machines. Also, because the data is so large, it's beneficial to process the data in parallel so that we can produce the index quickly. Now, to address these challenges, Google has made a number of innovations. One is the Google file system that's a general distributed file system that can help programmers manage files stored on a cluster of machines. The second is MapRecuce. This is a general software framework for supporting parallel computation. Hadoop is the most known open source implementation of map reduce, now used in many applications. So this is the architecture of the Google File System. It uses very simple centralized management mechanism to manage it all the specific locations of files, so that maintains the file name, space and look up a table to know where exactly each file is stored. The application client would then talk to this GFS master and then obtain specific locations of the files that they want to process. And once the GFS client obtained the. The specific information about the files, then the application client can talk to the specific servers where the data actually sit directly so that you can avoid involving other nodes in the network. So when this file system stores the files on machines the system also would create a fixed size of chunks so that data files are separated into many chunks. Each chunk is 64 MB, so it's pretty big, and that's a property for large data processing. These chunks are replicated to ensure reliability, so this is something that the programmer doesn't have to worry about. It's all taken care of by this fire system, so from the application perspective, the programmer would see this as if it's a normal file. The program doesn't have to know where exactly it's stored and can just invoke high level operators to process the file. (And) Another feature is that the data transfers directly between application and chunk servers, so it's efficient in this sense. On top of the Google File System and Google also proposed map reduce as a general framework for parallel programming. Now this is very useful to support a task like a building inverted index. So this framework is hiding a lot of low level features from the program. As a result, the programmer can make a minimum effort to create a application that can be run on large cluster in parallel. And so some of the low level details hidden in the framework, including the specific network communications or load balancing or where the tasks are executed. All these details are hidden from the programmer. There is also a nice feature which is the built-in fault tolerance. If one server is broken, let's say service down and then some tasks may not be finished, then the map reduce mechanism would know that the task has not been done, so automatically dispatches the task on other servers that can do the job and therefore again the program doesn't have to worry about that. So here's how MapReduce works. The input data will be separated into a number of key value pairs. Now, what exactly is in the value will depend on the data, and it's actually a fairly general framework to allow you to just partition the data into different parts, and each part can be then processed in parallel. Each key value pair would be send to a map function. The programmer would write map function of course. And then the map function would then process this key value pair and would generator a number of other key value pairs. Of course the new key is usually different from the old key that's given to the map as input. And these key value pairs are the output of the map function and all the outputs of all the map functions would be then collected. And then there will be for the sorted based on the key, and the result is that all the values that are associated with the same key would be then grouped together. So now we've got a pair of a key and a set of values that are attached to this key. So this will then be sent to a Reduce function. Now of course, each Reduce function will handle a different key, so we will send these output values to multiple, reduce functions, each handling unique key. A reduce function would then process the input. Which is a key and a set of values to produce another set of key values as the output. So these output values will be then collected together to form the final output. Right, so this is the general framework of MapReduce. Now the programmer only needs to write the Map function and the Reduce function. Everything else is actually taken care of by the MapReduce framework. So you can see the program really only needs to do minimum work. And with such a framework the input data can be partitioned into multiple parts, each is processed in parallel, first by map, and then in the process after we reach the reduce stage, then multiple reduce functions can also further process the different keys and their associated values in parallel, so it achieves (some) It achieves the purpose of parallel processing of large data set. So let's take a look at a simple example and that's what accounting. How the input is files containing words. And the output that we want to generate is the number of occurrences of each word, so it's the word account. We know this kind of counting would be useful to, for example, assess the popularity of a word in a large collection, and this is useful for achieving effect of IDF weiging. Or search. So how can we solve this problem? One natural thought is that. This task can be done in parallel by simply counting different parts of the file in parallel, and then in the end we just combine all the counts, and that's precisely the idea of what we can do with MapReduce. We can parallelize on lines in this input file. So more specifically, we can assume the input to each map function is key value pair that represents the line number and the stream on that line. So the first line, for example, has the key of 1 and the value is "Hello World" "Bye World" and just 4 words. On that line so this key value pair will be sent to a map function. The map function would then just count the words in this line, and in this case of course there are only four words. Each word gets a count of one, and these are the output that you see here on this slide. From this map function. So the map function is really very simple if you look at the what the pseudocode looks like on the right side you see it simply needs to iterate over all the words in this line and then just call the collect function, which means it would then send the world and the counter to the collector. The collector would then try to sort all these. key value pairs from different Map functions, so the function is very simple and the programmer specifies this function as a way to process each part of the data. Of course, the second line will be handled by a different map function, which will produce a similar output. OK, now the output from the map functions will be then send to a collector and the collector will do the internal grouping or sorting. So at this stage you can see we have collected multiple pairs, each pair is a word and its count in the line. So once we see all these pairs then we can sort them based on the key which is the word. So we will collect all the counts of a word like a "Bye" here together. An similarly we do that for other words like "Hadoop", "Hello" etc. So each world now is attached to a number of values, a number of counts. And these counts represent the occurrences of this word in different lines. So now we have got a new pair of a key and a set of values and this pair will then be feeding to reduce function. So the reduce function now would have to finish the job of counting the total occurrences of this word. Now it has already got all these partial accounts, so all it needs to do is similarly to add them up so the reduce function shown here is very simple as well. You have a counter and then iterate over all the words that you see in this array, and then you just accumulated the count. And then finally output the key and the total count, and that's precisely what we want as the output of this whole program. So you can see this is already very similar to building an inverted index, and if you think about it, the output here is indexed by world and we have already got the dictionary. Basically we have got the counts, but what's missing is the document IDs and the specific frequency counts of words in those documents, so we can modify this slightly to actually build inverted index in parallel. So here's one way to do that. So in this case we can assume the input to map function is a pair of a key, which denotes the document ID and the value denoting the stream for that document. So it's all the words in that document, and so the map function will do something very similar to what we have seen in the word count example. It simply groups all the counts of this word in this document together and it would then generate the set of key value pairs. Each key is a word. And the value is the count of this orld in this document, plus the document ID. Now you can easily see why we need to add document ID here. Of course later in the inverted index we would like to keep this information so the map function should keep track of it and this can be sent to the reduce function later. Similarly, another document D2 can be processed in the same way, so in the end that again there was a sorting mechanism that would group them together and then we will have just a key like "Java" associated with all the documents that match this key or the documents where "Java" occurred. And the counts. So the counts of Java in those documents. And this will be collected together and this will be also fed into the reduce function. So now you can see the reduce function has already got input that looks like a inverted index entry, right? So it's just the word and all the documents that contain the word and the frequencies of the word in those documents. So all it needs to do is simply to concatenate them into a continuous chunk of data, and this can be then written into a file system. So basically the reduce function is going to do very minimum work. So this is pseudocode for inverted index construction. Here we see two functions. Procedure Map and procedure Reduce. And a program with the specify these two functions to program on top of map reduce and you can see basically they are doing what I just described. In the case of map, it's going to count the occurrences of word using associative array and will output the old accounts together with the document ID here. So this is the reduce function On the other hand, simply concatenates all the input that it has been given and then put them together as one single entry for this key. So this is a very simple MapReduce function, yet it would allow us to construct the inverted index at very large scale and the data can be processed by different machines. The program doesn't have to take care of the details. So this is how we can do parallel index construction for web search. So to summarize, web scale indexing requires some new techniques that go beyond the standard traditional indexing techniques, mainly will have to store the index on multiple machines, and this is usually done by using file system like a Google File System, a distributed file system. And secondly, it requires creating the index in parallel because it's so large it takes a long time to create the index for all the documents. So if we can do it in parallel it will be much faster and this is done by using the MapReduce framework. Note that the post the GFS and MapReduce frameworks are very general so they can also support many other applications. 
This lecture is about link analysis for web search. In this lecture we're going to talk about web search. And particularly focusing on how to do link analysis and use the results to improve search. The main topic of this lecture is to look at the ranking algorithms for web search. In the previous lecture we talked about how to create index now that we have got index. We want to see how we can improve ranking of pages. Our standard IR models can be also applied here. In fact they are important building blocks for improvement for supporting web search, but they aren't sufficient and mainly for the following reasons. First, on the web we tend to have very different information needs. For example, people might search for a web page or entry page and this is different from the traditional library search where people are primarily interested in collecting literature information. So this kind of query is often called navigational queries. The purpose is to navigate into a particular target page. So for such queries we might benefit from using link information. Secondly, documents have additional information and on the web web pages are well format. There are a lot of other clues such as the layout, title or link information. Again, so this has provided the opportunity to use extra context information. Of the document to improve scoring and finally information quality varies a lot, so that means we have to consider many factors to improve the ranking algorithm. This would give us a more robust way to rank the pages, making it harder for any spammer to just manipulate the one signal to improve the ranking of a page. So as a result, people have made a number of major extensions to the ranking algorithms. One line is to exploit links to improve scoring. And that's the main topic of this lecture. People have also proposed algorithms to exploit the large scale implicit feedback information in the form of click throughs, and that's of course in the category of feedback techniques. And machine learning is often used there. In general in web search the ranking algorithms are based on machine learning algorithms to combine all kinds of features. Many of them are based on the standard visual models such as BM25 that we talked about. Or query likelihood to score different parts of documents or to provide additional features based on content matching, but link information is also very useful so they provide additional scoring. Signals. So let's look at links in more detail on the web. So this is a snapshot of some part of the web and say so we can see there are many links that link the different pages together, and in this case you can also look at the center here. There is a description of a link that's pointing to the document on the right side. Now this description text is called anchor text. Now if you think about the this text, it's actually quite a useful because it provides some extra description of that page being pointed to. So for example, if someone wants to bookmark Amazon.com, front page the person might say. The biggest online bookstore and then with the link to Amazon. Right, so the description here actually is very similar to what the user will type in the query box when they are looking for such a page, and that's why it's very useful for ranking pages. Suppose someone types in query like online bookstore or fixed online bookstore. The query would match this anchor text. In the page. Here and then, this actually provides evidence for matching the page that's being pointed to. That is the Amazon entry page. So if you match the anchor text that describes a link to a page, actually that provides good evidence for the relevance of the page being pointed to, so anchor text is very useful. If you look at the bottom part of this picture you can also see there are some patterns of links, and these links might indicate the utility of a document. So for example on the right side you can see this page has received, many in links. That means many other pages are pointing to this page and this shows that this page is quite useful. On the left side you can see, this is another page that points to many other pages, so this is a directory page that would allow you to actually see a lot of other pages. So we can call the first case authority page and the second case half page. This means the link information can help in two ways. One is to provide extra text for matching and the other is to provide some additional scores for the web pages to characterize how likely a pages have, how likely a pages authority. So people then of course proposed ideas to leverage these in this link information. Now Google's Pagerank, which was the main technique that they used in early days, is a good example and that is an algorithm to capture page popularity, basically to score authority. So the intuition's here are links are just like a citations in the literature. Think about one page pointing to another page. This is very similar to one paper citing another paper. So of course, then if a page is cited often, then we can assume this page to be more useful, in general. So that's a very good intuition. Now Pagerank is essentially to take advantage of this intuition to implement it with the principled approach. Intuitively, it's essentially doing citation counting or in link counting. It just improves this simple idea in two ways. One is it would consider indirect citations. So that means you don't just look at the how many in links you have. You also look at the what are those pages that are pointing to you. If those pages themselves have a lot of in links, well that means a lot. In some sense you will get some credit from them. But, if those pages that are pointing to you are not being pointed to by other pages, they themselves don't have many in links then, well, you don't get that much credit. So that's the idea of getting indirected citation. Alright you can also understand this idea by looking at again the research papers. If you're cited by let's say 10 papers and those 10 papers are just workshop papers and or some papers that are not very influential, right? So although you get 10 in links and that's not as good as if you were cited by 10 papers that themselves have attracted a lot of other citations. So this is. A case where we would like to consider indirect links and Pagerank does that. The other idea is it's going to smooth the citations or assume that basically every page is having a non zero pseudo citation count. Essentially we're trying to imagine there are many virtual links that will link all the pages together so that you actually get pseudo citations from everyone. The reason why they want to do that is this would allow them to solve the problem elegantly with. Linear algebra technique. So. I think maybe the best way to understand the page rank is through. Think of this as do computer the probability of random surfer, visiting every web page. 
So let's take a look at this in detail. So in this random surfing model. At any page would assume random surfer would choose the next page to visit, so this is a small graph here. That's of course oversimplification of the complicated web, but let's say there are four documents here, d1 d2 d3 and d4, and let's assume that a random surfer or random walker can be on any of these pages. And then the random surfer could decide to just randomly jump into any page. Or follow a link and then visit the next page. So if the random server is at d1. Then With some probability that random surfer will follow the links. Now there are two out links here. One is pointing to d3, the other is pointing to d4, so the random surfer could pick any of these two to reach d3 and d4. But it also assumes that the random surfer might get bored sometimes, so the random surfer will decide to ignore the actual links and simply randomly jump to any page on the web. So if it does that, it would be able to reach any of the other pages, even though there's no link directly from d1 to that page. So this is assumed random surfing model. Imagine a random surfer is really doing a surfing like this. Then we can ask the question how likely on average the surfer would actually reach a particular page like d1 or d2 or d3 really, that's the average probability of visiting a particular page. And this probability is precisely what Pagerank computes. So the Pagerank score of the document is the average probability that the surfer visits a particular page. Now, intuitively, this would basically capture the in link account. Why? Because if a page has a lot of in links, then it would have a higher chance of being visited because there will be more opportunities of having the surfer to follow a link to come to this page. And this is why. The random surfing model actually captures the idea of counting the in links. Note that it also considers the indirect in links. Why? Because if the pages that point to you have themselves a lot of in links. That would mean the random surfer will very likely reach one of them, and therefore it increases the chance of visiting you. So this is a nice way to capture both indirect and direct links. So mathematically, how can we compute this probability in order to see that we need to take a look at how this probability is computed. So first, let's take a look at the transition matrix here. And this is just the metrics with values indicating how likely I ran. The random surfer will go from one page to another, so each row stands for a starting page. For example, row one would indicate the probability of going to any other 4 pages from d1, and here we see there are only two non zero entries, each is 1 over 2. So, this is because if you look at the graph d1 is pointing to d3 and d4, there is no link from d1 to d1 itself or d2, so we've got zeros for the first 2 columns and .5 for d3 and d4. In general, the element in this matrix M sub i, j is the probability of going from d,i to d,j and obviously for each row the values should sum to 1 because the surfer would have to go to precisely one of these other pages, right? So this is the transition matrix. Now, how can we compute the probability of a surfer visiting a page? If you look at the surf model then basically we can compute the probability of reaching a page as follows. So. Here on the left hand side you see it's the probability of visiting page d,j at time T + 1, so it's the next time point. On the right hand side you can see the equation involves the probability of at Page d,i at time T. So you can see the subscript index t here, and that indicates that the probability that the surfer was at a document at time t. So. The equation basically captures the two possibilities of reaching at d,j at time T + 1. What are these two possibilities? One is through random surfing and one is through following a link as we just explained. So the first part captures the probability that the random surfer would reach this page by following a link, and you can see the random surfer chooses this strategy with probability 1 minus alpha as we assume and so there is a factor of 1 minus alpha here, but the main part is really sum over all the possible pages that the surfer could have been at time t. There are N pages, so it's a sum over all the possible N pages. Inside the sum is a product of two probabilities. One is the probability that the surfer was at d,i the time t. That's p sub t of d,i. The other is the transition probability from the d,i to d,j. And so in order to reach this d,j page, the surfer must first be at d,i at time t and then also would have to follow the link to go from the d,i to d,j. So the probability is the probability of being at d,i at time t multiplied by the probability of going from that page to the target page. d,j here. The second part is a similar sound. The only difference is that now the transition probability is a uniform transition probability of 1 / N and this part of captures the probability of reaching this page through random jumping. Right, so the form is exactly the same and is. This also allows us to see why Pagerank essentially assume the smoothing of the transition matrix. If you think about this 1 / N as coming from another transition matrix that has all the elements being 1 / N The uniform matrix, then you can see very clearly. Essentially we can merge the two parts. And because they are of the same form, we can imagine there's a different matrix that's a combination of this M and that uniform matrix, where every element is 1 / N, and in this sense Pagerank uses this idea of smoothing and ensuring that there's no zero entry in such a transition matrix. Now of course this is time dependent calculation of probabilities. Now we can imagine if we want to compute the average probabilities, the average probabilities probably would satisfy this equation without considering the time index. So let's drop the time index and just assume that they will be equal. Now this would give us N equations because for each page we have such equation and if you look at the what variables will have in these equations there are also precisely N variables. Right? So this basically means we now have a system of N equations with N variables. And these are linear equations. So basically the problem boils down to solve this system of equations. And here I also showed the equations in the matrix form. It's the vector p here. Equals a metrics of the transverse of the matrix here. And multiply by the vector again. Now, if you still remember some knowledge that you've learned from linear algebra and then you will realize this is precisely the equation for item vector, right when you multiply the matrix by this vector, you get the same value as this vector. And this can be solved by using iterative algorithm. So the equations here on above are basically taken from the previous slide, so you see the relation between the. The Pagerank scores of different pages and in this iterative approach or power approach, we simply start with. Randomly initialized vector p and then we repeatedly just updated this p by multiplying the matrix here by this p vector. So I also show a concrete example here. So you can see this now if we assume alpha is .2, then with the example that we show here on this slide we have the original transition matrix here. Right? That includes the graph, the actual links, and we have this smoothing transition matrix uniform transition matrix representing random jumping and we can combine them together with a linear interpolation to form another matrix. That would be like this. So essentially we can imagine now the web looks like this. Can be captured by that there are virtual links between all the pages now. So the Pagerank algorithm would just initialize the p vector first and then just compute the updating of this p vector by using this matrix multiplication. Now if you rewrite this matrix model multiplication in terms of just, individual equations, you will see this. And this is Basically the updating formula for this particular pages Pagerank score so you can also see you if you want to compute the value of this updated score for d1 you basically multiply this rule. Right, by this column. And we take the dot product of the two. That would give us the value for this value. So this is how we updated the vector. We started with some initial values for these guys. For this and then we just revise the scores we generate a new set of scores and the updating formula is this one. So we just repeatedly apply this and here it converges and when the metrics is like this where there's no zero values and it can be guaranteed to converge. And at that point that we will just have the Pagerank scores for all the pages. Now we typically set the initial values just to 1 / N. So Interestingly, this updating formula can be also interpreted as propagating scores over the graph. Can you see why? If you look at this formula and then compare that with this graph? And can you imagine how we might be able to interpret this as essentially propagating scores over the graph? I hope you will see that indeed we can imagine we have values initialized on each of these pages, so we can have values here. Let's say that's 1 /4 for each, and then we're going to use this matrix to update these scores. And if you look at the equation here. This one. Basically, we're going to combine the scores of the pages that possibly would lead to reaching this page, so we'll look at all the pages that are pointing to this page and then combine their scores and propagate the score. The sum of the scores to this document d1. So we look at the scores that represent the probability that the random surfer will be visiting the other pages before it reached d1, and then just do the propagation to simulate the probability of reaching this page. d1. So there are two interpretations. One is just the matrix multiplication and repeatedly multiply the vector by this matrix, but the other is to just think of it as propagating the scores repeatedly on the web. So in practice the computation of Pagerank score is actually efficient because the matrix is sparse and there are some ways to transform the equation so that you avoid actually literally computing the values for all those elements. Sometimes you may also normalize the equation and that would give you a somewhat different form of the equation, but then the ranking of pages will not change. The results of this potential problem of zero out link problem. In that case, if the page does not have any out link then the probability of these pages would not sum to one basically the probability of reaching the next page from this page will not sum to one. Mainly because we have lost some probability mass when we assume there's some probability that the surfer will try to follow links, but then there's no link to follow. And one possible solution is simply to use a page specific damping factor and that could easily fix this. Basically, that's to say alpha would be 1.0 for a page with no out out link. In that case the surfer would just have to randomly jump through another page instead of trying to follow a link. So there are many extensions of page rank. One extension is to do topic specific Pagerank. Noted that Pagerank doesn't really use the query information so. So we can make Pagerank query specific, however, so for example in the topic specific Pagerank we can simply assume when the surfer is bored the surfer is not going to randomly jump to any page on the web. Instead it's going to jump to only those pages that are relevant to a query. For example, if the queries is about the sports, then we could assume that when it's doing random jumping it's going to randomly jump to a sports page. By doing this, then we can bias and Pagerank to topic like sports and then if you know the current query is about sports and then you can use this specialized Pagerank score to rank documents that would be better than if you use a generic Pagerank score. Pagerank is also a general algorithm that can be used in many other applications for network analysis, particularly for example social networks. You can imagine if you compute the Pagerank scores for social network where a link might indicate friendship relation, you'll get some meaningful scores for people. 
So we talked about page rank as a way to. Capture the authorities now we also looked at the some other examples where a hub might be interesting, so there is another algorithm called hits and that's going to compute the scores for authorities and hubs. Intuitions are pages that are wider. Sites are good authorities, then where pages that cite many other pages are good hubs, right? But the I think the most interesting idea of this algorithm hits is. It's going to use reinforcement mechanism to kind of help improve the scoring for hubs and authorities, and here. So here's the idea. It would assume that good authorities are cited by good hubs. That means if you're cited by many pages with good hub scores, then that increases your authority score and similarly good hubs are those that pointed to good authorities. So if you get you pointed to a lot of good authority pages, then your hub score will be increased. So then we can iterate, reinforce each other 'cause you can point to some good hubs so that you can point to some good authorities to get a good hub score, whereas those authoritie scores. Would be also improved because they are pointed to by a good hub and this algorithm is also general. It can have many applications in graph and network analysis. So just briefly, here's how it works. We first also construct the matrix, but this time we're going to construct the adjacency matrix and we're not going to normalize the values. So if there's a link, there is one. If there's no link that's zero again it's the same graph. And then we're going to define the Hub score of page as the sum of the authority scores of all the pages that it points to. So whether you are a hub really depends on whether you're pointing to a lot of good authority pages. That's what it says in the first equation. In the second equation, we define the authorities score over page as the sum of the hub scores of all those pages that the point to you. So whether you are good authoritie would depend on whether those pages that are pointing to you are good hubs so you can see this forms iterative reinforcement mechanism. Now these two equations can be also written in the matrix format. I saw what we get here is then the hub vector is equal to the product of the edges of the adjacency matrix and the authority vector. And this is basically the first equation right? And similarly the second equation can be returned as the authoritie vector is equal to the product of A transpose multiplied by the hub vector and these are just different ways of expressing these equations. But what's interesting is that if you look at the metrics form, you can also plug in the authority equation. Into the first one. So if you do that, you can actually then eliminate the authoritie vector completely and you get the equation of only hub scores, right? The Hub score vector is equal to A * A transpose multiplied by the hub score vector again. And similarly, we can do a transformation to have equation for just the authority scores. So although we framed the problem as computing hubs and authorities, we can actually eliminate one of them to obtain equation just for one of them. The difference between this and page rank is that now the matrix is actually a multiplication of the edges in the matrix and its transpose, so this is different from page rank. But mathematically, then we will be computing the same problem. So in hits we typically would initialize the values that said, one for all these values and then we would iteratively apply these equations, essentially and This is equivalent to multiply that by the Matrix A and A transpose. So the algorithm is exactly the similar page rank, but here because the adjacency matrix is not normalized. So what we have to do, what we have to do is after each iteration, we're going to normalize and this would allow us to control the growth of value, otherwise they would grow larger and larger. And if we do that and then we'll basically get hits algorithm to compute the hub scores an authority scores for all the pages. And these scores can then be used in ranking just like a page rank scores. So to summarize, in this lecture we have seen that link information is very useful. In particular, the anchor text is very useful to increase the. The text representation of a page and we also talk about page rank and hits on as two major link analysis algorithms. Both can generate scores for web pages that can be used in the ranking function. Note that page rank and it's also very general algorithms, so they have many applications in analyzing other graphs or networks. 
This lecture is about learning to rank. In this lecture we are going to continue talking about web search. In particular, we're going to talk about the using machine learning to combine different features to improve the ranking function. So the question that we address in this lecture is how we can combine many features to generate a single ranking function to optimize search results. In the previous lectures we have talked about a number of ways to rank documents. We have talked about some retrieval models, like BM25 or query like code. They can generate the content based scores for matching documents with a query and we also talked about the link based approaches like page rank that can give additional scores to help us improve ranking. Now the question now is how can we combine all these features and potential many other features to do ranking and this will be very useful for ranking web pages. Not only just to improve accuracy, but also to improve the robustness of the ranking function so that it's not easy for a spammer to just perturb a one or a few features to promote the page. So the general idea of learning to rank is to use machine learning to combine these features to optimize the weights, different features to generate the optimal ranking function. So we will assume that the given a query document appear Q and D. We can define a number of features. And these features can vary from content based features such as a score of the document with respect to the query according to a retrieval function such as BM25, or query likelihood or pivot length normalization PL2 etc. It can also be linked based score like page rank score. It can be also application of retrieval models to the anchor text of the page, those are the text descriptions of links that point to this page. So these can all be clues about whether this document is relevant or not. We can even include a feature such as whether the URL has a tilde '~', because this might be indicator of homepage or engine page. So all these features can then be combined together to generate the ranking function. The question is of course, how can we combine them? In this approach we simply hypothesize that the probability that this document is relevant to this query is function of all these features. So we can hypothesize that the probability of relevance is related to these features through a particular form of the function that has some parameters. These parameters can control the influence of different features on the final relevance. This is of course just a assumption whether this assumption really makes sense is the big question and they have to empirically evaluate the function. But by hypothesising that relevance is related to these features in a particular way, we can then combine these features to generate the potentially more powerful ranking function and more robust ranking function. Naturally, the next question is how do we estimate those parameters and how do we know which features should have a higher weight than which features should have lower weight, so this is the task of training or learning, right? So in this approach, what we will do is to use some training data. Those are the data that have been judged by users so that we already know the relevance judgments we already know which documents should be ranked high for which queries. And this information can be based on real judgments by users, or this can also be approximated by just using click through information where we can assume the clicked documents are better than the skiped documents or clicked documents are relevant and skiped documents are non relevant. So in general, we would fit such a hypothesize the ranking function to the training data, meaning that we will try to optimize its retrieval accuracy on the training data, we can adjust these parameters to see how we can optimize the performance of the function on the training data in terms of some measures such as MAP or nDCG. So the training data would look like a table of tuples. Each tuple has three elements. The query, the document and judgment. So it looks very much like our relevance judgments that we talked about in evaluation of retrieval systems. 
So now let's take a look at the specific Method that's based on regression. Now this is one of the many different methods. In fact, it's one of the simplest methods and I choose this to explain the idea because it's simple. So in this approach, we simply assume that the relevance of document with respect to query is related to a linear combination of all the features. Here are used Xi to denote the feature, so Xi of Q and D is a feature. And we can have as many features as we would like. And we assume that these features can be combined in a linear manner. And each feature is controlled by a parameter here. And beta is a parameter that's a weighting parameter, a larger value would mean the feature would have a higher weight and would contribute more to the scoring function. The specific form of the function actually also involves a transformation of the probability of relevance. So this is the probability of relevance. We know that the probability of relevance is within the range from 0 to 1. And we could have just assumed the scoring function is related to this linear combination, right? So we can do a linear regression, but then the value of this linear combination could easily go beyond one, so this transformation here would map the zero to 1 range to the whole range of real values. You can verify it by yourself. So. This allows us then to connect the probability of relevance which is between zero and one to a linear combination of arbitrary features. And if we rewrite this into a probability function we would get the next one, so on this on this equation then we will have the probability of relevance. And on the right hand side we would have this form. Now this form is clearly non negative and it still involves the linear combination of features. And it's also clear that if this value is, this is actually negative of the linear combination in the equation above, if this, This value here. If. This value is large, then it would mean this value is small and therefore this probability this whole probability would be large and that's what we expect. Basically it would mean if this combination gives us a high value, then the documents more likely relevant. So this is our hypothesis. Again, this is not necessarily the best hypothesis, but this is a simple way to connect these features with the probability of relevance. So now we have this combination function. The next task is to see how we to estimate the parameters so that the function can actually be applied without knowing the beta values, it's harder to apply this function, so let's see how we can estimate beta values. Let's take a look at a simple example. In this example we have 3 features. One is BM25 score of the document and query one is the page rank score of the document, which might or might not depend on the query. We might have a topic sensitive Pagerank that would depend on query. Otherwise the general page rank doesn't really depend on query and then we have BM25 score on the anchor text of the document. These are then the feature values for a particular Doc document query pair. An in this case, the document is D1, and the judgment that says that is relevant. Here's another training instance and with these feature values. But in this case it's not relevant. OK, this is overly simplified case where we just have two instances. But it is sufficient to illustrate the point. So what we can do is we use the maximum likelihood estimator to actually estimate the parameters. Basically, we are going to predict the relevance status of the document that based on the feature values that is given that we observe these feature values here. Can we predict the relevance? Yeah, and of course the prediction will be using this function. That you see here. And we hypothesize that the probability of relevance is related to features in this way, so we're going to see for what values of beta. We can predict the relevance well. What do we? What do we mean by predicting the relevance well? we just mean in the first case for D1. This expression here right here should give a high values. In fact, we hope this to give a value close to one, why? because this is a relevant document. On the other hand, in the second case for D2, we hope this value will be small. Why? Because It's a non relevant document. So now let's see how this can be mathematically expressed. This is similar to expressing the probability of document. Only that we're not talking about the probability of words, but talking about probability of relevance one or zero. So what's the probability of this document? The relevant if it has these feature values. This is just this expression, right? We just need to plug in the Xi(s). So that's what we will get. It's exactly like. What we have seen above, only that we replaced these Xi(s) with now specific values, right? So for example, this .7 goes to here and this .11. goes to here. And these are different feature values and we combine them in this particular way. The beta values are still unknown, but this gives us the probability that this document is relevant if we assume such a model. OK, we would want to maximize this probability, since this is a relevant document. What we do for the second document? Well, we want to compute the probability that the prediction is non relevant. So. This would mean we have to compute 1 minus. This expression. Since this expression. Is actually the probability of relevance. So to compute the non relevance from relevance we just. Do 1 minus the probability of relevance. OK. So this whole expression then just is our probability of predicting these two. Relevance values one is 1 here, one is zero and this whole equation is our probability. Of observing a 1 here. And observing a zero here. Of course, this probability depends on the beta values. Right so then our goal is to adjust the beta values to make this whole thing reach its maximum. Make it as large as possible. So that means we're going to compute this. The beta is just the parameter values that would maximize this whole likelihood expression. And what that means is if look at the function is we are going to choose betas to make this as large as possible and make this. Also, as large as possible, which is equivalent to, say, make this the part as small as possible. And this is precisely what we want. So once we do the training now we will know the beta values. So then this function would be well defined once beta values are known. Both this and this. Would be completely specified, so for any new query and new documents, we can simply compute the features. For that pair and then we just use this formula to generate the ranking score and this scoring function can be used to rank documents for a particular query. So that's the basic idea of learning to rank. 
There are many more advanced learning algorithms, then the regression based approaches and they generally attempt to direct the optimizer retrieval measure. like MAP or nDCG. Note that the optimization objective function that we have seen on the previous slide is not directly related to retrieval measure By maximizing the prediction of one or zero, we don't necessarily optimize the ranking of those documents. One can imagine that why our prediction may not be too bad. Let's say both are around .5, so it's kinda of in the middle of zero and one for the two documents. But the ranking can be wrong, so we might have got a larger value for D2 and than D1. So that won't be good from retrieval perspective, even though by likelihood function is not bad. In contrast, we might have another case where we predicted values all around .9, let's say and by the objective functioning the error will be larger, but if we can get the order of the two documents correct, that's actually a better result. So these new, more advanced approaches will try to correct that problem. Of course, then the challenge is that the optimization problem would be harder to solve and then researchers have proposed many solutions to the problem and you can read more reference at the end to know more about the these approaches. Now these learning to rank approaches are actually general, so they can also be applied to many other ranking problems, not just the retrieval problem. So here I list some, for example recommender systems, computational advertising or summarization and there are many others that you can probably encounter in your applications. To summarize this lecture we have talked about the using machine learning to combine multiple features to improve ranking results. Actually the use of machine learning in information retrieval has started since many decades ago. So, for example, the Rocchio feedback approaches that we talked about earlier was machine learning approach applied to relevance feedback But the most recent use of machine learning has been driven by some changes in the environment of applications of retrieval systems, and first, it's mostly driven by the availability of a lot of training data in the form of click throughs. Such data won't available before, so the data can provide a lot of useful knowledge about the relevance and machine learning methods can be applied to leverage this. Secondly, it's also driven by the need for combining many features and this is not only just because there are more features available on the web that can be naturally used to improve scoring, it's also because by combining them we can improve the robustness of ranking, so this is desired for combating spams. Modern search engines are all used, some kind of machine learning techniques combined with many features to optimize ranking and this is a major feature of these commercial engines such as Google or Bing. The topic of learning to rank is still active research topic in the community and so you can expect to see new results being developed in the next few years, perhaps. Here are some additional readings that can give you more information about how learning to rank works and also some advanced methods. 
This lecture is about the future of web search. In this lecture we're going to talk about some possible future trends of web search and intelligent information retrieval systems in general. In order to further improve the accuracy of search engines, it's important to consider special cases of information need, so one particular trend could be to have more and more specialized and customized search engines and they can be called vertical search engines. These vertical search engines can be expected to be more effective than the current general search engines because they could assume that the users are a special group of users that might have a common information need, and then the search engine can be customized to serve such users. And because of the customization is also possible to do personalization, so search can be personalized. Because we have a better understanding of the users. Because of the restriction of the domain, we also have some advantages in handling the documents because we can have better understanding of documents. For example, particular words may not be ambiguous in such a domain, so we can bypass the problem of ambiguity. Another trend that we can expect to see is The search engine will be able to learn over time. It's like a lifetime learning or lifelong learning. And this is of course very attractive, because that means the search engine will self improve itself as more people are using it. Search engines will become better and better and this is already happening because search engines can learn from the implicit feedback, more users use it and the quality of the search results for the popular queries that are typed in by many users will likely become better. So this is another feature that we would see. The third trend might be the integration of multi models of information access. So search navigation and recommendation or filtering might be combined to form a full fledged information management system. And in the beginning of this course we talked about the push versus pull. These are different modes of information access, but these modes can be combined. And similarly, in the pull mode, querying and browsing could also be combined then in fact we're doing that basically today with the current search engines we are querying. Sometimes browsing, clicking on links. Sometimes we've got some information recommended, although most of the cases information recommended is because of advertising. But in the future you can imagine seamlessly integrate the system with multi mode for information access. And that would be convenient for people. Another trend is that we might see systems that try to go beyond, search it supports the user tasks. After all, the reason why people want to search is to solve a problem or to make a decision or perform a task. For example, consumers might search for opinions about products in order to purchase a product and choose a good product to buy. So in this case it would be beneficial to support the whole workflow of purchasing a product or choosing a product. In this area, after the current search engines already provided good support, for example, you can sometimes look at the reviews and then if you want to buy it and you can just click on the button to go to the shopping site directly, get it done. But it does not provide a good task support for many other tasks. For example for researchers, you might want to find the relevant literature or site of the literature, and then there's no support for finishing tasks such as writing a paper. So in general, I think there are many opportunities to innovate, and so in the following a few slides I would be talking a little bit more about some specific ideas or thoughts that hopefully you can help you imagine new application possibilities. Some of them might be already relevant to what you are currently working on. In general, you can think about any intelligent system, especially intended information system, as being specified by these three nodes, and so if we connect these three into a triangle, then we'll be able to specify information system, and I call this data user service triangle. So basically the three questions you ask would be who are you serving and what kind of data you are managing. And what kind of service you provide? Right there, this would help us basically specify your system and there are many different ways to connect them, depending on how you connect them, you will have a different kind of system, so let me give you some example. On the top, you can see different kinds of users, on the left side you can see different types of data or information, and on the bottom you can see different service functions. Now imagine we can connect all these in different ways. So for example if you can connect everyone with web pages. And the support search and browsing. What do you get? That's web search, right? What if we connect the UIUC employees with organization documents or enterprise documents to support the search and browsing. That's enterprise search. If you connect the scientists with literature information to provide all kinds of service, including search, browsing or alert of new relevant documents, or mining, analyzing research trends, or provide the task support or decision support, for example. You might be might be able to provide a support for automatically generating related work section for research paper, and this would be closer to task support, right? So then we can imagine this would be a literature assistant if we connect to online shoppers with blog articles or product reviews, then we can help these people to improve shopping experience so we can provide for example data mining capabilities to analyze the reviews to Compare products, compare sentiment of products and to provide a task support or decision support to help them choose what product to buy. Or we can connect the customer service people with emails from the customers. And we can imagine a system that can provide analysis of these emails to find the major complaints of the customers. We can imagine the system could provide task support by automatically generating a response to the customer email, or maybe intelligently attach also promotion. Message if appropriate if the detector that's a positive message, not a complaint, and then you might to take this opportunity to attach some promotion information. Whereas if it's a complaint that you might be able to. Automatically generate some generic response first and tell the customer that he or she can expect the detailed response later, etc. All these are trying to help people to improve the productivity. So this shows that the opportunities are really a lot, it's just only restricted by our imagination. So this picture shows the trend of the technology and also it characterizes the intelligent information system in three angles you can see in the center there is a triangle that connects keyword queries to search and bag of words representation. That means the current search engines basically provides search support. Users and mostly model users based on keyword queries. And it sees the data through a bag of words representation. So it's a very simple approximation of the actual information in the documents. But that's what the current system does. It connects these three nodes in such a simple way. Or it only provides you basic search function and doesn't really understand the user. And it doesn't really understand it that much information in the documents. Now I showed some trends to push each node. Toward more advanced function, so think about the user node here so we can go beyond the keyword queries. Look at the user search history and then further model the user completely to understand the users task, environment, task need context or other information. So this is pushing for personalization and complete the user modeling and this is a major direction in research. In order to build intelligent information systems. On the document side. We can also see we can go beyond bag of words representation to have entity relation representation. This means we'll recognize peoples names their relations, locations etc and this is already feasible with today's natural language processing technique and Google's recent initiative on the Knowledge Graph. If you have heard of it, it's a good step toward this direction and once we can get to that level of representation in a robust manner at large scale, it can enable the search engine to provide much better service. In the future, we would like to have knowledge representation where we can add perhaps inference rules and then the search engine will become more intelligent. So this calls for large scale semantic analysis, and perhaps this is more feasible for vertical search engines. It's easier to make progress in a particular domain. Now on the service side, we see that we need to go beyond the search if support information access in general, so search is only one way to get access to information as well. Recommender systems and push and pull are different ways to get access to relevant information, but going beyond access we also need to help people digest information. Once the information is found and this step has to do with analysis of information or data mining. We have to find the patterns or converted the text information into your knowledge that can be used in application or actionable knowledge that can be used for decision making. And furthermore the knowledge would be used to help user to improve productivity in finishing a task. For example, a decision making task. So this is a trend and so basically in this dimension we anticipate in the futur. Intelligent Information systems will provide intelligent and interactive task support. Now I should also emphasize interactive here because it's important to optimize the combined intelligence of the users and the system so we can get some help from users in some natural way and we don't have to assume the system has to do everything when the human user and the machine can collaborate in an intelligent way in an efficient way, then the combined intelligence will be high and in general we can minimize the user's overall effort in solving problem. So this is the big picture of future intelligent information systems. And this hopefully can provide us some insights about how to make further innovations on top of what we can do today. 
This lecture is about the recommender systems. So far we have talked about a lot of aspects of search engines. We have talked about the problem of search and ranking problem, different methods for ranking implementation of a&lt;br&gt; search engine and how to evaluate the search engine etc. The. This is probably because of we know that web search engines are by far the most important applications of text retrieval and they are the most useful tools to help people convert big raw text data into a small set of relevant documents. Another reason why we spend so many lectures on search engines is because many techniques used in search engines are actually also very useful for recommender systems, which is the topic of this lecture. And so overall, the two systems are actually well connected and there are many techniques that are shared by them. So this is a slide that you have seen before when we talked about the two different modes of text access - pull and push. An we mentioned that recommender systems are the main systems to serve users in the push mode where the systems would take initiative to recommend the information to user or push a relevant information to the user. And this often works well when the user has a relatively stable information need. When the system has a good knowledge about what the user wants. So a recommender system is sometimes called a filtering system and it's because recommending useful items to people is like discarding or filtering out the useless articles. And so in this sense they are kind of similar. And in all these cases, the system must make a binary decision, and usually there's a dynamic source of information items. And you have some knowledge about this user's interest and then the system would make a delivery decision whether this item is interesting to the user and then if he's interested in, then the system would recommend the article to the user. So the basic of filtering question here is really, will this user like this item. Will you like item X? And there are two ways to answer this question. If you think about it, I wanted to look at what items you like. And then we can see if X actually like those items. The other is to look at the who likes X and we can see if this user looks like a one of those users. or like most of those users. And these strategies can be combined if we follow the first strategy that look at item similarity. In the case of recommending text objects. Then we are talking about the content based filtering or content based recommendation. If we look at the second strategy, then it will compare users and in this case will exploit the user similarity and the technique is often called collaborative filtering. So let's first look at the content based filtering system. This is what the system would look like. Inside the system there will be a binary classifier that would have some knowledge above the users interest. And it's called a user interest profile. It maintains this profile to keep track of the users interest. And then there was a utility function to guide the user to make decisions, and I explained the utility function in the moment. It helps the system decide where to set the threshold. And then the accepted document will be those that have passed the threshold according to the classifier. There should be also an initialization module that would take a users input, maybe from a users specified keywords or chosen category etc. And this will be to feed the system with the initial user profile. There is also typically a learning module that will learn from users feedback overtime. Now. Note that in this case typical users information need is stable, so the system would have a lot of opportunities to observe the users, if the user has taken a recommended item has viewed that, and this is the signal to indicate that the recommended item may be relevant if the user discarded it, it's not relevant, and so such feedback can be a long term feedback and can last for a long time. And the system Clock collect a lot of information about these users interest and this can then be used to improve the classifier. Now what's the criteria for evaluating such a system? How do we know this filtering system actually performs well? Now, in this case we cannot use the ranking evaluation measures like a map because we can afford waiting for a lot of documents and then rank the documents to make a decision for the user. And so the system must make a decision in real time in general to decide whether the item is above the threshold or not. So in other words, we're trying to decide the absolute relevance. So in this case, one commonly used strategies is user utility function to evaluate the system. So here I show a linear utility function that's defined as for example 3 multiplied by the number of good items that you delivered minus 2 multiplied by the number of bad items you deliver. So in other words, we could kind of just. treat this as almost a in a gambling game. If you delete, if you deliver one good item, let's say you win $3, you gain $3. But if you deliver a better one and you will lose $2 and this utility function basically kind of measures how much money you will get by doing this kind of game. And so it's clear that if you want to maximize this utility function, your strategy should be to deliver as many good articles as possible and to minimize the delivery of bad articles, that's obvious. One interesting question here is how should we set these coefficients? Now I just showed 3 and negative 2 as a possible coefficients. But one can ask the question, are they reasonable? So what do you think? Do you think that's a reasonable choice? What about the other choices? And also for example we can have 10 and minus one. Or one minus ten. What's the difference? What do you think? How would this utility function affect the system's threshold decision? But you can think of these two extreme cases, 10&amp;nbsp; -1 versus 1 -10. Which one do you think it would encourage the system to overdeliver and which one would encourage the system to be conservative? If you think about it, they will see that when we get a big award for delivering a good document, you incur only a small penalty for delivering bad one. Intuitively, you would be encouraged to deliver more right? And you can try to deliver more. In hopes of getting a good one delivered, and then you'll get a big award. I saw, on the other hand, if you choose 1 -- 10, you don't really get such a big price if you deliver deliver a good document. On the other hand, you will have a big loss if you deliver a bad one. You can imagine that the system would be very reluctant to deliver a lot of documents. It has to be absolutely sure that it's not a non-relevant one. So this utility function has to be designed based on the specific application. Three basic problems in content based filtering are the following. Frst it has to make a filtering decision so it has to be a binary decision maker binary classifier given a text. Text document and profile description of the user. It has to say yes or no, whether this document should be delivered or not. So that's the decision module and there should be a initialization module. As you have seen earlier and this is to get the system started. And we have to initialize the system based on only very limited text description or very few examples from the user. And the third component is a learning module which he had to be able to learn from limited relevance judgments. Because we count in learn from the user about their preferences on the delivered documents if we don't deliver document to the user, we would never know would never be able to know whether the user likes it or not. And we can accumulate a lot of documents and learn from the entire history and all these modules would have to be optimized to maximize the utility. So how can we build such a system? And there are many different approaches. here. Here we're going to talk about how to extend retrieval system. A search engine for information filtering. Again, here's why. We've spent a lot of time to talk about the search engines because it's actually not very hard to extend the search engine for information filtering. So here's the basic idea for extending a retrieval system for information filtering. First we can reuse a lot of retrieval techniques to do scoring, right, so we know how to score documents against queries, etc. We can measure the similarity between profile text, description and document, and then we can use the score threshold for the filtering decision. We do retrieval, and then we kind of find the scores of documents and then we apply a threshold to see whether document is passing the threshold or not, and if it's passing the threshold we are going to say it's relevant, and we're going to deliver it to the user. And another component that we have to add is for is of course to learn from the history and here we can use the traditional feedback techniques to learn to improve scoring. And we know Rocchio can be used for scoring improvement. And but we have to develop a new approaches to learn how to set the threshold and we need to set it initially and then we have to learn how to update the threshold overtime. So here's what the system might look like if we just generalize the vector space model for filtering problems. Right, so you can see the document vector could be fed into a scoring module which is already exists in a search engine that implements a vector space model and the profile will be treated as a query essentially and then the profile vector can be matched with the document vector to generate the score. And then this score would be fed into a threshold module that would say yes or no, and then the evaluation would be based on utility for the filtering results. If it says yes and then the documents will be sent to the user and then the user could give some feedback and the feedback information would have been ,would be used to both adjust to the threshold and to adjust the vector representation so the vector learning is essentially the same as query modification or feedback. in the case of search. The threshold of learning is new component that we need to talk a little bit more about. 
There are some interesting challenges in threshold learning in the filtering problem. So here I show the historical data that you can collect in a filtering system so you can see the scores and the status of relevance. So the first one it has a score of 36.5 and it's relevant. The second one is non-relevant and etc. Of course we have a lot of documents for which we don't know the status because we have never delivered them to the user. So as you can see here, we only see the judgments of documents delivered to the user, so this is not a random sample, so it's censored data. It's kind of biased. So that creates some difficulty for learning, and secondly there are in general very little labeled data, and very few relevant data, so it's also challenging for machine learning approaches. Typically they require more training data, and in the extreme case at the beginning we don't even have any label data as well. The system still has to make a decision, so that's a very difficult problem at the beginning. Finally, there is also this issue of exploration versus exploitation tradeoff. Now this means we also want to explore the document space a little bit and to see if the user might be interested in documents that we haven't delivered. So in other words, we're going to explore the space of user interests by testing whether the user might be interested in some other documents that currently are not matching the users' interests so well. So how do we do that? Well, we could lower the threshold a little bit and do just deliver some near misses to the user to see what the user would respond, to see how the user would respond to this extra document. And this is the trade-off because on the one hand you want to explore, but on the other hand you don't want to really explore too much 'cause then you would overdeliver non-relevant information. So exploitation means you would exploit what you learned about user. Let's say you know the user is interested in this particular topic so you don't want to deviate that much. But if you don't deviate at all then you don't explore it all. That's also not good. You might miss opportunity to learn another interest of the user. So this is a dilemma. And that's also a difficult problem to solve. Now how do we solve these problems? In general, I think one can use the empirical utility optimization strategy, and this strategy is basically to optimize the threshold based on historical data, just as you have seen on the previous slide. So you can just compute the utility on the training data for each candidate score threshold. Pretend what if I cut at this point. What if I can cut at a different scoring threshold point what would happen, what's utility? Since these are training data, we can kind of compute the utility, right? We know their relevance status or we assume that we know relevant status that's based on approximation of clickthroughs. So then we can just choose the threshold that gives the maximum utility on the training data. But this of course doesn't account for exploration that we just talked about. And there is also the difficulty of bias training sample as we mentioned. So in general we can only get upper bound for the true optimal threshold, because the threshold might be actually lower than this. So it's possible that the discarded item might be actually interesting to the user. So how do we solve this problem where we generate and as I said, we can lower the threshold to explore a little bit, so here's one particular approach called better gamma threshold learning. So the idea is following. So here I show a ranked list of all the training documents that we have seen so far, and they are ranked by their positions. And on the y-axis, We show the utility. Of course this function depends on how you specify the coefficients in the utility function, but we can then imagine. that depending on the cut off position, we will have a utility that means. Suppose I cut at this position and that would be the utility. So we can, for example identify some cutting cut off point. The optimal point theta optimal is the point when we would achieve the maximum utility if we had chosen this threshold. And there is also zero threshold zero utility threshold, and you can see at this cut off the utility is 0. Now what does that mean? That means if I lower the threshold a little bit and now I reach this threshold, the utility would be lower, but it's still positive it's still non negative at least. So it's not as high as the optimal utility. But it gives us a safe point to explore the threshold. As I just explained, it's desirable to explore the interest space, so it's desirable to lower the threshold based on your training data. So that means in general we want to set the threshold somewhere in this range. Let's say we can use alpha to control the deviation from the optimal utility point so you can see the formula of the threshold would be just the interpolation of the zero utility threshold and the optimal utility threshold. Now the question is how should we set alpha? And when should we deviate more from the optimal utility point? Well this can depend on multiple factors and one way to solve the problem is to encourage this threshold mechanism to explore up to the zero point, and that's a safe point, but we're not going to necessarily reach all the way to the zero point, but rather we're going to use other parameters to further define alpha, and this specifically is as follows. So there will be a beta parameter to control the deviation from the optimal threshold, and this can be based on for example can be accounting for the overfitting to the training data let's say. And so this can be just adjustment factor. But what's more interesting is this gamma parameter here and you can see in this formula, gamma is controlling the influence of the number of examples in training dataset. So you can see it in this formula as N which denotes the number of training examples becomes bigger, then it would actually encourage less exploration. In other words, when N is very small, it would try to explore more, and that just means if we have seen few examples we're not sure whether we have exhausted the space of interests. So we would explore. But as we have seen many examples from the user, many data points, then we feel that we probably don't have to explore more. So this gives us a dynamic strategy for exploration, right? The more examples we have seen, the less explosion we're going to do, so the threshold would be closer to the optimal threshold. So that's the basic idea of this approach. Now this approach, it actually has been working well in some evaluation studies empirically effective. And also can work on arbitrary utility with a proper lower bound. And it explicitly addresses the exploration- exploitation tradeoff and it kind of uses the zero utility threshold point as a safeguard for exploration and exploitation tradeoff, we are never going to explore further than the zero utility point. So if you take the analogy of gambling and you don't want to risk on losing money, so it's a safe strategy. The conservative strategy for exploration. And the problem is, of course this approach is purely heuristic. And the zero utility lower bound is also often too conservative. And there are of course more advanced machine learning approaches that have been proposed for solving these problems, and this is the active research area. So to summarize, there are two strategies for recommender systems or filtering systems. One is content based which is looking at the item similarity. The other is collaborative filtering, which is looking at the user similarity. In this lecture, we've covered the content based filtering approach in the next lecture we're going to talk about collaborative filtering. In content-based filtering system, We generally have to solve several problems related to filtering decision and learning etc. And such a system can actually be built based on a search engine system by adding a threshold mechanism, and adding adaptive learning algorithm to allow the system to learn from long-term feedback from the user. 
This lecture is about the collaborative filtering. In this lecture, we're going to continue the discussion of recommender systems. In particular, we're going to look at the approach of collaborative filtering. You have seen this slide before when we talked about the two strategies to answer the basic question will user U like item X. In the previous lecture, we looked at the item similarity. That's content-based filtering. In this lecture, we will look at the user similarity. This is a different strategy called a collaborative filtering. So first, what is collaborative filtering? It is to make filtering decisions for individual user based on the judgments of other users. And that is, we say, we will infer individual's interest or preferences from that of other similar users. So the general idea is the following. Given a user U, we're going to 1st find the similar users u_1 through u_m. And then we're going to predict the user preferences based on the preferences of these similar users. u_1 through u_m. Now the user similarity here can be judged based on their similarity in preferences on a common set of items. Now here you can see the exact content of item doesn't really matter. We're going to look at the only the relation between the users and items. So this means this approach is very general. It can be applied to any items. Not just the text options, so this approach it would work well under the following assumptions. 1st. Users with the same interest where have similar preferences. Second, the users with similar preferences probably share the same interest. So for example. If the interest of the user is in information retrieval, then we can infer the user probably favor SIGIR papers. Alright, so those who are interested in information retrieval research probably all favor SIGIR papers. That's the assumption that we make and if this assumption is true, Then it would help collaborative filtering to work well. We can also assume that if we see people favor SIGIR papers, then we can infer their interest is probably information retrieval. So in this simple examples it seems to make sense. And in many cases, such assumption actually does make sense. So another assumption we have to make is that there are sufficiently large number of user preferences available to us. So for example, if you see a lot of ratings of users for movies, and those indicate their preferences for movies, and if you have a lot of such data, then collaborative filtering can be very effective. If not, there will be a problem, and that's often called cold start problem. That means you don't have many preferences available, so the system could not afford to take advantage of collaborative filtering yet. So let's look at the collaborative filtering problem in a more formal way, and so this picture shows that we are in general considering a lot of users showing we're showing M users here. So u_1 through u_m and we also considering a number of objects, let's say N objects denoted as o_1 through o_n and then we will assume that the users will be able to judge those objects and the user could for example give ratings for those items, for example, those items could be movies. Could be products and then the users would give ratings 1 through 5 let's say. So what you see here is that we have shown some ratings available for some combinations, so some users have watched some movies they have rated those movies. They obviously won't be able to watch all the movies and some users may actually only watch a few movies. So this is in general a sparse matrix. So many items, many entries. Have unknown values. And what's interesting here is we could potentially infer the value of an element in this matrix based on other values, and that's actually the central question in collaborative filtering, and that is we assume there's unknown function here F that would map a pair of a user and an object to the rating. And we have observed some values of this function. And we want to infer the value of this function for other pairs, That with that don't have values available here. So this is very similar to other machine learning problems where we know the values of the function on some training data sets and we hope to predict the values of this function on some test data, right? So this is a function approximation. And how can we figure out the function based on the observed ratings? So this is the setup. Now there are many approaches to solving this problem, and in fact this is a very active research area, a reason there are special conferences there are special conferences dedicated to the problem. R is the major conference devotes to the problem. the problem. 
And here we are going to talk about basic strategy and that would be based on similarity of users and then predicting the rating of. An object via active user. Using the ratings of similar users to this active user, this is called a memory based approach. Because it's a little similar to storing all the user information and when we are considering a particular user, we're going to try to retrieve the relevant users or the similar users to this user case and then try to use that user information about those users to predict the preference of this user. So here's the general idea, and we use some notations here so X_ij denotes the rating of object o_j by user u_i. And n_i is the average rating of all objects by this user. So. This n_i is needed because we would like to normalize the ratings of objects by this user. So how do you do normalization? Well, we're going to just subtract the average rating from all the ratings. Now this is it will normalize these ratings so that the ratings from different users would be comparable. Because some users might be more generous and they generally given high ratings. But some others might be more critical, so their ratings cannot be directly compared with each other or aggregate them together. So we need to do this normalization. Now the prediction of the rating on the item by another user or active user. u_ a here. Can be based on the average ratings of similar users. So the user u_a is the user that we're interested in recommending items to and We now are interested in recommending this o_j, so we're interested in knowing how likely this user will like this objec. How do we know that? The idea here is to look at whether similar users to this user have liked this object. So mathematically, this is to say the predicted the rating of this user on this object user A on object o_j is basically combination of the normalized ratings of different users. And in fact here we're taking a sum over all the users. But not all users contribute equally to the average, and this is controlled by the weights. So this. Weight Controls the influence of user on the prediction. And of course, naturally, this way that should be related to the similarity between u_a and this particular user u_i. The more similar they are, then the more contribution would like a user UI to make in predicting the preference of u_a So the formula is extremely simple. You can see it's a sum over all the possible users. And inside the sum, We have their ratings, well, their normalized ratings as I just explained, the ratings need to be normalized in order to be comparable with each other. And then these ratings are weighted by their similarity. So you can imagine W of a an I is just a similarity of user A and user I. Now what's k here, well k is simply normalizer, it's just it's just one over the sum of all the weights. over all the users. And so this means basically, if you consider the weight here together with K and we have coefficients or weights that would sum to one for all the users. And it's just a normalization strategy so that you get this predicted rating in the same range as the these ratings that we use to make the prediction. Right, so this is basically the main idea of memory based approaches for collaborative filtering. Once we make this prediction. We also would like to map back to the rating that the user would actually make and this is to Further add the mean rating or average rating of this user u_a to the predicted value. This would recover a meaningful rating for this user. So if this user is generous than the average would be somewhat high and when we add that the rating will be adjust to a relatively high rating. Now when you recommend. Item to a user. This actually doesn't really matter 'cause you're interested in. Basically the normalized rating that's more meaningful, but when they evaluate these collaborative filtering approaches. is that typically assume the actual ratings of the user on these objects to be unknown, and then you do the prediction and then you compare the Predicted ratings with their actual ratings so they you do have access to their actual ratings, but then you pretend you don't know. And then you compare your system's predictions with the actual ratings. In that case, obviously the systems prediction would have to be adjusted to match the actual ratings of the user, and this is what's happening here, basically. OK, so this is the memory based approach. Now of course if you look at the formula if you want to write the program to implement it, you still face the problem of determining what is this W function right? Once you know the W function, then the formula is very easy to implement. So indeed there are many different ways to compute this function or this weight, w and specific approaches generally differ in how this is computed. So here are some possibilities and you can imagine. There are many other possibilities. One popular approach is to use the Pearson correlation coefficient. This would be a sum over commonly rated items and the formula is a standard Pearson correlation coefficient formula as shown here. So this basically measures whether the two users tend to all give higher ratings to similar items, or lower ratings to similar items. Another measure is the cosine measure, and this is to treat the rating vectors as vectors in the vector space. And then we're going to measure the angle and then compute the cosine of the angles of the two vectors, and this measure has been used in the vector space model for retrieval as well. So as you can imagine, there are many different ways of doing that. In all these cases, note that the user similarity is based on their preferences on Items and we did not actually use any content information of these items. It didn't matter what these items are. They can be movies they can book so they can be products. They can be text documents. We just didn't care about the content. And So this allows such approach to be applied to a wide range of problems. Now, in some new approaches, of course we would like to use more information about the user. Clearly we know more about the user, not just these preferences on these items. So, in the actual filtering system using collaborative filtering. We could also combine that with content based filtering. We could use more context information and those are all interesting approaches that people are still studying. There are new approaches proposed, but this memory based approach. It has been shown to work reasonably well and it's easy to implement and in practical application this could be a starting point to see if the strategy works well for your application. So there are some obvious ways to also improve this approach. And mainly we would like to improve the user similarity measure and there are some practical issues to deal with here as well. So for example, there will be a lot of missing values. What do you do with them? Or you can set them to default values, or the average ratings of the user and that would be a simple solution, but there are the advanced approaches that can actually try to predict those missing values. And then use the predicted values to improve the similarity. So in fact the memory based approach you can predict those missing values right? So you can imagine you have iterative approach where you first do some preliminary prediction and then you can use the predicted values to further improve the similarity function. So this is. Here is a way to solve the problem and the strategy obviously would affect the performance of collaborative filtering. Just like any other heuristics to improve these similarity functions, another idea which is actually very similar to the idea of IDF that we have seen in Text research is called inverse user frequency. Or IUF Now here the idea is to look at the where the two users share similar ratings. If the item is a popular item that has been viewed by many people, and seeing these two people. Interested in this item May not be so interesting, but if it's a rare item it has not been viewed by many users, but these two users viewed this item and they give similar ratings and that says more about their similarity, right? So it's kind of to emphasize more on similarity on items that are not viewed by many users. 
So to summarize, our discussion of recommender systems in some sense, the filtering task or recommending task is easy and in some other senses, and the task is actually difficult, so it's easy because the users expectations, though in this case the system it takes initiative to push the information to the user so the user doesn't really make. An effort, so any recommendation is better than nothing, right? So unless you recommend the order, noisy items or useless documents, if you can recommend some useful information, users general would appreciate it, so that's in that sense that's easy. However, filtering is actually much harder task than retrieval because it has you have to make a binary decision and you can't afford waiting for a lot of items. And then you're going to see. whether 1 item is better than others. You have to make a decision when you see this item. The thing about the news filtering as soon as you see the news and you have to decide whether the news would be interesting to a user. If you wait for a few days. Even if you can make accurate recommendation of the most relevant news, the utility is going to be significantly decreased. Another reason why it's hard, it's because the data sparseness. If you think of this as a learning problem in collaborative filtering, for example, it's purely based on learning from the past ratings, so if you don't have many ratings, this really not that much you can do. And yeah, just mentioned this cold start problem. This is actually a very serious serious problem, but of course there are strategies that have been proposed to solve the problem and there are different strategies that you can use to alleviate the problem. You can use for example, more user information to assess their similarity instead of using the preferences of these users on these items, there may be additional information available about the user. etc and. And we also talked about the two strategies for filtering task one is content based, where we look at item similarity. The other is collaborative filtering where we look at the user similarity and they obviously can be combined in a practical system. You can imagine the general would have to be combined so that will give us a hybrid strategy for filtering. And we also could recall that we talked about push versus pull as two strategies for getting access to the text data and recommended system is to help users in the push mode and search engines are certain users in the pull mode. Obviously the tool should be combined and they can be combined to have a system that can support the user with multiple mode information access. So in future we could anticipate the such a system to be more useful to user. An either this is the active research area, so there are a lot of new algorithms being proposed all the time. In particular, those new algorithms tend to use a lot of context information. Now the context here could be the context of the user and that it could be also context of documents or items. The items are not isolated and they are connected in many ways. The users might form social network as well, so there's a rich context there that we can leverage in order to really solve the problem well and then that's an active research area where also machine learning algorithms that have been applied. Here are some additional readings in the Handbook called Recommender Systems and has a collection of a lot of good. Articles that can give you an overview of a number of specific approaches to recommended systems. 
This lecture is a summary of this course. This map shows the major topics we have covered in this course. And here are some key high level takeaway messages. First we talked about the natural language content analysis. Here the main takeaway message is natural language processing is the foundation for text retrieval. But current NLP isn't robust enough, so the bag of words representation is generally the main method used in modern search engines. And it's often sufficient for the most of the search tasks, but obviously for more complex such tasks than we need a deeper natural language processing techniques. And we then talked about the high level strategies for text access and we talked about the push vs pull. In pull, we talked about the querying versus browsing. In general, in future search engines we should integrate all these techniques to provide multiple information access. And then we talked about a number of issues related to search engines we talked about the search problem, and we framed that as a ranking problem. And we talked about a number of retrieval methods. We started with the overview of vector space model and the probabilistic model, and then we talked about the vector Space Model in depth. and we also later talked about the language modeling approaches, and that's a probabilistic model and here The main takeaway messages is that modern retrieval functions tend to look similar, and they generally use various heuristics. Most important ones are TF-IDF weighting, document length normalization, and TF is often transformed through a sublinear Transformation function. And then we talked about how to implement a retrieval system. And here the main techniques that we talked about are how to construct the inverted index so that we can prepare the system to answer query quickly. And we talked about how to perform search by using the inverted index. And we then talked about how to evaluate the text retrieval system. Mainly introduced the Cranefield evaluation methodology. This is a very important evaluation methodology that can be applied to many tasks. We talked about the major evaluation measures, so the most important measures for search engine--map(mean average precision), nDCG(normalized, discounted cumulative gain), an also precision and recall are the two basic measures. And we then talked about the feedback techniques and we talked about the Rocchio in the vector space model and the mixture model in the language modeling approach. Feedback is a very important technique, especially considering the opportunity of learning from a lot of clickthroughs on the web. We then talked about Web search, and here we talked about how to use parallel indexing to solve the scalability issue. In indexing we introduce the map reduce and then we talked about how to use linking information on the web to improve search. We talked about page rank and HITS as the major algorithms to analyze links on the web. We then talked about learning to rank. This is the use of machine learning to combine multiple features for improving scoring not only the effectiveness can be improved using this approach, but we can also improve the robustness of the ranking function so that it's not easy to spam the search engine with just. Some features to promote a page. And finally, we talked about the future of web search. We talked about some major directions that we might see in the future in improving the current generation of search engines. And then finally we talked about the recommender systems and these are systems to implement the push mode and we talked about the two approaches. One is content based, one is collaborative filtering and they can be combined together. Now. An obvious missing piece in this picture is the user, you can see, so user interface is also an important component in any search engine, even though the current searching interface is relatively simple, they actually have been a lot of studies of user interface related to visualization for example. And this is a topic that you can learn more by reading this book. It's an excellent book about all kinds of studies of search interface. If you want to know more about the topics that we talked about, you can also read some additional readings that are listed here in this short course. We only managed to cover some basic topics in text retrieval and search engines. And these resources provide additional information about the more advanced topics, and they gave a more thorough treatment of some of the topics that we talked about and a main source is synthesis digital library. Where you can see a lot of short textbook or textbooks or long tutorials, they tend to provide a lot of information to explain a topic and there are multiple serieses that are related to this course, and one is the information concepts retrieval and services and another is Human language technology and yet another is artificial intelligence and machine learning. There were also some major journals and conferences listed here that tend to have a lot of research papers related to the topic of this course, and finally, For more information about resources, including readings and toolkits, etc. You can check out this URL. So if you have not taken the text mining course in this data mining specialization series, then naturally the next step is to take that course as this picture shows to mine big text data we generally need two kinds of techniques, one is text retrieval, which is covered in this course and these techniques would help us convert the raw big text data into small relevant text data which are actually needed in the specific application. Human plays an important role in mining any text data becausw text data is written for humans to consume, so involving humans in the process of data mining is very important. And in this course we have covered various strategies to help users get access to the most relevant data. These techniques are also essential in any text mining system to help provide provenance, stand to help users interpret in the patterns that user would find through text data mining. So in general the user would have to go back to the original data to better understand the patterns. So the text mining course, or rather text mining analytics course will be dealing with what to do once the user has found the information. So this is the second step in this picture where we would convert the text data into actionable knowledge. And this has to do with helping users to further digest the found information or to find the patterns and to reveal knowledge buried in text and such knowledge can then be used in application system to help decision making or to help user finish a task. So if you have not taken that course, the natural step in the natural next step would be to take that course. Thank you for taking this course. I hope you have found this course to be useful to you and I look forward to interacting with you at a future opportunity. 
